[
  {
    "objectID": "guides.html",
    "href": "guides.html",
    "title": "Methods guides",
    "section": "",
    "text": "EGAP seeks to improve methods for rigorous impact evaluation throughout the social and behavioral science research community, as well as to make existing methods accessible to a wider audience. The methods guides are primers on a range of methodological and research topics. Each guide takes one issue and outlines the ten most important points to understand about it or the ten essential steps necessary to addressing that issue in your work."
  },
  {
    "objectID": "guides.html#causal-inference",
    "href": "guides.html#causal-inference",
    "title": "Methods guides",
    "section": "Causal Inference",
    "text": "Causal Inference\n\n\n\n\n\n\n\n\n\n\n10 Strategies for Figuring out whether X Causes Y\n\n\n\nMacartan Humphreys\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About Causal Inference\n\n\n\nMacartan Humphreys\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides.html#research-questions",
    "href": "guides.html#research-questions",
    "title": "Methods guides",
    "section": "Research Questions",
    "text": "Research Questions\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About the Local Average Treatment Effect\n\n\n\nPeter van der Windt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Heterogeneous Treatment Effects\n\n\n\nAlyssa René Heinze\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides.html#data-strategies",
    "href": "guides.html#data-strategies",
    "title": "Methods guides",
    "section": "Data Strategies",
    "text": "Data Strategies\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About Randomization\n\n\n\nLindsay Dolan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Adaptive Experimental Design\n\n\n\nDonald Green, Molly Offer-Westort\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Measurement in Experiments\n\n\n\nTara Slough\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Missing Data\n\n\n\nTara Slough\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Multisite or Block-Randomized Trials\n\n\n\nKristen Hunter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Sampling\n\n\n\nAnna M. Wilke\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Spillovers\n\n\n\nAlexander Coppock\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Survey Experiments\n\n\n\nChristopher Grady\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides.html#analysis-procedures",
    "href": "guides.html#analysis-procedures",
    "title": "Methods guides",
    "section": "Analysis Procedures",
    "text": "Analysis Procedures\n\n\n\n\n\n\n\n\n10 Randomization Inference Procedures with ri2\n\n\n\nAlexander Coppock\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About Multiple Comparisons\n\n\n\nAlexander Coppock\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things on Your Checklist for Analyzing an Experiment\n\n\n\nname\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Conducting a Meta-Analysis\n\n\n\nDonald Green\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Covariate Adjustment\n\n\n\nLindsay Dolan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Randomization Inference\n\n\n\nDonald Green\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides.html#assessing-designs",
    "href": "guides.html#assessing-designs",
    "title": "Methods guides",
    "section": "Assessing Designs",
    "text": "Assessing Designs\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About External Validity\n\n\n\nRenard Sexton\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About Statistical Power\n\n\n\nAlexander Coppock\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides.html#planning",
    "href": "guides.html#planning",
    "title": "Methods guides",
    "section": "Planning",
    "text": "Planning\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Pilot Studies\n\n\n\nKaylyn Jackson Schiff, Daniel S. Schiff, Natália S. Bueno\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Survey Design\n\n\n\nGabriella Sacramone-Lutz\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides.html#implementation",
    "href": "guides.html#implementation",
    "title": "Methods guides",
    "section": "Implementation",
    "text": "Implementation\n\n\n\n\n\n\n\n\n\n\n10 Conversations that Implementers and Evaluators Need to Have\n\n\n\nJake Bowers, Rebecca Wolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About Project Workflow\n\n\n\nMatthew Lisiecki\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Survey Implementation\n\n\n\nGabriella Sacramone-Lutz\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides.html#interpretation",
    "href": "guides.html#interpretation",
    "title": "Methods guides",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n\n\n\n\n\n\n\n\n10 Things Your Null Result Might Mean\n\n\n\nRekha Balu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Reading a Regression Table\n\n\n\nAbby Long\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "We collected statistical software tools useful for designing, implementing, and analyzing results from impact evaluations. You can search for functions and for software packages in R and Stata below.\n\n\n\n\n\n\npackage\nfn\nMember(s)\nFunctionality\nPlatform\nURL\n\n\n\n\nri2\nconduct_ri\nAlex Coppock\nRandomization Inference for Randomized Experiments\nR\nhttps://rdrr.io/cran/ri2/\n\n\nri2\ntidy\nAlex Coppock\nRandomization Inference for Randomized Experiments\nR\nhttps://rdrr.io/cran/ri2/\n\n\nhettx\ndetect_idiosyncratic\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nestimate_systematic\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nget.p.value\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nKS.stat\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nmake.linear.data\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nmake.quadradic.data\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nmake.randomized.compliance.dat\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nmake.randomized.dat\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nmake.skew.data\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nPenn46_ascii\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nR2\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nrq.stat\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nrq.stat.cond.cov\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nrq.stat.uncond.cov\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nSE\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nSKS.pool.t\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nSKS.stat\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nSKS.stat.cov\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nSKS.stat.cov.pool\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nSKS.stat.cov.rq\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nSKS.stat.int.cov\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nSKS.stat.int.cov.pool\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\ntest.stat.info\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nToyData\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nvariance.ratio.test\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nWSKS.t\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nsensemakr\nadd_bound_to_contour\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nadjusted_estimate\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nadjusted_partial_r2\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nadjusted_se\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nadjusted_t\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nbias\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\ncolombia\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\ndarfur\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\ngroup_partial_r2\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nmodel_helper\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\novb_bounds\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\novb_contour_plot\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\novb_extreme_plot\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\novb_minimal_reporting\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\novb_partial_r2_bound\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\npartial_f\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\npartial_f2\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\npartial_r2\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nrel_bias\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nrelative_bias\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nresid_maker\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nrobustness_value\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nsensemakr\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nsensitivity_stats\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nCBPS\nAsyVar\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nCBPS\nbalance\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nCBPS\nBlackwell\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nCBPS\nCBIV\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nCBPS\nCBMSM\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nCBPS\nCBPS\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nCBPS\nCBPS.fit\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nCBPS\nhdCBPS\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nCBPS\nLaLonde\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nCBPS\nnpCBPS\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nCBPS\nvcov_outcome\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nDeclareDesign\ncite_design\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ncompare_design_code\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ncompare_design_data\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ncompare_design_estimates\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ncompare_design_inquiries\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ncompare_design_summaries\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ncompare_designs\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ncompare_diagnoses\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_assignment\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_diagnosands\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_estimand\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_estimands\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_estimator\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_estimators\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_inquiries\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_inquiry\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_measurement\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_model\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_population\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_potential_outcomes\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_reveal\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_sampling\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_step\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_test\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndelete_step\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndiagnose_design\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndiagnose_designs\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndraw_data\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndraw_estimand\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndraw_estimands\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndraw_estimates\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nexpand_design\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nget_diagnosands\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nget_estimates\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nget_simulations\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ninsert_step\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nlabel_estimator\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nlabel_test\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nmodel_handler\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\npop.var\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nprint_code\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nredesign\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nreplace_step\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nreshape_diagnosis\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nrun_design\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nset_citation\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nset_diagnosands\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nsimulate_design\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nsimulate_designs\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ntidy\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ntidy_estimator\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ntidy_try\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nvars\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html",
    "href": "guides/implementation/survey-implementation_en.html",
    "title": "10 Things to Know About Survey Implementation",
    "section": "",
    "text": "There are two discrete skill sets needed on the ground when implementing a survey. The first skill set is focused on administration/logistics, and the second skill set is focused on research design. The first set of skills is needed for administration: budgeting, creating route plans, recruitment, and management of staff. Administration requires a level of familiarity with local conditions; for example, the ability to quickly estimate costs and troubleshoot logistical issues are important here. The second skill set requires knowledge of research methods to ensure that survey implementation is consistent with the research design. Researchers must be able to recognize any deviations from the protocol and address them in a way that leads to as little bias as possible. Important here is a deep understanding of the survey protocols and possible alternatives, in the case that changes need to be made on the ground.\nIf you will not be present during survey administration, you will need to either hire a firm or individuals who can work together to cover both sets of needs. There are clear advantages to hiring a firm if you have the budget, the biggest being that firms coordinate internally and balance both sets of needs, ensuring that logistics accommodate the design and vice versa. A possible drawback is that firms frequently have their own protocols, and these default procedures are usually at a lower standard than the latest protocol being used in academia. Upgrading protocols is a costly process and firms may push back against the use of stricter, or simply different, practices.\nIf you will be present during enumeration but you have a small budget, or do not feel you could manage the entire implementation (administration and design) yourself, a good alternative is to hire a field coordinator from a survey or research firm on a consultant basis. This person can help with administration while you take on the design-related work. Additionally, hiring someone for administration locally can do a lot to help with cross-cultural management. The types of management procedures that might work to motivate or sanction employees in the US may not work in another context, so someone who knows what is acceptable and effective can add a lot of value.\n\n\nWhen setting up contracts with local firms it is important to get the incentives right—thorough and good work should also be the most profitable for the firm. You can do a lot to set expectations and incentives in the contract. For example, pay on delivery where possible (although it is customary to pay some costs upfront to cover fixed expenses like transport and early salaries). You can also choose to impose financial penalties for late or low-quality data, but be sure to make these requirements clear up front and provide specific rules for what constitutes low-quality work and how late penalties will be assigned.\nIn addition to direct costs, it is reasonable for a local firm to charge overhead. This can vary from context to context, and it is best to check against the budgets of other similar projects to make sure the rate is reasonable."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#contracts-with-local-firms",
    "href": "guides/implementation/survey-implementation_en.html#contracts-with-local-firms",
    "title": "10 Things to Know About Survey Implementation",
    "section": "",
    "text": "When setting up contracts with local firms it is important to get the incentives right—thorough and good work should also be the most profitable for the firm. You can do a lot to set expectations and incentives in the contract. For example, pay on delivery where possible (although it is customary to pay some costs upfront to cover fixed expenses like transport and early salaries). You can also choose to impose financial penalties for late or low-quality data, but be sure to make these requirements clear up front and provide specific rules for what constitutes low-quality work and how late penalties will be assigned.\nIn addition to direct costs, it is reasonable for a local firm to charge overhead. This can vary from context to context, and it is best to check against the budgets of other similar projects to make sure the rate is reasonable."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#salaries",
    "href": "guides/implementation/survey-implementation_en.html#salaries",
    "title": "10 Things to Know About Survey Implementation",
    "section": "2.1 Salaries",
    "text": "2.1 Salaries\nEstimating total salary costs before drawing the sample (needed in order to determine the teams and route plans) requires a bit of guesswork. One approach is to estimate the work-hours needed to conduct the survey (survey length x sample size) and divide by some estimated number of enumerators to come up with the number of enumerator days you will need to pay. The per diem may need to cover food and lodging, and make this clear to enumerators so they can plan accordingly. For surveys that will require long fieldwork, it is good practice to pay salary on a rest day each week although some enumerators prefer to work continuously in order to finish sooner and return home. This choice is context-specific."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#per-diems",
    "href": "guides/implementation/survey-implementation_en.html#per-diems",
    "title": "10 Things to Know About Survey Implementation",
    "section": "2.2 Per Diems",
    "text": "2.2 Per Diems\nPer diems cover enumerator’s expenses associated with doing fieldwork. This means lodging for overnight stays, all meals, and sometimes also transportation. Per diems should also be paid on rest days that fall in between work days. In the case that the variation in lodging and food costs is low, it is not important to change the per diem rate according to location. Teams will know when to save and when to spend.\n\n2.2.0.1 Quick calculator:\n((survey time to complete * sample size)/workable hours in a day)/# of enumerators = number of days\nnumber of days * (daily rate + per diem) + supervisors = approx. total salary cost"
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#transportation",
    "href": "guides/implementation/survey-implementation_en.html#transportation",
    "title": "10 Things to Know About Survey Implementation",
    "section": "2.3 Transportation",
    "text": "2.3 Transportation\nIt’s important to, ex ante, be as accurate as possible in estimating the full cost of transportation as this is frequently both least flexible and most variable cost. Typically, it is good practice to build in contingency on the cost of fuel, as the price can change over the several months it takes to go from the grant application stage to the implementation stage. If you are budgeting before drawing your sample, pay particular attention to hard-to-reach areas in your population (islands, places without road access) and pad your transport line for the possibility you randomly sample enumeration areas that carry these higher costs."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#equipment",
    "href": "guides/implementation/survey-implementation_en.html#equipment",
    "title": "10 Things to Know About Survey Implementation",
    "section": "2.4 Equipment",
    "text": "2.4 Equipment\nLater on in this guide we present the benefits of using personal digital assistants (PDAs) or tablets for data collection (see section 3). PDAs/tablets can be either purchased using survey funds or leased from a research firm, university, or other researchers."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#why-personal-digital-assistants-pdas-or-tablets-are-better-if-you-have-the-budget",
    "href": "guides/implementation/survey-implementation_en.html#why-personal-digital-assistants-pdas-or-tablets-are-better-if-you-have-the-budget",
    "title": "10 Things to Know About Survey Implementation",
    "section": "3.1 Why Personal Digital Assistants [PDAs] or Tablets are better (if you have the budget)",
    "text": "3.1 Why Personal Digital Assistants [PDAs] or Tablets are better (if you have the budget)\nUse of a PDA/tablet allows the collection of more accurate and detailed data (Goldstein, 2012) because of:\n\nAutomated skip patterns\nMore detail, e.g. the ability to program a multi-stage code list\nThe ability to program some randomization algorithms (permuted block randomization, for example)\nSensitive questions can be recorded by the respondent themselves on the tablet (instead of the enumerator). There are even ways, using sound and video playback, to do this with illiterate respondents\nThe ability to audio record responses for later transcription\n\nPDAs/tablets have lower error rates than paper-based surveys (Caeyers, 2010) and have superior quality control options including:\n\nReal-time data upload\nReal-time survey modification in the case of error or oversight in terms of questions included\nAudio recording of portions of surveys to verify enumerator delivery\nTimers that measure how long respondents spend on the entire survey and each individual question\nReal-time validation checks to make sure numerical questions don’t have answers that are nonsensical\nThe ability to generate several orthogonal treatments within a single survey (either for multiple experiments or for conjoint experiments)"
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#tips-for-pda-or-tablet-use",
    "href": "guides/implementation/survey-implementation_en.html#tips-for-pda-or-tablet-use",
    "title": "10 Things to Know About Survey Implementation",
    "section": "3.2 Tips for PDA or tablet use",
    "text": "3.2 Tips for PDA or tablet use\n\nAlways buy extra equipment – chargers, battery packs, power strips, and tablets can go missing, be stolen, or get broken. In many countries you can’t buy extra equipment, even in capital cities, and it’s often more expensive and lower quality than what you can get at your home base. Buying 10-20% more equipment than you need can be expensive, but it is usually far cheaper than the salaries that you will have to pay for enumerators with equipment problems who do not have backups.\nPay close attention to battery life when you buy your equipment. If you want full days of enumeration, some of the cheaper tablets will not work.\nBudget for extra battery packs for your enumerators to carry in the field, particularly if they will travel to rural areas where they may not always be able to charge the tablets every night.\nForecast how frequently teams will be able to upload recorded data. The PDAs/tablets need to be able to store data from completed interviews until uploading is possible. In rural environments this can mean quite a lot of memory is needed, particularly if the survey is long and/or complex.\nBudget extra costs for charging of the PDAs or tablets in the field (i.e. paying for extra generator time from hotels) and for potential delays because of lack of power.\nBudget extra time for exhaustive testing of the PDA/tablet once the survey is fully coded. Code failures can be disastrous the more complex the code/randomization becomes. Run through as many different responses to the survey as you can yourself, and do a “fake” pre-test during training in which you collect data and inspect it to make sure there are no errors.\nHave someone available to make on-the-spot changes to the code in case problems are discovered in the field that stall enumeration until the change is implemented. In addition, give your enumerators enough paper versions of the survey to last for one or more days to serve as a holdover until the code is remedied.\nIf possible, name your variables in the survey software to avoid a really laborious process of manually re-naming later. This also makes it easier to inspect the data in real-time.\nCode answer values (i.e. the values that will be outputted into the dataset) in advance so that you can standardize scales and easily clean the data (for example, use different negative numbers for “don’t know” and “refuse” options so a 10-character command in R can clean the whole dataset).\nTake advantage of having more space for text by giving enumerators directions for complicated items in the displayed question text itself."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#surveying",
    "href": "guides/implementation/survey-implementation_en.html#surveying",
    "title": "10 Things to Know About Survey Implementation",
    "section": "4.1 Surveying",
    "text": "4.1 Surveying\nField teams are made up of enumerators and a team leader. Team leaders report to a field manager, or in a case of a large survey, a regional supervisor.\n\nThe enumerator’s role is sampling and selecting households and respondents within enumeration areas, gain consent, and conduct the interview. The enumerator’s tasks include:\n\nSelecting the household. For an enumerator this is the first stage in the random selection process and is done according to a clearly specified procedure, which should be easily referenced in both the manual and the survey instrument itself.\nSelecting respondents. Once the household has been selected, the enumerator should follow a similarly-specified random (or systematic) selection process in order to select the subject.\nConsent. Enumerators need to know the definition of informed consent and how to make sure the respondent understands his/her rights during the interview.\nConducting the interview. This will involve asking questions and closely following the instructions communicated in training and on the questionnaires. A hard copy of question-by-question instructions should be provided to enumerators for use as a reference.\nControlling the interview situation. The enumerator must work towards reducing or eliminating suspicion and prejudice within the interview environment. This may involve asking bystanders not to congregate or dealing with sensitive situations with respect to other family members within the home.\nAvoiding bias. The enumerator’s personal views must not be reflected in the data collected. This means, among other things, that the enumerator must remain neutral and respectful during the data collection by not expressing his or her opinion and ensuring that the respondent trusts that the enumerator will respect his or her privacy. Emphasize during training that there really is no right answer and that the goal of the survey is to find out what people really think – because without knowing that, we can’t find solutions to problems.\nPresentation. Interpersonal skills such as manners, dressing, body language and ability to persuade are all important for data quality and will help in obtaining the target respondents for each day.\n\nThe team leader manages a group of enumerators and can conduct interviews him/herself. The team leader is responsible for:\n\nLogistics. The team leader is responsible for organizing the transport of teams, gathering materials, transporting paper instruments, and managing the technology.\nPermissions. Team leaders make contact with local authorities to introduce and explain the survey and get permission to work.\nSupervising. Team Leaders ensure the team arrives on time to the enumeration area and proceeds to oversee within-enumeration area selection of households.\nCorrecting. Once interviewing has begun, team leaders should move between enumerators and check they are following protocols. Supervision should not, however, make subjects feel uncomfortable.\nData Quality. Team Leaders check all questionnaires in the field and at the end of the day. If PDAs/tablets are used either the team leader or an RA will check data. The team leader ensures that data errors are fixed by revisiting respondents."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#oversight",
    "href": "guides/implementation/survey-implementation_en.html#oversight",
    "title": "10 Things to Know About Survey Implementation",
    "section": "4.2 Oversight",
    "text": "4.2 Oversight\nEnumeration teams and field management can quite easily deviate from important protocols—these deviations can range from replacing sampled households based on the ease of getting respondents to creating fake data. In many cases, cutting corners is not easy to detect and can save money and time for the enumerators, field managers, and even the survey research firm. PDAs/tablets can reduce the number of total possible types of fraud, but some level of field supervision is always necessary. A parallel reporting structure, with independent oversight, can help guard against these deviations.\nOn the oversight side, there are two types of checks that should be conducted– audits and backchecks:\n\nAuditors arrive at randomly selected villages on the days they are slated for enumeration, without advance warning to the team. When an auditor visits a team, they make sure the team is in the correct enumeration area, that they have sought consent from local leaders, complied with the household selection procedure, and that all team members are working. The auditor then tracks the performance of the team throughout the day— as they seek consent, build rapport, conduct within-enumeration area sampling, and survey respondents.\nA backcheck consists of a revisit to a respondent who completed a survey not more than a few days prior. Using the data collected, they locate respondents and verify responses on a few key questions, for which the response was not likely to have changed (for example, age or household size) in the period since they were initially contacted. Backchecking can both help to identify enumerators who are not performing and establish an error rate. If phone numbers are being collected in the survey, this can be done more cheaply by telephone.\n\nConducting both audits and backchecks means that for each individual survey there is some non-zero probability that the work will be checked in some way. In the case that there are only backchecks, teams will never be monitored in terms of their adherence to protocols as they sample and conduct interviews. In the case that there are only audits, if a team is not visited on a particular day of work there is no chance to check that they actually interviewed subjects and recorded their responses accurately.\nAuditors and backcheckers must report directly to survey management. Imagine an example: Say a village is difficult to find and the team of enumerators chooses a replacement (rather than resampling by the PIs), and the auditors visit the sampled village and uncover it was not surveyed. If this error is communicated to someone also managing the enumerators, their best response is to cover this up or try to fix it without the PI knowing. This prevents having to admit a management mistake, and having to add a day of work to revisit the original or resampled village. If the survey team is notified directly, there is an opportunity to fix the mistake and make personnel changes as needed. Unmonitored communication between auditing teams and enumeration teams can result in a lot of unauthorized fixes and unexplainable data patterns."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#sources",
    "href": "guides/implementation/survey-implementation_en.html#sources",
    "title": "10 Things to Know About Survey Implementation",
    "section": "5.1 Sources",
    "text": "5.1 Sources\nIf you are working alone, recruit experienced enumerators through contacts at survey firms, NGOs, or universities. It is important that enumerators are experienced, literate, educated, and able to build rapport with subjects. Hiring enumerators who are connected, in some way, with the survey leaders or local coordinator, e.g. through a youth organization or other social tie, can help immensely with oversight as the enumerators have bigger reputational costs if they shirk their duties."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#languages",
    "href": "guides/implementation/survey-implementation_en.html#languages",
    "title": "10 Things to Know About Survey Implementation",
    "section": "5.2 Languages",
    "text": "5.2 Languages\nThe foremost requirement is that the enumerators speak the required local languages. We know that coethnicity between enumerators and subjects can reduce bias, so recruitment of coethnic interviewers, and balancing across the sample if using treatment and control groups, is important."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#gender-parity",
    "href": "guides/implementation/survey-implementation_en.html#gender-parity",
    "title": "10 Things to Know About Survey Implementation",
    "section": "5.3 Gender parity",
    "text": "5.3 Gender parity\nHaving a team of mostly male enumerators interview a sample with equal numbers of men and women there can introduce response bias. For sensitive questions, such as questions on sexual behavior or violence, it is strongly recommended that women interview other women. If it is difficult to recruit experienced women enumerators, it usually makes sense to hold a special training for women candidates with less experience in order to ensure teams are balanced in the end."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#standards",
    "href": "guides/implementation/survey-implementation_en.html#standards",
    "title": "10 Things to Know About Survey Implementation",
    "section": "7.1 Standards",
    "text": "7.1 Standards\nTrainings establish consistent standards for data collection. If you are contracting a survey firm and are not on the ground yourself, training is the most important part of the process to personally attend. It’s a key moment to communicate quality standards, expectations, the intended meaning of each question, and teach important procedures that may be more technical than what the firm is used to, such as a list experiment. It is also a key moment to motivate the team, by communicating the project’s goals and importance. In order to ensure that the each member of the team is prepared to a certain standard, it is a good idea to test each team member at the end of the training period. There should be an expectation that some team members will be asked not to proceed any further with the project as a result of the test, which will emphasize the importance of taking training to heart and taking the test seriously."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#assessment",
    "href": "guides/implementation/survey-implementation_en.html#assessment",
    "title": "10 Things to Know About Survey Implementation",
    "section": "7.2 Assessment",
    "text": "7.2 Assessment\nTrainings are a key moment for assessment as well. If you train teams together it is easy to spot management issues and leadership capabilities. A good practice is to train teams together, and select team leaders at the end of training—this gives you a few days to gauge skills and also incentivizes trainees to perform during the training."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#participation",
    "href": "guides/implementation/survey-implementation_en.html#participation",
    "title": "10 Things to Know About Survey Implementation",
    "section": "7.3 Participation",
    "text": "7.3 Participation\nTrainings set the tone for the rest of fieldwork. Beyond communicating standards and expectations, this is also a key moment to create a culture of participation. Encouraging trainees to speak out about issues with the survey can show that you are open to feedback and increase the chances that they will report adverse events or challenges during the actual data collection.\nTraining sections:\n\nProtocol review, e.g. how to get permission to work in village/household, how to select respondent, etc.\nDiscussion of unforeseen contingencies, e.g. what happens when you get a refusal, when enumerators are targeted or threatened, when enumerators observe perverse reactions to or consequences of the survey\nQuestion by question review\nInterviewing techniques (rapport building, discussions)\nReview translated instrument\nUsing a PDA/tablet and conducting the interview on a PDA/tablet\nUsing GPS\nGroup practice (enumerators interview each other and get feedback)\nField practice– at least one day of training (or more for complicated or long surveys) should be spent interviewing real people who are similar to the survey subjects\nCertification exam\n\nTraining usually takes several more days than you expect. See below for a rough guide to realistic training schedules.\n\nList of documents needed for training:\n\nInstrument + translated instrument\nQuestion-by-question guide\nManual with expectations, instructions for filling responses, tips on interviewing etc.\nProtocol for sampling, consent, reporting, and unforeseen contingencies\n\nBefore being deployed to the field, each enumerator must:\n\nBe able to correctly list, sample and interview individuals in the enumeration area\nUnderstand their role\nUnderstand and correctly follow interviewing protocols\nBe informed about oversight procedures\nComplete an IRB-approved module on human subjects protection\n\nData from mock surveys must be individually assessed and feedback given to each enumerator. You can check whether certain enumerators are entering data differently than their peers, for example by entering lots of “Don’t know” or “Refuse” answers, finding low prevalence of sensitive behaviors, or entering data that is logically inconsistent. However, there is a lot that you can’t tell from the data alone. Spending a lot of time observing enumerators while they run surveys can greatly increase the quality of the data by improving their training, allowing you to select the best enumerators more accurately, and allowing you to understand how the questions are being implemented in the field."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#expectations-and-quality-control",
    "href": "guides/implementation/survey-implementation_en.html#expectations-and-quality-control",
    "title": "10 Things to Know About Survey Implementation",
    "section": "8.1 Expectations and Quality Control",
    "text": "8.1 Expectations and Quality Control\nExpectations should be laid out clearly during training, in a manual, and reiterated in clearly worded contracts (signed after training).\nBasic expectations of enumerators:\n\nBeing on time\nAdhering to within-enumeration area sampling and replacement scheme\nGetting informed consent\nBuilding rapport with subjects\nAccurately recording responses\nCommunication with supervisors"
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#contracts-and-payment",
    "href": "guides/implementation/survey-implementation_en.html#contracts-and-payment",
    "title": "10 Things to Know About Survey Implementation",
    "section": "8.2 Contracts and Payment",
    "text": "8.2 Contracts and Payment\nAs much as possible, make payment dependent on delivery. Enumerators have less and less incentive to stick with the project towards the end of fieldwork. The marginal returns are lower and they may be concerned about finding new work. In order to offset this, it is good practice to withhold a portion of their total salary (+/- 30%) until the end of fieldwork, and sometimes until data has been thoroughly reviewed if using paper instruments that need to be entered manually. At the same time, enumerators are often living paycheck to paycheck and may have expenses to cover during their long absence in the field. It is important to pay an advance up front to allow enumerators to take care of personal expenses that may otherwise make them anxious and unhappy during fieldwork. Having a strong local manager who understands the enumerators financial situations can help you create incentives while still making sure that they perceive the compensation structure as fair and adequate."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#soft-incentives",
    "href": "guides/implementation/survey-implementation_en.html#soft-incentives",
    "title": "10 Things to Know About Survey Implementation",
    "section": "8.3 Soft Incentives",
    "text": "8.3 Soft Incentives\nSoft incentives help to keep teams happy and motivated throughout work. Some examples are:\n\nPerformance-based bonuses: Allowing managers to give performance-based bonuses for exceptional performance on a daily or weekly basis.\nCertificates: Survey trainings often involve learning portable skills, like the use of tablets or PDAs. Certificates can help enumerators prove to new employers that that they have these skills.\nLetters of Recommendation\nRecommendations to other survey firms, NGOs, etc.\nWrap party"
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#checking-data-entered-from-paper-instruments",
    "href": "guides/implementation/survey-implementation_en.html#checking-data-entered-from-paper-instruments",
    "title": "10 Things to Know About Survey Implementation",
    "section": "10.1 Checking data entered from paper instruments",
    "text": "10.1 Checking data entered from paper instruments\nAfter interviewing the team leader needs to review all instruments for completeness and accuracy. If there are missing data or other inconsistencies, the team leader should send the enumerator back to revisit the respondent to correct all problems before leaving the area.\nOnce instruments are collected, data entry should commence as soon as possible. All data should be entered twice, and any discrepancies should be checked by a supervisor against the paper instrument."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#checking-data-gathered-using-pdas-or-tablets",
    "href": "guides/implementation/survey-implementation_en.html#checking-data-gathered-using-pdas-or-tablets",
    "title": "10 Things to Know About Survey Implementation",
    "section": "10.2 Checking data gathered using PDAs or tablets",
    "text": "10.2 Checking data gathered using PDAs or tablets\nWhen using tablets or PDAs, checking the data is the responsibility of the RA and PIs. In addition to using a script that checks for patterns and outliers, it is also best practice to record selected portions of the interview and listen to a subsample of responses, both for errors and quality."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#footnotes",
    "href": "guides/implementation/survey-implementation_en.html#footnotes",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThanks to Brandon de la Cuesta for help with this section↩︎"
  },
  {
    "objectID": "guides/data-strategies/randomization_en.html",
    "href": "guides/data-strategies/randomization_en.html",
    "title": "10 Things You Need to Know About Randomization",
    "section": "",
    "text": "There are many ways to randomize. The simplest is to flip a coin each time you want to determine whether a given subject gets treatment or not. This ensures that each subject has a .5 probability of receiving the treatment and a .5 probability of not receiving it. Done this way, whether one subject receives the treatment in no way affects whether the next subject receives the treatment, every subject has an equal chance of getting the treatment, and the treatment will be independent from all confounding factors — at least in expectation.\nThis is not a bad approach but it has shortcomings. First, using this method, you cannot know in advance how many units will be in treatment and how many in control. If you want to know this, you need some way to do selections so that the different draws are not statistically independent from each other (like drawing names from a hat). Second, you may want to assert control over the exact share of units assigned to treatment and control. That’s hard to do with a coin. Third, you might want to be able to replicate your randomization to show that there was no funny business. That’s hard to do with coins and hats. Finally, as we show below, there are all sorts of ways to do randomization to improve power and ensure balance in various ways that are very hard to achieve using coins and hats.\nFortunately though, flexible replicable randomization is very easy to do with freely available software. The following simple R code can, for example, be used to generate a random assignment, specifying the number of units to be treated. Here, N (100) is the number of units you have and m (34) is the number you want to treat. The “seed” makes it possible to replicate the same draw each time you run the code (or you can change the seed for a different draw).1\n\nlibrary(randomizr)\nset.seed(343)\ncomplete_ra(100, 34)"
  },
  {
    "objectID": "guides/data-strategies/randomization_en.html#footnotes",
    "href": "guides/data-strategies/randomization_en.html#footnotes",
    "title": "10 Things You Need to Know About Randomization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRandom number generators are actually pseudo-random because they generate a vector of random numbers based on a small set of initial values, known as a seed state. Random number generators operate this way in order to improve computational speed. However, the series of random numbers generated is as random as you need to it to be for the purposes of random assignment because it is wholly unrelated to the potential outcomes of your subjects.↩︎\nFor more, see Moore and Moore (2013).↩︎\nFor a more detalied walkthrough on the randomization procedures available in the R package randomizr, see here.↩︎\nBut if we are certain about the loan’s effects, then it’s also unclear why we are running an experiment to test it. In medical research, randomized controlled trials often stop if it becomes clear early on that a drug is undoubtedly curing life-threatening diseases, and therefore withholding it from control subjects is dangerous. (Similarly, a trial would also stop if it were clear early on that a drug is undoubtedly causing negative and harmful effects.)↩︎"
  },
  {
    "objectID": "guides/data-strategies/adaptive-design_en.html",
    "href": "guides/data-strategies/adaptive-design_en.html",
    "title": "10 Things to Know About Adaptive Experimental Design",
    "section": "",
    "text": "1 What is an “adaptive” design?\nA static design applies the same procedures for allocating treatments and measuring outcomes throughout the trial. In contrast, an adaptive design may, based on interim analysis of the trial’s result, change the allocation of subjects to treatment arms or may change the allocation of resources to different outcome measures.\nOrdinarily, mid-course changes in experimental design are viewed with skepticism since they open the door to researcher interference in ways that could favor certain results. In recent years, however, statisticians have developed methods to automate adaptation in ways that either lessen the risk of interference or facilitate bias correction at the analysis stage.\n\n\n2 What are the potential advantages of an adaptive design?\nAdaptive designs have the potential to detect the best-performing experimental arm(s) more quickly than a static design (i.e., with fewer data-collection sessions and fewer subjects). When these efficiencies are realized, resources may be reallocated to achieve other research objectives.\nAdaptive designs also have the potential to lessen the ethical concerns that arise when subjects are allocated to inferior treatment arms. For therapeutic interventions, adaptive designs may reduce subjects’ exposure to inferior treatments; for interventions designed to further broad societal objectives, adaptive designs may hasten the discovery of superior interventions.\nTo illustrate the potential advantages of adaptive design, we simulate an RCT involving a control group and eight treatment arms. We administer treatments and gather 100 outcomes during each “period.” The simulation assumes that each subject’s outcome is binary (e.g., good versus bad). The adaptive allocation of subjects is based on interim analyses conducted at the end of each period. We allocate next period’s subjects according to posterior probabilities that a given treatment arm is best (see below). The simulation assumes that the probability of success is 0.10 for all arms except one, which is 0.20. The stopping rule is that the RCT is halted when one arm is found to have a 95% posterior probability of being best.\nIn the adaptive trial depicted below, the best arm (the red line) is correctly identified, and the trial is halted after 23 periods (total N=2300).\n\n\n\n3 What are the potential disadvantages of adaptive designs?\nThere is no guarantee that adaptive design will be superior in terms of speed or accuracy. For example, adaptive designs may result in a lengthy trial in cases where all of the arms are approximately equally effective. Even when one arm is truly superior, adaptive searches have some probability of resulting in long, circuitous searches (and considerable expense) if by chance they get off to a bad start (i.e., one of the inferior arms appears to be better than the other based on an initial round of results).\nFor instance, consider the following scenario in which all but one of the arms have a 0.10 probability of success, and the superior arm has a 0.12 probability of success (with the same trial design as in the previous example). The design eventually settles on the truly superior arm but only after more than 200 periods (N = 23,810). Even after 50 periods, the results provide no clear sense that any of the arms is superior.\n\nA further disadvantage of adaptive designs is they may produce biased estimates of the average treatment effect of the apparent best arm vis-à-vis the control group. Bias arises because the trial stops when the best arm crosses a threshold suggesting optimality; this stopping rule tends to favor lucky draws that suggest the efficacy of the winning arm. Conversely, when adaptive algorithms associate sampling probability with observed history, under-estimation for inferior arms, including the control group, may persist until stopping time (Nie et al. 2017).\nFor example, in the first scenario described above in which all arms have a 0.10 probability of success except for the best arm, which is 0.20, the average estimated success probability for the truly best arm is 0.202 across 1000 simulated experiments, while the control group average is found to 0.083. The average estimated difference in success probabilities (i.e., the average treatment effect) is 0.119, as compared to the true value of 0.10.\nIn the second scenario, in which the best arm’s success probability is just 0.12, the average estimated success probability for the best arm is 0.121, and the average estimated ATE is 0.027, as compared to the true ATE of 0.02. Bias in this case is relatively small on a percentage point scale due to the very large size of the average experiment.\n\n\n4 What kinds of experiments lend themselves to adaptive design?\nAdaptive designs require multiple periods of treatment and outcome assessment.\nAdaptive designs are well suited to survey, on-line, and lab experiments, where participants are treated and outcomes measured in batches over time.\nSome field experiments are conducted in stages, although the logistics of changing treatment arms may be cumbersome, as discussed below. One possible opportunity for adaptive design in a field context occurs when a given experiment is to be deployed over time in a series of different regions. This allows for adaptation based on region-by-region interim analyses.\nAdaptive designs are ill-suited to one-shot interventions with outcomes measured at a single point in time. For example, experiments designed to increase voter turnout in a given election do not lend themselves to adaptive design because everyone’s outcome is measured at the same time, leaving no opportunity for adaptation.\n\n\n5 What is the connection between adaptive designs and “multi-arm bandit problems”?\nThe multi-arm bandit problem (Scott 2010) is a metaphor for the following optimization problem. Imagine that you could drop a coin in one of several slot machines that may pay off at different rates. (Slot machines are sometimes nicknamed “one-arm bandits,” hence the name.) You would like to make as much money as possible. The optimization problem may be characterized as a trade off between learning about the relative merits of the various slot machines – exploration – and reaping the benefits of employing the best arm – exploitation. A static design may be viewed as an extreme case of allocating subjects solely for exploration.\nAs applied to RCTs, the aim is to explore the merits of the various treatment arms while at the same time reaping the benefits of the best arm or arms. Although the MAB problem is not specifically about estimating treatment effects, one could adjust the optimization objective so that the aim is to find the treatment arm with the greatest apparent superiority over the control group.\n\n\n6 What are some widely used algorithms for automating “adaptation”?\nThe most commonly used methods employ some form of “Thompson sampling” (Thompson 1933). Interim results are assessed periodically, and in the next period subjects are assigned to treatment arms in proportion to the posterior probability that a given arm is best. The more likely an arm is to be “best,” the more subjects it receives.\nMany variations on this basic assignment routine have been proposed, and some are designed to make it less prone to bias. If an adaptive trial is rolled out during a period in which success rates tend to be growing, increasing allocation of subjects to the best arm will tend to exaggerate that arm’s efficacy relative to the other arms, which receive fewer subjects during the high-yield period. In order to assess bias and correct for it, it may be useful to allocate some subjects in every period according to a static design. In this case, inverse probability weights for each period may be used to obtain unbiased estimates of the average treatment effect. (See Gerber and Green 2012 on the use of inverse probability weights for estimation of average treatment effects when the probability of assignment varies from block to block.)\n\n\n7 What are the symptoms of futile search?\nAlthough it is impossible to know for sure whether a drawn out search reflects an unlucky start or an underlying reality in which no arm is superior, the longer an adaptive trial goes, the more cause for concern. The following graphs summarize the distribution of stopping times for three scenarios. Stopping was dictated by at 10% value remaining criterion. Specifically, the trial stopped when the top of the 95% confidence interval showed that no other arm was likely to offer at least a 10 percent (not percentage point) gain in success rate. The first two scenarios were described above; the third scenario considers a case in which there are two superior arms. The graph illustrates how adaptive trials tend to conclude faster when the superiority of the best arm(s) is more clear-cut.\n\n\n\n8 What implications do adaptive designs have for pre-analysis plans?\nThe use of adaptive designs introduces additional decisions, which ideally should be addressed ex ante so as to limit researcher bias. For example, the researcher should specify which algorithms will be used for allocation. It is especially important to specify the stopping rule. Depending on the researcher’s objectives, this rule may focus on achieving a desired posterior probability, or it may use a “value remaining” criterion that considers whether one or more arms have shown themselves to be good enough vis-à-vis the alternative arms. Other hybrid stopping criteria may also be specified. The pre-analysis plan should also describe the analytic steps that will be taken to correct for bias.\n\n\n9 Are multi-arm bandit trials frequently used in social science?\nMany of the applications to date have taken place in commercial settings, such as website design for high-volume e-commerce, or in settings such as on-line fundraising. Relatively few applications have been written up in detail for a scholarly audience. Surprisingly rare are applications in biomedical research. As Villar, Bowden, and Wason (2015) note, “Despite this apparent near-perfect fit between a real-world problem and a mathematical theory, the MABP has yet to be applied to an actual clinical trial.”\nPutting aside the use of multi-arm bandit approaches, the use of adaptive trials is gradually winning acceptance in biomedical research. For details, see Chin (2016).\n\n\n10 What other considerations should inform the decision to use adaptive design?\nAs noted above, adaptive designs add to the complexity of the research design and analysis. They also may increase the challenges of implementation, particularly in field settings where the logistical or training costs associated with different arms vary markedly. Even when one arm is clearly superior (inferior), the lead-time necessary to staff or outfit this arm may make it difficult to scale it up (down). Adaptive designs are only practical if adaptation is feasible.\nOn the other hand, funders and implementation partners may welcome the idea of an experimental design that responds to on-the-ground conditions such that problematic arms are scaled back. A middle ground between static designs and designs that envision adaptation over many periods are adaptive designs involving only two or three interim analyses and adjustments. Such trials are winning increased acceptance in biomedical research (Chow and Chang 2008) and are likely to become more widely used in the social sciences too. The growing interest in replication and design-based extensions of existing experiments to aid generalization are likely to create opportunities for adaptive design.\n\n\n\n\n\n\n\n\n Back to top11 References\n\nChin, Richard. 2016. Adaptive and Flexible Clinical Trials. CRC Press.\n\n\nChow, Shein-Chung, and Mark Chang. 2008. “Adaptive Design Methods in Clinical Trials – a Review.” Orphanet Journal of Rare Diseases 3 (11).\n\n\nNie, Xinkun, Xiaoying Tian, Jonathan Taylor, and James Zou. 2017. “Why Adaptively Collected Data Have Negative Bias and How to Correct for It.”\n\n\nScott, Steven L. 2010. “A Modern Bayesian Look at the Multi-Armed Bandit.” Applied Stochastic Models in Business and Industry 26: 639–58.\n\n\nThompson, William R. 1933. “On the Likelihood That One Unknown Probability Exceeds Another in View of the Evidence of Two Samples.” Biometrika 25: 285–94.\n\n\nVillar, Sofı́a S., Jack Bowden, and James Wason. 2015. “Multi-Armed Bandit Models for the Optimal Design of Clinical Trials: Benefits and Challenges.” Statistical Science: A Review Journal of the Institute of Mathematical Statistics 30 (2): 199–215."
  },
  {
    "objectID": "guides/data-strategies/measurement.es.html",
    "href": "guides/data-strategies/measurement.es.html",
    "title": "10 cosas que debe saber sobre la medición en experimentos",
    "section": "",
    "text": "1 La validez de las conclusiones que sacamos de un experimento depende de la validez de las mediciones usadas.\nUsualmente usamos experimentos para estimar el efecto causal de un tratamiento, \\(Z\\), sobre una variable de resultado, \\(Y\\). Sin embargo, la razón por la que nos preocupamos por estimar este efecto causal es, en principio, para entender las características de la relación entre dos conceptos teóricos sin observar, medidos por las variables observadas \\(Z\\) y \\(Y\\).\nTeniendo en cuenta a Adcock y Collier (2001), considere en tres pasos el proceso de medición presentado en la Figura 1. Primero, los investigadores comienzan con un concepto sistematizado; un constructo teórico claramente definido. A partir de este concepto, el investigador desarrolla un indicador que mapea el concepto en una escala o conjunto de categorías. Finalmente, las unidades o casos se puntúan en el indicador, así proyectando una medida de tratamiento \\(Z\\) y un resultado \\(Y\\). Una medición es válida si la variación en el indicador se aproxima considerablemente a la variación en el subyacente concepto de interés.\nUn diseño de investigación experimental debería permitir al investigador estimar el efecto causal de \\(Z\\) sobre \\(Y\\) bajo supuestos estándar. Pero si el objetivo final es hacer una inferencia sobre el efecto causal del concepto que \\(Z\\) mide sobre el concepto que \\(Y\\) mide, las inferencias que podemos esperar hacer sobre la base de nuestra evidencia experimental son válidas, si y solo si, ambas medidas son válidas.\n\n\n\n\n\nEl proceso de medición en el contexto de un diseño de investigación experimental.\n\n\n\n\n\n\n2 Las mediciones son el enlace entre el argumento substantivo y/o teórico de un investigador y un diseño de investigación (experimental)\nCuando consideramos el diseño de un experimento, tendemos a enfocarnos en el proceso mediante el cual el tratamiento asignado aleatoriamente \\(Z\\) es asignado, y en la distribución conjunta de \\(Z\\) y una variable de resultado, \\(Y\\). En otras palabras, tendemos a separar las puntuaciones de \\(Z\\) y \\(Y\\) de conceptos más amplios cuando consideramos las propiedades estadísticas de un diseño de investigación. De esta forma, dos experimentos completamente distintos, con la misma distribución de \\(Z\\) y \\(Y\\), podrían tener propiedades idénticas.\nPor ejemplo: Un ensayo clínico sobre la eficacia de la aspirina en dolores de cabeza y un experimento que proporciona información sobre el nivel de corrupción de un político de turno, que luego pregunta a la persona encuestada si votaría por este político, podría tener muestras de tamaño y distribución idénticas, asignaciones, estimandos y realización de variables de resultado (datos). Sin embargo, esta caracterización como “equivalentes” de dos proyectos de investigación completamente distintos que buscan hacer inferencias completamente distintas, puede parecernos bastante extraña, incluso inquietante.\nSin embargo, cuando consideramos la medición como un componente fundamental del diseño de investigación, claramente estos experimentos son distintos. Observamos medidas de diferentes conceptos en los datos de ambos experimentos. Al considerar los indicadores y los conceptos más amplios que subyacen a los tratamientos y resultados, nos vemos obligados a examinar las respectivas teorías o argumentos de los investigadores. Al hacerlo, podemos plantear preguntas sobre la validez de las mediciones y la relación entre la validez de ellas y la validez de inferencias finales y sustantivas.\n\n\n3 Medir tratamientos incluye la operacionalización del tratamiento, además del cumplimiento de la asignación del tratamiento.\nEn un experimento, los tratamientos son normalmente diseñados, o como mínimo descritos, por el investigador. Los consumidores de investigación experimental deben estar interesados en las características del tratamiento y cómo este manipula un concepto de interés. La mayoría de los tratamientos en las ciencias sociales son compuestos o incluyen un conjunto de atributos. Podríamos estar interesados en el efecto de proporcionar a los votantes información sobre el desempeño de los funcionarios electos. Sin embargo, proporcionar información también incluye el modo de entrega y quién estaba entregando la información. Para comprender hasta qué punto el tratamiento manipula un concepto, debemos también entender qué manipulación adicional podría estar ejerciendo el tratamiento.\nSin embargo, a pesar de todo el esfuerzo de operacionalización de un tratamiento, el vínculo en la investigación experimental entre la operacionalización y el indicador de tratamiento es fundamentalmente distinto de la medición de covariables o variables de resultado por dos razones: En primer lugar, al asignar un tratamiento, los experimentadores buscan controlar los valores que adquiere una unidad determinada. En segundo lugar, para el indicador de tratamiento, la puntuación proviene de la asignación al tratamiento, que es un producto de la aleatorización. Un sujeto puede haber recibido el tratamiento o no, pero su puntuación en el indicador de tratamiento es simplemente el tratamiento al que se le asignó, no el tratamiento que recibió.\nCuando los sujetos reciben tratamientos distintos de aquellos a los que están asignados, normalmente buscamos en qué medir el cumplimiento; si los tratamientos se administraron y en qué medida. Para ello, definimos en qué consiste el cumplimiento de la asignación de tratamiento. Al determinar qué constituye el cumplimiento, los investigadores deben considerar el aspecto central de cómo el tratamiento manipula el concepto de interés. ¿En qué momento de la administración del tratamiento ocurre esta manipulación? Una vez que el cumplimiento está operacionalizado, buscamos codificar el indicador de cumplimiento de una manera fiel a esta definición.\nPor ejemplo, considere una campaña de sondeo puerta a puerta que distribuye información sobre el desempeño de un político en funciones. Los hogares están asignados para recibir la visita de un encuestador que comparte la información (tratamiento) o no visita (control). El indicador de tratamiento es simplemente si un hogar fue asignado al tratamiento o no. Sin embargo, si los habitantes de un hogar no están en casa cuando les visita el encuestador, no reciben la información. Nuestra definición de cumplimiento debería determinar qué constituye “tratado” en nuestra medida (endógena) de si un hogar recibió el tratamiento, que en este caso sería la información. Algunas definiciones comunes de cumplimiento pueden ser (a) que alguien del hogar abrió la puerta; o (b) que alguien del hogar escuchó el guión completo de información.\n\n\n4 La mayoría de variables de resultado de interés en las ciencias sociales son latentes.\nEn contraste con la medición de los indicadores de tratamiento y de cumplimiento, la medición de variables de resultado en la investigación experimental sigue mucho más de cerca el proceso descrito en la figura anterior. Teorizamos cómo el tratamiento puede influir en un concepto de variable de resultado. Luego operacionalizamos el concepto y registramos los puntajes o valores para completar nuestra medición de la variable de resultado.\nUn desafío particular en la medición de variables de resultado es que las más comunes de estas son latentes. Esto significa que no podemos observar el valor verdadero del concepto de variable de resultado de manera directa. De hecho, la naturaleza del valor verdadero puede entrar en discusión (por ejemplo, el debate sobre la medición de la “democracia” es un caso clásico en el que se cuestiona la definición del concepto en sí). Las variables de resultados que incluyen conocimientos, preferencias y actitudes están latentes. Por lo tanto, registramos o puntuamos los indicadores observables que se supone están relacionados con la variable de resultado latente en un esfuerzo por inferir características de la variable latente. Incluso las variables de resultados de comportamiento se utilizan frecuentemente en manifestaciones de conceptos latentes más amplios (es decir, el comportamiento de voto evaluado se utiliza para hacer inferencias sobre la “responsabilidad electoral” en un lugar).\nDebido a que estas variables son latentes, es un desafío delinear indicadores apropiados. Una pobre operacionalización tiene consecuencias bastante drásticas para la validez de nuestras inferencias sobre el concepto de interés por dos razones. Como en la sección # 1, si estos indicadores no miden el concepto de interés, entonces las inferencias que hacemos sobre sobre la relación entre \\(Z\\) y \\(Y\\) (incluso con un diseño “perfecto” en términos de poder estadístico y datos faltantes, etc.) puede que no nos enseñen acerca de la “inferencia última” que estamos tratando de hacer. Además, el error de medición puede socavar nuestra capacidad para estimar el efecto de \\(Z\\) y \\(Y\\), lo que lleva a inferencias incorrectas. El resto de esta guía se centra en este último problema.\n\n\n5 Hay dos tipos de error de medición que debemos considerar.\nPodemos formalizar los desafíos de medición de manera simple. Suponga que el tratamiento \\(Z_i\\) se plantea como hipótesis para cambiar las preferencias por las normas democráticas, \\(\\nu_i\\). En principio, la cantidad que nos gustaría estimar es $E[ nu_i|Z_i = 1] - E[ nu_i|Z_i = 0] $, el ATE (Average Treatment Effect - Efecto Promedio del Tratamiento) de nuestro tratamiento sobre las preferencias por las normas democráticas. Sin embargo, \\(\\nu_i\\) es una variable latente: no podemos medirla directamente. En cambio, preguntamos sobre el apoyo de varios comportamientos que se cree corresponden a estas normas. Este indicador, \\(Y_i\\), se puede descomponer en la variable latente, \\(\\nu_i\\) y dos formas de error de medición:\n\nError de medición No-Sistemático, \\(\\delta_i\\): Este error es independiente de la asignación de tratamiento, \\(\\delta_i \\perp Z_i\\).\n\nError de medición Sistemático*, \\(\\kappa_i\\): Este error no es independiente de la asignación de tratamiento, \\(\\kappa_i \\not\\perp Z_i\\).\n\n\n\\[Y_i = \\underbrace{\\nu_i}_{\\text{Valor latente}} + \\underbrace{\\delta_i}_{\\substack{\\text{Error de medición} \\\\ \\text{no sistemático}}} + \\underbrace{\\kappa_i}_{\\substack{\\text{Error de medición} \\\\ \\text{sistemático}}}\\]\n\n\n6 Los errores de medición reducen el poder de su experimento.\nEl error de medición no sistemático, representado arriba por \\(\\delta_i\\), se refiere al ruido con el que estamos midiendo la variable latente. En ausencia de un error de medición sistemático, medimos:\n\\[Y_i = \\underbrace{\\nu_i}_{\\text{Valor latente}} + \\underbrace{\\delta_i}_{\\substack{\\text{Error de medición} \\\\ \\text{no sistemático}}}\\]\nAhora, considere la [fórmula de poder analítico] (https://egap.org/resource/10-things-to-know-about-statistical-power) para un experimento de dos brazos. Podemos expresar \\(\\sigma\\), o la desviación estándar de la variable de resultado como \\(\\sqrt{Var(Y_i)}\\). Tenga en cuenta que en la fórmula siguiente, este término aparece en el denominador del primer término. A medida que \\(\\sqrt{Var(Y_i)}\\) aumenta,el poder estadístico disminuye.\n\\[\\beta = \\Phi \\left(\\frac{|\\mu_t− \\mu_c| \\sqrt{N}}{2 \\color{red}{\\sqrt{Var(Y_i)}}} − \\Phi^{−1}\\left(1 − \\frac{\\alpha}{2}\\right)\\right)\\]\n¿De qué manera puede el error de medición no-sistemático \\(\\delta_i\\) impactar el poder? Podemos descomponer \\(\\sqrt{Var(Y_i)}\\) de la siguiente manera:\n\\[\\sqrt{Var(Y_i)} = \\sqrt{Var(\\nu_i) + Var(\\delta_i) + 2 Cov(\\nu_i, \\delta_i)}\\]\nSiempre que \\(Cov(\\nu_i, \\delta_i)\\geq 0\\) (frecuentemente asumimos \\(Cov(\\nu_i, \\delta_i)= 0\\)), debe darse el caso de que \\(Var(Y_i)\\) esté aumentado a medida que el error de medición o \\(Var(\\delta_i)\\) aumenta. Esto implica que el poder disminuye a medida que aumenta el error de medición no-sistemático. En otras palabras, entre más ruidosas sean nuestras medidas de una variable latente, menor será nuestra capacidad para detectar los efectos de un tratamiento sobre una variable latente.\n¿Qué sucedería en el caso en que \\(Cov(\\nu_i, \\delta_i) &lt; 0\\)? Mientras esto reduce \\(Var(Y_i)\\) (manteniendo \\(Var(\\nu_i)\\) y \\(Var(\\delta_i)\\) constantes), también atenúa la variación que medimos en \\(Y_i\\). En principio, esto atenuaría el numerador \\(|\\mu_t-\\mu_c|\\), el cual, si es suficiente en relación con la reducción de la varianza, también reducirá el poder.\n\n\n7 El error de medición sistemático sesga las estimaciones de los efectos causales de los intereses.\nSi estamos estimando el Efecto Promedio del Tratamiento (Average Treatment Effect, ATE) de nuestro tratamiento \\(Z_i\\), sobre las preferencias por las normas democráticas, \\(\\nu_i\\), estamos tratando de recuperar el ATE, o \\(E[\\nu_i|Z_i = 1] - E[\\nu_i|Z_i = 0]\\). Sin embargo, en presencia de un error de medición sistemático, donde el error de medición está relacionado con la asignación del tratamiento en sí (por ejemplo, la variable de resultado se mide de manera diferente en el grupo de tratamiento que en el grupo de control), un estimador de diferencia de medias en el resultado observado, \\(Y_i\\), recupera una estimación sesgada del ATE. El efecto del tratamiento ahora incluye la diferencia de medición, así como la diferencia entre los grupos tratados y de control:\n\\[E[Y_i|Z_i = 1]−E[Y_i|Z_i = 0] =  E[\\nu_i + \\delta_i + \\kappa_i |Z_i = 1] − E[\\nu_i + \\delta_i + \\kappa_i|Z_i =0]\\] Debido a la medición de error no-sistemática, \\(\\delta_i\\) es independiente a la asignación de tratamiento, \\(E[\\delta_i|Z_i = 1] = E[\\delta_i |Z_i = 0]\\). Simplificando y reorganizando, podemos escribir:\n\\[E[Y_i|Z_i = 1]−E[Y_i|Z_i = 0] = \\underbrace{E[\\nu_i|Z_i = 1] − E[\\nu_i|Z_i =0]}_{ATE} +\n\\underbrace{E[\\kappa_i|Z_i = 1] - E[\\kappa_i|Z_i =0]}_{\\text{Sesgo}}\\]\nHay varias fuentes de errores de medición no-sistemáticos en los experimentos. Efectos de demanda y Efectos Hawthorne se pueden motivar como fuentes de errores de medición sistemáticos. Mas aún, los diseños que miden las variables de resultados de forma asimétrica en los tratamientos y grupos de control pueden ser propensos a errores de medición sistemáticos. En todos los casos, existe asimetría entre las condiciones de tratamiento en: (a) la forma en que los sujetos responden a ser observados; o (b) la forma en que observamos las variables de resultados, que es distinta de cualquier efecto del tratamiento sobre la variable latente de interés. La estimación sesgada del ATE se convierte en el neto de cualquier efecto sobre las variables latentes (el ATE) y el error de medición no-sistemático.\n\n\n8 Aproveche múltiples indicadores para evaluar la validez de una medida, pero tenga en cuenta las limitaciones de tales pruebas.\nMás allá de considerar la calidad del mapeo entre un concepto y una medida, a menudo podemos evaluar la calidad de la medida comparándola con medidas de operacionalizaciones alternativas del mismo concepto, conceptos estrechamente relacionados o conceptos distintos. En pruebas convergentes de la validez de una medida, evaluamos la correlación entre medidas alternativas de un concepto. Si están codificados en la misma dirección, esperamos que la correlación sea positiva y la validez de ambas mediciones aumente a medida que aumenta la magnitud de la correlación. Una limitación de las pruebas convergentes de validez es que si dos mediciones están débilmente correlacionadas, información adicional ausente, no sabemos si una medida es válida (y cuál) o si ambas medidas son inválidas.\nLa recopilación de múltiples indicadores también puede permitir a los investigadores evaluar la validez predictiva de una medición. ¿Hasta qué punto una medición de un concepto latente puede predecir un comportamiento que se cree ha sido moldeado por el concepto? Por ejemplo, ¿la ideología política (la variable latente) puede predecir la elección de voto para los partidos de izquierda? Esto proporciona medios adicionales para validar una medición. Aquí, cuanto mayor sea la capacidad de un indicador para predecir el comportamiento (u otras variables de resultados), mayor será la validez predictiva del indicador. Sin embargo, creemos que la mayoría de los comportamientos son el resultado de una compleja variedad de causas. Determinar si una medida es un predictor “suficientemente bueno” es una determinación algo arbitraria.\nPor último, es posible que deseemos determinar si estamos midiendo el concepto de interés de forma aislada en lugar de un grupo de conceptos. Las pruebas de validez discriminante analizan los indicadores de un concepto y un concepto relacionado pero distinto. En principio, buscamos correlaciones bajas (correlaciones cercanas a 0) entre ambos indicadores. Una limitación de las pruebas de validez discriminante es que no sabemos cómo covarían conceptos distintos subyacentes. Puede ser el caso de que tengamos indicadores válidos de ambos conceptos, pero que muestren una fuerte correlación (positiva o negativa) porque las unidades con niveles altos(bajos) de \\(A\\) tienden a tener niveles más altos (respectivamente bajos) de \\(B\\).\nEn resumen, la adición de más mediciones puede ayudar a validar un indicador, pero estas pruebas de validación están limitadas en lo que nos dicen cuando fallan. En este sentido, debemos ser conscientes de las limitaciones, además de la utilidad de recopilar medidas adicionales para simplemente validar un indicador.\n\n\n9 El uso de múltiples indicadores tiende a mejorar el poder de su experimento, pero puede introducir un reemplazo entre el sesgo y la eficiencia.\nRecopilar múltiples indicadores de un concepto o una variable de resultado también puede mejorar el poder de su experimento. Si múltiples indicadores miden el mismo concepto pero se miden con un error (no-sistemático), podemos mejorar la precisión con la que medimos la variable latente aprovechando múltiples mediciones.\nHay varias formas de agregar múltiples resultados en un índice. [“10 Cosas que saber sobre comparaciones múltiples”] (https://egap.org/resource/10-things-to-know-about-multiple-comparisons) describe índices construidos a partir de puntuación-\\(z\\) y ponderación de covarianza inversa de múltiples variables de resultados. También hay muchos otros modelos estructurales para estimar variables latentes a partir de múltiples mediciones.\nA continuación observamos un índice simple de puntuación-\\(z\\) de dos medidas ruidosas de una variable latente. Suponemos que las variables latentes y los indicadores “Medida 1” y “Medida 2” se extraen de una distribución normal multivariante y están correlacionados positivamente con la variable latente y entre sí. Para efectos de la simulación, asumimos que conocemos la variable latente, aunque en la práctica esto no es posible. Primero, podemos mostrar que en muchas simulaciones de los datos, la correlación entre el índice de puntuación-\\(z\\) de las dos medidas y la variable latente es, en promedio, más alta que la correlación entre cualquiera de los indicadores y la variable latente. Al graficar la correlación de las medidas individuales y la variable latente contra (\\(x\\) -ejes) la correlación del índice y la variable latente (\\(y\\) -eje), casi todos los puntos están por encima de la línea de 45 grados. Esto muestra que el índice se aproxima a la variable latente con mayor precisión.\n\nlibrary(mvtnorm)\nlibrary(randomizr)\nlibrary(dplyr)\nlibrary(estimatr)\n\nmake_Z_score &lt;- function(data, outcome){\n  ctrl &lt;- filter(data, Z == 0)\n  return(with(data, (data[,outcome] - mean(ctrl[,outcome]))/sd(ctrl[,outcome])))\n}\n\npull_estimates &lt;- function(model){\n  est &lt;- unlist(model)$coefficients.Z\n  se &lt;- unlist(model)$std.error.Z\n  return(c(est, se))\n}\n\ndo_sim &lt;- function(N, rhos, taus, var = c(1, 1, 1)){\n   measures &lt;- rmvnorm(n = N, \n                       sigma = matrix(c(var[1], rhos[1], rhos[2], \n                                        rhos[1], var[2], rhos[3], \n                                        rhos[2], rhos[3], var[3]), nrow = 3))\n   df &lt;- data.frame(Z = complete_ra(N = N),\n                      latent = measures[,1],\n                      Y0_1 = measures[,2],\n                      Y0_2 = measures[,3]) %&gt;%\n            mutate(Yobs_1 = Y0_1 + Z * taus[1],\n                   Yobs_2 = Y0_2 + Z * taus[2])\n   df$Ystd_1 = make_Z_score(data = df, outcome = \"Yobs_1\")\n   df$Ystd_2 = make_Z_score(data = df, outcome = \"Yobs_2\")\n   df$index = (df$Ystd_1 + df$Ystd_2)/2\n\n   cors &lt;- c(cor(df$index, df$latent), cor(df$Ystd_1, df$latent), cor(df$Ystd_2, df$latent))\n   ests &lt;- c(pull_estimates(lm_robust(Ystd_1 ~ Z, data = df)),\n             pull_estimates(lm_robust(Ystd_2 ~ Z, data = df)),\n             pull_estimates(lm_robust(index ~ Z, data = df)))\n   output &lt;- c(cors, ests)\n   names(output) &lt;- c(\"cor_index\", \"cor_Y1\", \"cor_Y2\", \"est_Y1\", \"se_Y1\",\n                      \"est_Y2\", \"se_Y2\", \"est_index\", \"se_index\")\n   return(output)\n}\n\nsims &lt;- replicate(n = 500, expr = do_sim(N = 200, \n                                         rhos = c(.6, .6, .6), \n                                         taus = c(.4, .4),\n                                         var = c(1, 3, 3)))\n\ndata.frame(measures = c(sims[\"cor_Y1\",], sims[\"cor_Y2\",]),\n           index = rep(sims[\"cor_index\",], 2),\n           variable = rep(c(\"Medida 1\", \"Medida 2\"), each = 500)) %&gt;%\n  ggplot(aes(x = measures, y = index)) + geom_point() + \n  facet_wrap(~variable) + \n  geom_abline(a = 0, b = 1, col = \"red\", lwd = 1.25) + \n  scale_x_continuous(\"Correlación entre la medición y la variable latente\", limits = c(0.1, .6)) +\n  scale_y_continuous(\"Correlación entre el índice y la variable latente\", limits = c(0.1, .6)) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nAhora, considere las implicaciones para el poder. En las simulaciones estimamos el ATE de un tratamiento en la Medida 1, la Medida 2 y el índice. El siguiente gráfico visualiza los estimados. Las líneas azules muestran intervalos de confianza del 95 por ciento. Los intervalos de confianza más pequeños sobre el índice visualizan las ganancias de precisión al aprovechar ambas mediciones. Vemos que esto se manifiesta en un mayor poder estadístico para el experimento.\n\ndata.frame(est = c(sims[\"est_index\",], sims[\"est_Y1\",], sims[\"est_Y2\",]),\n           se = c(sims[\"se_index\",], sims[\"se_Y1\",], sims[\"se_Y2\",]),\n           outcome = rep(c(\"Index\", \"Medida 1\", \"Medida 2\"), each = 500)) %&gt;%\n  mutate(T = est/se, \n         sig = 1 * (abs(T) &gt; 1.96)) %&gt;%\n  group_by(outcome) %&gt;%\n  mutate(power = sum(sig)/n(),\n          lab = paste0(\"Poder = \", round(power, 2))) %&gt;%\n  arrange(est) %&gt;%\n  mutate(order = 1:500/500) %&gt;%\n  ggplot(aes(x = order, y = est)) + \n  geom_errorbar(aes(ymin = est - 1.96 * se, ymax = est + 1.96 * se), width = 0,\n                col = \"light blue\", alpha = .25) +\n  geom_point() +\n  facet_wrap(~outcome) +\n  geom_text(x = 0.5, y = -.4, aes(label = lab), cex = 4) +\n  geom_hline(yintercept = 0, col = \"red\", lty = 3) + \n  theme_minimal() + xlab(\"Percentil del Estimado en 500 simulaciones\") + \n  ylab(\"ATE\")\n\n\n\n\n\n\n\n\nHemos examinado un índice compuesto por solo dos indicadores. En principio, se pueden lograr mayores ganancias de eficiencia al incorporar más indicadores en su índice. Sin embargo, a medida que aumentamos el número de indicadores, debemos considerar el grado en que la amalgama de indicadores se adhiere al concepto original. Al agregar mediciones para aprovechar las ganancias de eficiencia, podemos introducir sesgos en la medición del concepto latente. Los investigadores deben navegar este intercambio. El prerregistro de los componentes de un índice proporciona una forma de principios para navegar el tema que obliga a pensar el concepto a fondo en ausencia de datos. Esto también evita las preguntas ex-post sobre la elección de indicadores para un índice.\n\n\n10 Mientras los conceptos pueden ser globales, muchos indicadores son específicos a los contextos.\nMuchos estudios en las ciencias sociales se enfocan en conceptos que normalmente son asumidos como latentes; incluidas preferencias, conocimiento y actitudes. En la medida en que trabajamos sobre conceptos comunes, existe una tendencia a aprovechar de las operacionalizaciones existentes de estudios sobre conceptos relacionados en diferentes contextos. En estudios en múltiples contextos, como en la Iniciativa Metaketa de EGAP (http://egap.org/metaketa), los investigadores apuntan a estudiar la misma relación causal en variados contextos nacionales. Pero el deseo de estudiar conceptos comunes no implica que los mismos indicadores se deban utilizar en todos los contextos.\nPor ejemplo, considere un grupo de estudios que buscan medir la variación en el concepto de conocimiento político o sofisticación. El conocimiento sobre política puede evaluarse mediante preguntas que piden a los sujetos que recuerden un hecho sobre política. Una pregunta puede pedir a los sujetos que recuerden el nombre del ejecutivo actual (presidente / primer ministro, etc.), calificando las respuestas como “correctas” o “incorrectas”. En el país \\(A\\), el 50% de los encuestados responde correctamente la pregunta. En el País B$, el 100% de los encuestados responde correctamente la pregunta. En el país \\(B\\), no podemos identificar ninguna variación en el indicador porque todos podrían responder la pregunta. Esto no implica que no haya variación en el conocimiento político en el país \\(B\\), solo que este indicador es una mala medición de la variación que existe. En el país \\(A\\), sin embargo, esta pregunta puede ser un indicador completamente apropiado de conocimiento político. Si el conocimiento político fue la variable de resultado de un experimento, la falta de variación en la variable de resultado en el país \\(B\\) no nos permite identificar ninguna diferencia en el conocimiento político entre los grupos de tratamiento y de control.\nPor esta razón, si bien puede ser útil desarrollar indicadores basados ​​en trabajos existentes o instrumentos de otros contextos, esta no es necesariamente la mejor manera de desarrollar mediciones en un nuevo contexto. Las pruebas previas pueden proporcionar información adicional sobre si los indicadores son apropiados en un entorno determinado. En resumen, el mapeo entre conceptos e indicadores es, en muchos casos, específico del lugar. Los investigadores deben considerar estas limitaciones al operacionalizar conceptos comunes en distintos entornos.\n\n\n11 Referencias\nAdcock, Robert y David Collier. “Measurement Validity: A Shared Standard for Qualitative and Quantitative Research.” American Political Science Review. 95 (3): 529-546.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "guides/data-strategies/randomization.es.html",
    "href": "guides/data-strategies/randomization.es.html",
    "title": "10 cosas que debe saber sobre la aleatorización",
    "section": "",
    "text": "Hay muchas formas de aleatorizar. La más simple es lanzar una moneda para determinar si un sujeto recibe un tratamiento o no. Esto asegura que cada sujeto tenga una probabilidad de 0.5 de recibir el tratamiento y una probabilidad de 0.5 de no recibirlo. Hecho de esta manera, si un sujeto recibe el tratamiento no afecta para nada el que el siguiente sujeto reciba o no el tratamiento. Cada sujeto tiene la misma probabilidad de recibir el tratamiento y no estaría correlacionado con factores que generen una asociación engañosa, al menos en valor esperado.\nEste no es un procedimiento erróneo, pero tiene sus deficiencias. Primero, cuando usamos este método no podemos saber de antemano cuántas unidades estarán en tratamiento y cuántas en control. Si para usted es importante contar con esta información, deberá emplear alguna forma de muestreo en la que cada elemento selccionado no sea estadísticamente independientes de los otros (como sacar papeletas con nombres de un sombrero). En segundo lugar, puede que quiera controlar la proporción exacta de unidades asignadas al tratamiento y control, lo que es difícil de conseguir lanzando una moneda. En tercer lugar, es posible que quiera replicar su aleatorización para demostrar que no hubo nada raro con ella. Eso no se puede lograr tan fácil lanzando monedas o sacando papeletas de sombreros. Finalmente, como mostramos a continuación, hay todo tipo de formas de hacer la aleatorización para mejorar el poder estadístico y asegurar el balance de varias formas, que son muy difíciles de lograr usando monedas y sombreros.\nLa aleatorización replicable y flexible es muy fácil de hacer con software disponible gratuitamente. El siguiente código de R se puede utilizar, por ejemplo, para generar una asignación aleatoria, especificando el número de unidades a tratar. Aquí, N = 100 es el número de unidades que tiene y m = 34 es el número que desea tratar. La “semilla” permite replicar la misma simulación cada vez que ejecuta el código (o puede cambiar la semilla por iteración diferente de la simulación). 1 2\n\nlibrary(randomizr)\nset.seed(343)\ncomplete_ra(N = 100, m = 34)"
  },
  {
    "objectID": "guides/data-strategies/randomization.es.html#footnotes",
    "href": "guides/data-strategies/randomization.es.html#footnotes",
    "title": "10 cosas que debe saber sobre la aleatorización",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLos generadores de números aleatorios son en realidad pseudoaleatorios porque generan un vector de números aleatorios basado en un pequeño conjunto de valores iniciales, conocido como estado de semilla. Los generadores de números aleatorios funcionan de esta manera para mejorar la velocidad computacional. Sin embargo, la serie de números aleatorios generada es tan aleatoria como es necesario para los propósitos de la asignación aleatoria porque no tiene ninguna relación con los resultados potenciales de sus sujetos.↩︎\nTodos los fragmentos de código fueron actualizados por Alex Coppock el 25 de noviembre de 2020.↩︎\nPara obtener más información, consulte Moore, Ryan T. y Sally A. Moore. “Blocking for sequential political experiments.” Political Analysis 21.4 (2013): 507-523.↩︎\nPara obtener un tutorial más detallado sobre los procedimientos de aleatorización disponibles en el paquete R randomizr, consulte: https://declaredesign.org/r/randomizr/articles/randomizr_vignette.html↩︎\nPero si estamos seguros de los efectos del préstamo, tampoco estaría claro por qué estaríamos realizando un experimento para probarlo. En la investigación médica, a menudo se detienen los ensayos clínicos aleatorizados si se hace evidente desde el principio que un medicamento puede sin duda curar enfermedades potencialmente mortales y, por lo tanto, es peligroso negarlo a los sujetos de control. (De manera similar, un ensayo también se detendría si fuera claro desde el principio que un medicamento indudablemente está causando efectos negativos y dañinos).↩︎"
  },
  {
    "objectID": "guides/data-strategies/measurement_en.html",
    "href": "guides/data-strategies/measurement_en.html",
    "title": "10 Things to Know About Measurement in Experiments",
    "section": "",
    "text": "1 The validity of inferences we draw from an experiment depend on the validity of the measures used.\nWe typically experiment in order to estimate the causal effect of a treatment, \\(Z\\), on an outcome, \\(Y\\). Yet, the reason that we care about estimating this causal effect is, in principle, to understand characteristics of the relationship between two theoretical, unobserved, concepts measured by observed variables \\(Z\\) and \\(Y\\).\nFollowing Adcock and Collier (2001), consider the measurement process graphed in Figure 1 in three steps. First, researchers begin with systematized concept, a clearly-defined theoretical construct. From this concept, the researcher develops an indicator mapping the concept onto a scale or a set of categories. Finally, units or cases are scored on the indicator, yielding a measurement of treatment, \\(Z\\) and an outcome \\(Y\\). A measurement is valid if variation in the indicator closely approximates variation in the underlying concept of interest.\nAn experimental research design should allow a researcher to estimate the causal effect of \\(Z\\) on \\(Y\\) under standard assumptions. But if the ultimate goal is to make an inference about causal effect of the concept that \\(Z\\) measures on the concept that \\(Y\\) measures, the inferences that we can hope to make on the basis of our experimental evidence are valid if and only if both measures are valid.\n\n\n\n\n\nThe process of measurment in the context of an experimental research design.\n\n\n\n\n\n\n2 Measurement is the link between a researcher’s substantive and/or theoretical argument and an (experimental) research design.\nWhen we consider the design of an experiment, we tend to focus on the process by which the randomly assigned treatment, \\(Z\\) is assigned and the joint distribution of \\(Z\\) and an outcome, \\(Y\\). In other words, we tend to divorce scores of \\(Z\\) and \\(Y\\) from broader concepts when considering the statistical properties of a research design. In this telling, two completely separate experiments with the same distribution of \\(Z\\) and \\(Y\\) could have identical properties.\nFor example, a clinical trial on the efficacy of aspirin on headaches and an experiment that provides information on an incumbent politician’s level of corruption and then asks the respondent if she will vote for the incumbent could have identical sized and distributed samples, assignments, estimands, and realizations of outcomes (data). Yet, this characterization of two completely distinct research projects that seek to make completely distinct inferences as “equivalent” may strike us as quite strange or even unsettling.\nHowever, when we consider measurement as a fundamental component of research design, clearly these experiments are distinct. We observe measures of different concepts in the data for the two experiments. By considering the indicators and the broader concepts underlying the treatments and outcomes, we are forced to examine the researchers’ respective theories or arguments. In so doing, we can raise questions about the validity of the measures and the relationship between the validity of the measures and the validity of final, substantive, inferences.\n\n\n3 Measuring treatments includes the operationalization of treatment as well as compliance with treatment assignment.\nIn an experiment, treatments are typically designed, or at a minimum, described, by the researcher. Consumers of experimental research should be interested the characteristics of the treatment and how it manipulates a concept of interest. Most treatments in social science are compound, or include a bundle of attributes. We may be interested in the effect of providing voters with information on their elected officials’ performance. Yet, providing information also includes the mode of delivery and who was delivering the information. To understand the degree to which the treatment manipulates a concept, we must also understand what else the treatment could be manipulating.\nHowever, despite all the effort operationalizing a treatment, in experimental research, the link from the operationalization to the treatment indicator is fundamentally distinct from measurement of covariates or outcomes for two reasons. First, by assigning treatment, experimenters aim to control the values a given unit takes on. Second, for the treatment indicator, the score comes from assignment to treatment, which is a product of the randomization. A subject may or may not have received the treatment, but her score on the treatment indicator is simply the treatment that she was assigned to, not the treatment she received.\nWhen subjects receive treatments other than those to which they are assigned, we typically seek to measure compliance — whether the treatments were delivered and to what extent. To do so, we define what constitutes compliance with treatment assignment. In determining what constitutes compliance, researchers should consider the core aspect of how the treatment manipulates the concept of interest. At what point in the administration of the treatment does this manipulation occur? Once compliance is operationalized,we seek to code the compliance indicator in a manner faithful to this definition.\nFor example, consider a door-to-door canvassing campaign that distributes information about the performance of an incumbent politician. Households are assigned to receive a visit from a canvasser who shares the information (treatment) or no visit (control). The treatment indicator is simply whether a household was assigned to the treatment or not. However, if residents of a household are not home when the canvasser visits, they do not receive the information. Our definition of compliance should determine what constitutes “treated” on our (endogenous) measure of whether a household received the treatment, here the information. Some common definitions of compliance may be (a) that someone from the household answered the door; or (b) that someone from the household listened to the full information script.\n\n\n4 Most outcomes of interest in social science are latent.\nIn contrast to measuring treatment indicators and compliance, measuring outcomes in experimental research follows much more closely the process outlined in the figure above. We theorize how the treatment may influence an outcome concept. We then operationalize the concept and record scores or values to complete our measurement of the outcome.\nOne particular challenge in the measurement of outcomes is that many of the most common outcomes of interest in social science are latent. This means that we are unable to observe the true value of the outcome concept directly. In fact, the nature of the true value may itself be under debate (for example, the debate about the measurement of “democracy” is a classic case where the definition of the concept itself is contested). Outcomes including knowledge, preferences, and attitudes are latent. We thus record or score observable indicators assumed to be related to the latent outcome in an effort to infer characteristics of the latent variable. Even behavioral outcomes are often used as manifestations of larger latent concepts (i.e. assessed voting behavior is used to make inferences about “electoral accountability” in a place).\nBecause these variables are latent, it is challenging to devise appropriate indicators. Poor operationalization has rather drastic consequences for the validity of our inferences about the concept of interest for two reasons. As in section #1 above, if these indicators do not conceptually measure the concept of interest, then inferences we make about the relationship between \\(Z\\) and \\(Y\\) (even with a “perfect” design in terms of statistical power and missing data,etc.) may not teach us about the “ultimate inference” we are seeking to make. Furthermore, measurement error may undermine our ability to estimate the effect of \\(Z\\) and \\(Y\\) leading to incorrect inferences. The remainder of this guide focuses on the latter problem.\n\n\n5 There are two types of measurement error that we should consider.\nWe can formalize measurement challenges quite simply. Suppose a treatment, \\(Z_i\\) is hypothesized to change preferences for democratic norms, \\(\\nu_i\\). In principle, the quantity that we would like to estimate is \\(E[\\nu_i|Z_i = 1] - E[\\nu_i|Z_i =0]\\), the ATE of our treatment on preferences for democratic norms. However, \\(\\nu_i\\) is a latent variable: we cannot measure it directly. Instead we ask about support for various behaviors thought to correspond to these norms. This indicator, \\(Y_i\\), can be decomposed into the latent variable, \\(\\nu_i\\) and two forms of measurement error:\n\nNon-systematic measurement error, \\(\\delta_i\\): This error is independent of treatment assignment, \\(\\delta_i \\perp Z_i\\).\nSystematic measurement error, \\(\\kappa_i\\): This error is not independent of treatment assigniment, \\(\\kappa_i \\not\\perp Z_i\\).\n\n\\[Y_i = \\underbrace{\\nu_i}_{\\text{Latent outcome}} + \\underbrace{\\delta_i}_{\\substack{\\text{Non-systematic} \\\\ \\text{measurement error}}} + \\underbrace{\\kappa_i}_{\\substack{\\text{Systematic} \\\\ \\text{measurement error}}}\\]\n\n\n6 Measurement error reduces the power of your experiment.\nNon-systematic measurement error, represented by \\(\\delta_i\\) above, refers to the noise with which we are measuring the latent variable. In the absence of systematic measurement error, we measure:\n\\[Y_i = \\underbrace{\\nu_i}_{\\text{Latent outcome}} + \\underbrace{\\delta_i}_{\\substack{\\text{Non-systematic} \\\\ \\text{measurement error}}}\\]\nNow, consider the analytical power formula for a two-armed experiment. We can express \\(\\sigma\\), or the standard deviation of the outcome as \\(\\sqrt{Var(Y_i)}\\). Note that in the formula below, this term appears in the denominator of the first term. As \\(\\sqrt{Var(Y_i)}\\) increases, statistical power decreases.\n\\[\\beta = \\Phi \\left(\\frac{|\\mu_t− \\mu_c| \\sqrt{N}}{2 \\color{red}{\\sqrt{Var(Y_i)}}} − \\Phi^{−1}\\left(1 − \\frac{\\alpha}{2}\\right)\\right)\\]\nIn what way does non-systematic measurement error \\(\\delta_i\\) impact power? We can decompose \\(\\sqrt{Var(Y_i)}\\) as follows:\n\\[\\sqrt{Var(Y_i)} = \\sqrt{Var(\\nu_i) + Var(\\delta_i) + 2 Cov(\\nu_i, \\delta_i)}\\]\nSo long as \\(Cov(\\nu_i, \\delta_i)\\geq 0\\) (we often assume \\(Cov(\\nu_i, \\delta_i)= 0\\)), it must be the case that the \\(Var(Y_i)\\) is increasing as measurement error, or \\(Var(\\delta_i)\\) increases. This implies that power is decreasing as non-systematic measurement error increases. In other words, the noisier our measures of a latent variable, the lower our ability to detect effects of a treatment on a latent variable.\nWhat about the case in which \\(Cov(\\nu_i, \\delta_i) &lt; 0\\)? While this reduces \\(Var(Y_i)\\) (holding \\(Var(\\nu_i)\\) and \\(Var(\\delta_i)\\) constant), it also attenuates the variation that we measure in \\(Y_i\\). In principle, this should attenuate the numerator \\(|\\mu_t-\\mu_c|\\), which, if sufficient relative to the reduction in variance, will also reduce power.\n\n\n7 Systematic measurement error biases estimates of causal effects of interests.\nIf we are estimating the Average Treatment Effect (ATE) of our treatment \\(Z_i\\), on preferences for democratic norms, \\(\\nu_i\\), we are trying to recover the ATE, or \\(E[\\nu_i|Z_i = 1] - E[\\nu_i|Z_i =0]\\). However, in the presence of systematic measurement error, where measurement error is related to the treatment assignment itself (say, the outcome is measured differently in the treatment group than in the control group) a difference-in-means estimator on the observed outcome, \\(Y_i\\), recovers a biased estimate of the ATE. The effect of the treatment now includes the measurement difference as well as the difference between treated and control groups:\n\\[E[Y_i|Z_i = 1]−E[Y_i|Z_i = 0] =  E[\\nu_i + \\delta_i + \\kappa_i |Z_i = 1] − E[\\nu_i + \\delta_i + \\kappa_i|Z_i =0]\\] Because non-systematic measurement error, \\(\\delta_i\\) is independent of treatment assignment, \\(E[\\delta_i|Z_i = 1] = E[\\delta_i |Z_i = 0]\\). Simplifying and rearranging, we can write:\n\\[E[Y_i|Z_i = 1]−E[Y_i|Z_i = 0] = \\underbrace{E[\\nu_i|Z_i = 1] − E[\\nu_i|Z_i =0]}_{ATE} +\n\\underbrace{E[\\kappa_i|Z_i = 1] - E[\\kappa_i|Z_i =0]}_{\\text{Bias}}\\]\nThere are various sources of non-systematic measurement error in experiments. Demand effects and Hawthorne effects can be motivated as sources of systematic measurement error. Moreover, designs that measure outcomes asymmetrically in treatment and control groups may be prone to systematic measurement error. In all cases, there exists asymmetry across treatment conditions in: (a) the way that subjects respond to being observed; or (b) the way that we observe outcomes that is distinct from any effect of the treatment on the latent variable of interest. The biased estimate of the ATE becomes the net of any effects on the latent variables (the ATE) and the non-systematic measurement error.\nWhen designing an experiment, researchers can take various steps to limit systematic measurement error. First and foremost, they can attempt to use identical measurement strategies across all experimental groups. To limit demand and Hawthorne effects, researchers often aim to design treatments and measurement strategies that are as naturalistic and unobtrusive as possible. For example, sometimes it can be beneficial to avoid baseline surveys at the outset of an experiment that would reveal the purpose of a study to participants. Researchers may also want to separate treatment from outcome measurement phases to limit the apparent connection between the two. Ensuring that study staff are blind towards the treatment status of study participants can also help maintain measurement symmetry across treatment conditions. The use of placebo treatments sometimes helps to hide their treatment status even from study participants themselves. Finally, researchers sometimes supplement outcome measures such as survey questions that are particularly susceptible to demand effects with behavioral measures for which experimenter demand may be less of a concern. See Quidt, Haushofer, and Roth (2018) for a way to assess the robustness of your experimental results to demand effects.\n\n\n8 Leverage multiple indicators to assess the validity of a measure but be aware of the limitations of such tests.\nBeyond consideration of the quality of the mapping between a concept and a measure, we can often assess the quality of the measure by comparing it to measures from alternate operationalizations of the same concept, closely related concepts, or distinct concepts. In convergent tests of the validity of a measure, we assess the correlation between alternate measures of a concept. If they are coded in the same direction, we expect the correlation to be positive and validity of both measures increases as the magnitude of the correlation increases. One limitation of convergent tests of validity is if two measures are weakly correlated, absent additional information, we do not know whether one measure is valid (and which) or whether both measures are invalid.\nGathering multiple indicators may also allow for researchers to assess the predictive validity of a measure. To what extent does a measure of a latent concept predict behavior believed to be shaped by the concept? For example, does political ideology (the latent variable) predict reported vote choice for left parties? This provides an additional means of validating a measure. Here, the higher the ability of an indicator to predict behavior (or other outcomes), the stronger the predictive validity of the indicator. Yet, we believe that most behaviors are a result of a complex array of causes. Determining whether a measure is a “good enough” predictor is a somewhat arbitrary determination.\nFinally, we may want to determine whether we are measuring the concept of interest in isolation rather than a bundle of concepts. Tests of discriminant validity look at indicators of a concept and a related but distinct concept. In principle, we look for low correlations (correlations close to 0) between both indicators. One limitation of tests of discriminant validity is that we don’t know how underlying distinct concepts covary. It may be the case that we have valid indicators of both concepts, but they exhibit strong correlation (positive or negative) because units with high levels of \\(A\\) tend to have higher (resp. low) levels of \\(B\\).\nIn sum, the addition of more measures can help validate an indicator, but these validation tests are limited in what they tell us when they fail. To this extent, we should remain cognizant of the limitations in addition to the utility of collecting additional measures to simply validate an indicator.\n\n\n9 The use of multiple indicators often improves the power of your experiment, but may introduce a bias-efficiency tradeoff.\nGathering multiple indicators of a concept or outcome may also improve the power of your experiment. If multiple indicators measure the same concept but are measured with (non-systematic) error, we can improve the precision with which we measure the latent variable by leveraging multiple measures.\nThere are multiple ways to aggregate multiple outcomes into an index. 10 Things to Know about Multiple Comparisons describes indices built from \\(z\\)-score and inverse covariance weighting of multiple outcomes. There are also many other structural models for estimating latent variables from multiple measures.\nBelow, we look at simple \\(z\\)-score index of two noisy measures of a latent variable. We assume that the latent variables and both indicators “Measure 1” and “Measure 2” are drawn from a multivariate normal distribution and are positively correlated with the latent variable and with each other. For the purposes of simulation, we assume that we know the latent variable, though in practice this is not possible. First, we can show that across many simulations of the data, the correlation between the \\(z\\)-score index of the two measures and the latent variable is, on average, higher than the correlation between either of the indicators and the latent variable. When graphing the correlation of the individual measures and the latent variable against (\\(x\\)-axes) the correlation of the index and the latent variable (\\(y\\)-axis), almost all points are above the 45-degree line. This shows that the index approximates the latent variable with greater precision.\n\nlibrary(mvtnorm)\nlibrary(randomizr)\nlibrary(dplyr)\nlibrary(estimatr)\nmake_Z_score &lt;- function(data, outcome){\n  ctrl &lt;- filter(data, Z == 0)\n  return(with(data, (data[,outcome] - mean(ctrl[,outcome]))/sd(ctrl[,outcome])))\n}\npull_estimates &lt;- function(model){\n  est &lt;- unlist(model)$coefficients.Z\n  se &lt;- unlist(model)$std.error.Z\n  return(c(est, se))\n}\ndo_sim &lt;- function(N, rhos, taus, var = c(1, 1, 1)){\n   measures &lt;- rmvnorm(n = N, \n                       sigma = matrix(c(var[1], rhos[1], rhos[2], \n                                        rhos[1], var[2], rhos[3], \n                                        rhos[2], rhos[3], var[3]), nrow = 3))\n   df &lt;- data.frame(Z = complete_ra(N = N),\n                      latent = measures[,1],\n                      Y0_1 = measures[,2],\n                      Y0_2 = measures[,3]) %&gt;%\n            mutate(Yobs_1 = Y0_1 + Z * taus[1],\n                   Yobs_2 = Y0_2 + Z * taus[2])\n   df$Ystd_1 = make_Z_score(data = df, outcome = \"Yobs_1\")\n   df$Ystd_2 = make_Z_score(data = df, outcome = \"Yobs_2\")\n   df$index = (df$Ystd_1 + df$Ystd_2)/2\n   cors &lt;- c(cor(df$index, df$latent), cor(df$Ystd_1, df$latent), cor(df$Ystd_2, df$latent))\n   ests &lt;- c(pull_estimates(lm_robust(Ystd_1 ~ Z, data = df)),\n             pull_estimates(lm_robust(Ystd_2 ~ Z, data = df)),\n             pull_estimates(lm_robust(index ~ Z, data = df)))\n   output &lt;- c(cors, ests)\n   names(output) &lt;- c(\"cor_index\", \"cor_Y1\", \"cor_Y2\", \"est_Y1\", \"se_Y1\",\n                      \"est_Y2\", \"se_Y2\", \"est_index\", \"se_index\")\n   return(output)\n}\nsims &lt;- replicate(n = 500, expr = do_sim(N = 200, \n                                         rhos = c(.6, .6, .6), \n                                         taus = c(.4, .4),\n                                         var = c(1, 3, 3)))\ndata.frame(measures = c(sims[\"cor_Y1\",], sims[\"cor_Y2\",]),\n           index = rep(sims[\"cor_index\",], 2),\n           variable = rep(c(\"Measure 1\", \"Measure 2\"), each = 500)) %&gt;%\n  ggplot(aes(x = measures, y = index)) + geom_point() + \n  facet_wrap(~variable) + \n  geom_abline(a = 0, b = 1, col = \"red\", lwd = 1.25) + \n  scale_x_continuous(\"Correlation between measure and latent variable\", limits = c(0.1, .6)) +\n  scale_y_continuous(\"Correlation between index and latent variable\", limits = c(0.1, .6)) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nNow, consider the implications for power. In the simulations, we estimate of the ATE of a treatment on Measure 1, Measure 2, and the index. The following graph visualizes the estimates. The blue lines show 95 percent confidence intervals. The smaller confidence intervals about the index visualize the precision gains from leveraging both measures. We see that this manifests in higher statistical power for the experiment.\n\ndata.frame(est = c(sims[\"est_index\",], sims[\"est_Y1\",], sims[\"est_Y2\",]),\n           se = c(sims[\"se_index\",], sims[\"se_Y1\",], sims[\"se_Y2\",]),\n           outcome = rep(c(\"Index\", \"Measure 1\", \"Measure 2\"), each = 500)) %&gt;%\n  mutate(T = est/se, \n         sig = 1 * (abs(T) &gt; 1.96)) %&gt;%\n  group_by(outcome) %&gt;%\n  mutate(power = sum(sig)/n(),\n          lab = paste0(\"Power = \", round(power, 2))) %&gt;%\n  arrange(est) %&gt;%\n  mutate(order = 1:500/500) %&gt;%\n  ggplot(aes(x = order, y = est)) + \n  geom_errorbar(aes(ymin = est - 1.96 * se, ymax = est + 1.96 * se), width = 0,\n                col = \"light blue\", alpha = .25) +\n  geom_point() +\n  facet_wrap(~outcome) +\n  geom_text(x = 0.5, y = -.4, aes(label = lab), cex = 4) +\n  geom_hline(yintercept = 0, col = \"red\", lty = 3) + \n  theme_minimal() + xlab(\"Percentile of Estimate in 500 simulations\") + \n  ylab(\"ATE\")\n\n\n\n\n\n\n\n\nWe have examined an index composed of just two indicators. In principle, there are further efficiency gains to be made by incorporating more indicators into your index. Yet, as we increase the number of indicators, we should consider the degree to which the amalgamation of indicators adheres to the original concept. By adding measures to leverage efficiency gains, we may introduce bias into the measure of the latent concept. Researchers must navigate this tradeoff. Pre-registration of the components of an index provides one principled way to navigate the issue which forces thorough consideration of the concept in the absence of data. This also avoids the ex-post questions about the choice of indicators for an index.\n\n\n10 While concepts may be global, many indicators are context-specific.\nMany studies in the social sciences focus on concepts that are typically assumed to be latent including preferences, knowledge, and attitudes. To the extent that we work on common concepts, there is a tendency to draw from existing operationalizations from studies on related concepts in different contexts. In studies in multiple contexts, as in EGAP’s Metaketa Initiative, researchers aim to study the same causal relationship in multiple national contexts. But a desire to study common concepts need not imply that the same indicators should be used across contexts.\nFor example, consider a set of studies that seek to measure variation in the concept of political knowledge or sophistication. Political knowledge may be assessed through questions that ask subjects to recall a fact about politics. One question may ask subjects to recall the current executive’s (president/prime minister etc.) name, scoring answers as “correct” or “incorrect.” In country \\(A\\), 50% of respondents answer the question correctly. In Country \\(B\\), 100% of respondents answer the question correctly. In Country \\(B\\), we are unable to identify any variation in the indicator because everyone could answer the question. This does not imply that there is no variation in political knowledge in Country \\(B\\), just that this indicator is a poor measure of the variation that exists. In Country \\(A\\), however, this question may be a completely appropriate indicator of political knowledge. If political knowledge was the outcome of an experiment, the lack of variation in the outcome in Country \\(B\\) fails to allow us to identify any difference in political knowledge between treatment and control groups.\nFor this reason, while it may be useful to develop indicators based on existing work or instruments from other contexts, this is not necessarily the best way to develop measures in a new context. Pre-testing can provide insights into whether indicators are appropriate in a given setting. In sum, the mapping between concepts and indicators is site-specific in many cases. Researchers should consider these limitations when operationalizing common concepts in distinct settings.\n\n\n\n\n\n\n\n\n Back to top11 References\n\nAdcock, Robert, and David Collier. 2001. “Measurement Validity: A Shared Standard for Qualitative and Quantitative Research.” American Political Science Review 95 (3): 529–46.\n\n\nQuidt, Jonathan de, Johannes Haushofer, and Christopher Roth. 2018. “Measuring and Bounding Experimenter Demand.” American Economic Review 108 (11): 3266–3302."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html",
    "href": "guides/data-strategies/multisite-experiments_en.html",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "",
    "text": "A multisite or block-randomized trial is a randomized experiment “in which sample members are randomly assigned to a program or a control group within each of a number of sites” (S. W. Raudenbush and Bloom 2015).\nThis guide focuses on multisite educational trials for illustration, although multisite trials are not unique to education. Multisite trials are a subset of multilevel randomized controlled trials (RCTs), in which units are nested within hierarchical structures, such as students nested within schools nested within districts. This guide uses as an illustrative example the case where each site is a school, although they could also be districts or classrooms; thus the term “site” and “school” are used interchangeably.\nAn advantage of multisite trials is that they allow a researcher to study average impact across units or sites, while also getting a sense of heterogeneity across sites (S. W. Raudenbush and Bloom 2015). However, the opportunities provided by multisite trials also come with their own challenges. Much of the rest of this guide will discuss the choices that researchers must make when analyzing multisite trials, and the consequences of these choices.\n\n\nBefore diving in, let’s introduce the definitions of estimand, estimator, and estimate. These concepts are sometimes conflated, but disentangling them increases clarity and understanding. The main distinction is that the estimand is the goal, while the estimator is the analysis we do in order to reach that goal.\nAn estimand is an unobserved quantity of interest about which the researcher wishes to learn. In this guide, the only type of estimand considered is the overall average treatment effect (ATE). Other options include focusing on treatment effect for only a subgroup, or calculating a different summary, such as an odds ratio. After choosing an estimand, the researcher chooses an estimator, which is a method used to calculate the final estimate which should tell the researcher something about the estimand. Finally, the researcher must also choose a standard error estimator if she wants to summarize how the estimates might vary if the research design or underlying data generating process were repeated.\nFirst, to provide context, let’s consider an example. The researcher decides their estimand will be the average treatment effect for the pool of subjects in the experiment. In this example, the researchers observe all of the subjects for whom they want to estimate an effect. As with any causal analysis, the researchers do not observe the control outcomes of the subjects assigned to the active treatment, or the treated outcomes of the subjects assigned to the control treatment. Thus, causal inference is sometimes referenced as a missing data problem, because it is impossible to observe both potential outcomes (the potential outcome given active treatment and the potential outcome given control treatment). See 10 Things to Know About Causal Inference and 10 Types of Treatment Effect You Should Know About for a discussion of other common estimands.\nGiven an estimand, the researchers choose their estimator to be the coefficient from an OLS regression of the observed outcome on site-specific fixed effects and the treatment indicator. To calculate standard errors, they use Huber-White robust standard errors. All these choices result in a point estimate (e.g. the program increased reading scores by \\(5\\) points) and a measure of uncertainty (e.g. a standard error of \\(2\\) points).\nWe’ll also need some notation. This guide follows the Neyman-Rubin potential outcomes notation (Splawa-Neyman, Dabrowska, and Speed (1923/1990), Imbens and Rubin (2015)). The observed outcomes are \\(Y_{ij}\\) for unit \\(i\\) in site \\(j\\). The potential outcomes are \\(Y_{ij}(1)\\), the outcome given active treatment, and \\(Y_{ij}(0)\\), the outcome given control treatment. The quantity \\(B_{ij}\\) is the unit-level intention-to-treat effect (ITT) \\(B_{ij} =  Y_{ij}(1) - Y_{ij}(0)\\). If there is no noncompliance, the ITT is the ATE, as defined above. Then \\(B_j\\) is the average impact at site \\(j\\), \\(B_j = 1/N_j \\sum_{i = 1}^{N_j} B_{ij}\\) where \\(N_j\\) is the number of units at site \\(j\\). Finally, \\(N = \\sum_{j = 1}^{J} N_j\\).\nThis guide is structured around the choices an analyst must make concerning estimand and estimators, and the resulting consequences. The choice of estimand impacts the substantive conclusion that a researcher makes. The choice of estimator and standard error estimator results in different statistical properties, including a potential trade off between bias and variance. This guide summarizes material using the framework provided by Miratrix, Weiss, and Henderson (2021)."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#preliminaries-estimands-estimators-and-estimates",
    "href": "guides/data-strategies/multisite-experiments_en.html#preliminaries-estimands-estimators-and-estimates",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "",
    "text": "Before diving in, let’s introduce the definitions of estimand, estimator, and estimate. These concepts are sometimes conflated, but disentangling them increases clarity and understanding. The main distinction is that the estimand is the goal, while the estimator is the analysis we do in order to reach that goal.\nAn estimand is an unobserved quantity of interest about which the researcher wishes to learn. In this guide, the only type of estimand considered is the overall average treatment effect (ATE). Other options include focusing on treatment effect for only a subgroup, or calculating a different summary, such as an odds ratio. After choosing an estimand, the researcher chooses an estimator, which is a method used to calculate the final estimate which should tell the researcher something about the estimand. Finally, the researcher must also choose a standard error estimator if she wants to summarize how the estimates might vary if the research design or underlying data generating process were repeated.\nFirst, to provide context, let’s consider an example. The researcher decides their estimand will be the average treatment effect for the pool of subjects in the experiment. In this example, the researchers observe all of the subjects for whom they want to estimate an effect. As with any causal analysis, the researchers do not observe the control outcomes of the subjects assigned to the active treatment, or the treated outcomes of the subjects assigned to the control treatment. Thus, causal inference is sometimes referenced as a missing data problem, because it is impossible to observe both potential outcomes (the potential outcome given active treatment and the potential outcome given control treatment). See 10 Things to Know About Causal Inference and 10 Types of Treatment Effect You Should Know About for a discussion of other common estimands.\nGiven an estimand, the researchers choose their estimator to be the coefficient from an OLS regression of the observed outcome on site-specific fixed effects and the treatment indicator. To calculate standard errors, they use Huber-White robust standard errors. All these choices result in a point estimate (e.g. the program increased reading scores by \\(5\\) points) and a measure of uncertainty (e.g. a standard error of \\(2\\) points).\nWe’ll also need some notation. This guide follows the Neyman-Rubin potential outcomes notation (Splawa-Neyman, Dabrowska, and Speed (1923/1990), Imbens and Rubin (2015)). The observed outcomes are \\(Y_{ij}\\) for unit \\(i\\) in site \\(j\\). The potential outcomes are \\(Y_{ij}(1)\\), the outcome given active treatment, and \\(Y_{ij}(0)\\), the outcome given control treatment. The quantity \\(B_{ij}\\) is the unit-level intention-to-treat effect (ITT) \\(B_{ij} =  Y_{ij}(1) - Y_{ij}(0)\\). If there is no noncompliance, the ITT is the ATE, as defined above. Then \\(B_j\\) is the average impact at site \\(j\\), \\(B_j = 1/N_j \\sum_{i = 1}^{N_j} B_{ij}\\) where \\(N_j\\) is the number of units at site \\(j\\). Finally, \\(N = \\sum_{j = 1}^{J} N_j\\).\nThis guide is structured around the choices an analyst must make concerning estimand and estimators, and the resulting consequences. The choice of estimand impacts the substantive conclusion that a researcher makes. The choice of estimator and standard error estimator results in different statistical properties, including a potential trade off between bias and variance. This guide summarizes material using the framework provided by Miratrix, Weiss, and Henderson (2021)."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#a-multisite-trial-is-fundamentally-a-blocked-or-stratified-rct.",
    "href": "guides/data-strategies/multisite-experiments_en.html#a-multisite-trial-is-fundamentally-a-blocked-or-stratified-rct.",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "2.1 A multisite trial is fundamentally a blocked or stratified RCT.",
    "text": "2.1 A multisite trial is fundamentally a blocked or stratified RCT.\nA multisite trial is a blocked RCT with 2 levels: randomization occurs at the student level (level 1) within blocks defined by sites/schools (level 2). For example, in a study of a new online math tool for high school students, randomization occurs at the student level within blocks defined by sites/schools. Perhaps half of students at each school are assigned to the status quo / control treatment (no additional math practice), and half are assigned to the active treatment (an offer of additional math practice at home using an online tool).\nBecause of the direct correspondence between multisite trials and blocked experiments, statistical properties of blocked experiments also translate directly to multisite experiments. The main difference between a traditional blocked RCT and a multisite experiment is that in many blocked RCTs, the researcher is able to choose the blocks. For example, in a clinical trial, a researcher may decide to block based on gender or specific age categories. Blocking can help increase statistical power overall or ensure statistical power to assess effects within subgroups (such as those defined by time of entering the study, or defined by other important covariates that might predict the outcome) (Moore (2012), Moore and Moore (2013), Bowers (2011)). Pashley and Miratrix (2021) makes the distinction between fixed blocks, where the number and covariate distribution of blocks is chosen by the researcher, and structural blocks, where natural groupings determine the number of blocks and their covariate distributions. Multisite experiments have structural blocks, such as districts, schools, or classrooms. The type of block can impact variance estimation, as shown in Pashley and Miratrix (2021) and Pashley and Miratrix (2022).\nThe EGAP Metaketa Projects are also multisite trials: the 5 to 7 countries that contain sites for each study are fixed and chosen in advance by the different research teams."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#a-multisite-trial-is-not-a-cluster-randomized-trial",
    "href": "guides/data-strategies/multisite-experiments_en.html#a-multisite-trial-is-not-a-cluster-randomized-trial",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "2.2 A multisite trial is not a cluster-randomized trial",
    "text": "2.2 A multisite trial is not a cluster-randomized trial\nA different type of RCT is a cluster-randomized design, in which entire schools are assigned to either the active treatment or control treatment. This video explains the difference between cluster and block-randomized designs. In a multisite trial, treatment is assigned within a block to individual units. In a cluster-randomized trial, treatment is assigned to groups of units. Some designs combine cluster- and block-randomization.\nAnother design that is not a multisite or block-randomized trial is an experiment that takes place in only one school and assigns individual students to active treatment and control treatment. This type of study has only one site and thus differences between sites do not matter in this design."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#why-choose-a-multisite-or-block-randomized-trial-design",
    "href": "guides/data-strategies/multisite-experiments_en.html#why-choose-a-multisite-or-block-randomized-trial-design",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "2.3 Why choose a multisite or block-randomized trial design?",
    "text": "2.3 Why choose a multisite or block-randomized trial design?\nIn most contexts, blocking reduces estimation error over an unblocked (completely randomized) experiment (Moore (2012), Gerber and Green (2012)). Thus, blocked experiments generally offer higher statistical power than unblocked experiments. Blocking is most helpful in increasing precision and statistical power in the setting where there is variation in the outcome, and where the blocks are related to this variation.\nIn multisite trials as compared to block-randomized trials, the researcher typically cannot purposely construct blocks to reduce variation, because they are defined by pre-existing sites. However, the researcher can hope, and often expect, that sites naturally explain some between-site variation. For example, if some schools tend to have higher outcomes than others, then blocked randomization using the school as a block improves efficiency over complete randomization.\nRandomizing with purposefully created blocks or pre-existing sites also helps analysts learn about how treatment effects may vary across the sites or groups of people categorized into the blocks. If a new treatment should help the lowest performing students most, but in any given study most students are not the lowest performing, then researchers may prefer to create blocks of students within schools with the students divided by their previous performance. This blocking within site would allow comparisons of the treatment effects on the relatively rare lowest performing students with the treatment effects on the relatively rare highest performing students."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#why-not-block",
    "href": "guides/data-strategies/multisite-experiments_en.html#why-not-block",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "2.4 Why not block?",
    "text": "2.4 Why not block?\nOften, in a multisite trial with treatment administered by site administrators (like principals of schools), an analyst has no choice but to randomize within site. In other studies, the construction and choice of blocking criteria is a choice. Pashley and Miratrix (2022) shows that blocking is generally beneficial, but also explores settings in which it may be harmful. Blocking does result in fewer degrees of freedom, but in practice this reduction is rarely an issue, unless an experiment is very small (Imai, King, and Stuart 2008). Any use of blocking requires that an analyst keep track of the blocks and also that an analyst reflect the blocks in subsequent analysis: in many circumstances estimating average treatment effects from a block-randomized experiment while ignoring the blocks will yield biased estimates of the underlying targeted estimands (see “The trouble with ‘controlling for blocks’” and “Estimating Average Treatment Effects in Block Randomized Experiments” for demonstrations of bias arising from different approaches to weighting by blocks)."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#common-linear-regression-models",
    "href": "guides/data-strategies/multisite-experiments_en.html#common-linear-regression-models",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "5.1 Common linear regression models",
    "text": "5.1 Common linear regression models\nFixed effects with a constant treatment (FE)\nWith this model, the researcher assumes that there are site-specific fixed effects (intercepts), but a common overall ATE. The assumed model is \\(Y_{ij} = \\sum_{k = 1}^{J} \\alpha_k \\text{Site}_{k,ij} + \\beta T_{ij} + e_{ij}\\), where \\(\\text{Site}_{k,ij}\\) is an indicator for unit \\(ij\\) being in site \\(k\\) (out of \\(J\\) sites), \\(T_{ij}\\) is a treatment indicator, and \\(e_{ij}\\) is an \\(iid\\) error term. For more discussion, see S. W. Raudenbush and Bloom (2015).\nFixed effects with interactions (FE-inter)\nWith this model, the researcher assumes site-specific heterogeneous treatment effects, so in addition to fitting a separate fixed effect for the intercepts for each site, a separate treatment impact coefficient is found for each site. \\[Y_{ij} = \\sum_{k = 1}^{J} \\alpha_k \\text{Site}_{k,ij} +\n\\sum_{k = 1}^{J} \\beta_k \\text{Site}_{k,ij} T_{ij} + e_{ij}\\]\nGiven a series of site-specific treatment estimates \\(\\hat{\\beta}_j\\), these estimates are then averaged, with weights by either simple weighting (see Clark and Silverberg (2011)) or by site size."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#common-multilevel-models",
    "href": "guides/data-strategies/multisite-experiments_en.html#common-multilevel-models",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "5.2 Common multilevel models",
    "text": "5.2 Common multilevel models\nOnce an analyst selects multilevel modeling, for site intercepts and site impacts they must decide: what is considered random, and what is considered fixed?\nFixed intercept, random treatment coefficient (FIRC)\nThis model is similar to the fixed effects models above, but assumes that the site impact \\(\\beta_j\\) is drawn from a shared distribution. The FIRC model was more recently designed to handle bias issues that arise when the proportion of units treated varies across sites.\n\\[\\begin{align*}\n\\text{Level 1}\\qquad & Y_{ij} = \\sum_{k = 1}^{J} \\alpha_k\n\\text{Site}_{k,ij} + \\beta_j T_{ij} + e_{ij}\\\\\n\\text{Level 2}\\qquad & \\beta_j = \\beta + b_j\n\\end{align*}\\] See S. W. Raudenbush and Bloom (2015) and Bloom and Porter (2017).\nRandom intercept, random treatment coefficient (RIRC)\nThis model is an older version of multilevel models, and assumes that both the site intercept and site impact are drawn from shared distributions. \\[\\begin{align*}\n\\text{Level 1}\\qquad & Y_{ij} = A_j + \\beta_j T_{ij} + e_{ij}\\\\\n\\text{Level 2}\\qquad & \\beta_j = \\beta + b_j\\\\\n& A_j = \\alpha + a_j\n\\end{align*}\\]\nRandom intercept, constant treatment coefficient (RICC)\nFinally, this model assumes that the site intercepts are drawn from a shared distribution, but the treatment impact is shared. \\[\\begin{align*}\n\\text{Level 1}\\qquad & Y_{ij} = A_j + \\beta T_{ij} + e_{ij}\\\\\n\\text{Level 2}\\qquad & A_j = \\alpha + a_j\\\\\n\\end{align*}\\] As noted previously, the multilevel framework generally naturally corresponds to the super population perspective. However, for RICC models, the site impacts are not assumed to be drawn from a super population; only the site intercepts are assumed to be random. Thus, when it comes to estimating treatment impacts, RICC models actually take a finite population perspective.\nThere are also weighted versions of both traditional regressions and multilevel models. For example, a fixed-effects model can weigh each person by their inverse chance of treatment to help increase precision. Weighted regression for traditional regression is discussed in Miratrix, Weiss, and Henderson (2021), and weighted regression for multilevel models is discussed in Raudenbush S. W. and Schwartz (2020)."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#design-based-estimators",
    "href": "guides/data-strategies/multisite-experiments_en.html#design-based-estimators",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "6.1 Design-based estimators",
    "text": "6.1 Design-based estimators\nDesign-based estimators are the most straightforward, as they are composed of simple weighted combinations of means. First, the site-specific treatment impact estimates \\(\\hat{B_j}\\) are calculated by taking differences in means between the active treatment and control treatment groups for each site. Then, the overall estimate is a weighted combination of these estimates, weighted by either person or site weighting.\nThe design-based estimators are \\[\\begin{align*}\n\\hat{\\beta}_{DB-persons} &= \\sum_{j = 1}^{J} \\frac{N_j}{N} \\hat{B_j} \\\\\n\\hat{\\beta}_{DB-sites} &= \\sum_{j = 1}^{J} \\frac{1}{J} \\hat{B_j}.\n\\end{align*}\\] Design-based estimators are generally unbiased for their corresponding estimands (person-weighted or site-weighted). Unbiasedness does not hold for one super population model; see Pashley and Miratrix (2022) for more details."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#linear-regression-estimators",
    "href": "guides/data-strategies/multisite-experiments_en.html#linear-regression-estimators",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "6.2 Linear regression estimators",
    "text": "6.2 Linear regression estimators\nConsider the FE model (fixed effects with a constant treatment). This regression model results in a precision-weighted estimate, in which each site impact is weighted by the estimated precision of estimating that site’s impact. The estimator is \\(\\hat{\\beta}_{FE} = \\sum_{j = 1}^{J} \\frac{N_j p_j (1 - p_j)}{Z} \\hat{B_j}\\), where \\(p_j\\) is the proportion treated at site \\(j\\). The quantity \\(Z\\) is a normalizing constant, so \\(Z\\) is defined as \\(\\sum_{j = 1}^{J} N_j p_j (1-p_j)\\) to ensure the weights sum to one. The weights are \\(N_j p_j (1 - p_j)\\), which is the inverse of \\(Var(\\hat{\\beta_j})\\), so the weights are related to the precision of the estimate for each site. This expression shows that sites with larger \\(N_j\\), or that have \\(p_j\\) closer to \\(0.5\\), have larger weights.\nThe FE estimator is not generally unbiased for either person-weighted or site-weighted estimands. If the impact size \\(B_j\\) is related to the weights (\\(N_j p_j (1 - p_j)\\)), then the estimator could be biased. For example, if sites that treat a higher proportion of treated units also experience a larger treatment impact, then \\(B_j\\) can be related to \\(p_j (1- p_j)\\). This setting is plausible for example if sites with more resources to intervene on more students also implement the intervention more effectively. If larger sites are more effective, then \\(B_j\\) can be related to \\(N_j p_j (1- p_j)\\).\nInstead, the FE estimator is unbiased for an estimand that weights the site impacts by \\(N_j p_j (1- p_j)\\). However, this estimand does not have a natural substantive interpretation. Although the FE estimator is generally biased for the estimands of interest, it may have increased precision and thus a lower mean squared error.\nIn contrast, the FE-inter model ends up with weights identical to the design-based estimators, depending on if the estimated site impacts are weighted equally or by size."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#multilevel-model-estimators",
    "href": "guides/data-strategies/multisite-experiments_en.html#multilevel-model-estimators",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "6.3 Multilevel model estimators",
    "text": "6.3 Multilevel model estimators\nMultilevel models also result in precision weighting, but in these models the estimated precision also takes into account the assumed underlying variance in site impacts. For example, the FIRC model can be expressed roughly as: \\[\\hat{\\beta}_{ML-FIRC*} = \\sum_{j = 1}^{J} \\frac{1}{Z}\n\\left(\\frac{\\sigma^2}{N_j p_j ( 1 - p_j)} + \\tau^2\\right)^{-1} \\hat{B_j}\\], where \\(Z\\) is again a normalizing constant, \\(Z = \\sum_{j = 1}^{J} \\left(\\frac{\\sigma^2}{N_j p_j ( 1 - p_j)} + \\tau^2\\right)^{-1}\\). This equation assumes that the \\(b_j\\) have known variance \\(\\tau^2\\), and the \\(e_{ij}\\) have known variance \\(\\sigma^2\\). In general, we do not know these quantities, and instead must estimate them. However, we can see that the implied precision weights incorporate the additional uncertainty assumed in the value of \\(b_j\\).\nThe RIRC model imposes the same structure on the site impacts, and thus the weights are similar to the FIRC model. The RICC model assumes a constant treatment impact, and thus is essentially equivalent to the precision-weighted fixed effects with constant treatment model (FE) when it comes to estimating the site impacts.\nWe summarize the weights in the table below. The following table includes additional estimators that are not discussed in this guide; for more information about these additional estimators, see Miratrix, Weiss, and Henderson (2021).\n\n\n\n\n\n\n\n\nWeight name\nWeight\nEstimators\n\n\n\n\nUnbiased person-weighting\n\\(w_j \\propto N_j\\)\n\\(\\hat{\\beta}_{DB-FP-person}\\), \\(\\hat{\\beta}_{DB-SP-person}\\), \\(\\hat{\\beta}_{FE-weight-person}\\), \\(\\hat{\\beta}_{FE-inter-person}\\)\n\n\nFixed-effect precision-weighting\n\\(w_j \\propto N_j p_j (1 - p_j)\\)\n\\(\\hat{\\beta}_{FE}\\), \\(\\hat{\\beta}_{FE-HW}\\), \\(\\hat{\\beta}_{FE-CR}\\), \\(\\hat{\\beta}_{ML-RICC}\\) (approximately)\n\n\nRandom-effect precision-weighting\n\\(w_j \\propto \\left[\\hat{\\tau} + N_j p_j (1 - p_j)\\right]^{-1}\\)  (approximately)\n\\(\\hat{\\beta}_{ML-FIRC}\\), \\(\\hat{\\beta}_{ML-RIRC}\\)\n\n\nUnbiased site-weighting\n\\(w_j \\propto 1\\)\n\\(\\hat{\\beta}_{DB-FP-site}\\), \\(\\hat{\\beta}_{DB-SP-site}\\), \\(\\hat{\\beta}_{FE-weight-site}\\), \\(\\hat{\\beta}_{FE-inter-site}\\)"
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#point-estimates",
    "href": "guides/data-strategies/multisite-experiments_en.html#point-estimates",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "8.1 Point estimates",
    "text": "8.1 Point estimates\nFirst, they consider the impact of choices on point estimates. The authors ask, “to what extent can the choice of estimator of the overall average treatment effect result in a different impact estimate?” In general, the authors find that the choice of estimator can substantially impact the point estimates, although the degree of impact depends on the choice. The authors reach the following conclusions.\nPerson-weighted estimands can result in a different conclusion than site-weighted estimands.\nIn some trials, estimates resulting from person-weighted estimands differed substantially from estimates resulting from site-weighted estimands. These discrepancies could be due to a difference in the true underlying values of the estimands, but they could also be due to estimation error from the estimation procedure. Through empirical exploration, they found that the difference is likely due to the estimands themselves being different. They found that “the range of estimates across all estimators is rarely meaningfully larger than the range between the person- and site-weighted estimates alone.”\nFor person-weighted estimands, the choice of estimator generally does not matter.\nThe unbiased design-based estimator and the precision-weighted fixed effect estimate both target the person-weighted estimand. There was little difference in estimates between these estimators. Most likely, “this implies that the potential bias in the bias-precision trade off to the fixed effect estimators is negligible in practice.” Other authors have been able to create situations in which the bias-precision trade off is more severe.\nFor site-weighted estimands, the choice of estimator can matter.\nFIRC estimates did differ from the unbiased design-based site estimator. FIRC can be seen as an adaptive estimator: when there is little estimated variation in impacts between sites, it tends to be more similar to the person-weighted estimate instead of the site-weighted estimate.\nDifferent estimators have different bias-variance trade offs.\nFinally, the authors consider the empirical bias-variance trade off of different estimators, and find:\n\nFE estimators have little bias, but also do not improve precision much over design-based estimators.\nFIRC tends to have lower mean squared error than design-based estimators.\nLarger site impact heterogeneity results in more biased estimates for FIRC.\nEven with more site impact heterogeneity, the mean squared error for FIRC estimators is still generally lower.\nCoverage for design-based estimators is more reliable, especially when site size is variable and site size is correlated with impact."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#standard-errors",
    "href": "guides/data-strategies/multisite-experiments_en.html#standard-errors",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "8.2 Standard errors",
    "text": "8.2 Standard errors\nThe second question concerns the choice of standard error estimators. The authors ask, “to what extent can the choice of estimator of the standard error of the overall average treatment effect result in a different estimated standard error?”\nThe choice of standard error estimator can substantially impact the estimated standard error. The authors reach the following conclusions.\nThe choice of estimand impacts the standard error.\nSuper population estimators generally have larger standard errors than finite population estimators. Site-weighted estimators generally have larger standard errors than person-weighted estimators.\nGiven a particular estimand, the choice of estimator matters in some contexts and not others.\nFor finite population estimands (including both person and site-weighted estimands) or super population person-weighted estimands, the choice of standard error estimator generally does not matter. In practice, Miratrix, Weiss, and Henderson (2021) found that estimators that attempt to improve precision by trading bias may not actually result in gains in precision in practice. The use of robust standard errors also does not differ much from non-robust standard errors in practice.\nFor super population site-weighted estimands, the choice of standard error estimator can matter a lot. In most cases, standard error estimates differed substantially between the design-based super population estimator and FIRC. The authors further conclude that for super population site-weighted estimands, the wide-ranging standard error estimates stem from instability in estimation. Through a simulation study, they find that super population standard errors can underestimate the true error. The design-based super population standard error estimator is particularly prone to underestimate the standard error compared to multilevel models, and can be unstable, in that it estimates a wide range of different values across simulations."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#footnotes",
    "href": "guides/data-strategies/multisite-experiments_en.html#footnotes",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Linear regression can be used as a tool in both design-based approaches (to calculate the difference in means) and model-based approaches (to estimate the parameters of a Normal data-generating process). In general, this guide considers linear regression as used in a model-based approach.↩︎"
  },
  {
    "objectID": "guides/analysis-procedures/covariates_en.html",
    "href": "guides/analysis-procedures/covariates_en.html",
    "title": "10 Things to Know About Covariate Adjustment",
    "section": "",
    "text": "“Covariates” are baseline characteristics of your experimental subjects. When you run an experiment, you are primarily interested in collecting data on outcome variables that your intervention may affect, e.g. expenditure decisions, attitudes toward democracy, or contributions for a public good in a lab experiment. But it’s also a good idea to collect data on baseline characteristics of subjects before treatment assignment occurs, e.g. gender, level of education, or ethnic group. If you do this you can explore how treatment effects vary with these characteristics (see 10 Things to Know About Heterogeneous Treatment Effects). But doing this also lets you perform covariate adjustment.\nCovariate adjustment is another name for controlling for baseline variables when estimating treatment effects. Often this is done to improve precision. Subjects’ outcomes are likely to have some correlation with variables that can be measured before random assignment. Accounting for variables like gender will allow you to set aside the variation in outcomes that is predicted by these baseline variables, so that you can isolate the effect of treatment on outcomes with greater precision and power.\nCovariate adjustment can be a cheaper route to improved precision than increasing the number of subjects in the experiment. Partly for that reason, researchers often collect extensive data on covariates before random assignment. Pre-tests (measures that are analogous to the outcome variable but are restricted to time periods before random assignment) may be especially valuable for predicting outcomes, and baseline surveys can ask subjects about other background characteristics."
  },
  {
    "objectID": "guides/analysis-procedures/covariates_en.html#footnotes",
    "href": "guides/analysis-procedures/covariates_en.html#footnotes",
    "title": "10 Things to Know About Covariate Adjustment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee, e.g., pages 217–219 of Bruhn and McKenzie (2013).↩︎\nA brief review of bias and precision: Imagine replicating the experiment many times (without changing the experimental sample and conditions, but re-doing random assignment each time). An unbiased estimator may overestimate or underestimate the ATE on any given replication, but its expected value (the average over all possible replications) will equal the true ATE. We usually prefer unbiased or approximately unbiased estimators, but we also value precision (which is formally defined as the inverse of the variance). Imagine you’re throwing a dart at a dartboard. If you hit the center of the dartboard on average but your shots are often far from the mark, you have an unbiased but imprecise estimator. If you hit close to the center every time, your estimator is more precise. A researcher may choose to accept a small bias in return for a large improvement in precision. One possible criterion for evaluating estimators is the mean squared error, which equals the variance plus the square of the bias. See, e.g., Lohr (2010), pp. 31-32↩︎\n“Sampling variability” refers to the spread of estimates that will be produced just because of the different random assignments that could have been drawn. When the luck of the draw of random assignment produces a treatment group with more As and a control group with more Bs, it is more difficult to separate background characteristics (A and B) from treatment assignment as the predictor of the observed outcomes.↩︎\nThe estimated bias is 0.0003 with a margin of error (at the 95% confidence level) of 0.0018.↩︎\nSee, e.g.: Cox and Reid (2000), (pp. 29-32), Holt and Smith (1979), and Royall (1976). For an introduction to philosophical disagreements about statistical inference, see Efron (1978).↩︎\nLin, Green, and Coppock (2016). Italics in the original.↩︎\nThe estimated bias is \\(-\\) 0.459 with a margin of error (at the 95% confidence level) of 0.002.↩︎\nSee Freedman (2008). See also Winston Lin’s blog posts (part I and part II) discussing his response to Freedman.↩︎\nFor more discussion of pre-analysis plans, see, e.g., Olken (2015).↩︎"
  },
  {
    "objectID": "guides/analysis-procedures/ri2_en.html",
    "href": "guides/analysis-procedures/ri2_en.html",
    "title": "10 Randomization Inference Procedures with ri2",
    "section": "",
    "text": "Randomization inference is a procedure for conducting hypothesis tests that takes explicit account of a study’s randomization procedure. See 10 things about Randomization Inference for more about the theory behind randomization inference. In this guide, we’ll see how to use the ri2 package for r to conduct 10 different analyses. This package was developed with funding from EGAP’s innaugural round of standards grants, which are aimed at projects designed to improve the quality of experimental research.\nTo illustrate what you can do with ri2, we’ll use some data from a hypothetical experiment involving 200 students in 20 schools. We’ll consider how to do randomization inference using a variety of different designs, including complete random assignment, block random assignment, cluster random assignment, and a multi-arm trial. You can check the kinds of random assignment methods guide for more on the varieties of random assignment.\nFollow the links below to download the four datasets we’ll use in the examples:\n\ncomplete randomization assignment dataset\nblocked randomization assignment dataset\nclustered randomization assignment dataset- three-arm randomization assignment datase\n\n\n1 Randomization inference for the Average Treatment Effect\nWe’ll start with the most common randomization inference task: testing an observed average treatment effect estimate against the sharp null hypothesis of no effect for any unit.\nIn ri2, you always “declare” the random assignment procedure so the computer knows how treatments were assigned. In the first design we’ll consider, exactly half of the 200 students were assigned to treatment using complete random assignment.\n\nlibrary(ri2)\ncomplete_dat &lt;- read.csv(\"ri2_complete_dat.csv\")\ncomplete_dec &lt;- declare_ra(N = 200)\n\nNow all that remains is a call to conduct_ri. The sharp_hypothesis argument is set to 0 by default corresponding to the sharp null hypothesis of no effect for any unit. We can see the output using the summary and plot commands.\n\nsims &lt;- 10000\nri_out &lt;-\n  conduct_ri(\n    Y ~ Z,\n    declaration = complete_dec,\n    sharp_hypothesis = 0,\n    data = complete_dat,\n    sims = sims\n  )\nsummary(ri_out)\n\n  term estimate two_tailed_p_value\n1    Z    41.98             0.1144\n\nplot(ri_out)\n\n\n\n\n\n\n\n\nYou can obtain one-sided p-values with a call to summary:\n\nsummary(ri_out, p = \"upper\")\n\n  term estimate upper_p_value\n1    Z    41.98        0.0564\n\nsummary(ri_out, p = \"lower\")\n\n  term estimate lower_p_value\n1    Z    41.98        0.9436\n\n\n\n\n2 Randomization inference for alternative designs\nThe answer that ri2 produces depends deeply on the randomization procedure. The next example imagines that the treatment was blocked at the school level.\n\nblocked_dat &lt;- read.csv(\"ri2_blocked_dat.csv\")\nblocked_dec &lt;- declare_ra(blocks = blocked_dat$schools)\nri_out &lt;-\n  conduct_ri(\n    Y ~ Z,\n    declaration = blocked_dec,\n    data = blocked_dat,\n    sims = sims\n  )\nsummary(ri_out)\n\n  term estimate two_tailed_p_value\n1    Z    91.98              2e-04\n\nplot(ri_out)\n\n\n\n\n\n\n\n\nA very similar syntax accommodates a cluster randomized trial.\n\nclustered_dat &lt;- read.csv(\"ri2_clustered_dat.csv\")\nclustered_dec &lt;- declare_ra(clusters =  clustered_dat$schools)\nri_out &lt;-\n  conduct_ri(\n    Y ~ Z,\n    declaration = clustered_dec,\n    data = clustered_dat,\n    sims = sims\n  )\nsummary(ri_out)\n\n  term estimate two_tailed_p_value\n1    Z    79.32             0.0111\n\nplot(ri_out)\n\n\n\n\n\n\n\n\n\n\n3 Randomization inference with covariate adjustment\nCovariate adjustment can often produce large gains in precision. To analyze an experiment with covariate adjustment, simply include the covariates in the formula argument of conduct_ri:\n\ncomplete_dec &lt;- declare_ra(N = 200)\nri_out &lt;-\n  conduct_ri(\n    Y ~ Z + PSAT,\n    declaration = complete_dec,\n    data = complete_dat,\n    sims = sims\n  )\nsummary(ri_out)\n\n  term estimate two_tailed_p_value\n1    Z 59.27132                  0\n\nplot(ri_out)\n\n\n\n\n\n\n\n\n\n\n4 Randomization inference for a balance test\nYou can use randomization inference to conduct a balance test (or randomization check). In this case, we write a function of data that return some balance statistic (the F-statistic from a regression of the treatment assignment on two covariates).\n\nbalance_fun &lt;- function(data) {\n  summary(lm(Z ~ professionalism + PSAT, data = data))$f[1]\n}\nri_out &lt;-\n  conduct_ri(\n    test_function = balance_fun,\n    declaration = complete_dec,\n    data = complete_dat,\n    sims = sims\n  )\n\nWarning in data.frame(est_sim = test_stat_sim, est_obs = test_stat_obs, : row\nnames were found from a short variable and have been discarded\n\nsummary(ri_out)\n\n                   term  estimate two_tailed_p_value\n1 Custom Test Statistic 0.2924994             0.7489\n\nplot(ri_out)\n\n\n\n\n\n\n\n\n\n\n5 Randomization inference for treatment effect heterogeneity by subgroups\nYou can assess whether the treatment engenders different treatment effects among distinct subgroups by comparing the model fit (using an F-statistic) of two nested models:\n\nA regression of the outcome on the treatment assignment and the subgroup indicator\nA regression of the outcome on the treatment assignment, subgroup indicator, and their interaction\n\nThe null hypothesis we’re testing against in this example is the sharp hypothesis that the true treatment effect for each unit is the observed average treatment effect, i.e., that effects are constant.\n\nate_obs &lt;- with(complete_dat, mean(Y[Z == 1]) - mean(Y[Z == 0]))\nri_out &lt;-\n    conduct_ri(\n      model_1 = Y ~ Z + high_quality,\n      model_2 = Y ~ Z + high_quality + Z * high_quality,\n      declaration = complete_dec,\n      sharp_hypothesis = ate_obs,\n      data = complete_dat, \n      sims = sims\n    )\nsummary(ri_out)\n\n         term estimate two_tailed_p_value\n1 F-statistic 1.095294             0.7015\n\nplot(ri_out)\n\n\n\n\n\n\n\n\n\n\n6 Randomization inference for unmodeled treatment effect heterogeneity\nAnother way to investigate treatment effect heterogeneity is to consider whether the variance in the treatment and control groups are different. We can therefore test whether the difference-in-variances is larger in magnitude than what we would expect under the sharp null hypothesis of no effect for any unit.\n\nd_i_v &lt;- function(dat) {\n    with(dat, var(Y[Z == 1]) - var(Y[Z == 0]))\n  }\nri_out &lt;-\n    conduct_ri(\n      test_function = d_i_v,\n      declaration = complete_dec,\n      data = complete_dat, \n      sims = sims\n    )\nsummary(ri_out)\n\n                   term  estimate two_tailed_p_value\n1 Custom Test Statistic -8408.684             0.2824\n\nplot(ri_out)\n\n\n\n\n\n\n\n\n\n\n7 Randomization inference for multi-arm trials\nIn a three-arm trial, the research might wish to compare each treatment to control separately. To do this, we must change the null hypothesis in a subtle way: we are going to assume the sharp null for each pairwise comparison. For example, when comparing treatment 1 to control, we exclude the subjects assigned to treatment 2 and pretend we simply have a two arm trial conducted among the subjects assigned to control and treatment 1.\n\nthree_arm_dat &lt;- read.csv(\"ri2_three_arm_dat.csv\")\nthree_arm_dec &lt;- declare_ra(N = 200, \n                            conditions = c(\"Control\", \"Treatment 1\", \"Treatment 2\"))\nri_out &lt;-\n    conduct_ri(\n      formula = Y ~ Z,\n      declaration = three_arm_dec,\n      data = three_arm_dat,\n      sims = sims\n    )\nsummary(ri_out)\n\n          term  estimate two_tailed_p_value\n1 ZTreatment 1  26.72546                 NA\n2 ZTreatment 2 -48.52827                 NA\n\n## plot(ri_out)\n\n\n\n8 Randomization inference for joint significance\nIn that same three-arm trial that compares two treatments to a control, we might be interested in testing whether, jointly, the treatments appear to change outcomes relative to the control. This is analogous to a joint F-test, conducted via randomization inference. We assume the sharp null that a unit would express their observed outcome in any of the three conditions.\n\nF_statistic &lt;- function(data) {\n  summary(lm(Y ~ Z, data = data))$f[1]\n}\nri_out &lt;-\n   conduct_ri(\n     test_function = F_statistic,\n     declaration = three_arm_dec,\n     data = three_arm_dat,\n     sims = sims\n   )\n\nWarning in data.frame(est_sim = test_stat_sim, est_obs = test_stat_obs, : row\nnames were found from a short variable and have been discarded\n\nsummary(ri_out)\n\n                   term estimate two_tailed_p_value\n1 Custom Test Statistic 2.802927             0.0664\n\nplot(ri_out)\n\n\n\n\n\n\n\n\n\n\n9 Randomization inference under noncompliance\nSome experiments encounter noncompliance, the slippage between treatment as assigned and treatment as delivered. The Complier Average Causal Effect (\\(CACE\\)) can be shown (under standard assumptions plus monotonicity) to be the ratio of the effect of assignment on the outcome – the “Intention-to-Treat” (\\(ITT_y\\)) and the effect of assignment on treatment receipt the (\\(ITT_D\\)). (The \\(CACE\\) is also called Local Average Treatment Effect. See our guide 10 Things to Know About the Local Average Treatment Effect for more details.) Because the \\(CACE\\) is just a rescaled, \\(ITT_y\\), a hypothesis test with respect to the \\(ITT_y\\) is a valid test for the \\(CACE\\). In practice, researchers can simply conduct a randomization inference test exactly as they would for the ATE, ignoring noncompliance altogether.\n\nITT_y = with(complete_dat, mean(Y[Z == 1]) - mean(Y[Z == 0])) \nITT_d = with(complete_dat, mean(D[Z == 1]) - mean(D[Z == 0])) \nCACE &lt;- ITT_y / ITT_d\n           \nri_out &lt;-\n  conduct_ri(\n    Y ~ Z, # notice we do inference on the ITT_y\n    declaration = complete_dec,\n    data = complete_dat,\n    sims = sims\n  )\nsummary(ri_out)\n\n  term estimate two_tailed_p_value\n1    Z    41.98             0.1156\n\nplot(ri_out)\n\n\n\n\n\n\n\n\n\n\n10 Randomization inference for arbitrary test statistics\nri2 can accommodate any scalar test statistic. A favorite among some analysts is the Wilcox rank-sum statistic, which can be extracted from the wilcox.test() function:\n\nwilcox_fun &lt;- function(data){\n  wilcox_out &lt;- with(data, wilcox.test(Y ~ Z))\n  wilcox_out$statistic\n}\nri_out &lt;-\n  conduct_ri(\n    test_function = wilcox_fun,\n    declaration = complete_dec,\n    data = complete_dat,\n    sims = sims\n  )\n\nWarning in data.frame(est_sim = test_stat_sim, est_obs = test_stat_obs, : row\nnames were found from a short variable and have been discarded\n\nsummary(ri_out)\n\n                   term estimate two_tailed_p_value\n1 Custom Test Statistic     4320             0.9565\n\nplot(ri_out)\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "guides/analysis-procedures/randomization-inference_en.html",
    "href": "guides/analysis-procedures/randomization-inference_en.html",
    "title": "10 Things to Know About Randomization Inference4",
    "section": "",
    "text": "One of the advantages of conducting a randomized trial is that the researcher knows the precise procedure by which the units were allocated to treatment and control. Randomization inference considers what would have happened under all possible random assignments, not just the one that happened to be selected for the experiment at hand. Against the backdrop of all possible random assignments, is the actual experimental result unusual? How unusual is it?"
  },
  {
    "objectID": "guides/analysis-procedures/randomization-inference_en.html#footnotes",
    "href": "guides/analysis-procedures/randomization-inference_en.html#footnotes",
    "title": "10 Things to Know About Randomization Inference4",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI am grateful to Winston Lin and Gareth Nellis, who commented on an earlier draft.↩︎\nI am grateful to Winston Lin and Gareth Nellis, who commented on an earlier draft.↩︎\nI am grateful to Winston Lin and Gareth Nellis, who commented on an earlier draft.↩︎\nWe focus here on randomization inference as applied to hypothesis testing. Randomization inference may also be used for construction of confidence intervals, but this application requires stronger assumptions. See Gerber and Green (2012), chapter 3.↩︎\nAs explained in other guides, the fundamental problem of causal inference is that we cannot observe what would have happened to those in control group had they be treated, nor can we observe what would have happened to those in the treatment group had they not been treated.↩︎\nOne-tailed tests consider the null hypothesis of no effect against an alternative hypothesis that the average treatment effect is positive (negative). Two-tailed tests evaluate a null hypothesis against the alternative that the ATE is nonzero, whether positive or negative. In that case, the p-value may be assessed by calculating the proportion of simulated random assignments that are at least as large as the observed test statistic in absolute value.↩︎\nSee Chung and Romano (2013). This “studentized” approach makes sense when there is reason to believe that the treatment changes the variance in outcomes in an experiment with different numbers of subjects in treatment and control.↩︎\nSee 10 Things to Know about Heterogeneous Treatment Effects, especially section 7 on multiple comparisons.↩︎\nThe alternative is to make stronger assumptions. See Small, Ten Have, and Rosenbaum (2008).↩︎"
  },
  {
    "objectID": "guides/interpretation/regression-table_en.html",
    "href": "guides/interpretation/regression-table_en.html",
    "title": "10 Things to Know About Reading a Regression Table",
    "section": "",
    "text": "The table below that will be used throughout this methods guide is adapted from a study done by EGAP members Miriam Golden, Eric Kramon and their colleagues (Asunka et al. 2013). The authors performed a field experiment in Ghana in 2012 to test the effectiveness of domestic election observers on combating two common electoral fraud problems: ballot stuffing and overvoting. Ballot stuffing occurs when more ballots are found in a ballot box than are known to have been distributed to voters. Overvoting occurs when more votes are cast at a polling station than the number of voters registered. This table reports a multiple regression (this is a concept that will be further explained below) from their experiment that explores the effects of domestic election observers on ballot stuffing. The sample consists of 2,004 polling stations."
  },
  {
    "objectID": "guides/interpretation/regression-table_en.html#standard-error",
    "href": "guides/interpretation/regression-table_en.html#standard-error",
    "title": "10 Things to Know About Reading a Regression Table",
    "section": "4.1 Standard Error",
    "text": "4.1 Standard Error\nThe standard error (SE) is an estimate of the standard deviation of an estimated coefficient.3 It is often shown in parentheses next to or below the coefficient in the regression table. It can be thought of as a measure of the precision with which the regression coefficient is estimated. The smaller the SE, the more precise is our estimate of the coefficient. SEs are of interest not so much for their own sake as for enabling the construction of confidence intervals (CIs) and significance tests. An often-used rule of thumb is that when the sample is reasonably large, the margin of error for a 95% CI is approximately twice the SE. However, explicit CI calculations are preferable. We discuss CIs in more detail in the next section.\nThe table above from Asunka et al. (2013) shows “robust” standard errors, which have attractive properties in large samples because they remain valid even when some of the regression model assumptions are violated. The key assumptions that “conventional” or “classical” SEs make and robust SEs relax are that (1) the expected value of Y, given X, is a linear function of X, and (2) the variance of Y does not depend on X (conditional homoskedasticity). Robust SEs do assume (unless they are “clustered”) either that the observations are statistically independent or that the treatment was randomly assigned to the units of observation (the polling stations in this example).4"
  },
  {
    "objectID": "guides/interpretation/regression-table_en.html#t-statistic",
    "href": "guides/interpretation/regression-table_en.html#t-statistic",
    "title": "10 Things to Know About Reading a Regression Table",
    "section": "4.2 t-Statistic",
    "text": "4.2 t-Statistic\nThe t-statistic (in square brackets in the example table) is the ratio of the estimated coefficient to its standard error. T-statistics usually appear in the output of regression procedures but are often omitted from published regression tables, as they’re just a tool for constructing confidence intervals and significance tests."
  },
  {
    "objectID": "guides/interpretation/regression-table_en.html#p-values-and-significance-tests",
    "href": "guides/interpretation/regression-table_en.html#p-values-and-significance-tests",
    "title": "10 Things to Know About Reading a Regression Table",
    "section": "4.3 p-Values and Significance Tests",
    "text": "4.3 p-Values and Significance Tests\nIn the table above, if an estimated coefficient (in bold) is marked with one or more asterisks,5 that means the estimate is “statistically significant” at the 1%, 5%, or 10% level—in other words, the p-value (from a two-sided test6 of the null hypothesis that the true coefficient is zero) is below 0.01, 0.05, or 0.1.\nTo calculate a p-value, we typically assume that the data on which you run your regression are a random sample from some larger population. We then imagine that you draw a new random sample many times and run your regression for every new sample. (Alternatively, we may imagine randomly assigning some treatment many times. See our guide on hypothesis testing for more details.) This procedure would create a distribution of estimates and t-statistics. Given this distribution, the p-value captures the probability that the absolute value of the t-statistic would have been at least as large as the value that you actually observed if the true coefficient were zero. If the p-value is greater than or equal to some conventional threshold (such as 0.05 or 0.1), the estimate is “not statistically significant” (at the 5% or 10% level). According to convention, estimates that are not statistically significant are not considered evidence that the true coefficient is nonzero.\nIn the table, the only estimated coefficient that is statistically significant at any of the conventional levels is the intercept (which is labeled “Constant/Intercept” because in the algebra of regression, the intercept is the coefficient on the constant 1). The intercept is the predicted value of the outcome when the values of the explanatory variables are all zero. In this example, the question of whether the true intercept is zero is of no particular interest, but the table reports the significance test for completeness. The research question is about observer effects on ballot stuffing (as shown in the heading of the table). The estimated coefficient on “Observer Present (OP)” is of main interest, and it is not statistically significant.\nIt is easy to misinterpret p-values and significance tests. Many scholars believe that although significance tests can be useful as a restraining device, they are often overemphasized. Helpful discussions include the American Statistical Association’s 2016 statement on p-values; the invited comments on the statement, especially Greenland et al. (2016); and short posts by David Aldous and Andrew Gelman."
  },
  {
    "objectID": "guides/interpretation/regression-table_en.html#f-test-and-degrees-of-freedom",
    "href": "guides/interpretation/regression-table_en.html#f-test-and-degrees-of-freedom",
    "title": "10 Things to Know About Reading a Regression Table",
    "section": "4.4 F-Test and Degrees of Freedom",
    "text": "4.4 F-Test and Degrees of Freedom\nThe bottom section of the table includes a row with the heading “F(5, 59)”, the value 1.43 (the F-statistic), and the p-value .223. This F-test is a test of the null hypothesis that the true values of the regression coefficients, excluding the intercept, are all zero. In other words, the null hypothesis is that none of the explanatory variables actually help predict the outcome. In this example, the p-value associated with the F-statistic is 0.223, so the null hypothesis is not rejected at any of the conventional significance levels. However, since our main interest is in the effects of observers, the F-test isn’t of much interest in this application. (We already knew that the estimated coefficient on “Observer Present” is not statistically significant, as noted above.)\nThe numbers 5 and 59 in parentheses are the degrees of freedom (df) associated with the numerator and denominator in the F-statistic formula. The numerator df (5) is the number of parameters that the null hypothesis claims are zero. In this example, those parameters are the coefficients on the 5 explanatory variables shown in the table. The denominator df (59) equals the sample size minus the total number of parameters estimated. (In this example, the sample size is 2,004 and there are only 6 estimated parameters shown in the table, but the regression also included many dummy variables for constituencies that were used in blocking.)"
  },
  {
    "objectID": "guides/interpretation/regression-table_en.html#footnotes",
    "href": "guides/interpretation/regression-table_en.html#footnotes",
    "title": "10 Things to Know About Reading a Regression Table",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor in-depth discussions, see: Angrist and Pischke (2009), chapters 3 and 8; Berk et al. (2014); Buja et al. (2015); Hansen (2022)↩︎\nOn regression in observational studies, see: 10 Strategies for Figuring out if X Caused Y; Freedman (1991); Angrist and Pischke (2015); Angrist and Pischke (2009); Imbens and Wooldridge (2009); Imbens (2015). On regression adjustment in randomized experiments, see 10 Things to Know About Covariate Adjustment and Winston Lin’s Development Impact blog posts (here and here).↩︎\nStrictly speaking, the true SE is the standard deviation of the estimated coefficient, while what we see in the regression table is the estimated SE. However, in common parlance, people often say “standard error” when they mean the estimated SE, and we’ll do the same.↩︎\nRobust SEs are also known as Huber–White or sandwich SEs. On the properties of robust SEs, see: Angrist and Pischke (2009), section 3.1.3 and chapter 8; Imbens and Kolesár (2016); Reichardt and Gollog (1999); Samii and Aronow (2012); Lin (2013); Abadie et al. (2014).↩︎\nThe use of asterisks to flag statistically significant results is common but not universal. Our intention here is merely to explain what the asterisks mean, not to recommend that they should or should not be used.↩︎\nTwo-sided tests are the default in most software packages and in some research fields, so when tables do not explicitly note whether the p-values associated with regression coefficients are one- or two-sided, they are usually two-sided. Olken (2015) (pp. 67, 70) notes that since “convention typically dictates two-sided hypothesis tests,” researchers who prefer one-sided tests should commit to that choice in a pre-analysis plan so “they cannot be justly accused of cherry-picking the test after the fact.” Greenland et al. (2016) (p. 342) argue against the view that one should always use two-sided p-values, but write, “Nonetheless, because two-sided P values are the usual default, it will be important to note when and why a one-sided P value is being used instead.”↩︎\nThe framework where “the only thing that varies from one replication to another is which units are randomly assigned to treatment” is known as randomization-based inference. This isn’t the only framework for frequentist inference. In the classical regression framework, the only thing that varies is that on each replication, different values of ε are randomly drawn. And in the random sampling framework, on each replication a different random sample is drawn from the population. On randomization-based inference, see Reichardt and Gollog (1999), Samii and Aronow (2012), Lin (2013), and Abadie et al. (2014); on the random sampling framework, see the references in note 2.↩︎\nSee, e.g., Humphreys, Sanchez de la Sierra, and Windt (2013).↩︎\nSimmons, Nelson, and Simonsohn (2011).↩︎\nGelman and Loken (2013).↩︎\nSee Humphreys, Sanchez de la Sierra, and Windt (2013), Monogan (2013), Anderson (2013), and Gelman (2013).↩︎\nSee Olken (2015) and Coffman and Niederle (2015).↩︎\nFor more discussion of attrition bias in randomized experiments, see, e.g., chapter 7 of Gerber and Green (2012).↩︎\nSee also: Angrist and Pischke (2009), section 3.2.3; Rosenbaum (1984).↩︎\nRosenthal (1979). On reforms to counter publication bias, see: Nyhan (2015); Findley et al. (2016).↩︎\nWeighting by the inverse of the variance of ε is a form of generalized least squares (GLS). The classical argument is that GLS is more efficient (i.e., has lower variance) than OLS under heteroskedasticity. However, when the goal is to estimate an average treatment effect, some researchers question the relevance of the classical theory, because if treatment effects are heterogeneous, GLS and OLS are not just more efficient and less efficient ways to estimate the same treatment effect. Instead, they estimate different weighted average treatment effects. In other words, they answer different questions, and choosing GLS for efficiency is arguably like looking for your keys where the light’s better. Angrist and Pischke (2010) write: “Today’s applied economists have the benefit of a less dogmatic understanding of regression analysis. Specifically, an emerging grasp of the sense in which regression and two-stage least squares produce average effects even when the underlying relationship is heterogeneous and/or nonlinear has made functional form concerns less central. The linear models that constitute the workhorse of contemporary empirical practice usually turn out to be remarkably robust, a feature many applied researchers have long sensed and that econometric theory now does a better job of explaining. Robust standard errors, automated clustering, and larger samples have also taken the steam out of issues like heteroskedasticity and serial correlation. A legacy of White (1980)’s paper on robust standard errors, one of the most highly cited from the period, is the near death of generalized least squares in cross-sectional applied work. In the interests of replicability, and to reduce the scope for errors, modern applied researchers often prefer simpler estimators though they might be giving up asymptotic efficiency.” Similarly, Stock (2010) comments: “The 1970s procedure for handling potential heteroskedasticity was either to ignore it or to test for it, to model the variance as a function of the regressors, and then to use weighted least squares. While in theory weighted least squares can yield more statistically efficient estimators, modeling heteroskedasticity in a multiple regression context is difficult, and statistical inference about the effect of interest becomes hostage to the required subsidiary modeling assumptions. White (1980)’s important paper showed how to get valid standard errors whether there is heteroskedasticity or not, without modeling the heteroskedasticity. This paper had a tremendous impact on econometric practice: today, the use of heteroskedasticity-robust standard errors is standard, and one rarely sees weighted least squares used to correct for heteroskedasticity.” (Emphasis added in both quotations.)↩︎\nLogit, probit, and other limited dependent variable (LDV) models do not immediately yield estimates of the average treatment effect (ATE). To estimate ATE, one needs to compute an average marginal effect (or average predictive comparison) after estimating the LDV model (see, e.g., Gelman and Pardoe (2007)). Some researchers argue that the complexity of marginal effect calculations for LDV models is unnecessary because OLS tends to yield similar ATE estimates (see Angrist and Pischke (2009), section 3.4.2, and the debate between Angrist and his discussants in Angrist (2001)). In randomized experiments, the robustness of OLS is supported by both asymptotic theory and simulation evidence. For theory, see Lin (2013). For simulations, see Humphreys, Sanchez de la Sierra, and Windt (2013) and Judkins and Porter (2016). See also Lin’s comments on this MHE blog post.↩︎"
  },
  {
    "objectID": "guides/causal-inference/causal-inference.es.html",
    "href": "guides/causal-inference/causal-inference.es.html",
    "title": "10 cosas que debe saber sobre la inferencia causal",
    "section": "",
    "text": "Para la mayoría de investigadores que realizan experimentos, el enunciado “\\(X\\) causó \\(Y\\)” significa que \\(Y\\) ocurrió y que no habría ocurrido si \\(X\\) no hubiera estado presente. Esta definición requiere que tengamos una noción de lo que podría haber sucedido, pero no sucedió.1 De manera similar, el “efecto” de \\(X\\) en \\(Y\\) se considera la diferencia entre el valor que \\(Y\\) habría tomado dado un valor de \\(X\\) y el valor que \\(Y\\) habría tomado dado otro valor de \\(X\\). Debido al enfoque en la diferencia de los resultados, este enfoque a veces se conoce como el enfoque de causalidad basado en “hacer diferencias” o en lo “contrafactual”.\nNota técnica: Los estadísticos emplean el marco de “resultados potenciales” para describir las relaciones contrafactuales. En este marco, dejamos que \\(Y_i(1)\\) denote el valor que la unidad \\(i\\) tomaría bajo la condición uno (por ejemplo, si la unidad \\(i\\) recibió un tratamiento) y \\(Y_i(0)\\) el valor que habría sido observado en otra condición (por ejemplo, si la unidad \\(i\\) no recibió el tratamiento). Un efecto causal del tratamiento para la unidad \\(i\\) puede ser una simple diferencia de los resultados potenciales \\(\\tau_i = Y_i(1)-Y_(0)\\). Un tratamiento tiene un efecto causal (positivo o negativo) en \\(Y\\) para la unidad \\(i\\) si \\(Y_i (1) \\neq Y_i (0)\\)."
  },
  {
    "objectID": "guides/causal-inference/causal-inference.es.html#footnotes",
    "href": "guides/causal-inference/causal-inference.es.html#footnotes",
    "title": "10 cosas que debe saber sobre la inferencia causal",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHolland, Paul W. “Statistics and causal inference.” Journal of the American Statistical Association 81.396 (1986): 945-960.↩︎\nHolland, Paul W. “Statistics and causal inference.” Journal of the American Statistical Association 81.396 (1986): 945-960.↩︎\nEsto se conoce a veces como el “Problema de las causas excesivas”.↩︎\nDe acuerdo a Mackie, a veces se invoca la idea de condiciones “INUS” para capturar la dependencia de las causas de otras causas. Según esta explicación, una causa puede ser parte Insuficiente pero Necesaria de una condición que en sí misma es Innecesaria pero Suficiente. Por ejemplo, marcar un número de teléfono es una causa de contacto con alguien, ya que tener una conexión y marcar un número es suficiente (S) para hacer una llamada telefónica, mientras que marcar solo sin una conexión no sería suficiente (I), ni tener una conexión (N). Por supuesto, hay otras formas de contactar a alguien sin hacer llamadas telefónicas (U). Mackie, John L. “El cemento del universo”. Londres: Oxford Uni (1974).↩︎\nNota técnica: La idea técnica clave es que la diferencia de promedios es la misma que el promedio de diferencias. Es decir, usando el “operador de expectativas”, \\(\\text{E}(\\tau_i) = \\text{E}(Y_i (1) -Y_i (0)) = \\text{E}(Y_i (1)) - \\text{E}(Y_i (0))\\). Los términos dentro del operador de esperanzas en la segunda cantidad no se pueden estimar, pero los términos dentro de los operadores de expectativas en la tercera cantidad si se pueden ser estimados5 Vea la ilustración [aquí] (https://raw.githubusercontent.com/egap/ guías-métodos / maestro / inferencia-causal / PO.jpg).↩︎\nPor esta razón usar las pruebas \\(t\\) para verificar si “la asignación aleatoria funcionó bien” no tiene mucho sentido, al menos si se sabe que se siguió una procedimiento aleatorio: por simple chance, 1 de cada 20 de esas pruebas mostrará diferencias estadísticamente detectables entre los grupos tratados y de control. Si existen dudas sobre si la asignación aleatoria se realizó correctamente, estas pruebas se pueden utilizar para probar la hipótesis de que los datos se generaron efectivamente mediante un procedimiento aleatorio. Esta última razón para las pruebas de aleatorización puede ser especialmente importante en experimentos de campo donde las cadenas de comunicación entre la persona que crea los números aleatorios y la persona que implementa la asignación del tratamiento son largas y complejas.↩︎\nNota técnica: Sea \\(D_i\\) un indicador de si la unidad \\(i\\) ha recibido un tratamiento o no. Entonces la diferencia en los resultados promedio entre los que reciben el tratamiento y los que no lo reciben se puede escribir como \\(\\frac{\\sum_i D_i × Y_i (1)} {\\sum_iD_i} - \\frac {\\sum_i(1 - D_i) \\times Y_i (0)}{\\sum_i (1 - D_i)}\\). Sin información sobre cómo se asignó el tratamiento, no hay mucho por decir sobre si esta diferencia es un buen estimador del efecto promedio del tratamiento. Es decir, de la diferencia en los resultados potenciales promedio de las unidades en el grupo de tratamiento y control para todas las unidades. Lo que importa es si \\(\\frac{\\sum_i D_i × Y_i (1)} {\\sum_iD_i}\\) es una buena estimación de \\(\\frac{\\sum_i 1 × Y_i (1)} {\\sum_i1}\\) y si \\(\\frac{\\sum_i (1 - D_i) × Y_i (0)}{\\sum_i(1 - D_i)}\\) es una buena estimación de \\(\\frac{\\sum_i 1 × Y_i (0)} {\\sum_i1}\\). Este puede ser el caso si los que recibieron tratamiento son una muestra representativa de todas las unidades, pero de lo contrario no hay razón para esperar que así sea.↩︎\nEntiéndase la expresión “\\(A\\) causa \\(B\\), en promedio” como “el efecto promedio de \\(A\\) sobre \\(B\\) es positivo”.↩︎\nA veces se reinterpreta la pregunta “causas de los efectos” en el sentido de: ¿cuáles son las causas que tienen efectos sobre las variable de resultado? Véase Andrew Gelman and Guido Imbens, “Why ask why? Forward causal inference and reverse causal questions”, NBER Working Paper No. 19614 (Nov. 2013).↩︎\nVer, por ejemplo, Tian, J., Pearl, J. 2000. “Probabilities of Causation: Bounds and Identification.” Annals of Mathematics and Artificial Intelligence 28:287–313.↩︎"
  },
  {
    "objectID": "guides/causal-inference/causal-inference_en.html",
    "href": "guides/causal-inference/causal-inference_en.html",
    "title": "10 Things You Need to Know About Causal Inference",
    "section": "",
    "text": "For most experimentalists, the statement “\\(X\\) caused \\(Y\\)” means that \\(Y\\) is present and \\(Y\\) would not have been present if \\(X\\) were not present. This definition requires a notion of what could have happened, but did not happen (Holland 1986). Similarly, the “effect” of \\(X\\) on \\(Y\\) is thought of as the difference between the value that \\(Y\\) would have taken given one value of \\(X\\) and the value that \\(Y\\) would have taken given another value of \\(X\\). Because of the focus on differences in outcomes, this approach is sometimes called the “difference making” or “counterfactual” approach to causation.\nTechnical Note: Statisticians employ the “potential outcomes” framework to describe counterfactual relations. In this framework, we let \\(Y_i(1)\\) denote the outcome for unit \\(i\\) that would be observed under one condition (e.g., if unit \\(i\\) received a treatment) and \\(Y_i(0)\\) the outcome that would be observed in another condition (e.g., if unit \\(i\\) did not receive the treatment). One causal effect of the treatment for unit \\(i\\) might be a simple difference of the potential outcomes \\(τ_i=Y_i(1)−Y_i(0)\\). A treatment has a (positive or negative) causal effect on \\(Y\\) for unit \\(i\\) if \\(Y_i(1)≠Y_i(0)\\)."
  },
  {
    "objectID": "guides/causal-inference/causal-inference_en.html#footnotes",
    "href": "guides/causal-inference/causal-inference_en.html#footnotes",
    "title": "10 Things You Need to Know About Causal Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFollowing Mackie (1974), sometimes the idea of “INUS” conditions is invoked to capture the dependency of causes on other causes. Under this account, a cause may be an Insufficient but Necessary part of a condition which is itself Unnecessary but Sufficient. For example dialing a phone number is a cause of contacting someone since having a connection and dialing a number is sufficient (S) for making a phone call, whereas dialing alone without a connection alone would not be enough (I), nor would having a connection (N). There are of course other ways to contact someone without making phone calls (U).↩︎\nTechnical Note: The key technical insight is that the difference of averages is the same as the average of differences. That is, using the “expectations operator,” \\(𝔼(τ_i)=𝔼(Y_i(1)−Y_i(0))=𝔼(Y_i(1))−𝔼(Y_i(0))\\). The terms inside the expectations operator in the second quantity cannot be estimated, but the terms inside the expectations operators in the third quantity can be (Holland 1986). See illustration here.↩︎\nFor this reason \\(t\\)-tests to check whether “randomization worked” do not make much sense, at least if you know that a randomized procedure was followed — just by chance 1 in 20 such tests will show statistically detectable differences between treated and control groups. If there are doubts about whether a randomized procedure was correctly implemented these tests can be used to test the hypothesis that the data was indeed generated by a randomized procedure. This later reason for randomization tests can be especially important in field experiments where chains of communication from the person creating random numbers and the person implementing treatment assignment may be long and complex.↩︎\nTechnical Note: Let \\(D_i\\) be an indicator for whether unit \\(i\\) has received a treatment or not. Then the difference in average outcomes between those that receive the treatment and those that do not can be written as \\(\\frac{∑_i D_i×Y_i(1)}{∑_iD_i}−\\frac{∑_i (1−D_i)×Y_i(0)}{∑_i (1−D_i)}\\). In the absence of information about how treatment was assigned, we can say little about whether this difference is a good estimator of the average treatment effect, i.e., of the difference in average treated and control potential outcomes across all units. What matters is whether \\(\\frac{∑_i D_i×Y_i(1)}{∑_iD_i}\\) is a good estimate of \\(\\frac{∑_i 1×Y_i(1)}{∑_i1}\\) and whether \\(\\frac{∑_i (1−D_i)×Y_i(0)}{∑_i (1−D_i)}\\) is a good estimate of \\(\\frac{∑_i 1×Y_i(0)}{∑_i1}\\). This might be the case if those who received treatment are a representative sample of all units, but otherwise there is no reason to expect that it would be.↩︎\nInterpret “\\(A\\) causes \\(B\\), on average” as “the average effect of \\(A\\) on \\(B\\) is positive.”↩︎\nSome reinterpret the “causes of effects” question to mean: what are the causes that have effects on outcomes. See Gelman and Imbens (2013).↩︎\nSee, for example, Tian and Pearl (2000)↩︎"
  },
  {
    "objectID": "guides/causal-inference/causal-inference.fr.html#footnotes",
    "href": "guides/causal-inference/causal-inference.fr.html#footnotes",
    "title": "10 choses à savoir sur l’inférence causale",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHolland, Paul W. “Statistics and causal inference.” Journal of the American Statistical Association 81.396 (1986): 945-960.↩︎\nHolland, Paul W. “Statistics and causal inference.” Journal of the American Statistical Association 81.396 (1986): 945-960.↩︎\nCertains appellent cela le “problème des causes de prodigalité”.↩︎\nMackie a présenté l’idée de conditions dites “INSS” (“INUS” en anglais) pour capturer la dépendance des causes sur d’autres causes. Une cause peut être une partie Insuffisante mais Nécessaire d’une condition qui est elle-même Superflue mais Suffisante. Par exemple, composer un numéro de téléphone est une cause de “contacter quelqu’un” car avoir une connexion et composer un numéro est suffisant (S) pour passer un appel téléphonique, alors que composer seul sans connexion ne suffirait pas (I), ni avoir un connexion (N). Il existe bien sûr d’autres moyens de contacter quelqu’un sans passer d’appels téléphoniques (S). Mackie, John L. “The cement of the universe.” London: Oxford Uni (1974).↩︎\nNote technique : La principale idée technique est que la différence des moyennes est la même que la moyenne des différences. C’est-à-dire, en utilisant “l’opérateur d’espérance”, \\(𝔼(τ_i)=𝔼(Y_i(1)−Y_i(0))=𝔼(Y_i(1))−𝔼(Y_i(0))\\). Les termes à l’intérieur de l’opérateur d’espérance dans la deuxième quantité ne peuvent pas être estimés, mais les termes à l’intérieur de l’opérateur d’espérance dans la troisième quantité peuvent l’être.5Voir l’illustration ici.↩︎\nPour cette raison, les tests-\\(t\\) pour vérifier si “la randomisation a fonctionné” n’ont pas beaucoup de sens, du moins si vous savez qu’une procédure randomisée a été suivie — simplement par hasard, 1 test sur 20 montrera des différences statistiquement détectables entre les groupes de traitement et de contrôle. En cas de doute sur la mise en œuvre correcte d’une procédure randomisée, ces tests peuvent être utilisés pour tester l’hypothèse selon laquelle les données ont bien été générées par une procédure randomisée. Ces tests peuvent alors être particulièrement importants pour des expériences de terrain où les chaînes de communication entre la personne randomisant et la personne mettant en œuvre l’assignation du traitement peuvent être longues et complexes.↩︎\nNote technique: soit \\(D_i\\) un indicateur pour savoir si l’unité \\(i\\) a reçu un traitement ou non. Alors, la différence des résultats moyens entre ceux qui reçoivent le traitement et ceux qui ne le reçoivent pas peut s’écrire \\(\\frac{∑_i D_i×Y_i(1)}{∑_iD_i}−\\frac{∑_i (1−D_i)× Y_i(0)}{∑_i (1−D_i)}\\). En l’absence d’informations sur la manière dont le traitement a été assigné, nous ne pouvons pas dire si cette différence est un bon estimateur de l’effet moyen du traitement, c’est-à-dire de la différence entre les résultats potentiels moyens pour les groupes de traitement et de contrôle pour toutes les unités. Ce qui importe est de savoir si \\(\\frac{∑_i D_i×Y_i(1)}{∑_iD_i}\\) est une bonne estimation de \\(\\frac{∑_i 1×Y_i(1)}{∑_i1}\\) et si \\(\\frac {∑_i (1−D_i)×Y_i(0)}{∑_i (1−D_i)}\\) est une bonne estimation de \\(\\frac{∑_i 1×Y_i(0)}{∑_i1}\\). Cela pourrait être le cas si ceux qui ont reçu un traitement sont un échantillon représentatif de toutes les unités, mais sinon il n’y a aucune raison de s’attendre à ce qu’il le soit.↩︎\nInterprétez “\\(A\\) cause \\(B\\), en moyenne” comme “l’effet moyen de \\(A\\) sur \\(B\\) est positif”.↩︎\nCertains réinterprètent la question des “causes des effets” comme suit : quelles sont les causes qui ont des effets sur les résultats. Voir Andrew Gelman and Guido Imbens, “Why ask why? Forward causal inference and reverse causal questions”, NBER Working Paper No. 19614 (Nov. 2013).↩︎\nVoir, par exemple, Tian, J., Pearl, J. 2000. “Probabilities of Causation: Bounds and Identification.” Annals of Mathematics and Artificial Intelligence 28:287–313.↩︎"
  },
  {
    "objectID": "guides/assessing-designs/power.fr.html",
    "href": "guides/assessing-designs/power.fr.html",
    "title": "10 choses à savoir sur la puissance statistique",
    "section": "",
    "text": "La puissance est la capacité de distinguer le signal du bruit.\nLe signal qui nous intéresse est l’impact d’un traitement sur certains résultats. L’éducation augmente-t-elle les revenus ? Les campagnes de santé publique diminuent-elles l’incidence des maladies ? Les observateurs internationaux peuvent-ils réduire la corruption gouvernementale ?\nLe bruit qui nous préoccupe provient de la complexité du monde. Les résultats varient selon les personnes et les lieux pour une myriade de raisons. En termes statistiques, vous pouvez considérer cette variation comme l’écart type de la variable de résultat. Par exemple, supposons qu’une expérience utilise les taux d’une maladie rare comme résultat. Le nombre total de personnes touchées ne fluctue probablement pas énormément d’un jour à l’autre, ce qui signifie que le bruit de fond dans cet environnement sera faible. Lorsque le bruit est faible, les expériences peuvent détecter même de petits changements dans les résultats moyens. Un traitement qui réduirait l’incidence de la maladie de 1% serait facilement détecté, car les taux de base sont constants.\nSupposons maintenant qu’une expérience utilise plutôt le revenu des sujets comme variable de résultat. Les revenus peuvent varier assez considérablement - dans certains endroits, il n’est pas rare que les gens aient des voisins qui gagnent deux, dix ou cent fois leur salaire quotidien. Lorsque le bruit est élevé, les expériences sont plus difficiles. Un traitement qui augmenterait les revenus des travailleurs de 1% serait difficile à détecter, car les revenus diffèrent tellement en premier lieu.\nUne préoccupation majeure avant de se lancer dans une expérience est le danger d’un faux négatif. Supposons que le traitement ait réellement un impact causal sur les résultats. Il serait dommage de se donner autant de peine et de dépenser autant pour randomiser le traitement, collecter des données sur les groupes de traitement et de contrôle et analyser les résultats, pour que l’effet soit finalement noyé par le bruit de fond.\nSi nos expériences ont une grande puissance statistique, nous pouvons être sûrs de ceci : s’il y a vraiment un effet de traitement, nous pourrons le voir.",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/fr.png' width=20px>&nbsp;&nbsp;French"
    ]
  },
  {
    "objectID": "guides/assessing-designs/power.fr.html#footnotes",
    "href": "guides/assessing-designs/power.fr.html#footnotes",
    "title": "10 choses à savoir sur la puissance statistique",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFormule reproduite de Gerber et Green 2012, page 93↩︎\nPour un outil supplémentaire de visualisation de puissance en ligne, consultez le blog R Psychologist de Kristoffer Magnusson.↩︎",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/fr.png' width=20px>&nbsp;&nbsp;French"
    ]
  },
  {
    "objectID": "guides/assessing-designs/external-validity.fr.html",
    "href": "guides/assessing-designs/external-validity.fr.html",
    "title": "10 choses à savoir sur la validité externe",
    "section": "",
    "text": "La validité externe est un autre nom pour la généralisation des résultats, demandant “si une relation causale tient pour une variation des personnes, des paramètres, des traitements et des résultats”.1 Un exemple classique de problème de validité externe est de savoir si les expériences traditionnelles en laboratoire d’économie ou de psychologie menées sur des étudiants universitaires produisent des résultats généralisables au grand public. Dans l’économie politique du développement, nous pourrions considérer comment un programme de développement communautaire en Inde pourrait s’appliquer (ou non) en Afrique de l’Ouest ou en Amérique centrale.\nLa validité externe devient particulièrement importante lors de la formulation de recommandations politiques issues de la recherche. L’extrapolation des effets causaux d’une ou plusieurs études à un contexte politique donné nécessite un examen attentif à la fois de la théorie et des preuves empiriques. Ce guide des méthodes aborde certains concepts clés, les pièges à éviter et les références utiles à prendre en compte lors du passage d’un effet moyen local du traitement au monde plus vaste.",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/fr.png' width=20px>&nbsp;&nbsp;French"
    ]
  },
  {
    "objectID": "guides/assessing-designs/external-validity.fr.html#footnotes",
    "href": "guides/assessing-designs/external-validity.fr.html#footnotes",
    "title": "10 choses à savoir sur la validité externe",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference. Houghton, Mifflin and Company.↩︎\nCampbell, D. T. (1957). Factors relevant to the validity of experiments in social settings. Psychological bulletin, 54(4), 297.↩︎\nPlus de détails disponibles dans le guide des méthodes d’inférence causale: http://egap.org/resource/10-things-to-know-about-causal-inference↩︎\nImbens, G. (2013). Book Review Feature: Public Policy in an Uncertain World: By Charles F. Manski. The Economic Journal,123(570), F401-F411.↩︎\nManski, C. F. (2013). Response to the review of ‘public policy in an uncertain world’. The Economic Journal 123: F412–F415.↩︎\nSamii, Cyrus. (2016). “Causal Empiricism in Quantitative Research.” Journal of Politics 78(3):941–955.↩︎\nRosenbaum, Paul R. (1999). “Choice as an Alternative to Control in Observational Studies” (avec discussion). Statistical Science 14(3): 259–304.↩︎\nGerber, A. S., & Green, D. P. (2012). Field experiments: Design, analysis, and interpretation. WW Norton.↩︎\nBisbee, James; Rajeev Dehejia; Cristian Pop-Eleches & Cyrus Samii. (2016). “Local Instruments, Global Extrapolation: External Validity of the Labor Supply-Fertility Local Average Treatment Effect.” Journal of Labor Economics↩︎\nBisbee, James; Rajeev Dehejia; Cristian Pop-Eleches & Cyrus Samii. (2016). “Local Instruments, Global Extrapolation: External Validity of the Labor Supply-Fertility Local Average Treatment Effect.” Journal of Labor Economics↩︎\nKern, H. L., Stuart, E. A., Hill, J., & Green, D. P. (2016). Assessing methods for generalizing experimental impact estimates to target populations. Journal of Research on Educational Effectiveness, 9(1), 103-127.↩︎\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference. Houghton, Mifflin and Company.↩︎\nOpen Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716.↩︎",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/fr.png' width=20px>&nbsp;&nbsp;French"
    ]
  },
  {
    "objectID": "guides/assessing-designs/power.es.html",
    "href": "guides/assessing-designs/power.es.html",
    "title": "10 cosas que debes saber sobre el poder estadístico",
    "section": "",
    "text": "El poder estadístico sirve para diferenciar la señal del ruido.\nLa señal en la que estamos interesados es el impacto de un tratamiento sobre alguna variable de resultado. ¿Podemos decir que entre más alto nivel de educación mayor el ingreso? ¿Reducen las campañas de salud pública la incidencia de enfermedades? ¿Puede el monitoreo internacional disminuir la corrupción gubernamental?\nEl ruido que nos importa proviene de la complejidad del mundo. Las variables de resultado varían según las personas y los lugares por innumerables razones. En términos estadísticos, puede pensar en esta variación como la desviación estándar de la variable de resultado. Por ejemplo, suponga que para un experimento, la variable de resultado que nos interesa es la tasa de una enfermedad rara. Es poco probable que el número total de personas afectadas fluctúe mucho día a día, lo que significa que el ruido de fondo en este caso sería bajo. Cuando el ruido es bajo, los experimentos pueden detectar incluso cambios pequeños en la variable de resultado promedio. Un tratamiento que reduzca la incidencia de la enfermedad en un 1% en puntos porcentuales se detectaría fácilmente, porque las tasas de referencia son constantes.\nAhora suponga que en un experimento se utiliza el ingreso de los sujetos como variable de resultado. El nivel de ingresos pueden variar mucho; no es raro que en algunos lugares las personas tengan vecinos que ganan dos, diez o cien veces el valor de su salario diario. Cuando el ruido es alto, tenemos mayores dificultades para detectar cambios. Un tratamiento que incremente los ingresos de los trabajadores en un 1% sería difícil de detectar, porque los ingresos difieren mucho en primer lugar.\nUna de las principales preocupaciones antes de embarcarse en un experimento es el peligro de un falso negativo. Suponga que el tratamiento realmente tiene un impacto causal en los resultados. Sería una pena tomarse todas las molestias y los gastos de aleatorizar el tratamiento, recopilar datos sobre los grupos de tratamiento y de control y analizar los resultados, solo para que el efecto se vea abrumado por el ruido de fondo.\nSi nuestros experimentos tienen suficiente poder, podemos estar seguros de que si realmente hay un efecto del tratamiento, seremos capaces de observarlo.",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/es.png' width=20px>&nbsp;&nbsp;Spanish"
    ]
  },
  {
    "objectID": "guides/assessing-designs/power.es.html#footnotes",
    "href": "guides/assessing-designs/power.es.html#footnotes",
    "title": "10 cosas que debes saber sobre el poder estadístico",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nReproducido de Gerber y Green 2012, página 93↩︎\nPuede encontrar una herramienta visual para el cálculo del poder en Kristoffer Magnusson’s R Psychologist blog.↩︎",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/es.png' width=20px>&nbsp;&nbsp;Spanish"
    ]
  },
  {
    "objectID": "guides/how-to-read.html",
    "href": "guides/how-to-read.html",
    "title": "How to Read the Methods Guides",
    "section": "",
    "text": "1 Who are these for?\n\nacademic researchers\nmeasurement and evaluation practitioners\npartner organizations\npolicymakers\n\n\n\n2 How each guide is organized\n\nfirst two sections are at a general level\nwhat?\nwhy?\ndetails\nhow? [code]\n\n\n\n3 Getting set up to use the code\n\ninstall.packages(c(\"tidyverse\", \"estimatr\", \"DeclareDesign\"))\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "guides/planning/pap.en.html",
    "href": "guides/planning/pap.en.html",
    "title": "10 Things to Know About Pre-Analysis Plans",
    "section": "",
    "text": "A PAP is a document that formalizes and declares the design and analysis plan for your study. It is written before the analysis is conducted and is generally registered on a third-party website.1\nThe objectives of the PAP are to improve research design choices, increase research transparency, and allow other scholars to replicate your analysis. As a result, we recommend focusing the PAP on analytic details that will help you analyze your study and allow other researchers to replicate your analysis. A brief section on theory should be included insofar as it helps articulate hypotheses, but a detailed theory and literature review need not be included. The PAP does not need to include the front-end of an academic paper if these sections do not help you think about your analysis or help readers replicate your analysis.\nIn the following sections, we provide guidelines for the details you should include in PAPs, including example text. We also recommend that you include as much code and analysis of simulated data as possible.2 Many PAPs will not be able to include everything on our list, but a PAP should, at a minimum, include the full list of hypotheses that you intend to test, how you will measure variables relevant to those hypotheses, and a verifiable time stamp.",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/gb.png' width=20px>&nbsp;&nbsp;English"
    ]
  },
  {
    "objectID": "guides/planning/pap.en.html#footnotes",
    "href": "guides/planning/pap.en.html#footnotes",
    "title": "10 Things to Know About Pre-Analysis Plans",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPAPs are encouraged as part of the Transparency and Openness Promotion (TOP) Guidelines (Nosek et al. 2015), published in Science, with leading social science journals committing to implementing TOP Guidelines.↩︎\nResources like the DeclareDesign project can assist you with simulating and analyzing fake data that mimics the real data your project will gather. Power analysis, an important component of a PAP, requires simulated data.↩︎\nAs R.A. Fisher said and has often been requoted, “Make your theories elaborate…when constructing a causal hypothesis one should envisage as many different consequences of the truth as possible,” (Cochran (1965); cited in Rosenbaum (2010), pp. 327). Though this was said about determining causation in observational studies, the logic also applies to experimental studies.↩︎\nFor an example of an omnibus test, see Caughey, Dafoe, and Seawright (2017).↩︎\nYou may also be interested in estimating other types of effects. See this guide on types of treatment effects for more information about effect types.↩︎\nFor example, you could calculate standard errors and \\(p\\)-values using permutation-based randomization inference. Or you could closely approximate standard errors and \\(p\\)-values using analytic methods as in Samii and Aronow (2012).↩︎\nNote that some organizations do not use third party sites. For example, the U.S. General Services Administration Office of Evaluation Sciences process uses Github, which has timestamps that verify the PAP was created before the analysis was conducted.↩︎",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/gb.png' width=20px>&nbsp;&nbsp;English"
    ]
  },
  {
    "objectID": "guides/planning/survey-design_en.html",
    "href": "guides/planning/survey-design_en.html",
    "title": "10 Things to Know About Survey Design",
    "section": "",
    "text": "Surveys are the most frequently-used tool for collecting experimental data in social science research, and the design of these surveys can have a profound effect on the conclusions we draw about the treatments we study. Therefore, the stakes are high when designing surveys around your experimental projects.\nAt a minimum, all that is needed to estimate a treatment effect after an experiment has been conducted is a measure of the outcome, collected after the treatment has been delivered, with a sufficient number of observations across the treatment and control groups. If time and budget allow, baseline surveys, conducted before the implementation of an experiment, serve important functions as well and can improve analysis greatly.\n\n\nBaseline surveys should produce data that describe the experimental subject population as they were before the treatment is delivered. This is accomplished through the measurement of covariates, which are observed pre-treatment characteristics of experimental subjects. Using covariate data you can: 1) describe the subject population, 2) improve the precision with which you estimate treatment effects, 3) report balance, and 4) estimate heterogeneous treatment effects.\n\n\n\nCovariates improve the precision with which you can estimate treatment effects by reducing variance in three ways; covariates can be used to rescale your dependent variable, as controls when using regression to estimate treatment effects, and to construct blocks in order to conduct blocked random assignment.1 In order for covariate data to be used to reduce variance in our estimates of treatment effects, they need to be unaffected by treatment assignment, i.e. collected sometime before treatment is delivered. See the guide on covariate adjustment for more about how to use covariate data.\nThe greater the predictive power of included covariates, the greater increase in the power of your design and the precision with which you can estimate effects. If you believe covariates will likely predict outcomes in your experiment, then that is grounds to include them in your survey. For example, if the intervention involves providing a service at a cost to treated users, income will likely explain some variation in outcomes and is therefore a useful covariate to measure at the baseline stage.\nBecause pre-treatment covariates can improve precision, conducting a baseline becomes more important when the sample size is limited.\nCovariates also allow you to conduct sub-group analyses. Heterogeneous effects are not causal, and so interpretation is limited. Still, understanding how treatment effects vary by subejcts’ attributes can provide you with important clues about mechanisms. The implication for design is to include covariates for which you would like to report heterogeneous effects. Covariate data will also be used to show “balance”, or the extent to which the treatment and control populations resemble each other. Although random assignment alone ensures that outcomes across the treatment and control group are in expectation the same, it is standard practice to show that random assignment resulted in two groups that are “balanced” on covariates of interest. If, for example, the treatment group included 25% more men than the control group, we might worry that random assignment failed in some way. Collecting pre-treatment covariate data allows us to evaluate and report balance.\n\n\n\nThe baseline provides an opportunity to measure the outcome before the experiment was conducted, later allowing you to use change scores as your outcome and the difference-in-differences estimator. The difference-in-differences estimator will improve precision only when a covariate strongly predicts outcomes.2\n\n\n\nEndline surveys, conducted after the treatment is delivered, are primarily used to measure outcomes. Including questions about implementation can improve analysis and interpretation greatly.\nSurveys conducted after treatments are delivered are one way to understand if there were compliance issues or other implementation issues that may have consequences for analysis. Survey data can help to determine the scope of non-compliance with the assigned treatment, and the underlying causes. This is an opportunity to ask subjects directly about reasons for noncompliance. Subjects may have understood the treatment differently than the researchers, and survey data can be used to both show this and speculate why this may have happened.\nIn the endline, you can learn about spillover by asking subjects in the control condition about their knowledge and access to the treatment. Interviews with treated subjects are useful for understanding spillover as well, because survey data can be used to understand the networks through which the treatment could have “spilled-over” into the control group.\nDescribing the population is important here as well, but covariate data collected after implementation are less useful for improving precision. Ordinarily, covariates collected after treatment assignment are considered suspect, as they could conceivably be affected by treatment.\n\n\n\n\n\n\n\nBaseline checklist\nEndline checklist\n\n\n\n\nWill your data allow you to:\nWill your data allow you to:\n\n\n• Describe the population\n• Estimate effects\n\n\n• Adjust treatment effect estimates (are the covariates included likely to be prognostic of outcomes?)\n• Assess if spillover occurred, or if there was interference\n\n\n• Estimate heterogeneous treatment effects\n• Measure non-compliance (and the reasons for non-compliance if it occurred)\n\n\n• Design a blocked randomization procedure\n• Look for causal mechanisms (how are effects transmitted?)\n\n\n• Describe balance across treatment and control conditions"
  },
  {
    "objectID": "guides/planning/survey-design_en.html#baseline-surveys",
    "href": "guides/planning/survey-design_en.html#baseline-surveys",
    "title": "10 Things to Know About Survey Design",
    "section": "",
    "text": "Baseline surveys should produce data that describe the experimental subject population as they were before the treatment is delivered. This is accomplished through the measurement of covariates, which are observed pre-treatment characteristics of experimental subjects. Using covariate data you can: 1) describe the subject population, 2) improve the precision with which you estimate treatment effects, 3) report balance, and 4) estimate heterogeneous treatment effects."
  },
  {
    "objectID": "guides/planning/survey-design_en.html#covariates",
    "href": "guides/planning/survey-design_en.html#covariates",
    "title": "10 Things to Know About Survey Design",
    "section": "",
    "text": "Covariates improve the precision with which you can estimate treatment effects by reducing variance in three ways; covariates can be used to rescale your dependent variable, as controls when using regression to estimate treatment effects, and to construct blocks in order to conduct blocked random assignment.1 In order for covariate data to be used to reduce variance in our estimates of treatment effects, they need to be unaffected by treatment assignment, i.e. collected sometime before treatment is delivered. See the guide on covariate adjustment for more about how to use covariate data.\nThe greater the predictive power of included covariates, the greater increase in the power of your design and the precision with which you can estimate effects. If you believe covariates will likely predict outcomes in your experiment, then that is grounds to include them in your survey. For example, if the intervention involves providing a service at a cost to treated users, income will likely explain some variation in outcomes and is therefore a useful covariate to measure at the baseline stage.\nBecause pre-treatment covariates can improve precision, conducting a baseline becomes more important when the sample size is limited.\nCovariates also allow you to conduct sub-group analyses. Heterogeneous effects are not causal, and so interpretation is limited. Still, understanding how treatment effects vary by subejcts’ attributes can provide you with important clues about mechanisms. The implication for design is to include covariates for which you would like to report heterogeneous effects. Covariate data will also be used to show “balance”, or the extent to which the treatment and control populations resemble each other. Although random assignment alone ensures that outcomes across the treatment and control group are in expectation the same, it is standard practice to show that random assignment resulted in two groups that are “balanced” on covariates of interest. If, for example, the treatment group included 25% more men than the control group, we might worry that random assignment failed in some way. Collecting pre-treatment covariate data allows us to evaluate and report balance."
  },
  {
    "objectID": "guides/planning/survey-design_en.html#pre-treatment-measurement-of-outcomes",
    "href": "guides/planning/survey-design_en.html#pre-treatment-measurement-of-outcomes",
    "title": "10 Things to Know About Survey Design",
    "section": "",
    "text": "The baseline provides an opportunity to measure the outcome before the experiment was conducted, later allowing you to use change scores as your outcome and the difference-in-differences estimator. The difference-in-differences estimator will improve precision only when a covariate strongly predicts outcomes.2"
  },
  {
    "objectID": "guides/planning/survey-design_en.html#endline-surveys",
    "href": "guides/planning/survey-design_en.html#endline-surveys",
    "title": "10 Things to Know About Survey Design",
    "section": "",
    "text": "Endline surveys, conducted after the treatment is delivered, are primarily used to measure outcomes. Including questions about implementation can improve analysis and interpretation greatly.\nSurveys conducted after treatments are delivered are one way to understand if there were compliance issues or other implementation issues that may have consequences for analysis. Survey data can help to determine the scope of non-compliance with the assigned treatment, and the underlying causes. This is an opportunity to ask subjects directly about reasons for noncompliance. Subjects may have understood the treatment differently than the researchers, and survey data can be used to both show this and speculate why this may have happened.\nIn the endline, you can learn about spillover by asking subjects in the control condition about their knowledge and access to the treatment. Interviews with treated subjects are useful for understanding spillover as well, because survey data can be used to understand the networks through which the treatment could have “spilled-over” into the control group.\nDescribing the population is important here as well, but covariate data collected after implementation are less useful for improving precision. Ordinarily, covariates collected after treatment assignment are considered suspect, as they could conceivably be affected by treatment.\n\n\n\n\n\n\n\nBaseline checklist\nEndline checklist\n\n\n\n\nWill your data allow you to:\nWill your data allow you to:\n\n\n• Describe the population\n• Estimate effects\n\n\n• Adjust treatment effect estimates (are the covariates included likely to be prognostic of outcomes?)\n• Assess if spillover occurred, or if there was interference\n\n\n• Estimate heterogeneous treatment effects\n• Measure non-compliance (and the reasons for non-compliance if it occurred)\n\n\n• Design a blocked randomization procedure\n• Look for causal mechanisms (how are effects transmitted?)\n\n\n• Describe balance across treatment and control conditions"
  },
  {
    "objectID": "guides/planning/survey-design_en.html#gathering-behavioral-data-doesnt-have-to-be-expensive.-here-is-how-to-develop-low-cost-measures",
    "href": "guides/planning/survey-design_en.html#gathering-behavioral-data-doesnt-have-to-be-expensive.-here-is-how-to-develop-low-cost-measures",
    "title": "10 Things to Know About Survey Design",
    "section": "5.1 Gathering behavioral data doesn’t have to be expensive. Here is how to develop low-cost measures:",
    "text": "5.1 Gathering behavioral data doesn’t have to be expensive. Here is how to develop low-cost measures:\n\nBrainstorm a set of actions that subjects would do if the treatment had had an effect or would not do in the case that the treatment did not have an effect. It also works to think about behavior on a continuum—i.e., what would people be more likely to do if affected, and less likely to do if not? Local context matters a lot here; rely on your enumerators, local survey staff, or implementation partners to help you think through a set of possibilities. One nice way to think about this is to challenge yourself to think about “hints.” In the above example, we might think about a group of opposition activists wearing wristbands publicly as a “hint” that people are more likely to take risks. Keep in mind your eventual audience: what behaviors are frequently tracked in the literature you hope to speak to?\nIsolate the set of behaviors that are feasible to measure. This will most likely be the set of behaviors that can be immediately observed by the enumerator and involve minimal materials. What is the least expensive or costly action that would be associated with the behavioral change you want to detect?\nIdeally, pre-test the measures either with the rest of your survey, or in smaller focus groups. Learning why respondents did or did not behave a certain way will increase confidence in your results."
  },
  {
    "objectID": "guides/planning/survey-design_en.html#how-do-you-construct-questions-that-accomplish-these-goals",
    "href": "guides/planning/survey-design_en.html#how-do-you-construct-questions-that-accomplish-these-goals",
    "title": "10 Things to Know About Survey Design",
    "section": "7.1 How do you construct questions that accomplish these goals?",
    "text": "7.1 How do you construct questions that accomplish these goals?\n\nUse the simplest possible form of each question, using the most widely-understood words. Avoid jargon or technical terms, and be straightforward and brief.\nBe specific, such that if the question were to be lifted from the section and asked without context you would get the same response.\nIt helps to begin the question by providing a context. For example, you can prime a time period (“Thinking of the last year: has your income been better, the same, or worse?”), or a place (“Thinking of people in this village: have people earned more or less this year as compared to last year?”)\nAvoid measuring multiple things at once. For example, the following question measures attitudes about both the president and government concurrently, making it difficult to draw a clear conclusion from the data: “Do you think the president and the government are doing a good job in terms of protecting basic freedoms?”\nWhen constructing response categories, be as comprehensive as possible. Include all possible responses. You don’t want to record a lot of “don’t know” responses and miss important information. See below for a discussion of scales.\nKeep in mind the concerns with social desirability discussed above; the wording of the question shouldn’t lead the respondent towards a certain response."
  },
  {
    "objectID": "guides/planning/survey-design_en.html#footnotes",
    "href": "guides/planning/survey-design_en.html#footnotes",
    "title": "10 Things to Know About Survey Design",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Gerber and Green (2012).↩︎\nSee Gerber and Green (2012), equation 4.2.↩︎\nLink to fishing article: http://www.columbia.edu/~mh2245/papers1/PA_2012b.pdf. Link to Casey et al: http://qje.oxfordjournals.org/content/127/4/1755.full.pdf+html↩︎\nIf employing multiple survey measures for the same outcome; pre-commit to how you will analyze them e.g. in an index/with accounting for multiple comparisons etc. to avoid cherry-picking measures post-hoc.↩︎\nSee Young (2018).↩︎\nThis approach involves estimating the upper and lower “bounds,” which are the largest and smallest ATEs we would obtain if the missing information were filled in with the highest and lowest outcomes that appear in the data we have.↩︎"
  },
  {
    "objectID": "guides/research-questions/mechanisms.es.html",
    "href": "guides/research-questions/mechanisms.es.html",
    "title": "10 cosas sobre los mecanismos",
    "section": "",
    "text": "Desde hace ya mucho tiempo, los mecanismos han sido parte principal de la medicina. Cada vez que un médico prescribe un tratamiento, lo hace a partir de la comprensión de los factores químicos o físicos que causan una enfermedad, y prescribe un tratamiento que es eficaz porque interrumpe estos factores. Por ejemplo, muchos psicólogos clínicos recomiendan hacer ejercicio a los pacientes que sufren depresión. El ejercicio aumenta las endorfinas en la química del cuerpo, lo cual desencadena estados de ánimo positivos y actúan como analgésicos, que reducen el dolor. Las endorfinas, por lo tanto, son un mecanismo por el cual el ejercicio ayuda a reducir la depresión. El ejercicio puede tener efectos positivos sobre otras variables dependientes (por ejemplo, las enfermedades cardíacas) a través de otros mecanismos (por ejemplo, elevando la frecuencia cardíaca), pero el mecanismo que influye en la depresión en particular, son las endorfinas. También podríamos concluir que otro tratamiento, como un fármaco que eleve las endorfinas, pueda tener efectos similares sobre la depresión.\nLos mecanismos son igual de importantes para las ciencias sociales. Consideremos como ejemplo las investigaciones recientes que han relacionado el cambio climático con un aumento de los conflictos civiles. Un estudio1 busca identificar el efecto causal de las perturbaciones climáticas en los conflictos violentos al estudiar la tasa de conflictos civiles en los países afectados por El Niño; comparando los años en que estos países sufrieron El Niño frente a los años en los que no los sufrieron. Supongamos que este estudio es correcto. ¿Por qué una perturbación climática podría aumentar los conflictos en un país? Un mecanismo podría ser la pobreza: las perturbaciones climáticas perjudican a la economía y, al reducirse los costos de oportunidad, los individuos son más propensos a unirse a grupos armados. Un mecanismo alternativo es el fisiológico: las personas están físicamente predispuestas a ser más agresivas con temperaturas más altas. Tal vez el mecanismo sea la migración: las perturbaciones climáticas desplazan a las personas que habitan regiones costeras, lo que produce un conflicto social entre los migrantes y los nativos. En realidad todos estos mecanismos (así como otros no enumerados aquí) podrían estar operando simultáneamente ¡Incluso en el mismo caso! En muchas de las preguntas más interesantes de las ciencias sociales, hay varios canales (“M”) que podrían transmitir el efecto total de X sobre Y.",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/es.png' width=20px>&nbsp;&nbsp;Spanish"
    ]
  },
  {
    "objectID": "guides/research-questions/mechanisms.es.html#footnotes",
    "href": "guides/research-questions/mechanisms.es.html#footnotes",
    "title": "10 cosas sobre los mecanismos",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSolomon M. Hsiang, Kyle C. Meng y Mark A. Cane, “Civil Conflicts Are Associated with the Global Climate”, Nature 476.7361 (2011): 438-441.↩︎\nTed Gurr, Why Men Rebel, Princeton University Press, 1970.↩︎\nAlberto Chong, Ana L. de la O, Dean Karlan y Leonard Wantchekon, “Does Corruption Information Inspire the Fight or Quash the Hope? A Field Experiment in Mexico on Voter Turnout, Choice, and Party Identification”, The Journal of Politics 77.1 (2015): 55-71.↩︎\nNótese que Chong et al. prueban su argumento utilizando datos a nivel de distrito electoral, no a nivel individual, pero hemos adaptado su argumento al nivel individual para facilitar la explicación.↩︎\nPara una discusión más rigurosa sobre estas falacias, léase Adam N. Glynn, “The Product and Difference Fallacies for Indirect Effects”, American Journal of Political Science 56.1 (2012): 257-269.↩︎\nExplicación adaptada de Alan Gerber y Donald Green, Field Experiments, W.W. Norton and Company, 2012, capítulo 10.↩︎\nHeather Sarsons, “Rainfall and Conflict: A Cautionary Tale”. Journal of Development Economics 115 (2015): 62-72.↩︎\nMarion Joseph Levy, Modernization and the Structure of Societies, Princeton University Press, 1966.↩︎\nFrantz Fanon, The Wretched of the Earth, Grove Press, 1964. John Lott, Jr., “Public Schooling, Indoctrination and Totalitarianism,” Journal of Political Economy 107(6), 1999.↩︎\nGabriel Almond and Sidney Verba, The Civic Culture: Political Attitudes and Democracy in Five Nations, Sage Publications, 1963. Robert Mattes and Michael Bratton, “Learning about Democracy in Africa: Awareness, Performance, and Experience,” American Journal of Political Science, 51(1), 2007.↩︎\nWilla Friedman, Michael Kremer, Edward Miguel, and Rebecca Thornton, “Education as Liberation?” NBER Working Paper 16939, 2011.↩︎\nEn el estudio actual, los autores se sorprendieron al descubrir evidencias de que la educación también aumentaba la aceptación de la violencia política por parte de los individuos. Aunque siguen sosteniendo que el empoderamiento individual es el responsable de la relación entre educación y democracia, advierten que la educación no siempre conduce a la democratización (es decir, M3→Y pero también es posible que M3→NO Y). No obstante, su enfoque es una útil demostración de cómo múltiples variables de resultado pueden aclarar cosas sobre los mecanismos.↩︎\nSarah Baird, Craig McIntosh, Berk Ozler, “Cash or Condition? Evidence from a Cash Transfer Experiment,” Quarterly Journal of Economics 126, 2011.↩︎",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/es.png' width=20px>&nbsp;&nbsp;Spanish"
    ]
  },
  {
    "objectID": "guides/research-questions/effect-types.en.html",
    "href": "guides/research-questions/effect-types.en.html",
    "title": "10 Types of Treatment Effect You Should Know About",
    "section": "",
    "text": "This guide describes estimands, theoretical parameters of interest, of researchers might want to identify. This guide should be used to help researchers choose an estimand of interest to address their specific research question."
  },
  {
    "objectID": "guides/research-questions/effect-types.en.html#footnotes",
    "href": "guides/research-questions/effect-types.en.html#footnotes",
    "title": "10 Types of Treatment Effect You Should Know About",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Imai, King, and Stuart (2008) for a more detailed review of the issues discussed in this section.↩︎\nSee Imbens and Wooldridge (2007).↩︎\nSee 10 Things You Need to Know About the Local Average Treatment Effect for more on non-compliance.↩︎"
  },
  {
    "objectID": "guides/research-questions/late_en.html",
    "href": "guides/research-questions/late_en.html",
    "title": "10 Things You Need to Know About the Local Average Treatment Effect",
    "section": "",
    "text": "When subjects do not receive the treatment to which they were assigned, the experimenter faces a “noncompliance” problem. Some subjects may need the treatment so badly that they will always take up treatment, irrespective of whether they are assigned to the treatment or to the control group. These are called “Always-Takers”. Other subjects may not take the treatment even if they are assigned to the treatment group: the “Never-Takers”. Some subjects are “Compliers”. These are the subjects that do what they are supposed to do: they are treated when assigned to the treatment group, and they are not treated when they are assigned to the control group. Finally, some subjects do the exact opposite of what they are supposed to do. They are called “Defiers”. Table 1 shows these four different types of subjects in the population.\n\nNoncompliance can make it impossible to estimate the average treatment effect (ATE) for the population. For example, say that in a population of 200, 100 people are randomly assigned to treatment and we find that only 80 people are actually treated. What is the impact of the treatment? One method to answer this question is simply to ignore the noncompliance and compare the outcome in the treatment (100 people) and control (100 people) groups. This method estimates the average intention-to-treat effect (ITT). (See our guide on different kinds of treatment effects for more on the ITT.) While informative, this method does not give a measure of the effect of the treatment itself. Another approach would be to compare the 120 really-untreated and 80 really-treated subjects. Doing so, however, might give you biased estimates. The reason is that the 20 subjects that did not comply with their assignment are likely to be a nonrandom subset of those that were assigned to treatment.\nSo what now? In some cases it is possible to estimate the “Local Average Treatment Effect” (LATE), also known as the “Complier Average Causal Effect” (CACE). The LATE is the average treatment effect for the Compliers. Under assumptions discussed below, the LATE equals the ITT effect divided by the share of compliers in the population."
  },
  {
    "objectID": "guides/research-questions/late_en.html#footnotes",
    "href": "guides/research-questions/late_en.html#footnotes",
    "title": "10 Things You Need to Know About the Local Average Treatment Effect",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor more extensive overviews, see: Angrist, Imbens, and Rubin (1996); Angrist (2006); Angrist and Pischke (2009), sections 4.4-4.6 and 6.2; Dunning (2012); Baiocchi, Cheng, and Small (2014) (with correction in Statistics in Medicine 33: 4859-4860).↩︎\nSee Angrist and Evans (1998).↩︎\nSee: Imbens (2010); Imbens and Angrist (1994); Imbens and Wooldridge (2007).↩︎\nSee Hirano et al. (2000).↩︎\nSee Green and Gerber (2002).↩︎\nSee Gerber et al. (2010).↩︎\nSee Qi Long, Little, and Lin (2010); Jin and Rubin (2009); Jin and Rubin (2008).↩︎"
  },
  {
    "objectID": "guides/research-questions/mechanisms.fr.html",
    "href": "guides/research-questions/mechanisms.fr.html",
    "title": "10 choses sur les mécanismes",
    "section": "",
    "text": "Les mécanismes sont depuis longtemps au cœur de la médecine. Chaque fois qu’une médecin prescrit un traitement, elle le fait en sachant quels facteurs chimiques ou physiques provoquent une maladie, et elle prescrit un traitement efficace car il interrompt ces facteurs. Par exemple, de nombreux cliniciens psychologues recommandent l’exercice aux patients souffrant de dépression. L’exercice augmente les endorphines dans la chimie du corps, qui déclenchent des sentiments positifs et agissent également comme analgésiques, ce qui réduit la perception de la douleur. Les endorphines sont donc un mécanisme par lequel l’exercice aide à réduire la dépression. L’exercice peut avoir des effets positifs sur un certain nombre d’autres variables dépendantes (par exemple, les maladies cardiaques) par voie d’autres mécanismes (par exemple, l’élévation du rythme cardiaque), mais le mécanisme qui l’amène à affecter la dépression en particulier est l’endorphine. Nous pourrions également conclure qu’un autre traitement, tel qu’un médicament qui augmente les endorphines, peut avoir des effets similaires sur la dépression.\nLes mécanismes sont tout aussi importants pour les sciences sociales. Prenez, par exemple, des recherches récentes qui ont associé le changement climatique à une augmentation des conflits civils. Une étude1 prétend identifier l’effet causal des chocs climatiques sur les conflits violents en étudiant le taux de conflits civils dans les pays touchés par El Niño pendant les années El Niño par rapport aux années sans El Niño. Supposons que cette étude soit correcte. Pourquoi subir un choc climatique causerait des niveaux de conflit élevés dans un pays ? Un mécanisme pourrait être la pauvreté : les chocs climatiques nuisent à l’économie, et avec des coûts d’opportunité inférieurs, les individus sont plus enclins à rejoindre des groupes armés. Un mécanisme alternatif est physiologique : les gens sont physiquement câblés pour être plus agressifs par temps chaud. Le mécanisme est peut-être la migration : les chocs climatiques déplacent les populations des régions côtières, ce qui produit des conflits sociaux entre migrants et autochtones. En réalité, plusieurs ou tous ces mécanismes (ainsi que d’autres non listés ici) pourraient fonctionner simultanément ! Dans bon nombre des questions les plus intéressantes en sciences sociales, il existe plusieurs canaux (ou mécanismes “M”) qui pourraient transmettre l’effet total de X sur Y.",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/fr.png' width=20px>&nbsp;&nbsp;French"
    ]
  },
  {
    "objectID": "guides/research-questions/mechanisms.fr.html#footnotes",
    "href": "guides/research-questions/mechanisms.fr.html#footnotes",
    "title": "10 choses sur les mécanismes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSolomon M. Hsiang, Kyle C. Meng et Mark A. Cane, “Civil Conflicts Are Associated with the Global Climate,” Nature 476.7361 (2011): 438-441.↩︎\nTed Gurr, Why Men Rebel, Princeton University Press, 1970.↩︎\nAlberto Chong, Ana L. de la O, Dean Karlan et Leonard Wantchekon, “Does Corruption Information Inspire the Fight or Quash the Hope? A Field Experiment in Mexico on Voter Turnout, Choice, and Party Identification,” The Journal of Politics 77.1 (2015): 55-71.↩︎\nNotez que Chong et al. testent leur argument en utilisant des données au niveau de l’arrondissement, pas au niveau individuel, mais nous avons adapté leur argument au niveau individuel pour faciliter l’explication.↩︎\nPour une discussion plus rigoureuse de ces fausses conceptions, voir Adam N. Glynn, “The Product and Difference Fallacies for Indirect Effects,” American Journal of Political Science 56.1 (2012): 257-269.↩︎\nExplication adaptée d’Alan Gerber et Donald Green, Field Experiments, W.W. Norton and Company, 2012, chapter 10.↩︎\nHeather Sarsons, “Rainfall and Conflict: A Cautionary Tale.” Journal of Development Economics 115 (2015): 62-72.↩︎\nMarion Joseph Levy, Modernization and the Structure of Societies, Princeton University Press, 1966.↩︎\nFrantz Fanon, The Wretched of the Earth, Grove Press, 1964. John Lott, Jr., “Public Schooling, Indoctrination and Totalitarianism,” Journal of Political Economy 107(6), 1999.↩︎\nGabriel Almond et Sidney Verba, The Civic Culture: Political Attitudes and Democracy in Five Nations, Sage Publications, 1963. Robert Mattes et Michael Bratton, “Learning about Democracy in Africa: Awareness, Performance, and Experience,” American Journal of Political Science, 51(1), 2007.↩︎\nWilla Friedman, Michael Kremer, Edward Miguel et Rebecca Thornton, “Education as Liberation?” NBER Working Paper 16939, 2011.↩︎\nDans l’étude proprement dite, les auteurs ont été surpris de découvrir des preuves que l’éducation augmentait également l’acceptation de la violence politique par les individus. S’ils soutiennent toujours que l’autonomisation individuelle est responsable de la relation entre l’éducation et la démocratie, ils avertissent que l’éducation ne conduit pas toujours à la démocratisation (c’est-à-dire M3→Y mais il est également possible que M3→NON Y). Néanmoins, leur approche est une démonstration utile de la façon dont de multiples résultats peuvent éclairer les mécanismes.↩︎\nSarah Baird, Craig McIntosh, Berk Ozler, “Cash or Condition? Evidence from a Cash Transfer Experiment,” Quarterly Journal of Economics 126, 2011.↩︎",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/fr.png' width=20px>&nbsp;&nbsp;French"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Methods guides",
    "section": "",
    "text": "Figure 1\n\n\n\nEvidence in Governance and Politics (EGAP) is a global research, evaluation, and learning network that promotes rigorous knowledge accumulation, innovation, and evidence-based policy in various governance and accountability domains.\nOur methods resources are aimed at learners and teachers of all levels. Guides are for self-learning, and our coursebook is for teachers of impact evaluation methods.\n\nMethods guides\nLearn methods for impact evaluations just-in-time to use them with guides for impact evaluation methods – for researchers, evidence users, and students\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About External Validity\n\n\n\nRenard Sexton\n\n\n\n\n\n\n\n\nNo matching items\n\n\nMore…\n\n\nSoftware\nSoftware tools in R and Stata can be useful for designing, implementing, analysis, and reporting on impact evaluations. We collected relevant software and provide a search interface for it.\nMore…\n\n\nTeaching materials\n\n\n\nLearning days host cities (red) and participant home countries (blue).\n\n\nOver the past decade, Evidence in Governance and Politics (EGAP) has organized Learning Days workshops with the aim of building experimental social-science research capacity among principal investigators (PIs) – both researchers and practitioners – in Africa and Latin America. By sharing the practical and statistical methods of randomized field experiments with workshop participants, the Learning Days effort hopes to identify and nurture researcher networks around the world and to create strong, productive connections between these researchers and EGAP members. The Learning Days Book, which grew out of a desire to share the materials we developed for the Learning Days, is a comprehensive overview of causal inference methods for researchers developing an experimental research design.\nMore…\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "guides/research-questions/effect-types.fr.html",
    "href": "guides/research-questions/effect-types.fr.html",
    "title": "10 types d’effets de traitement à connaître",
    "section": "",
    "text": "Nous commençons par examiner comment, avec la randomisation, une simple différence des moyennes fournit une estimation non biaisée de l’ATE. Nous prenons plus de temps pour présenter certains concepts et notations statistiques courants utilisés tout au long de ce guide.\nTout d’abord, nous définissons un effet de traitement pour une observation individuelle (une personne, un ménage, une ville, etc.) comme la différence entre le comportement de cette unité sous le traitement \\(Y_{i}(1)\\) et sous le contrôle \\(Y_{i}( 0)\\) :\n\\[τ_{i}=Y_{i}(1)−Y_{i}(0)\\]\nPuisque nous ne pouvons observer que \\(Y_{i}(1)\\) ou \\(Y_{i}(0)\\), l’effet du traitement individuel est impossible à connaître. Soit \\(D_{i}\\) un indicateur pour savoir si c’est une observation sous traitement ou sous contrôle. Si le traitement est assigné de manière aléatoire, \\(D_{i}\\) est indépendant, non seulement des résultats potentiels mais aussi de toutes les covariables (observées et non observées) qui pourraient également prédire ces résultats \\((Y_{i}(1), Y_{i }(0) et X_{i}⊥⊥D_{i})\\).2\nSupposons que notre conception implique \\(m\\) unités sous traitement et \\(N−m\\) sous contrôle. Supposons que nous devions réassigner à plusieurs reprises le traitement de manière aléatoire et à chaque fois calculer la différence des moyennes entre les groupes de traitement et de contrôle, puis enregistrer cette valeur dans une liste. La moyenne des valeurs de cette liste sera la même que la différence des moyennes des véritables résultats potentiels si nous avions observé tous les résultats potentiels pour toutes les observations.3 Autrement dit, l’estimateur (i.e. la différence des moyennes observées) est un estimateur sans biais de l’effet causal moyen du traitement.\n\\[ATE≡\\frac{1}{N}∑^{N}_{i=1}τ_{i}=\\frac{∑^{N}_{1}Y_{i}(1)}{N}−\\frac{∑^{N}_{1}Y_{i}(0)}{N}\\]\nEt nous estimons souvent l’ATE en utilisant la différence des moyennes observées:4\n\\[\\widehat{ATE} =\\frac{∑^m_1Z_{i}Y_{i}}{m}−\\frac{∑^{N}_{m+1}(1−Z_{i})Y_{i}}{N−m}\\]\nL’inférence statistique sur l’ATE estimé nécessite que nous sachions comment il variera selon les randomisations. Il s’avère que nous pouvons écrire la variance de l’ATE à travers les randomisations comme suit :\n\\[V(ATE) = \\frac{N}{N−1} [\\frac{V(Y_{i}(1))}{m}+\\frac{V(Y_{i}(0))}{N−m}]−\\frac{1}{N−1}[V(Y_{i}(1))+V(Y_{i}(0))−2∗Cov(Y_{i}(1),Y_{i}(0))]\\]\net estimer cette quantité à partir des estimations de l’échantillon de la variance dans chaque groupe.5\nUn modèle linéaire de régression du résultat observé \\(Y_{i}\\) sur un indicateur de traitement \\(D_{i}\\) fournit un estimateur pratique de l’ATE (et avec quelques ajustements supplémentaires, de la variance de l’ATE) :\n\\[Y_{i}=Y_{i}(0)∗(1−D_{i})+Y_{i}(1)∗D_{i}=β_{0}+β_{1}D_{i}+u\\]\npuisque nous pouvons réarranger les termes de sorte que \\(β_{0}\\) représente la moyenne parmi les observations de contrôle \\((Y_{i}(0)∣D_{i}=0)\\) et \\(β_{1}\\) représente la différence des moyennes \\((Y_{i}(1)∣D_{i}=1)–(Y_{i}(1)∣D_{i}=0)\\). Dans le code ci-dessous, nous créons un échantillon de 1 000 observations et assignons de manière aléatoire un traitement Di avec un effet unitaire constant à la moitié des unités. Nous estimons l’ATE en utilisant la méthode des moindres carrés ordinaires (MCO) pour calculer la différence des moyennes observée. Le calcul des moyennes dans chaque groupe et la prise de leur différence produiraient également une estimation non biaisée de l’ATE. Notez que l’ATE estimé à partir de MCO n’est pas biaisé, mais les erreurs dans ce modèle linéaire sont supposées être indépendantes et distribuées de manière identique. Lorsque notre traitement affecte à la fois la valeur moyenne du résultat et la distribution des réponses, cette hypothèse ne tient plus et nous devons ajuster l’erreur type de MCO à l’aide d’un estimateur sandwich Huber-White pour obtenir les estimations correctes (basées sur la variance de l’ATE) pour l’inférence statistique.6 Enfin, nous démontrons également l’absence de biais de ces estimateurs par simulation.\n\nset.seed(1234) # pour la reproductibilité\nN = 1000 # taille de la population\nY0 = runif(N) # Résultat potentiel sous la condition de contrôle\nY1 = Y0 + 1 # Résultat potentiel sous condition de traitement\nD = sample((1:N)%%2) # Traitement: 1 si traité, 0 sinon\nY = D*Y1 + (1-D)*Y0 # Résultat dans la population\nsamp = data.frame(D,Y) \nATE = coef(lm(Y~D,data=samp))[2] # idem avec (samp,mean(Y[Z==1])-mean(Y[Z==0]))\n# SATE avec Neyman ou avec une erreur type justifiée par randomisation\n# qui sont les mêmes que les erreurs types MCO en l'absence de covariables ou de découpage par bloc\nlibrary(lmtest) \nlibrary(sandwich) \nfit&lt;-lm(Y~D,data=samp) \ncoef(summary(fit))[\"D\",1:2]\n\n  Estimate Std. Error \n1.01820525 0.01841784 \n\nATE.se&lt;-coeftest(fit,vcovHC(fit,type=\"HC2\"))[\"D\",2] \n# idem avec (samp,sqrt(var(Y[D==1])/sum(D)+var(Y[D==0])/(n-sum(D)))\n# Évaluer le biais et simuler l'erreur type\ngetATE&lt;-function() {\n  D = sample((1:N)%%2) # Traitement: 1 si traité, 0 sinon\n  Y = D*Y1 + (1-D)*Y0 \n  coef(lm(Y~D))[[\"D\"]] \n} \nmanyATEs&lt;-replicate(10000,getATE()) \n## Évaluer le biais:\nc(ATE=mean(Y1)-mean(Y0), ExpEstATE=mean(manyATEs)) \n\n      ATE ExpEstATE \n1.0000000 0.9999077 \n\n## Erreur type\n### en utilisant la formule classique\nV&lt;-var(cbind(Y0,Y1)) \nvarc&lt;-V[1,1] \nvart&lt;-V[2,2] \ncovtc&lt;-V[1,2] \nn&lt;-sum(D) \nm&lt;-N-n \nvarestATE&lt;-((N-n)/(N-1))*(vart/n) + ((N-m)/(N-1))* (varc/m) + (2/(N-1)) * covtc \n### Comparer les erreurs types\nc(SimulatedSE= sd(manyATEs), TrueSE=sqrt(varestATE), ConservativeSE=ATE.se) \n\n   SimulatedSE         TrueSE ConservativeSE \n    0.01835497     0.01842684     0.01841784",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/fr.png' width=20px>&nbsp;&nbsp;French"
    ]
  },
  {
    "objectID": "guides/research-questions/effect-types.fr.html#footnotes",
    "href": "guides/research-questions/effect-types.fr.html#footnotes",
    "title": "10 types d’effets de traitement à connaître",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nVoir Holland (1986), Angrist et Pischke, Angrist, Joshua et Jörn-Steffen Pischke (2008). Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton University Press.↩︎\nVoir Holland et Angrist & Pischke encore une fois pour une discussion plus formelle de l’indépendance et des hypothèses nécessaires pour estimer les effets causaux.↩︎\ni.e \\((E(Y_i(1)|D=1)=E(Y_i(1)|D=0)=E(Y_i(1)))\\) et \\((E(Y_i(0)|D=1)=E(Y_i(0)|D=0)=E(Y_i(0)))\\)↩︎\nLes estimations sont souvent écrites avec un chapeau (\\(\\widehat{ATE}\\)) pour refléter la différence entre l’estimation de notre échantillon particulier et le paramètre, cible de notre estimation qui n’est pas observée. Sauf indication contraire, dans ce guide, nous nous concentrons sur la génération d’estimations d’échantillons et omettons par la suite cette notation explicite par souci de concision. Voir Gerber et Green (2012) pour une introduction concise à cette distinction et Imbens et Wooldridge (2007) pour un traitement approfondi de ces concepts.↩︎\nLa covariance de \\(Y_{i}(1),Y_{i}(0)\\) est impossible à observer mais l’estimateur “Neyman” de la variance en omettant le terme de covariance fournit une estimation conservatrice (trop grande) de la vraie variance de l’échantillon parce que nous avons tendance à supposer que la covariance est positive. Étant donné que nous sommes généralement soucieux de minimiser le taux d’erreur de type I (rejetant à tort l’hypothèse nulle vraie), nous préférons utiliser des estimations conservatrices de la variance. Voir aussi Dunning (2010) et Gerber & Green (2012) pour la justification de l’estimateur de variance conservateur.↩︎\nLin (2013)↩︎\nBrambor, Clark et Golder (2006)↩︎\nAngrist, Joshua et Jörn-Steffen Pischke. 2008. Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton university press.↩︎\nNous supposons généralement la monotonie, ce qui signifie qu’il n’y a pas de non-conformistes, c’est à dire de personnes qui ne prennent le traitement que lorsqu’elles sont assignées au contrôle (\\(D_{i}=1\\) quand \\(Z_i=0\\)) et refusent le traitement lorsqu’elles sont assignées au traitement (\\(D_{i}=0\\) quand \\(Z_{i}=1\\)).↩︎\nAngrist, Joshua et Jörn-Steffen Pischke. 2008. Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton university press.; Bound, Jaeger, and Baker (1995)↩︎\nImai, King et Stuart (2008) pour un examen plus détaillé des questions abordées dans cette section.↩︎\nImbens, Guido et Jeffrey M Wooldridge. 2007. What’s New in Econometrics? NBER.↩︎\nAngrist et Pischke (2008) fournissent une brève introduction des sujets traités plus en détail par Hirano, Imbens et Ridder (2003), Aronow et Middleton (2013), Glynn et Quinn (2010), et Hartman et al. (à paraître)↩︎\nGlynn, Adam N et Kevin M Quinn. 2010. “An Introduction to the Augmented Inverse Propensity Weighted Estimator.” Political Analysis 18 (1):36–56.↩︎\nHartman et al. (à paraître) pour voir les efforts visant à combiner des données expérimentales et observationnelles pour passer d’un ATE pour un échantillon à un ATT pour une population.↩︎\nRosenbaum et Rubin (1983)↩︎\nAbadie, Angrist et Imbens (2002)↩︎\nChernozhukov et Hansen (2005). C’est-à-dire que le traitement peut avoir des effets hétérogènes, mais l’ordre des résultats potentiels est préservé. Voir Angrist, Joshua et Jörn-Steffen Pischke. 2008. Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton university press.↩︎\nVoir Koenker, Roger et Kevin Hallock. 2001. “Quantile Regression: An Introduction.” Journal of Economic Perspectives 15 (4): 43–56. pour un aperçu concis de la régression par quantile↩︎\nFormellement, Imai, Keele et Yamamoto (2010) définissent les conditions nécessaires d’ignorabilité séquentielle comme : \\({Y_i(d',m),M_i(d)}⊥D_i|X_i=x, Y_i(d',m)⊥M_i(d)|D_i=d,X_i=x\\). C’est-à-dire que, premièrement, étant donné les covariables de pré-traitement, les résultats potentiels de Y et M sont indépendants du traitement D et, deuxièmement, conditionnés aux covariables de pré-traitement et au statut du traitement, les résultats potentiels sont également indépendants du médiateur.↩︎\nVoir par exemple Imai, Keele et Yamamoto (2010), Imai et al. (2011), Imai, Tingley et Yamamoto (2013), ou Imai et Yamamoto (2013). Aussi voir la discussion de Imai, Tingley et Yamamoto (2013) pour différentes perspectives quand aborder une affirmation sur la médiation avec une analyse de sensibilité ou une analyse des bornes de sensibilité.↩︎",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/fr.png' width=20px>&nbsp;&nbsp;French"
    ]
  },
  {
    "objectID": "guides/research-questions/mechanisms.en.html",
    "href": "guides/research-questions/mechanisms.en.html",
    "title": "10 Things to Know About Mechanisms",
    "section": "",
    "text": "Mechanisms have long been at the heart of medicine. Every time a doctor prescribes a treatment, she does so out of an understanding of which chemical or physical factors cause a disease, and she prescribes a treatment that is effective because it interrupts these factors. For example, many clinical psychologists recommend exercise to patients dealing with depression. Exercise raises endorphins in the body’s chemistry, which trigger positive feelings and also act as analgesics, which reduce the perception of pain. Endorphins, therefore, are a mechanism by which exercise helps reduce depression. Exercise may have positive effects on a number of other dependent variables (e.g. heart disease) through other mechanisms (e.g. elevating heart rates), but the mechanism that causes it to affect depression in particular is endorphins. We could also conclude that another treatment, such as a drug that raised endorphins, may have similar effects on depression.\nMechanisms are just as important for social sciences. Take, for example, recent research that has connected climate change to an increase in civil conflict. One study1 claims to identify the causal effect of climate shocks on violent conflict by studying the rate of civil conflict in El Nino-affected countries during El Nino versus non-El Nino years. Suppose this study is correct. Why would experiencing a climate shock cause a country to have elevated levels of conflict? One mechanism could be poverty: climate shocks hurt the economy, and with lower opportunity costs, individuals are more inclined to join armed groups. An alternative mechanism is physiological: people are physically wired to be more aggressive in hotter temperatures. Perhaps the mechanism is migration: climate shocks displace people in coastal regions, and this produces social conflict between migrants and natives. In reality, several or all of these mechanisms (as well as others not listed here) could be operating simultaneously, even in the same case! In many of the most interesting social science questions, there are several channels (“M”s) that could transmit the total effect of X on Y.",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/gb.png' width=20px>&nbsp;&nbsp;English"
    ]
  },
  {
    "objectID": "guides/research-questions/mechanisms.en.html#footnotes",
    "href": "guides/research-questions/mechanisms.en.html#footnotes",
    "title": "10 Things to Know About Mechanisms",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Hsiang, Meng, and Cane (2011).↩︎\nSee Gurr (1970).↩︎\nNote that Chong et al. (2015) test their argument using data at the precinct level, not the individual level, but we’ve adapted their argument to the individual level for ease of exposition.↩︎\nFor a more rigorous discussion of these fallacies, see Glynn (2012).↩︎\nExplanation adapted from Gerber and Green (2012), chapter 10.↩︎\nSee Levy (1966).↩︎\nSee Fanon (1964); Lott Jr. (1999).↩︎\nSee Almond and Verba (1963); Mattes and Bratton (2007).↩︎\nIn the actual study, the authors were surprised to uncover evidence that education also increased individuals’ acceptance of political violence. While they still argue that individual empowerment is responsible for the relationship between education and democracy, they caution that education does not always lead to democratization (that is, M3→Y but it is also possible that M3→NOT Y). Nonetheless, their approach is a useful demonstration of how multiple outcomes may shed light on mechanisms.↩︎",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/gb.png' width=20px>&nbsp;&nbsp;English"
    ]
  },
  {
    "objectID": "guides/research-questions/heterogeneous-effects_en.html",
    "href": "guides/research-questions/heterogeneous-effects_en.html",
    "title": "10 Things to Know About Heterogeneous Treatment Effects",
    "section": "",
    "text": "A treatment may affect individuals or groups in different ways: this is treatment effect heterogeneity. The study of treatment effect heterogeneity involves estimating how treatment effects vary across individuals or groups within an experiment. For whom are there big effects? For whom are there small effects? For whom does treatment generate beneficial or adverse effects?"
  },
  {
    "objectID": "guides/research-questions/heterogeneous-effects_en.html#footnotes",
    "href": "guides/research-questions/heterogeneous-effects_en.html#footnotes",
    "title": "10 Things to Know About Heterogeneous Treatment Effects",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis guide draws heavily from Gerber and Green (2012) and from Don Green’s course notes on experimental methods at Columbia University and builds on an older version of this guide by Albert Fang.↩︎\nThis is known as the Fundamental Problem of Causal Inference. For more background, see 10 Things You Need to Know About Causal Inference.↩︎\nWe can also define ATEs for subgroups defined by the individuals’ treatment status (e.g., the ATE among those who were assigned to treatment, also called the ATT or average treatment effect on the treated) or individuals’ post-treatment outcomes. We do not focus on these types of CATEs because of estimation challenges described below.↩︎\nFor further reading (at an advanced technical level), see Vansteelandt and Goetghebeur (2003); Vansteelandt and Goetghebeur (2004); Vansteelandt (2010); Stephens, Keele, and Joffe (2016).↩︎\nSee Bansak (2021) for more on causal moderation.↩︎\nFor more background and a range of views on the multiple comparisons problem, see, e.g.: 10 Things You Need to Know About Multiple Comparisons; Cook and Farewell (1996); Schulz and Grimes (2005a); Schulz and Grimes (2005b); Anderson (2008); Westfall, Tobias, and Wolfinger (2011); Gelman, Hill, and Yajima (2012).↩︎\nSee, for example, Imai and Ratkovic (2013).↩︎\nSee Chipman, George, and McCulloch (2010); Hill (2011); Green and Kern (2012).↩︎\nSee Hainmueller and Hazlett (2013).↩︎\nSee Laan, Polley, and Hubbard (2007); Grimmer, Messing, and Westwood (2014).↩︎"
  },
  {
    "objectID": "guides/research-questions/effect-types.es.html",
    "href": "guides/research-questions/effect-types.es.html",
    "title": "10 tipos de los efectos causales que debe conocer",
    "section": "",
    "text": "Comenzamos revisando cómo, con la aleatorización, una simple diferencia de medias proporciona una estimación no sesgada del ATE. A continuación, presentamos algunos conceptos estadísticos comunes y la notación que utilizamos a lo largo de esta guía.\nPrimero definimos el efecto del tratamiento para una observación individual (una persona, hogar, ciudad, etc.) como la diferencia entre el comportamiento de esa unidad bajo tratamiento \\((Y_ {i} (1))\\) y su control \\[\\tau_{i}=Y_{i}(1)-Y_{i}(0).\\] Dado que solo podemos observar $Y_{i}(1) $ o \\(Y_ {i} (0)\\), no tenemos forma de saber cuál es el efecto del tratamiento individual. Ahora sea \\(D_{i}\\) un indicador de si una observación está en el grupo de tratamiento o control. Si el tratamiento se asigna al azar, \\(D_ {i}\\) es independiente, no solo de los resultados potenciales sino también de cualquier covariable (observada y no observada) que pueda predecir también esos resultados \\(((Y_ {i} (1), Y_ {i } (0),X_ {i} \\perp \\perp D_ {i}))\\) 2.\nSuponga que nuestro diseño tiene \\(m\\) unidades en el grupo de tratamiento y \\(N - m\\) en el de control. Supongamos que reasignamos repetidamente el tratamiento al azar muchas veces y cada vez calculamos la diferencia de medias entre los grupos de tratamiento y de control y luego registramos este valor en una lista. El promedio de los valores en esa lista será el mismo que la diferencia de los promedios de los resultados potenciales reales si hubiéramos observado todos los resultados potenciales posibles para todas las observaciones 3. Otra forma de expresar esta característica del efecto promedio del tratamiento y el estimador del mismo es decir que la diferencia de medias observadas es un estimador insesgado del efecto del tratamiento causal promedio.\n\\[ATE\\equiv\\frac{1}{N}\\sum^{N}_{i=1}\\tau_{i}=\\frac{\\sum^{N}_{1}Y_{i}(1)}{N}-\\frac{\\sum^{N}_{1}Y_{i}(0)}{N}\\]\nA menudo estimamos el ATE utilizando la diferencia de medias observada: 4\n\\[\\widehat{ATE} =\\frac{\\sum^m_1Z_{i}Y_{i}}{m}-\\frac{\\sum^{N}_{m+1}(1-Z_{i})Y_{i}}{N-m}\\]\nLa inferencia estadística sobre el ATE estimado requiere que sepamos cómo este variaría entre cada aleatorización. Para eso podemos escribir la varianza entre aleatorizaciones de la siguiente manera:\n\\[V(ATE) = \\frac{N}{N-1} [\\frac{V(Y_{i}(1))}{m}+\\frac{V(Y_{i}(0))}{N-m}]-\\frac{1}{N-1}[V(Y_{i}(1))+V(Y_{i}(0))-2Cov(Y_{i}(1),Y_{i}(0))]\\]\ny estimar esta cantidad a partir de las estimaciones muestrales de la varianza en cada grupo. 5\nUn modelo lineal que regresa el resultado observado \\(Y_ {i}\\) en un indicador de la asignación del tratamiento \\(D_{i}\\) nos sirve como estimador del ATE (y con algunos ajustes adicionales de la varianza del ATE):\n\\[Y_{i}=Y_{i}(0)(1-D_{i})+Y_{i}(1)D_{i}=\\beta_{0}+\\beta_{1}D_{i}+u\\] ya que podemos reorganizar los términos de modo que \\(\\beta_{0}\\) estime el promedio entre las observaciones de control \\((Y_ {i} (0) ∣D_ {i} = 0)\\) y \\(\\beta_{1}\\) estime las diferencias de las medias \\((Y_ {i} (1) ∣D_ {i} = 1) - (Y_ {i} (1) ∣D_ {i} = 0)\\). En el siguiente código, creamos una muestra de 1000 observaciones y asignamos aleatoriamente un tratamiento \\(D_i\\) con un efecto constante para la mitad de las unidades. Estimamos el ATE utilizando la regresión de mínimos cuadrados ordinarios (MCO) para calcular la diferencia de medias observada. El cálculo de las medias en cada grupo y su diferencia también produciría una estimación no sesgada del ATE. Tenga en cuenta que el ATE estimado del estimador de MCO es insesgado porque suponemos que los errores en este modelo lineal son independientes y están distribuidos de manera idéntica. Cuando nuestro tratamiento afecta tanto al valor promedio del resultado como a la distribución de las respuestas, esta supuesto ya no se cumple y necesitamos ajustar los errores estándar de MCO utilizando un estimador sándwich de Huber-White para obtener las estimaciones correctas (basadas en la varianza del ATE) para hacer inferencia estadística. 6 Finalmente, a través de la simulación demostramos que estos estimadores son insesgados.\n\nset.seed(1234) # Por replicabilidad \nN = 1000 # Tamaño de la población\nY0 = runif(N) # Resultado potencial bajo la condición de control\nY1 = Y0 + 1 # Resultado potencial bajo la condición de tratamiento\nD = sample((1:N)%%2) #igual a 1 si la unidad es tratada, 0 de lo contrario\nY = D*Y1 + (1-D)*Y0 # Variable de resultado de la población\nsamp = data.frame(D,Y) \n\nATE = coef(lm(Y~D,data=samp))[2] #lo mismo que with(samp,mean(Y[Z==1])-mean(Y[Z==0])) \n\n# ATE de la muestra (SATE) con error estándar Neyman \n# o justificados en la aleatorización\n# que son iguales al error estándar del MCO cuando no hay covariables o bloques\nlibrary(lmtest) \nlibrary(sandwich) \nfit&lt;-lm(Y~D,data=samp) \ncoef(summary(fit))[\"D\",1:2]\n\n  Estimate Std. Error \n1.01820525 0.01841784 \n\nATE.se&lt;-coeftest(fit,vcovHC(fit,type=\"HC2\"))[\"D\",2] \n #lo mismo que with(samp,sqrt(var(Y[D==1])/sum(D)+var(Y[D==0])/(n-sum(D))) \n\n# Evaluar si no hay sesgo y simular el error estándar\ngetATE&lt;-function() {\n  D = sample((1:N)%%2) #  igual a 1 si la unidad es tratada, 0 de lo contrario\n  Y = D*Y1 + (1-D)*Y0 \n  coef(lm(Y~D))[[\"D\"]] \n} \n\nmanyATEs&lt;-replicate(10000,getATE()) \n\n## Sesgo\nc(ATE=mean(Y1)-mean(Y0), ExpEstATE=mean(manyATEs)) \n\n      ATE ExpEstATE \n1.0000000 0.9999077 \n\n## Error estándar\n### Fórmula del error estándar real\nV&lt;-var(cbind(Y0,Y1)) \nvarc&lt;-V[1,1] \nvart&lt;-V[2,2] \ncovtc&lt;-V[1,2] \nn&lt;-sum(D) \nm&lt;-N-n \nvarestATE&lt;-((N-n)/(N-1))*(vart/n) + ((N-m)/(N-1))* (varc/m) + (2/(N-1)) * covtc \n\n### Compararar errores estándar \nc(SimulatedSE= sd(manyATEs), TrueSE=sqrt(varestATE), ConservativeSE=ATE.se) \n\n   SimulatedSE         TrueSE ConservativeSE \n    0.01835497     0.01842684     0.01841784",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/es.png' width=20px>&nbsp;&nbsp;Spanish"
    ]
  },
  {
    "objectID": "guides/research-questions/effect-types.es.html#footnotes",
    "href": "guides/research-questions/effect-types.es.html#footnotes",
    "title": "10 tipos de los efectos causales que debe conocer",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPuede consultar Holland (1986), Angrist y Pischke Angrist, Joshua y Jörn-Steffen Pischke. 2008. Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton University Press.↩︎\nVer Holland, Angrist y Pischke para una discusión más formal sobre la independencia y los supuestos necesarios para estimar los efectos causales.↩︎\nEs decir, \\((E(Y_i(1)|D=1)=E(Y_i(1)|D=0)=E(Y_i(1)))\\) and \\((E(Y_i(0)|D=1)=E(Y_i(0)|D=0)=E(Y_i(0)))\\)↩︎\nLas estimaciones a menudo se escriben con un acento (\\(\\widehat {ATE}\\)) para reflejar la diferencia entre la estimación de nuestra muestra particular y el estimando, es decir la cantidad objetivo de nuestra estimación que no podemos observar. A menos que se indique lo contrario, en esta guía nos enfocamos en generar estimaciones a partir de la muestra y posteriormente omitimos esta notación explícita. Ver Gerber y Green (2012) para una introducción concisa a esta distinción e Imbens y Wooldridge (2007) para una revisión completa de estos conceptos.↩︎\nLa covarianza de \\(Y_{i} (1), Y_ {i} (0)\\) es imposible de observar, pero el estimador “Neyman” de la varianza que omite el término de covarianza proporciona una estimación conservadora (demasiado grande) de la varianza muestral verdadera porque tendemos a suponer que la covarianza es positiva. Dado que en general nos preocupa minimizar la tasa de error de tipo I (rechazando incorrectamente la hipótesis nula verdadera), preferimos utilizar estimaciones conservadoras de la varianza. Ver también Dunning (2010) y Gerber y Green (2012) para la justificación del estimador de varianza conservador.↩︎\nLin (2013)↩︎\nAngrist, Joshua y Jörn-Steffen Pischke. 2008. Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton university press.↩︎\nPor lo general, asumimos monotonicidad, lo que significa que no hay quienes desobedecen o desafían la asignación o personas que solo toman el tratamiento cuando se les asigna el control (\\(D_{i} = 1\\) cuando \\(Z_i = 0\\) ) y rechazan el tratamiento cuando se les asigna al tratamiento.↩︎\nAngrist, Joshua, and Jörn-Steffen Pischke. 2008. Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton university press.; Bound, Jaeger, and Baker (1995)↩︎\nVea Imai, King y Stuart (2008) para una revisión más detallada de los temas discutidos en esta sección.↩︎\nImbens, Guido y Jeffrey M Wooldridge. 2007. What’s New in Econometrics? NBER.↩︎\nGlynn, Adam N y Kevin M Quinn. 2010. “An Introduction to the Augmented Inverse Propensity Weighted Estimator.” Political Analysis 18 (1):36–56.↩︎\nEn Hartman et al. (próximo a publicar) puede encontrar un ejemplo en el que se combinan datos experimentales and observacionales para pasar de un ATE de la muestra a una estimación del ATT de la población.↩︎\nRosenbaum y Rubin (1983)↩︎\nAbadie, Angrist y Imbens (2002)↩︎\nChernozhukov y Hansen (2005). Es decir, el tratamiento puede tener efectos heterogéneos pero se conserva el orden de los resultados potenciales. Ver Joshua Angrist y Jörn-Steffen Pischke. 2008. Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton university press.↩︎\nKoenker, Roger Y Kevin Hallock. 2001. “Quantile Regression: An Introduction.” Journal of Economic Perspectives 15 (4): 43–56 para una revisión concisa de la regresión por cuantiles.↩︎\nKoenker, Roger Y Kevin Hallock. 2001. “Quantile Regression: An Introduction.” Journal of Economic Perspectives 15 (4): 43–56 para una revisión concisa de la regresión por cuantiles.↩︎\nImai, Keele y Yamamoto (2010) definen formalmente las condiciones necesarias para la ignorabilidad secuencial como: \\({Y_i(d',m),M_i(d)}\\perp D_i|X_i=x, Y_i(d',m)\\perp M_i(d)|D_i=d,X_i=x\\). Esto quiere decir, primero, dado unas covariables de pretatamiento, los resultados potenciales de Y y M son independientes del tratamiento D y segundo que condicionado a las covariables previas al tratamiento y al estado del tratamiento, los resultados potenciales también son independientes del mediador.↩︎\nVea por ejemplo Imai, Keele, y Yamamoto (2010), Imai et al. (2011), Imai, Tingley y Yamamoto (2013), Imai y Yamamoto (2013). Así como también Imai, Tingley, y Yamamoto (2013) para perspectivas diferentes en la desabi for different perspectives sobre la conveniencia de abordar hacer aserciones sobre posible mediación con análisis de sensibilidad o de límites (bounds).↩︎",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/es.png' width=20px>&nbsp;&nbsp;Spanish"
    ]
  },
  {
    "objectID": "guides/planning/pilots_en.html",
    "href": "guides/planning/pilots_en.html",
    "title": "10 Things to Know About Pilot Studies",
    "section": "",
    "text": "1 What is a pilot, and what is it good for?\nDuring the process of planning an experiment, researchers often face questions regarding their study’s theoretical and conceptual underpinnings, its measurement approach, and associated logistics. Pilot studies can help you to consider and improve these elements of your research whether you are running a survey, lab, or field experiment. In particular, a pilot study is a smaller scale preliminary test or trial run, used to assist with the preparation of a more comprehensive investigation. Pilots are typically administered before a research design is finalized in order to evaluate and improve the feasibility, reliability, and validity of the proposed study.\nWhile it may be tempting to think about a pilot as simply a miniature version of one’s final study, helpful for doing an initial test of one’s hypotheses, pilot studies are neither especially appropriate for hypothesis testing, nor are they limited to it. Given smaller sample sizes, pilots are typically underpowered for evaluating hypotheses. Besides, deciding whether to continue a study based on initial results contributes to the “file drawer” problem where important studies and results—including null results—are never published, leading to misrepresentative bodies of published research (Franco, Malhotra, and Simonovits 2014).\nFortunately, as depicted in the table below, pilots can be useful for a wide range of research purposes including theory development, research design, improving measurement, sampling considerations, evaluating logistics, pre-planning analysis, weighing ethical considerations, and communicating one’s research (Teijilngen et al. (2001); Thabane et al. (2010)). Each of these activities can help improve the quality of one’s main study and render it more compelling and ultimately successful.\nIn the sections below, we review these many benefits. First, we consider how pilots can assist with a study’s theory and measurement approach, evaluate its logistical feasibility, and provide information about the sample size needed to test hypotheses. In addition, we discuss how piloting may help you secure research funding and institutional support, gather feedback, and incorporate best practices in research ethics. Finally, we offer recommendations on how to use pilots to inform your main study design, reveal important unknowns, and contribute to your broader research agenda.\n\n\n\n\n\n\n\nTheory Development\nResearch Design\n\n\n\n\nRefining your research questions and hypotheses\nEvaluating suitability of study context for answering research questions\n\n\nDetermining initial plausibility of theorized mechanisms\nDetermining construction and strength of treatments\n\n\nIdentifying additional research questions of interest\nDetermining number, levels, and timing of treatments\n\n\nExploring unknowns related to your research questions and hypotheses\nConsidering alternative and/or multiple measures of participant response to treatment\n\n\n\n\n\n\n\n\n\n\nMeasurement\nSample\n\n\n\n\nTesting alternative treatments and outcomes to improve clarity, validity, and reliability\nDetermining power and sample size needed for main study\n\n\nUncovering issues related to asymmetry in delivery and measurement of treatments and/or outcomes\nEvaluating recruitment procedures and eligibility criteria\n\n\nConsidering covariate measurement approach\nTesting participant understanding of research tools and procedures\n\n\nEnsuring successful data collection and data entry\nAssessing participant compliance and treatment uptake\n\n\nIdentifying additional information that may need to be collected\nAssessing participant response rate, quality, and attrition\n\n\nConsidering the use of combined measures such as scales and indices\n\n\n\n\n\n\n\n\n\n\n\nLogistics\nAnalysis\n\n\n\n\nDetermining time and resources needed to conduct main study\nConsidering feasibility of software, statistical techniques, or other analytic tools given preliminary data\n\n\nEvaluating randomization procedure\nPreparing analysis procedures for main study\n\n\nAssessing research team understanding of protocols and procedures\nConsidering feasibility of incorporating data from partners or other sources\n\n\nDetermining partner capacity and willingness\nConsidering responses to missingness in data\n\n\nTraining research team members and partners\n\n\n\nAssessing inter-researcher reliability in data collection, procedures, and analysis\n\n\n\n\n\n\n\n\n\n\n\nEthics\nCommunicating Research\n\n\n\n\nEnsuring participant safety and well-being\nCollecting preliminary data to share externally\n\n\nEvaluating consent procedures\nSoliciting feedback on research design\n\n\nAssessing participant time and burdens\nLeveraging pilot results transparently to explain design choices for main study\n\n\nEnsuring normative acceptability of study elements in given context\nDemonstrating research team capacity and competence\n\n\nAssessing privacy and confidentiality procedures\nPersuading funders, ethics committees, and other stakeholders of study feasibility and value\n\n\nAssessing security of data and other study materials\nInforming interested scholars and other stakeholders\n\n\n\nEducating students about research practices\n\n\n\n\n\n2 Pilots are useful for improving your study’s measurement approach in relation to your theory\nFewer things are more frustrating for a researcher than investing significant resources and time into a study only to find that one’s outcome measures lack reliability to accurately assess the concepts of interest, or that participants did not receive treatments in the way that was anticipated. Unexpected results are common, even when one approaches measurement carefully, as operationalizing concepts into an effective measurement strategy in the social sciences is a complex endeavor.\nAs a remedy, pilots are great for testing different versions of treatments and outcomes to see which of them “work” or whether any changes may be needed to increase validity, reliability, and clarity. For example, you can include a larger set of outcome measures in a pilot than is feasible in the main study, and perform simple factor analysis to identify a preferred subset of measures. Pilot results can also inform whether it makes sense to create indices or scales in order to improve reliability and decrease variance. Subtle variation in the strength, nature, timing, or number of treatments can also significantly alter study findings. Pilots offer researchers the opportunity to evaluate multiple possibilities for one’s treatment design, and to assess how these options influence participant compliance, uptake of treatment, attrition, and more.\nWhen refining one’s approach to measuring treatments, outcomes, and covariates, it is especially important to keep in mind how these elements of one’s research design speak to the broader concepts and theory under study. Will the data you receive provide the necessary information regarding the theoretical elements and causal mechanisms under study? Are there other causal channels that may be in play, or heterogeneous effects within subgroups that you hadn’t thought about previously? These kinds of considerations can inform changes to your research design, such as alterations to your randomization strategy, the introduction of new dimensions in your treatments, and decisions about which aspects of your study are core and which can be saved for a later date.\nWhen you begin a pilot study, you may have an initial conception of your research questions, hypotheses, and measurement approach; but with a careful pilot, you have the opportunity to refine all of these aspects in a way that can increase your (and others’) confidence in the overall quality of the study.\n\n\n3 Pilots can also help you to prepare for the logistics of running your experiment\nPerhaps the most often emphasized purpose of pilots is to work out any logistical kinks that might impede the main study (Thabane et al. 2010). Logistical considerations include those implicating a project’s overall resources, the study team, the participants, and the administration of study instruments.\nIn terms of participant logistics, is important to establish whether your study participants understand the research tools and procedures, are able to receive treatment, and feel comfortable answering questions or performing tasks. Based on how successful the pilot is, you may find ways to improve your recruitment strategy, adjust your eligibility criteria, and improve the clarity of your research instruments. It is also important to ensure that basic elements like randomization and data collection are working as anticipated. Data simulations and pilots with very small samples can also be used to test certain study elements.\nSimilar considerations apply to your study team and partners. Do they understand the protocols or might they require additional training, for example, to promote reliability in procedures such as data collection? Are there any asymmetries in delivery or measurement of treatment or outcomes of which you were not aware? A pilot can also be very helpful for determining the resources needed to conduct the main study. For example, how much time does it take—for both members of the study team and participants—and what might this imply for the size of the sample and complexity of the research design that is ultimately feasible for the main study?\nWhile the specific logistical considerations will vary depending on whether your research design is centered around a field experiment, survey experiment, lab experiment, or something else, pilots will help you ensure that your experiment goes as planned. Thus, when constructing a pilot study, consider making a list of the logistical elements you want to evaluate and designing the pilot to facilitate answering associated questions. Asking your participants whether they were able to “hear the video” or “understood the instructions” can make your research easier down the road.\n\n\n4 You can use pilots for power analysis or for calculating minimum detectable effects\nAnother common purpose of pilot studies, in light of limited resources, is assessing statistical power, which helps to avoid the risk of false negatives (or false positives) from underpowered studies. As described in the EGAP methods guide about power analysis, a researcher’s goal is to answer the following: “supposing there truly is a treatment effect and you were to run your experiment a huge number of times, how often will you get a statistically significant result?” To improve the likelihood that your main study will achieve a typical target power value of 80%, you can use a pilot study combined with careful simulations. These results are helpful for determining appropriate sample sizes, the extent to which you can subset your sample for various analyses, and whether adjustments may be necessary for increasing power.\nTraditionally, a pilot study is used to obtain an estimate of the effect size, which becomes the presumed estimate for simulations used to determine power and sample size in one’s main study. However, the DeclareDesign cautions that effect size estimates are very noisy in small pilots, especially when true effect sizes are small (for example, under 0.2 sd). As an alternative, they recommend using pilot studies to estimate the standard deviation of the outcome variable. Using this estimate, one can easily obtain an unbiased estimate of a main study’s minimum detectable effect (MDE) for a given outcome and with 80% power as 2.8 times the estimated standard error of the associated outcome variable (Gelman and Hill 2006). The goal is to ensure that the MDE is small enough that the study would capture any substantively meaningful effect.\nUsing the recommendations from DeclareDesign, we provide sample code based on a hypothetical pilot study. In the code below, we assume we have already conducted a pilot study and have calculated the standard deviation of a key outcome measure for both the control and treatment groups. Next, we use these estimates to calculate MDEs for different possible sample sizes, in order to inform our target sample size for a future main study.\n\nrequire(tidyverse)\n#Standard deviations of outcome measure for treatment & control groups calculated from pilot study results\n#The ones provided are for purposes of demonstration\n#We assume one treatment and control group--you will need to adjust to study specifics\nsd_control &lt;- 1.5\nsd_treat &lt;- 1.2\n#MDE calculation for various possible sample sizes\nN &lt;- seq(from=500, to=3000, by=100)\nMDE &lt;- vector()\nfor (i in 1:length(N)){\n  #We assume an equal number of participants in treatment and control--you will need to adjust to study specifics\n  MDE[i] &lt;- 2.8 * sqrt((sd_control^2/(N[i]/2)) + (sd_treat^2/(N[i]/2))) #The true effect size must be at least 2.8 standard errors from zero to detect it with 80% probability using 95% confidence intervals (Gelman and Hill 2006). To estimate the standard error of the ATE, we use equation 3.6 in @gerber_green_2012. Thus, simply multiply 2.8 by the standard error of the ATE to calculate the MDE. \n}\n#Visualizing results to identify target sample size for main study\nmde_data &lt;- as.data.frame(cbind(N, MDE))\nmde_plot &lt;- ggplot(mde_data, aes(x=N, y=MDE)) + geom_point() +\n  xlab(\"Sample Size\") + ylab(\"MDE\") + ggtitle(\"Minimum Detectable Effect (MDE) for Main Study by Sample Size\") + \n  theme_bw() +\n  geom_hline(yintercept = .2, lty = \"dashed\")\nmde_plot\n\n\n\n\n\n\n\n\nBased on the hypothetical standard deviations in the control and treatment groups from the pilot study, the main study would need a sample size of approximately 1,500 to ensure that effect sizes as small as 0.2 sd are detectable. While calculating the MDE based on pilot results is straightforward, determining how small an MDE should be is more subjective, and should be informed by theory and prior work.\nMDE calculations are based on the design of the pilot and the specific outcome measures. Thus, you may need to perform MDE calculations for each estimand of interest to determine what sample sizes are needed and which hypotheses can be addressed with sufficient power given your experimental design. Keep in mind that as you use pilot results to refine your study design, including treatments and outcomes, the relevant standard deviations of the outcome measures and the resultant MDE calculations may change. Another quick pilot is an option, though it is important to take resource constraints into account.\n\n\n5 Piloting may help you secure funding and support for your research\nPilots are not only helpful for improving the quality of your main study, they may also help to ensure you have the support and funding to enable your study to go forward. Given a general trend of tightening of per-researcher funding, particularly for smaller projects (Bloch and Sørensen 2015), and a movement towards evidence-based decision-making, you may wish to draw on pilots to provide initial evidence that your study is worthwhile. For example, the National Science Foundation (NSF) recognizes the need for more early-stage exploratory studies that can provide a basis for future larger-scale studies, and Time-sharing Experiments for the Social Sciences (TESS) notes that “Proposals that report trial runs of novel and focal ideas will be viewed as more credible.”\nThis does not entail that one needs to show that effect sizes are large enough nor that hypotheses are likely to be confirmed. Instead, a pilot can demonstrate that your research project is feasible in terms of time and resources, that your study design is adequate for answering the research questions proposed, and that your research team has the expertise and capacity to administer the study, perform analyses, and even present results in a compelling fashion (Teijilngen et al. 2001). In a similar fashion, piloting can help you to recruit study team members and organizational partners, or solicit institutional support.\n\n\n6 You can use a pilot to get feedback on your study design\nSharing initial findings, successes, and challenges is a great way to help you prepare for your main study. In light of the growth of the open science movement (Christensen et al. 2020), conferences and workshops are increasingly open to accepting submissions based on pre-analysis plans and pilot results. Whether through these more formal venues, or by reaching out to colleagues or experts, you can use pilot results to receive feedback about your study design, such as strategies to address possible challenges and unexplored theoretical or empirical directions that you can incorporate in your main study.\nFurther, it can take a long time to complete an experimental study and publish results. Sharing intermediate findings allows you to coordinate with other researchers in the field, helping you to align your work and incorporate recent theoretical and empirical innovations relevant to your study.\nUltimately, the design and piloting stage is the best time to receive feedback, as you still have time to make improvements. In contrast, most key research design decisions will already have been finalized by the time your study undergoes formal peer review for publication.\n\n\n7 Keep an eye out for ethical considerations when piloting\nWhen you design your study initially, all of the relevant ethical considerations and risks may not be immediately apparent. The piloting stage is thus a good opportunity to review whether any risks or harms you anticipated may come into play and whether still other ethical considerations should be incorporated into your main study.\nYou can use your pilot to evaluate whether your procedures around informed consent are adequate and to assess the extent of burdens such as time required of participants. You may find, for example, that certain topics—such as mental health and personal identity—are more sensitive than anticipated, or that survey questions (even those based on validated and popular measures) use outdated and offensive language. The best way to find out is to ask. Consider including open-ended survey questions or talking to participants directly to determine what participants think of the study in terms of its normative and cultural acceptability in a given context.\nIn addition, the piloting process is a good time to practice your procedures around privacy, confidentiality, and security of data and other materials. You may find that other procedures for obtaining consent, ensuring participant safety and well-being, and promoting privacy and anonymity are necessary to improve the ethical dimensions of your main study. This can include decreasing risks as well as increasing benefits to participants, such as by providing helpful resources and information that may help to mitigate possible harms.\nNote that this doesn’t imply any less ethical consideration should be given to your pilot itself. All appropriate safeguards, including IRB review, apply to human subjects research for a pilot study as well.\n\n\n8 Be transparent about how your pilot informs the design of your main study\nAs noted, it’s helpful to pre-identify a set of questions about logistics, measurement, or other features of your study that you believe a pilot can help to answer. Putting these questions into writing, designing your pilot to facilitate answering them, and reporting on how the answers shape any modifications to your main study is a way in which you can promote transparency in research. This includes transparency within one’s study team about the purposes of a pilot, as well as for external audiences such as funders or peer reviewers. Transparency helps to alleviate concerns such as the file drawer problem, for example by demonstrating that a pilot study does not function as a method for cherry-picking statistically significant results. It also facilitates understanding and receipt of feedback.\nThe table below is a simple illustrative example of how one could transparently present lessons learned from a hypothetical pilot, for example, in a pre-analysis plan or research proposal. The first column indicates the question the pilot is intended to help answer, in this case questions related to logistical adequacy, manipulation checks, delivery of treatments, and measurement of outcomes. The second column presents the associated findings from the pilot, and the third column discusses how the lessons learned will inform design choices for the main study.\n\n\n\n\n\n\n\n\nQuestions\nPilot Results\nDesign Decision\n\n\n\n\nCan participants hear the video treatments?\n10% reported difficulty.\nWe will add subtitles to the video treatments.\n\n\nCan participants identify the politician party, indicating successful treatment?\n75% correctly identified the politician party.\nWe will mention the politician party additional times to make this treatment component more memorable to participants.\n\n\nWhich video treatments from the available options are best to use?\nParticipants were more familiar with some politicians in the videos than others.\nWe will randomize subjects to a more familiar or less familiar politician and may explore heterogeneous effects by politician familiarity.\n\n\nWhat is the best way to measure uncertainty as an outcome?\nRespondents appear to have been confused by bidirectional scales and questions with reversed scales.\nWe will use unidirectional scales and also consider the distribution (variance) of other key outcome measures.\n\n\n\n\n\n9 Explore unknowns\nWhile you may begin your study with a set of preestablished questions—or “known unknowns”— that your pilot can help to address, keep in mind that there is a whole universe of “unknown unknowns” left to be explored. Of course, not all of these will be relevant to your study, but some are likely to be. For example, you may find that the participants have vastly different interpretations of an informational treatment, understand survey scales differently, or are reluctant to share responses given a perceived political bias.\nThere are a few ways to explore these unknowns in your pilot study. You might wish to include additional exploratory outcome questions, collect extra covariate data, or use a variety of treatments drawing on innovative or untested ideas. Another great way to identify unknowns is through open-ended questions of study participants. Regardless of your study design, you might consider surveying or interviewing participants to ask “what they think about topic X,” or “what comes to mind when they hear the term Y.” The world is often more complex than researchers model in their study designs, and study participants often have more diverse perspectives and relationships with the issues at hand than researchers expect.\nThrough this process, you may identify novel research questions, potential theoretical dimensions or causal mechanisms, and hypotheses that you had not originally formulated when you began studying the topic. Exploring unknowns can therefore lead to refining the ideas you had originally conceived of, as you are unlikely to have determined the best version of your study from the beginning. It can also lead to entirely novel considerations and ideas altogether, some of which you may be able to incorporate into the current study or future studies.\nOverall, pilots afford a wonderful opportunity for open-ended exploration and idea generation.\n\n\n10 A pilot is part of a broader sequence of research activities\nWhile we’ve talked about research in terms of “pilots” and “main studies,” this is an oversimplification of the research process. Indeed one can (and often should) employ multiple pilot studies, perhaps with different sample sizes, to evaluate different questions and to continue refining one’s research design as new questions and possibilities emerge. Pilot studies certainly do set the stage for main studies as well as subsequent “follow-up” studies, but there may not be a sharp demarcation between these kinds of studies. As depicted in the graphic, pilot studies do not merely lead to main studies. They can also help one develop new ideas and they may serve as a venue through which one can contribute to or coordinate with the broader research community around topics of shared interest.\n\n\n\n\nThe Role of Pilots in Experimental Research\n\n\n\n\nAs pilots are part of this broader sequence of research activities, an important consideration is what portion of one’s funding and resources should be allocated to a pilot study versus one’s main study. Researchers may be discouraged from devoting research resources to pilots due to a perception that pilots only provide preliminary data that cannot be used for final research products. While there is no simple “rule of thumb” about the share of the budget that researchers should apply to pilots, we believe that pilots can pay off when they empower researchers to improve their study designs and make more grounded decisions about their research.\nAdministering a second pilot study is especially prudent when pilot results or procedures deviate significantly from your expectations, or if you make substantial alterations to your design. Piloting one or more times is particularly beneficial for field experiments, as researchers often have just one opportunity to successfully implement their full study. However, when resources are constrained, you may opt instead to use simulations to evaluate alternative research designs based on results from your single pilot study. The DeclareDesign package in R is a helpful resource.\nIn short, pilots are neither pre-tests of hypotheses nor merely checks on basic study logistics that deplete one’s research funds. Instead, pilots are living and breathing elements of the broader research process that provide value in their own right.\n\n\n\n\n\n\n\n\n Back to top11 References\n\nBloch, Carter, and Mads P. Sørensen. 2015. “The Size of Research Funding: Trends and Implications.” Science and Public Policy 42 (1): 30–43.\n\n\nChristensen, Garret, Zenan Wang, Elizabeth Levy Paluck, Nicholas Swanson, David Birke, Edward Miguel, and Rebecca Littman. 2020. “Open Science Practices Are on the Rise: The State of Social Science (3S) Survey.”\n\n\nFranco, Annie, Neil Malhotra, and Gabor Simonovits. 2014. “Publication Bias in the Social Sciences: Unlocking the File Drawer.” Science 345 (6203): 1502–5.\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge, United Kingdom: Cambridge University Press.\n\n\nTeijilngen, Edwin R. van, Anne-Marie Rennie, Vanora Hundley, and Wendy Graham. 2001. “The Importance of Conducting and Reporting Pilot Studies: The Example of the Scottish Births Survey.” Journal of Advanced Nursing 34 (3): 289–95.\n\n\nThabane, Lehana, Jinhui Ma, Rong Chu, Ji Cheng, Afisi Ismaila, Lorena P. Rios, Robson Reid, Marroon Thabane, Lora Giangregorio, and Charles H. Goldsmith. 2010. “A Tutorial on Pilot Studies: The What, Why and How.” BMC Medical Research Methodology 10 (1): 1–10."
  },
  {
    "objectID": "guides/planning/pap.fr.html",
    "href": "guides/planning/pap.fr.html",
    "title": "10 choses à savoir sur les plans de pré-analyse",
    "section": "",
    "text": "Un PAP est un document qui formalise et décrit la conception et le plan d’analyse de votre étude. Il est rédigé avant la réalisation de l’analyse et est généralement enregistré sur un site web tiers.1\nLes objectifs du PAP sont d’améliorer les choix de conception de recherche, d’accroître la transparence de la recherche et de permettre à d’autres chercheurs de reproduire votre analyse. Par conséquent, nous recommandons de concentrer le PAP sur les détails analytiques qui vous aideront à analyser votre étude et permettront à d’autres chercheurs de reproduire votre analyse. Une brève section sur la théorie devrait être incluse dans la mesure où elle aide à formuler les hypothèses, mais une analyse détaillée de la théorie et de la littérature n’a pas besoin d’être incluse. Le PAP n’a pas besoin d’inclure le début d’un article académique si ces sections n’aident pas à réfléchir sur votre analyse ou à aider les lecteurs à reproduire votre analyse.\nDans les sections suivantes, nous fournissons des directives pour les détails que vous devez inclure dans les PAP, y compris un exemple de texte. Nous vous recommandons également d’inclure autant de code et d’analyse de données simulées que possible.2 De nombreux PAP ne pourront pas inclure toute notre liste, mais un PAP devrait, au minimum, inclure la liste complète des hypothèses que vous avez l’intention de tester, comment vous allez mesurer les variables pertinentes pour ces hypothèses et un horodatage vérifiable.",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/fr.png' width=20px>&nbsp;&nbsp;French"
    ]
  },
  {
    "objectID": "guides/planning/pap.fr.html#footnotes",
    "href": "guides/planning/pap.fr.html#footnotes",
    "title": "10 choses à savoir sur les plans de pré-analyse",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLes PAP sont encouragés dans le cadre des directives pour la promotion de la transparence et de l’ouverture (Transparency and Openness Promotion, TOP) (Nosek et al. 2015), publiées dans Science, avec les principales revues scientifiques en sciences sociales s’engageant à mettre en œuvre les directives TOP.↩︎\nDes ressources telles que le projet DeclareDesign peuvent vous aider à simuler et à analyser de fausses données qui imitent les données réelles que votre projet collectera. L’analyse de puissance statistique est un élément important d’un PAP et nécessite des données simulées.↩︎\nComme R.A. Fisher a dit, souvent cité par la suite : “élaborez vos théories … lors de la construction d’une hypothèse causale, il faut envisager autant de conséquences différentes de la vérité que possible”, (Cochran, 1965; cité dans Rosenbaum (2010), pp. 327). Bien que cela ait été dit à propos de la détermination de la causalité dans les études d’observation, la logique s’applique également aux études expérimentales.↩︎\nPour un exemple de test omnibus, voir Caughey, Dafoe, and Seawright (2017).↩︎\nVous pouvez également être intéressé par l’estimation d’autres types d’effets. Voir ce guide sur les types d’effets de traitement pour plus d’informations sur les types d’effet.↩︎\nPar exemple, vous pouvez calculer les erreurs types et les \\(p\\)-valeurs en utilisant l’inférence de randomisation basée sur la permutation. Ou vous pouvez approcher précisement les erreurs types et les \\(p\\)-valeurs en utilisant des méthodes analytiques (Samii and Aronow 2012)↩︎\nNotez que certaines organisations n’utilisent pas de sites tiers. Par exemple, le U.S. General Services Administration Office of Evaluation Sciences Process utilise Github, qui permet de vérifier que le PAP a été créé avant la réalisation de l’analyse.↩︎",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/fr.png' width=20px>&nbsp;&nbsp;French"
    ]
  },
  {
    "objectID": "guides/planning/pap.es.html",
    "href": "guides/planning/pap.es.html",
    "title": "1 Resumen",
    "section": "",
    "text": "Esta guía explica qué es un Plan de Análisis Previo (PAP) y para qué se usa, presenta preguntas orientativas para facilitar la redacción de planes de preanálisis para sus estudios y ofrece un modelo de PAP. Al final de este documento se incluyen enlaces a registros de PAP aprobados que pueden servir como ejemplos prácticos.",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/es.png' width=20px>&nbsp;&nbsp;Spanish"
    ]
  },
  {
    "objectID": "guides/planning/pap.es.html#footnotes",
    "href": "guides/planning/pap.es.html#footnotes",
    "title": "1 Resumen",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLos PAP son promovidos como parte de las pautas para la promoción de la transparencia y apertura (TOP)(https://osf.io/4kdbm/?_ga=2.259736077.62256863.1547257566-317545181.1501862097) (Nosek et al. 2015), publicadas en la revista Science. Al firmar dicho documento, las principales revistas de ciencias sociales se comprometen a aplicar las pautas TOP.↩︎\nPara un ejemplo de una prueba ómnibus, véase Caughey, Dafoe, and Seawright (2017).↩︎\nPor ejemplo, se podrían calcular los errores estándar y los valores \\(p\\) utilizando métodos de permutación (“inferencia por aleatorización”) (https://egap.org/resource/10-things-to-know-about-randomization-inference). O se podrían aproximar los errores estándar y los valores \\(p\\) utilizando métodos analíticos (Samii and Aronow 2012)↩︎\nTenga en cuenta que algunas organizaciones no utilizan sitios de terceros. Por ejemplo, el proceso de la Oficina de Ciencias de la Evaluación de la Administración de Servicios Generales de EE.UU. utiliza Github, que tiene marcas de tiempo que verifican que el PAP fue creado antes de que se realizara el análisis↩︎",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/es.png' width=20px>&nbsp;&nbsp;Spanish"
    ]
  },
  {
    "objectID": "guides/assessing-designs/external-validity.es.html",
    "href": "guides/assessing-designs/external-validity.es.html",
    "title": "10 cosas que debe saber sobre la validez externa",
    "section": "",
    "text": "La validez externa es un nombre que se da a la generalización de resultados, y consiste en preguntarse “si una relación causal se mantiene a pesar de la variación de personas, entornos, tratamientos o variables de resultado”1. Por ejemplo, una de las preocupaciones frente a la validez externa es si los experimentos de laboratorio tradicionales en economía o psicología llevados a cabo con estudiantes universitarios, producen resultados que son generalizables al público común. Por ejemplo, en la economía política del desarrollo, podríamos evaluar la posibilidad de que un programa de desarrollo impulsado por la comunidad en la India, pueda aplicarse (o no) en África Occidental o en América Central.\nLa validez externa adquiere especial importancia cuando se formulan recomendaciones sobre políticas públicas derivadas de la investigación. La extrapolación de los efectos causales de uno o más estudios a un contexto político determinado requiere una cuidadosa consideración tanto de la teoría como de las evidencias empíricas. En esta guía de métodos se analizan algunos conceptos clave, los obstáculos que hay que evitar y las referencias útiles que hay que tener en cuenta al pasar de un Efecto Promedio Local a un mundo más amplio y tangible.",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/es.png' width=20px>&nbsp;&nbsp;Spanish"
    ]
  },
  {
    "objectID": "guides/assessing-designs/external-validity.es.html#footnotes",
    "href": "guides/assessing-designs/external-validity.es.html#footnotes",
    "title": "10 cosas que debe saber sobre la validez externa",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference. Houghton, Mifflin and Company.↩︎\nCampbell, D. T. (1957). Factors relevant to the validity of experiments in social settings. Psychological bulletin, 54(4), 297.↩︎\nSe pueden encontrar más detalles en la guía de métodos de inferencia causal: http://egap.org/resource/10-cosas-que-debe-saber sobre-la-inferencia-causal↩︎\nImbens, G. (2013). Artículo de reseña del libro: Public Policy in an Uncertain World: By Charles F. Manski. The Economic Journal,123(570), F401-F411.↩︎\nManski, C. F. (2013). Response to the review of ‘public policy in an uncertain world’. The Economic Journal 123: F412-F415.↩︎\nSamii, Cyrus. (2016). “Causal Empiricism in Quantitative Research.” Journal of Politics 78(3):941–955.↩︎\nRosenbaum, Paul R. (1999). “Choice as an Alternative to Control in Observational Studies” (con discusión). Statistical Science 14(3): 259–304.↩︎\nGerber, A. S., & Green, D. P. (2012). Field experiments: Design, analysis, and interpretation. WW Norton.↩︎\nBisbee, James; Rajeev Dehejia; Cristian Pop-Eleches & Cyrus Samii. (2016). “Local Instruments, Global Extrapolation: External Validity of the Labor Supply-Fertility Local Average Treatment Effect.” Journal of Labor Economics↩︎\nBisbee, James; Rajeev Dehejia; Cristian Pop-Eleches & Cyrus Samii. (2016). “Local Instruments, Global Extrapolation: External Validity of the Labor Supply-Fertility Local Average Treatment Effect.” Journal of Labor Economics↩︎\nKern, H. L., Stuart, E. A., Hill, J., & Green, D. P. (2016). Assessing methods for generalizing experimental impact estimates to target populations. Journal of Research on Educational Effectiveness, 9(1), 103-127.↩︎\nShadish, W. R., Cook, T. D., y Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference. Houghton, Mifflin and Company.↩︎\nOpen Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716.↩︎",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/es.png' width=20px>&nbsp;&nbsp;Spanish"
    ]
  },
  {
    "objectID": "guides/assessing-designs/external-validity.en.html",
    "href": "guides/assessing-designs/external-validity.en.html",
    "title": "10 Things You Need to Know About External Validity",
    "section": "",
    "text": "External validity is another name for the generalizability of results, asking “whether a causal relationship holds over variation in persons, settings, treatments and outcomes.” (Shadish, Cook, and Campbell 2002). A classic example of an external validity concern is whether traditional economics or psychology lab experiments carried out on college students produce results that are generalizable to the broader public. In the political economy of development, we might consider how a community-driven development program in India might apply (or not) in West Africa, or Central America.\nExternal validity becomes particularly important when making policy recommendations that come from research. Extrapolating causal effects from one or more studies to a given policy context requires careful consideration of both theory and empirical evidence. This methods guide discusses some key concepts, pitfalls to avoid, and useful references to consider when going from a Local Average Treatment Effect to the larger world.",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/gb.png' width=20px>&nbsp;&nbsp;English"
    ]
  },
  {
    "objectID": "guides/assessing-designs/external-validity.en.html#footnotes",
    "href": "guides/assessing-designs/external-validity.en.html#footnotes",
    "title": "10 Things You Need to Know About External Validity",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMore details can be found in the causal inference methods guide.↩︎",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/gb.png' width=20px>&nbsp;&nbsp;English"
    ]
  },
  {
    "objectID": "guides/assessing-designs/power.en.html",
    "href": "guides/assessing-designs/power.en.html",
    "title": "10 Things You Need to Know About Statistical Power",
    "section": "",
    "text": "Power is the ability to distinguish signal from noise.\nThe signal that we are interested in is the impact of a treatment on some outcome. Does education increase incomes? Do public health campaigns decrease the incidence of disease? Can international monitoring decrease government corruption?\nThe noise that we are concerned about comes from the complexity of the world. Outcomes vary across people and places for myriad reasons. In statistical terms, you can think of this variation as the standard deviation of the outcome variable. For example, suppose an experiment uses rates of a rare disease as an outcome. The total number of affected people isn’t likely to fluctuate wildly day to day, meaning that the background noise in this environment will be low. When noise is low, experiments can detect even small changes in average outcomes. A treatment that decreased the incidence of the disease by 1% percentage points would be easily detected, because the baseline rates are so constant.\nNow suppose an experiment instead used subjects’ income as an outcome variable. Incomes can vary pretty widely – in some places, it is not uncommon for people to have neighbors that earn two, ten, or one hundred times their daily wages. When noise is high, experiments have more trouble. A treatment that increased workers’ incomes by 1% would be difficult to detect, because incomes differ by so much in the first place.\nA major concern before embarking on an experiment is the danger of a false negative. Suppose the treatment really does have a causal impact on outcomes. It would be a shame to go to all the trouble and expense of randomizing the treatment, collecting data on both treatment and control groups, and analyzing the results, just to have the effect be overwhelmed by background noise.\nIf our experiments are highly-powered, we can be confident that if there truly is a treatment effect, we’ll be able to see it.",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/gb.png' width=20px>&nbsp;&nbsp;English"
    ]
  },
  {
    "objectID": "guides/assessing-designs/power.en.html#footnotes",
    "href": "guides/assessing-designs/power.en.html#footnotes",
    "title": "10 Things You Need to Know About Statistical Power",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nReproduced from Gerber and Green (2012), page 93.↩︎\nFor an additional online power visualization tool, see Kristoffer Magnusson’s R Psychologist blog.↩︎",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/gb.png' width=20px>&nbsp;&nbsp;English"
    ]
  },
  {
    "objectID": "guides/causal-inference/x-cause-y.en.html",
    "href": "guides/causal-inference/x-cause-y.en.html",
    "title": "10 Strategies for Figuring out whether X Causes Y",
    "section": "",
    "text": "1 Randomization\nThe strategy used in randomized control trials (or randomized interventions, randomized experiments) is to use some form of a lottery to determine who, among some group, will or won’t get access to a treatment or program (or perhaps who will get it first and who will get it later, or who will get one version and who will get another). The elegance of the approach is that it uses randomness to work out what the systematic effects of a program are. The randomness reduces the chance that an observed relationship between treatment and outcomes is due to “confounders”—other things that are different between groups (for example one might be worried that things look better in treatment areas precisely because programs choose to work in well-functioning areas, but knowing that the selection was random completely removes this concern). It is powerful because it guarantees that there is no systematic relationship between treatment and all other features that can affect outcomes, whether you are aware of them or not. For this reason it is often considered to be the gold standard. Randomization cannot be used always and everywhere however, both for ethical and practical reasons. But it can be used in many more situations than people think. See Humphreys and Weinstein (2009) for a discussion of strengths and limitations of the approach for research in the political economy of development.\n\n\n2 Experimental Control (induced unit homogeneity)\nA second strategy used more in lab settings and also in the physical sciences is to use experimental control to ensure that two units are identical to each other in all relevant respects except for treatment. For example if you wanted to see if a heavy ball falls faster than a lighter ball you might make sure that they have the same shape and size and drop them both at the same time, under the same weather conditions, and so on. You then attribute any differences in outcomes to the feature that you did not keep constant between the two units. This strategy is fundamentally different to that used in randomized trials. In randomized trials you normally give up on the idea of keeping everything fixed and seek instead to make sure that natural variation—on variables that you can or cannot observe—does not produce bias in your estimates; in addition you normally seek to assess average effects across a range of background conditions rather than for a fixed set of background conditions. The merits of the control approach depend on your confidence that you can indeed control all relevant factors; if you cannot, then a randomized approach may be superior.\n\n\n3 Natural experiments (as-if randomization)\nSometimes researchers are not able to randomize, but causal inference is still possible because nature has done the randomization for you. The key feature of the “natural experiment” approach is that you have reason to believe that variation in some natural treatment is “as-if random.” For example say that seats in a school are allocated by lottery. Then you might be able to analyze the effects of school attendance as if it were a randomized control trial. One clever study of the effects of conflict on children by Blattman and Annan (2010) used the fact that the Lord’s Resistance Army (LRA) in Uganda abducted children in a fairly random fashion. Another clever study on Disarmament, Demobilization, and Reintegration (DDR) programs by Gilligan, Samii, and Mvukiyehe (2012) used the fact that an NGO’s operations were interrupted because of a contract dispute, which resulted in a “natural” control group of ex-combatants that did not receive demobilization programs. See Dunning (2012) for a guide to finding and analyzing natural experiments.\n\n\n4 Before/after comparisons\nOften the first thing that people look to in order to work out causal effects is the comparison of units before and after treatment. Here you use the past as a control for the present. The basic idea is very intuitive: you switch the lightswitch off and you see the light switch off; attributing the light change to the action seems easy even in the absence of any randomization or control. But for many social interventions the approach is not that reliable, especially in changing environments. The problem is that things get better or worse for many reasons unrelated to treatments or programs you are interested in. In fact it is possible that because of all the other things that are changing, things can get worse in a program area even if the programs had a positive effect (so they get worse but are still not as bad as they would have been without the program!). A more sophisticated approach than simple before/after comparison is called “difference in differences” – basically you compare the before/after difference in treatment areas with those in control areas. This is a good approach but you still need to be sure that you have good control groups and in particular that control and treatment groups are not likely to change differently for reasons other than the treatment.\n\n\n5 Ex Post Controlling I: Regression\nPerhaps the most common approach to causal identification in applied statistical work is the use of multiple regression to control for possible confounders. The idea is to try to use whatever information you have about why treatment and control areas are not readily comparable and adjust for these differences statistically. This approach works well to the extent that you can figure out and measure the confounders and how they are related to treatment, but is not good if you don’t know what the confounders are. In general we just don’t know what all the confounders are and that exposes this approach to all kinds of biases (indeed if you control for the wrong variables it is possible to introduce bias where none existed previously).\n\n\n6 Ex Post Controlling II: Matching and Weighting\nA variety of alternative approaches seek to account for confounding variables by carefully matching treatment units to one or many control units. Matching has some advantages over regression (for example, estimates can be less sensitive to choices of functional form), but the basic idea is nevertheless similar, and indeed matching methods can be implemented in a regression framework using appropriate weights. Like regression, at its core, this strategy depends on a conviction that there are no important confounding variables that the researcher is unaware of or is unable to measure. Specific methods include:\n\nOptimal full- and pair-matching (Hansen 2004), and see the optmatch package\nOptimal pair-matching with fine-balance via mixed integer programming (Zubizarreta, Paredes, and Rosenbaum 2014). See also the designmatch package and the paper comparing approaches (Los Angeles Resa and Zubizarreta 2016)\nOptimal multi-level matching (for designs with schools and students) (Pimentel et al. 2018)\nSparse optimal matching\nGeneralized full matching (Sävje, Higgins, and Sekhon 2017)\nCoarsened exact matching\nGenetic matching (Diamond and Sekhon 2013)\nEntropy balancing (Hainmueller 2012)\nInverse propensity weighting (Glynn and Quinn 2010)\nStable balancing weights (Zubizarreta 2015), and the use of\nSynthetic controls (Abadie, Diamond, and Hainmueller 2015).\n\n\n\n7 Instrumental variables (IV)\nAnother approach to identifying causal effects is to look for a feature that explains why a given group got a treatment but which is otherwise unrelated to the outcome of interest. Such a feature is called an instrument. For example say you are interested in the effect of a livelihoods program on employment, and say it turned out that most people who got access to the livelihoods program did so because they were a relative of a particular program officer. Now suppose that being a relative of the program officer does not affect job prospects in any way other than through its effect on getting access to the livelihoods program. If so, then you can work out the effect of the program by understanding the effect of being a relative of the program officer on job prospects. This has been a fairly popular approach but the enthusiasm for it has died a bit, basically because it is hard to find a good instrument. One smart application are studies on the effects of poverty on conflict which use rainfall in Africa as an instrument for income/growth. While there are worries that the correlation between conflict and poverty may be due to the fact that conflict causes poverty, it does not seem plausible that conflict causes rainfall! So using rainfall as an instrument here gave a lot more confidence that really there is a causal, and not just correlational, relationship between poverty and conflict (Miguel, Satyanath, and Sergenti 2004).\n\n\n8 Regression discontinuity designs (RDD)\nThe regression discontinuity approach works as follows. Say that some program is going to be made available to a set of potential beneficiaries. These potential beneficiaries are all ranked on a set of relevant criteria, such as prior education levels, employment status, and so on. These criteria can be quantitative; but they can also include qualitative information such as assessments from interviews. These individual criteria are then aggregated into a single score and a threshold is identified. Candidates scoring above this threshold are admitted to the program, while those below are not. “Project” and “comparison” groups are then identified by selecting applicants that are close to this threshold on either side. Using this method we can be sure that treated and control units are similar, at least around the threshold. Moreover, we have a direct measure of the main feature on which they differ (their score on the selection criteria). This information provides the key to estimating a program effect from comparing outcomes between these two groups. The advantage of this approach is that all that is needed is that the implementing agency uses a clear set of criteria (which can be turned into a score) upon which they make treatment assignment decisions. The disadvantage is that really reliable estimates of impact can only be made for units right around the threshold. For overviews of RDD, see Skovron and Titiunik (2015) and Lee and Lemieux (2013); for two interesting applications, see Manacorda, Miguel, and Vigorito (2011) on Uruguay and Samii (2013) on Burundi.\n\n\n9 Process tracing\nIn much qualitative work researchers try to establish causality by looking not just at whether being in a program is associated with better outcomes but (a) looking for steps in the process along the way that would tell you whether a program had the effects you think it had and (b) looking for evidence of other outcomes that should be seen if (or perhaps: if and only if) the program was effective. For example not just whether people in a livelihoods program got a job but whether they got trained in something useful, got help from people in the program to find an employer in that area, and so on. If all these steps are there, that gives confidence that the relationship is causal and not spurious. If a program was implemented but no one actually took part in it, this might give grounds to suspect that any correlation between treatment and outcomes is spurious. The difficulty with this approach is that it can be hard to know whether any piece of within-case evidence has probative value. For example a program may have positive (or negative) effects through lots of processes that you don’t know anything about and processes that you think are important, might not be. See Humphreys and Jacobs (2015) for a description of the Bayesian logic underlying process tracing and illustrations of how to combine it with other statistical approaches.\n\n\n10 Front Door Strategies (Argument from mechanisms)\nA final approach, conceptually close to process tracing, is to make use of mechanisms. Say you know, as depicted in the picture below, that \\(A\\) can cause \\(C\\) only through \\(B\\). Say moreover that you know that no third variable causes both \\(B\\) and \\(C\\) (other than, perhaps, via \\(A\\)) and no third variable causes both \\(A\\) and \\(B\\). Then covariation between \\(A\\) and \\(B\\) and between \\(B\\) and \\(C\\) can be used to assess the effect of \\(A\\) on \\(C\\). The advantage is that causality can be established even in the presence of confounders — for example even if, as in the picture below, unobserved variables cause both \\(A\\) and \\(C\\). The difficulty however is that the strategy requires a lot of confidence in your beliefs about the structure of causal relations. For more see Pearl (2000).\n\n\n\n\n\n\n\n\n\n Back to topReferences\n\nAbadie, Alberto, Alexis Diamond, and Jens Hainmueller. 2015. “Comparative Politics and the Synthetic Control Method.” American Journal of Political Science 59 (2): 495–510.\n\n\nBlattman, Christopher, and Jeannie Annan. 2010. “The Consequences of Child Soldiering.” The Review of Economics and Statistics 92 (4): 882–98.\n\n\nDiamond, Alexis, and Jasjeet S. Sekhon. 2013. “Genetic Matching for Estimating Causal Effects: A General Multivariate Matching Method for Achieving Balance in Observational Studies.” The Review of Economics and Statistics 95 (3): 932–45.\n\n\nDunning, Thad. 2012. Natural Experiments in the Social Sciences Natural Experiments in the Social Sciences: A Design-Based Approach. Cambridge University Press.\n\n\nGilligan, Michael J., Cyrus Samii, and Eric N. Mvukiyehe. 2012. “Reintegrating Rebels into Civilian Life: Quasi-Experimental Evidence from Burundi.” Journal of Conflict Resolution 57 (4).\n\n\nGlynn, Adam N., and Kevin M. Quinn. 2010. “An Introduction to the Augmented Inverse Propensity Weighted Estimator.” Political Analysis 18 (1): 36–56.\n\n\nHainmueller, Jens. 2012. “Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies.” Political Analysis 20: 25–46.\n\n\nHansen, Ben B. 2004. “Full Matching in an Observational Study of Coaching for the SAT.” Journal of the American Statistical Association 99 (467): 609–18.\n\n\nHumphreys, Macartan, and Alan Jacobs. 2015. “Mixing Methods: A Bayesian Approach.” American Political Science Review 109 (4): 653–73.\n\n\nHumphreys, Macartan, and Jeremy M. Weinstein. 2009. “Field Experiments and the Political Economy of Development.” Annual Review of Political Science 12: 367–78.\n\n\nLee, David S., and Thomas Lemieux. 2013. “The SAGE Handbook of Regression Analysis and Causal Inference.” In. SAGE Publications Ltd.\n\n\nLos Angeles Resa, Marı́a de, and José R. Zubizarreta. 2016. “Evaluation of Subset Matching Methods and Forms of Covariate Balance.” Statistics in Medicine 35 (27): 4961–79.\n\n\nManacorda, Marco, Edward Miguel, and Andrea Vigorito. 2011. “Government Transfers and Political Support.” American Economic Journal: Applied Economics 3 (3): 1–28.\n\n\nMiguel, Edward, Shanker Satyanath, and Ernest Sergenti. 2004. “Economic Shocks and Civil Conflict: An Instrumental Variables Approach.” Journal of Political Economy 112 (4): 725–53.\n\n\nPearl, Judea. 2000. Causality: Models, Reasoning, and Inference. Cambridge University Press.\n\n\nPimentel, Samuel D., Lindsay C. Page, Matthew Lenard, and Luke Keele. 2018. “Optimal Multilevel Matching Using Network Flows: An Application to a Summer Reading Intervention.” Ann. Appl. Stat. 12 (3): 1479–1505.\n\n\nSamii, Cyrus. 2013. “Perils or Promise of Ethnic Integration? Evidence from a Hard Case in Burundi.” American Political Science Review 107 (3): 558–73.\n\n\nSävje, Fredrik, Michael J. Higgins, and Jasjeet S. Sekhon. 2017. “Generalized Full Matching and Extrapolation of the Results from a Large-Scale Voter Mobilization Experiment.”\n\n\nSkovron, Christopher, and Rocı́o Titiunik. 2015. “A Practical Guide to Regression Discontinuity Designs in Political Science.” In.\n\n\nZubizarreta, José R. 2015. “Stable Weights That Balance Covariates for Estimation with Incomplete Outcome Data.” Journal of the American Statistical Association 110 (511): 910–22.\n\n\nZubizarreta, José R., Ricardo D. Paredes, and Paul R. Rosenbaum. 2014. “Matching for Balance, Pairing for Heterogeneity in an Observational Study of the Effectiveness of for-Profit and Not-for-Profit High Schools in Chile.” Ann. Appl. Stat. 8 (1): 204–31.",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/gb.png' width=20px>&nbsp;&nbsp;English"
    ]
  },
  {
    "objectID": "guides/causal-inference/x-cause-y.fr.html",
    "href": "guides/causal-inference/x-cause-y.fr.html",
    "title": "10 stratégies pour déterminer si X cause Y",
    "section": "",
    "text": "1 Randomisation\nLa stratégie utilisée dans les essais randomisés contrôlés (interventions randomisées, expériences randomisées) consiste à utiliser une forme de loterie pour déterminer qui, dans un groupe, aura ou n’aura pas accès à un traitement ou à un programme (ou peut-être qui aura accès en premier et qui aura accès plus tard, ou qui aura accès à une version et qui aura accès à une autre). L’élégance de l’approche est qu’elle utilise le hasard pour déterminer quels sont les effets systématiques d’un programme. Le caractère aléatoire réduit le risque qu’une relation observée entre le traitement et les résultats soit due à des “facteurs de confusion” - i.e. d’autres différences entre les groupes (par exemple, on pourrait craindre que les choses s’améliorent dans les zones de traitement précisément parce que les programmes choisissent une zone qui fonctionne déjà bien, mais savoir que la sélection est aléatoire efface complètement cette préoccupation). La randomisation est puissante car elle garantit qu’il n’y a pas de relation systématique entre le traitement et toutes les autres caractéristiques qui peuvent affecter les résultats, que vous en soyez conscient ou non. Pour cette raison, c’est souvent considéré comme l’étalon-or. Cependant, la randomisation ne peut être utilisée toujours et partout, à la fois pour des raisons éthiques et pratiques. Mais elle peut être utilisée dans beaucoup plus de situations que les gens ne le pensent. Voir Humphreys et Weinstein pour une discussion sur les forces et les limites de l’approche en économie politique du développement.\n\n\n2 Contrôle expérimental (homogénéité unitaire induite)\nUne deuxième stratégie plus utilisée en laboratoire et également en sciences physiques consiste à utiliser un contrôle expérimental pour s’assurer que deux unités sont identiques à tous les égards pertinents, à l’exception du traitement. Par exemple, si vous vouliez voir si une balle lourde tombe plus vite qu’une balle plus légère, vous pouvez vous assurer qu’elles ont la même forme et la même taille et les laisser tomber toutes les deux en même temps, dans les mêmes conditions météorologiques, et ainsi de suite. Vous attribuez ensuite toute différence de résultats à la caractéristique que vous n’avez pas maintenue constante entre les deux unités. Cette stratégie est fondamentalement différente de celle utilisée dans les essais randomisés. Dans les essais randomisés, vous abandonnez l’idée de tout garder fixe et cherchez plutôt à vous assurer que la variation naturelle - sur des variables que vous pouvez ou ne pouvez pas observer - ne produit pas de biais dans vos estimations ; en outre, vous cherchez à évaluer les effets moyens pour une plage de conditions de base plutôt que pour un ensemble fixe de conditions de base. Les mérites de l’approche du contrôle expérimental dépendent de votre confiance dans le contrôle effectif de tous les facteurs pertinents ; si le contrôle n’est pas effectif, alors une approche randomisée peut être meilleure.\n\n\n3 Expériences naturelles (randomisation supposée)\nParfois, les chercheurs ne sont pas en mesure de randomiser, mais l’inférence causale est toujours possible car la nature a fait la randomisation pour vous. La caractéristique clé de l’approche “expérience naturelle” est que vous avez des raisons de croire que la variation de certains traitements naturels est “supposée aléatoire”. Par exemple, disons que les places dans une école sont attribuées par tirage au sort. Ensuite, vous pourrez peut-être analyser les effets de la fréquentation scolaire comme s’il s’agissait d’un essai randomisé contrôlé. Une étude intelligente des effets des conflits sur les enfants par Annan et Blattman a utilisé le fait que la Lord’s Resistance Army (LRA) en Ouganda a enlevé des enfants de façon assez aléatoire. Une autre étude intelligente sur les programmes de Désarmement, Démobilisation et Réintégration (DDR) par Gilligan, Mvukiyehe et Samii a utilisé le fait que les opérations d’une ONG ont été interrompues en raison d’un différend contractuel, ce qui a donné lieu à un groupe de contrôle “naturel” d’ex-combattants qui n’ont pas bénéficié de programmes de démobilisation. Voir le livre de Dunning pour trouver et analyser des expériences naturelles.\n\n\n4 Comparaisons avant/après\nSouvent, la première chose vers laquelle les gens se tournent pour déterminer les effets causaux est la comparaison des unités avant et après le contrôle. Ici, vous utilisez le passé comme un contrôle pour le présent. L’idée de base est très intuitive : vous éteignez la lumière et vous voyez la lumière s’éteindre ; attribuer le changement de lumière à l’action semble facile même en l’absence de toute randomisation ou contrôle. Mais pour de nombreuses interventions sociales, l’approche n’est pas si fiable, en particulier dans des environnements changeants. Le problème est que les choses s’améliorent ou empirent pour de nombreuses raisons sans rapport avec les traitements ou les programmes qui vous intéressent. En fait, il est possible qu’en raison de toutes les autres choses qui changent, les choses peuvent empirer dans un domaine du programme même si les programmes ont eu un effet positif (donc les choses empirent mais ne sont toujours pas aussi mauvaises qu’elles l’auraient été sans le programme!). Une approche plus sophistiquée que la simple comparaison avant/après est appelée “méthode des doubles différences” – en gros, vous comparez la différence avant/après dans les zones de traitement avec celles dans les zones de contrôle. C’est une bonne approche mais vous devez toujours être sûr que vous avez de bons groupes de contrôle et en particulier que les groupes de contrôle et de traitement ne sont pas susceptibles de changer différemment pour des raisons autres que le traitement.\n\n\n5 Contrôle ex post I : régression\nL’approche la plus courante de l’identification causale dans les travaux de statistique appliqués est peut-être l’utilisation de la régression multiple pour contrôler les facteurs de confusion possibles. L’idée est d’essayer d’utiliser toutes les informations dont vous disposez sur les raisons pour lesquelles les zones de traitement et de contrôle ne sont pas facilement comparables et d’ajuster statistiquement ces différences. Cette approche fonctionne bien dans la mesure où vous pouvez déterminer et mesurer les facteurs de confusion et savoir comment ils sont liés au traitement, mais elle n’est pas correcte si vous ne savez pas quels sont les facteurs de confusion. En général, nous ne savons tout simplement pas quels sont tous les facteurs de confusion et cela expose cette approche à toutes sortes de biais (en effet, si vous contrôlez les mauvaises variables, il est possible d’introduire un biais là où il n’y en avait pas auparavant).\n\n\n6 Contrôle ex post II: appariement et pondération\nDiverses approches alternatives cherchent à tenir compte des variables de confusion en associant soigneusement les unités de traitement à une ou plusieurs unités de contrôle. L’appariement présente certains avantages par rapport à la régression (par exemple, les estimations peuvent être moins sensibles aux choix de la forme fonctionnelle), mais l’idée de base est néanmoins similaire. En effet, les méthodes d’appariement peuvent être mises en œuvre dans un cadre de régression en utilisant des poids appropriés. Comme la régression, cette stratégie repose sur la conviction qu’il n’y a pas de variables de confusion importantes que le chercheur ignore ou est incapable de mesurer. Les méthodes spécifiques comprennent :\n\nappariement complet optimal, voir le package optmatch\nappariement optimal avec équilibrage précis via une programmation mixte en nombres entiers, voir aussi le package designmatch et l’article comparant les approches\nappariement multi-niveaux optimal (pour les conceptions avec des écoles et des étudiants)\nappariement creux optimal\nappariement généralisé complet\nappariement grossier exact\nappariement génétique\néquilibre d’entropie\npondération inverse du score de propension\npondération de l’équilibre stable\ncontrôles synthétiques.\n\n\n\n7 Variables instrumentales\nUne approche très différente pour estimer les effets causaux peut être utilisée si les chercheurs peuvent trouver une caractéristique qui explique pourquoi un groupe donné a reçu un traitement mais qui n’est par ailleurs pas liée au résultat d’intérêt. Une telle caractéristique est appelée un instrument. Par exemple, disons que vous êtes intéressé par l’effet sur l’emploi d’un programme d’aide à la précarité, et que la plupart des personnes ont accédé à ce programme parce qu’elles avaient un lien de parenté avec un agent travaillant pour ce programme particulier. Ensuite, s’il n’y avait pas d’autres façons d’établir une relation entre le lien de parenté de cette personne et ses perspectives d’emploi, alors vous pouvez déterminer l’effet du programme en calculant l’effet d’être un parent de cette personne sur les perspectives d’emploi. Cela a été une approche assez populaire, mais l’enthousiasme pour cette approche est un peu retombé, essentiellement parce qu’il est difficile de trouver un bon instrument. Une application intelligente pour examiner les effets de la pauvreté sur les conflits a utilisé les précipitations en Afrique comme instrument de revenu/croissance. Bien que l’on s’inquiète que la corrélation entre conflit et pauvreté soit due au fait que le conflit pourrait causer la pauvreté, il ne semble pas plausible que le conflit provoque des précipitations ! Ainsi, l’utilisation des précipitations comme instrument ici a augmenté la certitude qu’il existe réellement une relation causale, et pas seulement une corrélation, entre pauvreté et conflit.\n\n\n8 La méthode de régression par discontinuité (regression discontinuity design, RDD)\nLa régression par discontinuité est l’une des approches les plus sous-utilisées, mais elle a un potentiel énorme. La stratégie fonctionne comme suit. Disons qu’un programme va être mis à la disposition d’un ensemble de bénéficiaires potentiels. Ces bénéficiaires potentiels sont tous classés selon un ensemble de critères pertinents, tels que le niveau d’éducation antérieur, le statut d’emploi, etc. Ces critères peuvent être quantitatifs ; mais ils peuvent également inclure des informations qualitatives telles que des évaluations issues d’entretiens. Ces critères individuels sont ensuite agrégés en un seul score et un seuil est identifié. Les candidats dont le score est supérieur à ce seuil sont admis au programme, tandis que les autres ne le sont pas. Les groupes “projet” et “comparaison” sont ensuite identifiés en sélectionnant des candidats proches de ce seuil de part et d’autre. En utilisant cette méthode, nous pouvons être sûrs que les unités traitées et témoins sont similaires, au moins autour du seuil. De plus, nous avons une mesure directe de la principale caractéristique sur laquelle ils diffèrent (leur score sur les critères de sélection). Cette information fournit la clé pour estimer l’effet d’un programme en comparant les résultats entre ces deux groupes. L’avantage de cette approche est que tout ce qui est nécessaire est que l’organisme en charge utilise un ensemble clair de critères (qui peuvent être transformés en un score) sur lesquels ils prennent la décision d’assignation au traitement. L’inconvénient est que des estimations vraiment fiables de l’impact ne peuvent être faites que pour les unités situées juste autour du seuil. Pour un aperçu de la régression par discontinuité, voir Skovron et Titiunik, Lee et Lemieux. Pour deux applications intéressantes, voir Manacorda et al. en Uruguay et Samii au Burundi.\n\n\n9 Traçage de processus\nDans de nombreux travaux qualitatifs, les chercheurs essaient d’établir la causalité en cherchant à savoir si le fait d’être dans un programme est associé à de meilleurs résultats, mais aussi (a) en recherchant les étapes du processus qui montreraient que le programme a eu les effets que vous pensez qu’il a eu (b) et en recherchant des preuves d’autres résultats qui devraient être observés si (ou peut-être : si et seulement si) le programme était efficace. Par exemple, ils veulent savoir si les personnes participant à un programme d’aide à la précarité ont obtenu un emploi, mais aussi si elles ont été formées à quelque chose d’utile, ont obtenu de l’aide des personnes participant au programme pour trouver un employeur dans ce domaine, etc. Si toutes ces étapes sont présentes, cela donne l’assurance que la relation est causale et non fausse. Si un programme a été mis en œuvre mais que personne n’y a réellement participé, cela peut laisser penser que toute corrélation entre le traitement et les résultats est fausse. La difficulté avec cette approche est qu’il peut être difficile de savoir si un élément de preuve au sein du processus a une valeur probante. Par exemple, un programme peut avoir des effets positifs (ou négatifs) à travers de nombreux processus dont vous ne savez rien et des processus que vous jugez importants, peuvent ne pas l’être. Voir Humphreys et Jacobs pour une description de la logique bayésienne sous-jacente au traçage des processus et des illustrations sur la façon de le combiner avec d’autres approches statistiques.\n\n\n10 Stratégies “porte d’entrée” (argument basé sur les mécanismes)\nUne dernière approche, conceptuellement proche du traçage de processus, consiste à argumenter à partir des mécanismes. Supposons que vous sachiez que seul \\(A\\) peut causer \\(C\\) uniquement via \\(B\\). De plus, vous savez que rien d’autre ne cause à la fois \\(B\\) et \\(C\\) (autre que, peut-être, via \\(A\\)) et que rien d’autre ne cause à la fois \\(A\\) et \\(B\\). Ensuite, la covariation entre \\(A\\) et \\(B\\) et entre \\(B\\) et \\(C\\) peut être utilisée pour évaluer l’effet de \\(A\\) sur \\(C\\). L’avantage est que la causalité peut être établie même en présence de facteurs de confusion — par exemple si des variables non observées causent à la fois \\(A\\) et \\(C\\). La difficulté cependant est que la stratégie nécessite une confiance forte en vos croyances sur la structure des relations causales. Pour en savoir plus, voir Pearl (2000).\n\n\nReferences\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/fr.png' width=20px>&nbsp;&nbsp;French"
    ]
  },
  {
    "objectID": "guides/causal-inference/x-cause-y.es.html",
    "href": "guides/causal-inference/x-cause-y.es.html",
    "title": "10 estrategias para descubrir si X causa Y",
    "section": "",
    "text": "La estrategia utilizada en pruebas de control aleatorias (o intervenciones aleatorias, experimentos aleatorios) consiste en utilizar algún tipo de sorteo para determinar quién, dentro de algún grupo, tendrá o no acceso a un tratamiento o programa (o quizás quién lo recibirá primero y quién después, o quién recibirá una versión y quién otra). El atractivo de este enfoque es que utiliza la aleatorización para averiguar cuáles son los efectos sistemáticos de un programa. La aleatorización reduce la posibilidad de que una relación observada entre el tratamiento y las variables de resultado se deba a “posibles factores que puedan generar confusión” (confounding factors), es decir, a otras cosas que son diferentes entre los grupos (por ejemplo, podría preocuparnos que las cosas parezcan mejores en áreas tratadas precisamente porque los programas son implementados en áreas donde las cosas funcionan bien, pero al saber que la selección fue aleatoria esta preocupación se elimina por completo). Esta estrategia es poderosa porque garantiza que no hay una relación sistemática entre el tratamiento y todas las demás características que pueden afectar a las variables de resultado, seamos consciente de ellas o no. Por esta razón, a menudo se considera el mejor modelo a seguir. Sin embargo la aleatorización no puede utilizarse siempre y en todas partes, tanto por razones éticas como prácticas. Pero puede utilizarse en muchas más situaciones de lo que se tiende a creer. Léase Humphreys y Weinstein para una discusión de las ventajas y las limitaciones de este método para investigación en economía política de desarrollo.",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/es.png' width=20px>&nbsp;&nbsp;Spanish"
    ]
  },
  {
    "objectID": "guides/causal-inference/x-cause-y.es.html#footnotes",
    "href": "guides/causal-inference/x-cause-y.es.html#footnotes",
    "title": "10 estrategias para descubrir si X causa Y",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAuteur d’origine : Macartan Humphreys. Révisions mineures : Winston Lin, 30 août 2016. Le guide est un document vivant et sujet à une mise à jour par les membres de EGAP à tout moment ; les contributeurs répertoriés ne sont pas responsables des modifications ultérieures.↩︎",
    "crumbs": [
      "Home",
      "Language",
      "<img src='https://flagcdn.com/w20/es.png' width=20px>&nbsp;&nbsp;Spanish"
    ]
  },
  {
    "objectID": "guides/interpretation/null-results_en.html",
    "href": "guides/interpretation/null-results_en.html",
    "title": "10 Things Your Null Result Might Mean",
    "section": "",
    "text": "Imagine you lead the department of education for a government and are wondering about how to boost student attendance. You decide to consider a text message intervention that offers individual students counseling. Counselors at each school can help students address challenges specifically related to school attendance. Your team runs a randomized trial of the intervention, and tells you there is a null result.\nHow should you understand the null result, and what should you do about it? It could be a result of unmet challenges at several stages of your work – in the way the intervention is designed, the way the intervention is implemented, or the way study is designed Below are 10 things to consider when interpreting your null result."
  },
  {
    "objectID": "guides/interpretation/null-results_en.html#your-intervention-theory-and-approach-are-mismatched-to-the-problem.",
    "href": "guides/interpretation/null-results_en.html#your-intervention-theory-and-approach-are-mismatched-to-the-problem.",
    "title": "10 Things Your Null Result Might Mean",
    "section": "1.1 1. Your intervention theory and approach are mismatched to the problem.",
    "text": "1.1 1. Your intervention theory and approach are mismatched to the problem.\nYou delivered a counseling intervention because you thought that students needed support to address challenges in their home life. However, students who had the greatest needs never actually met with a counselor, in part because they did not trust adults at the school. The theory of change assumed that absenteeism was a function primarily of a student’s personal decisions or family circumstances and that the offer of counseling without changes to school climate would be sufficient; it did not account appropriately for low levels of trust in teacher-student relationships. Therefore, this null effect does not suggest that counseling per se cannot boost attendance, but that counseling in the absence of other structural or policy changes or in the context of low-trust schools may not be sufficient.\nHow can you tell if…you have a mismatch between your theory of change and the problem that needs to be solved? List all potential barriers and consider how they connect. Does the intervention as designed address only one of those barriers, and, if so, can it succeed without addressing others? Are there assumptions made about one source or one cause that may undermine the success of the intervention?"
  },
  {
    "objectID": "guides/interpretation/null-results_en.html#your-intervention-strength-or-dosage-is-too-low-for-the-problem-or-outcome-of-interest.",
    "href": "guides/interpretation/null-results_en.html#your-intervention-strength-or-dosage-is-too-low-for-the-problem-or-outcome-of-interest.",
    "title": "10 Things Your Null Result Might Mean",
    "section": "1.2 2. Your intervention strength or dosage is too low for the problem or outcome of interest.",
    "text": "1.2 2. Your intervention strength or dosage is too low for the problem or outcome of interest.\nAfter talking to experts, you learn that counseling interventions can build trust, but usually require meetings that are more frequent and regular than your intervention offered to have the potential for an effect. Maybe your “dose” of services is too small.\nHow can you tell if…you did not have a sufficient “dose”? Even if no existing services tackle your problem of interest, consider what is a minimum level, strength, or dose that is both feasible to implement and could yield an effect. When asking sites what they are willing to take on, beware of defaulting to the lowest dose. The more complex the problem or outcome is to move, the stronger or more comprehensive the intervention may need to be."
  },
  {
    "objectID": "guides/interpretation/null-results_en.html#your-intervention-does-not-represent-a-large-enough-enhancement-over-usual-services.",
    "href": "guides/interpretation/null-results_en.html#your-intervention-does-not-represent-a-large-enough-enhancement-over-usual-services.",
    "title": "10 Things Your Null Result Might Mean",
    "section": "1.3 3. Your intervention does not represent a large enough enhancement over usual services.",
    "text": "1.3 3. Your intervention does not represent a large enough enhancement over usual services.\nIn your position at the state department of education, you learn that students at the target schools were already receiving some counseling and support services. Even though the existing services were not sufficient to boost attendance to the targeted levels, the new intervention did not add enough content or frequency of the counseling services to reach those levels either—the intervention yielded show-up rates that were about the same as existing services. So this null effect does not reflect that counseling has no effect, but rather that the version of counseling your intervention offered was not effective over and above existing counseling services.\nHow can you tell if…the relative strength of your intervention was not sufficient to yield an effect? Take stock of the structure and content of existing services, and consider if the extent or form in which clients respond to existing services indicates that the theory of change or approach needs to be revised. If the theory holds, use existing services as a benchmark and consider whether your proposed intervention needs to include something supplementary and/or something complementary."
  },
  {
    "objectID": "guides/interpretation/null-results_en.html#your-implementation-format-was-not-reliable.",
    "href": "guides/interpretation/null-results_en.html#your-implementation-format-was-not-reliable.",
    "title": "10 Things Your Null Result Might Mean",
    "section": "2.1 4. Your implementation format was not reliable.",
    "text": "2.1 4. Your implementation format was not reliable.\nIn the schools in your study, counseling interventions sometimes occurred in person, sometimes happened by text message, sometimes by phone. Anticipating and allowing for some variation and adaptation is important. Intervention dosage and strength is often not delivered as designed nor to as many people as expected.\nBut unplanned variations in format can reflect a host of selection bias issues, such that you cannot disentangle whether counseling as a concept does not work or whether certain formats of outreach did not work. This is especially important to guard against if you intend to test specific channels or mechanisms critical to your theory of change.\nHow can you tell if…an unreliable format is the reason for your null? Were you able to specify or standardize formats in a checklist? Could you leave enough discretion but still incentivize fidelity? Pre-specifying what the intervention should look like can help staff and researchers monitor along the way and correct inconsistencies or deviations that may affect the results. This could include a training protocol for those implementing the intervention. If nothing was specified or no one was trained, then the lack of consistency may be part of the explanation."
  },
  {
    "objectID": "guides/interpretation/null-results_en.html#your-intervention-and-outcome-measure-are-mismatched-to-your-randomization-design.",
    "href": "guides/interpretation/null-results_en.html#your-intervention-and-outcome-measure-are-mismatched-to-your-randomization-design.",
    "title": "10 Things Your Null Result Might Mean",
    "section": "2.2 5. Your intervention and outcome measure are mismatched to your randomization design.",
    "text": "2.2 5. Your intervention and outcome measure are mismatched to your randomization design.\nYou expected counseling to be more effective in schools with higher student-to-teacher ratios, but did not block randomize by class size (for more on block randomization, see our guide on 10 Things to Know About Randomization). then it may no longer have the potential to be more effective for students in high class size schools.\nHow can you tell if…you have a mismatch between your intervention and randomization design? Consider whether treatment effects could vary, or service delivery might occur in a cluster, or intervention concepts could spill over, and to what extent your randomization design accounted for that."
  },
  {
    "objectID": "guides/interpretation/null-results_en.html#your-study-sample-includes-people-whose-behavior-could-not-be-moved-by-the-intervention.",
    "href": "guides/interpretation/null-results_en.html#your-study-sample-includes-people-whose-behavior-could-not-be-moved-by-the-intervention.",
    "title": "10 Things Your Null Result Might Mean",
    "section": "3.1 6. Your study sample includes people whose behavior could not be moved by the intervention.",
    "text": "3.1 6. Your study sample includes people whose behavior could not be moved by the intervention.\nYou ask schools to randomize students into an intervention or control (business-as-usual) group. Some students in both your intervention and control groups will always attend school, while some students will rarely attend school, regardless of what interventions are or are not offered to them. Your intervention’s success depends on not just whether students actually receive the message and/or believe it, but also on whether it can shift behavior among such potential responders.\nIf the proportion of potential responders is too small, then it may be difficult to detect an effect. In addition, your intervention may need to be targeted and modified in some way to address the needs of potential responders.\nHow can you tell if…the proportion of potential responders may be too small? Take a look at the pre-intervention attendance rate. If it is extremely low, does that rate reflect low demand or structural barriers that may limit the potential for response? Is it so high that it tells us that most people who could respond have already done so (say, 85% or higher)? Even if there is a large proportion of hypothetical potential responders, is it lower when you consider existing barriers preventing students from using counseling services that your intervention is not addressing?"
  },
  {
    "objectID": "guides/interpretation/null-results_en.html#your-measure-is-not-validated-or-reliable-it-varies-too-much-and-systematically-across-sites.",
    "href": "guides/interpretation/null-results_en.html#your-measure-is-not-validated-or-reliable-it-varies-too-much-and-systematically-across-sites.",
    "title": "10 Things Your Null Result Might Mean",
    "section": "3.2 7. Your measure is not validated or reliable: It varies too much and systematically across sites.",
    "text": "3.2 7. Your measure is not validated or reliable: It varies too much and systematically across sites.\nAs a leader of the state’s department of education, you want to measure the effectiveness of your intervention using survey data on student attitudes related to attendance. You learn that only some schools administer a new survey measuring student attitudes, and those with surveys changed the survey items so that there is not the same wording across surveys or schools. If you observe no statistically significant difference on a survey measure that is newly developed or used by only select schools, it may be difficult to know whether the intervention “has no effect” or whether the outcome is measuring something different in each school because of different wording.\nHow can you tell if… outcome measurement is the problem? Check to see whether the outcome is (1) collected in the same way across your sites and (2) if it means the same thing to the participants as it means to you. In addition, check on any reporting bias and if your study participants or sites face any pressure from inside or outside of their organizations to report or answer in a particular way."
  },
  {
    "objectID": "guides/interpretation/null-results_en.html#your-outcome-is-not-validated-or-reliable-it-varies-too-little.",
    "href": "guides/interpretation/null-results_en.html#your-outcome-is-not-validated-or-reliable-it-varies-too-little.",
    "title": "10 Things Your Null Result Might Mean",
    "section": "3.3 8. Your outcome is not validated or reliable: It varies too little.",
    "text": "3.3 8. Your outcome is not validated or reliable: It varies too little.\nGiven the problems with the survey measure, you then decide to use administrative data from student records to measure whether students show up on time to school. But it turns out that schools used a generic definition of “on time” such that almost every student looks like they arrive on time. An outcome that does not have enough variation in it to detect an effect between intervention and control groups can be especially limiting if your intervention potentially could have had different effects on different types of students, but the outcome measure used in the study lacks the precision to capture the effects on different subgroups.\nHow can you tell if…your null result arises from measures that are too coarse or subject to response biases? Pressures to report a certain kind of outcome faced by people at your sites could again yield this kind of problem with outcome measurement. So, it is again worth investigating the meaning of the outcomes as reported by the sites from the perspective of those doing the reporting. This problem differs from the kind of ceiling and floor effects discussed elsewhere in this guide; it arises more from the strategic calculations of those producing administrative data and less from the natural behavior of those students whose behavior you are trying to change."
  },
  {
    "objectID": "guides/interpretation/null-results_en.html#your-statistical-power-is-insufficient-to-detect-an-effect-for-the-intervention-as-implemented.",
    "href": "guides/interpretation/null-results_en.html#your-statistical-power-is-insufficient-to-detect-an-effect-for-the-intervention-as-implemented.",
    "title": "10 Things Your Null Result Might Mean",
    "section": "3.4 9. Your statistical power is insufficient to detect an effect for the intervention as implemented.",
    "text": "3.4 9. Your statistical power is insufficient to detect an effect for the intervention as implemented.\nThis may sound obvious to people with experience testing interventions at scale. But researchers and policymakers can fall into two traps:\n\nThinking about statistical significance rather than what represents a meaningful and feasible effect. Although a study with an incredibly large sample size can detect small effects with precision, one does not want to trade precision for meaning. Moreover, an intervention known to be weak during the intervention design is likely to be weaker when implemented, especially across multiple sites or months. So it may not be sufficient to simply enroll more subjects to study an intervention known to be weak (even though strong research design cannot compensate for a weak intervention in any easy or direct way);\nThinking that the only relevant test statistic for an experiment effect is a difference of means (even though we have long known that differences of means are valid but low-powered test statistics when outcomes do not neatly fit into a normal distribution).\n\nHow can you tell if…your null result arises mostly from low statistical power? Recall that statistical power depends on (a) effect size or intervention strength, (b) variability in outcomes, (c) the number of independent observations (often well measured with sample size), and (d) the test statistic you use. The previous discussions pointed out ways to learn whether an intervention you thought might be strong was weak, or whether an outcome that you thought might be clear could turn out to be very noisy.\nA formal power analysis could also tell you that, given the variability in your outcome and the size of your effect, you would have needed a larger sample size to detect this effect reliably. For example, if you had known about the variability in administration of the treatment or the variability in the outcome (let alone surprises with missing data) in advance, your pre-field power analysis would have told you to use a different sample size.\nA different test statistic can also change a null result into a positive result if, say, the effect is large but it is not an effect that shifts means as much as moves people who are extreme, or has the effect of making moderate students extreme. A classic example of this problem occurs with outcomes that have very long tails – such as those involving money, like annual earnings or auction spending. A t-test might produce a p-value of .20 but a rank-based test might produce a p-value of &lt; .01. The t-test is using evidence of a shift in averages (means) to reflect on the null hypothesis of no effects. The rank-based test is merely asking whether the treatment group outcomes tend to be bigger than (or smaller than) the control group outcomes (whether or not they differ in means)."
  },
  {
    "objectID": "guides/interpretation/null-results_en.html#your-null-needs-to-be-published.",
    "href": "guides/interpretation/null-results_en.html#your-null-needs-to-be-published.",
    "title": "10 Things Your Null Result Might Mean",
    "section": "4.1 10. Your null needs to be published.",
    "text": "4.1 10. Your null needs to be published.\nIf you addressed all the issues above related to intervention design, sample size and research design, and have a precisely estimated, statistically significant null result, it is time to publish. Your colleagues and other researchers need to learn from this finding, so do not keep it to yourself.\nWhen you have a precise null, you do not have a gap in evidence–you are generating evidence.\nWhat can you do to convince editors and reviewers they should publish your null results? This guide should help you reason about your null results and thus explain their importance. If other studies on your topic exist, you can also contextualize your results; for example, follow some of the ideas from Abadie (2020).\nFor an example, see how Bhatti et al. (2018)–in their study of a Danish governmental voter turnout intervention–used previous work on face-to-face voter turnout (reported on as a meta-analysis in Bhatti et al. (2016)) to contextualize their own small effects.\nIf you are unable to find a publication willing to include a study with null results in their journal, you can still contribute to the evidence base on the policy area under examination by making your working paper, data, and/or analysis code publicly available. Many researchers choose to do so via their personal websites; in addition, there are repositories (such as the Open Science Framework) that provide a platform for researchers to share their in-progress and unpublished work."
  },
  {
    "objectID": "guides/analysis-procedures/how-to-analyze-experiments_en.html",
    "href": "guides/analysis-procedures/how-to-analyze-experiments_en.html",
    "title": "10 Things on Your Checklist for Analyzing an Experiment",
    "section": "",
    "text": "This guide will provide practical tools on what to do with your data once you’ve run an experiment. This guide is geared towards anyone involved in the life cycle of an experimental project: from analysts to implementers to project managers. If you’re not the one directly involved in analyzing the data, these are some key things to look for in reports of analysis of experimental data. If you are analyzing the data, this guide provides instructions and R code on how to do so."
  },
  {
    "objectID": "guides/analysis-procedures/how-to-analyze-experiments_en.html#footnotes",
    "href": "guides/analysis-procedures/how-to-analyze-experiments_en.html#footnotes",
    "title": "10 Things on Your Checklist for Analyzing an Experiment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote: the Bernoulli coin need not be defined by 50-50 chances; the researcher defines ex-ante the probability of receiving treatment versus control, which could be 70-30 instead.↩︎\nSee the methods guide on balance tests for more on this. However, the key (unobservable) assumption that we seek to test is whether or not treatment assignment is independent of potential outcomes.↩︎\nHartman and Hidalgo (2018) write on the meaning and implications of equivalence tests, vis-a-vis conventional balance tests.↩︎\nHowever, protecting against ‘bad draws’ is possible through blocked designs: see 10 Things You Need to Know About Randomization.↩︎"
  },
  {
    "objectID": "guides/analysis-procedures/meta-analysis_en.html",
    "href": "guides/analysis-procedures/meta-analysis_en.html",
    "title": "10 Things to Know About Conducting a Meta-Analysis",
    "section": "",
    "text": "Meta-analysis is a method for summarizing the statistical findings from a research literature. For example, if five experiments have been conducted using the same intervention and outcome measure on the same population of people with five separate estimates of an average treatment effect, one might imagine pooling these five studies together into a single dataset and analyzing them jointly. In broad strokes, in such a case, we could act as though the studies came from five blocks within a single experiment rather than five separate experiments. The benefit of such an approach would be more statistical power in the estimation of one overall average treatment effect. In essence, a meta-analysis produces a weighted average of the five studies’ results. As explained below, this method is also used to summarize research literatures that comprise a diverse array of interventions and outcomes measured in diverse settings, under the assumption that the interventions are theoretically similar and the outcome measures tap into a shared underlying trait."
  },
  {
    "objectID": "guides/analysis-procedures/meta-analysis_en.html#footnotes",
    "href": "guides/analysis-procedures/meta-analysis_en.html#footnotes",
    "title": "10 Things to Know About Conducting a Meta-Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://twitter.com/uri_sohn/status/471318552470126592↩︎\nFor a list of R packages useful in conducting meta-analysis, see here.↩︎\nSee for example https://www.stata.com/support/faqs/statistics/meta-analysis/ and https://cran.r-project.org/web/views/MetaAnalysis.html.↩︎"
  },
  {
    "objectID": "guides/analysis-procedures/multiple-comparisons_en.html",
    "href": "guides/analysis-procedures/multiple-comparisons_en.html",
    "title": "10 Things You Need to Know About Multiple Comparisons",
    "section": "",
    "text": "Typically, researchers are not interested in just one treatment versus control comparison per experiment. There are three main ways that comparisons proliferate:\n\nMultiple treatment arms. When an experiment has \\(n\\) treatment arms, there are \\(n(n-1)/2\\) possible comparisons between the arms.\nHeterogeneous treatment effects. Often, we are interested in whether the treatment has different impacts on different subgroups. For example, a treatment might be more effective for women than for men.\nMultiple estimators. Often, experimenters will apply multiple estimators to the same dataset: for example, difference-in-means and covariate adjustment. There is of course nothing wrong with employing multiple treatment arms, exploring treatment effect heterogeneity, or using multiple estimators of the treatment effect. However, these design and analysis choices sometimes require that researchers correct their statistical tests to account for multiple comparisons.\nMultiple outcomes. Researchers often assess the effects of an intervention on multiple distinct outcomes or multiple operationalizations of the outcome variable.\n\nThese concerns are especially problematic when making a “family claim,” that is, when you are summarizing a series of results. For example, a family claim might be that treatments A, B, C, and D had no effect, but treatment E did. Or, similarly, the treatment had no effect among group 1, group 2, or group 3, but had a strong effect among group 4.\nThe multiple comparisons problem is related to, but different from, the problem of “fishing.” Fishing occurs when an unscrupulous analyst conducts many tests but only reports the “interesting” ones. In essence, fishing withholds the necessary information we would need in order to correct for multiple comparisons."
  },
  {
    "objectID": "guides/analysis-procedures/multiple-comparisons_en.html#footnotes",
    "href": "guides/analysis-procedures/multiple-comparisons_en.html#footnotes",
    "title": "10 Things You Need to Know About Multiple Comparisons",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFormally \\(E[FP/R|R&gt;0]P(R&gt;0)\\) to avoid dividing by 0.↩︎\nSection 9 updated by Tara Lyn Slough, 4 February 2016↩︎"
  },
  {
    "objectID": "guides/data-strategies/spillovers_en.html",
    "href": "guides/data-strategies/spillovers_en.html",
    "title": "10 Things to Know About Spillovers",
    "section": "",
    "text": "Spillovers refer to a broad class of instances in which a given subject is influenced by whether other subjects are treated.\nHere are some examples of how spillovers (or “interference”) might occur:\n\nPublic Health: Providing an infectious disease vaccine to some individuals may decrease the probability that nearby individuals become ill.\nCriminology: Increased enforcement may displace crime to nearby areas.\nEducation: Students may share newly acquired knowledge with friends.\nMarketing: Advertisements displayed to one person may increase product recognition among her work colleagues.\nPolitics: Election monitoring at some polling stations may displace fraud to neighboring polling stations.\nEconomics: Lowering the cost of production for one firm may change the market price faced by other firms.\nWithin-subjects experiments across many domains: the possibility that treatment effects persist or that treatments are anticipated can be modeled as a kind of spillover.\n\nThese examples share some features:\n\nAn intervention: the vaccine, increased enforcement, election monitoring;\nAn outcome: incidence of disease, crime rates, electoral fraud; and\nA “network” that links units together: face-to-face social interaction, geographic proximity within a city, road distance between polling stations.\n\nThe network is a crucial feature of any spillover analysis. For each unit, it describes the set of other units whose treatment assignments “matter.” To take the education example: it may matter to me if you treat another student in my classroom, but it probably doesn’t matter if you treat a student in a different city. I’m connected to the other students in my classroom but not to students in other cities."
  },
  {
    "objectID": "guides/data-strategies/spillovers_en.html#footnotes",
    "href": "guides/data-strategies/spillovers_en.html#footnotes",
    "title": "10 Things to Know About Spillovers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote though, it is generally very difficult to guess the direction of the bias that would be induced by spillover. Claims like, “spillover would only make our treatment effects appear stronger” usually depend on assumptions of treatment (and spillover) effect homogeneity.↩︎"
  },
  {
    "objectID": "guides/data-strategies/survey-experiments_en.html",
    "href": "guides/data-strategies/survey-experiments_en.html",
    "title": "10 Things to Know About Survey Experiments",
    "section": "",
    "text": "Survey experiments are widely used by social scientists to study individual preferences. This guide discusses the functions and considerations of survey experiments."
  },
  {
    "objectID": "guides/data-strategies/survey-experiments_en.html#footnotes",
    "href": "guides/data-strategies/survey-experiments_en.html#footnotes",
    "title": "10 Things to Know About Survey Experiments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Hainmueller, Hopkins, and Yamamoto (2014) and Green and Rao (1971).↩︎\nSee Horiuchi, Markovich, and Yamamoto (2022).↩︎\nSee Warner (1965), Boruch (1971), D. Gingerich (2015), and D. W. Gingerich (2010).↩︎\nSee Edgell, Himmelfarb, and Duchan (1982) and Yu, Tian, and Tang (2008).↩︎\nSee Dafoe, Zhang, and Caughey (2018) on information equivalence.↩︎\nSee Krosnick (1991) and Simon and March (2006).↩︎\nSee Haaland, Roth, and Johannes (2023) for a review of information provision experiments.↩︎"
  },
  {
    "objectID": "guides/data-strategies/sampling_en.html",
    "href": "guides/data-strategies/sampling_en.html",
    "title": "10 Things to Know About Sampling",
    "section": "",
    "text": "Researchers are rarely able to collect measurements on all units that make up the target population of a study; instead, they select a subset of units — a process called sampling. This guide provides an overview of different ways to sample and how these affect what can be learned from a study."
  },
  {
    "objectID": "guides/data-strategies/sampling_en.html#simple",
    "href": "guides/data-strategies/sampling_en.html#simple",
    "title": "10 Things to Know About Sampling",
    "section": "3.1 Simple",
    "text": "3.1 Simple\nSimple random sampling is the most basic survey design. In this design, each sample of size \\(n\\) and hence each unit has the same probability of being sampled. One way of drawing a simple random sample of size \\(n\\) from a sampling frame with \\(N\\) units is to enumerate all possible samples of size \\(n\\) and randomly select one of those samples. This is the procedure described in the example above. However, this approach tends to be impractical in real world applications, since actual populations are typically much larger than \\(N=6\\), and the number of possible samples will thus be vast. An alternative procedure is to number all units from 1 to \\(N\\), to generate \\(n\\) random numbers, ideally after setting a random seed (like set.seed() in R), and to select the corresponding units.2"
  },
  {
    "objectID": "guides/data-strategies/sampling_en.html#stratified",
    "href": "guides/data-strategies/sampling_en.html#stratified",
    "title": "10 Things to Know About Sampling",
    "section": "3.2 Stratified",
    "text": "3.2 Stratified\nSuppose we know upfront that the characteristic in which we are interested varies across sub-populations. For example, if we aim to estimate average income, we may suspect that men earn more than women. If we draw a simple random sample, it is possible that we end up with a sample that contains more women than men. In this case, our average income estimate will be subject to a large sampling error. A way to guard against this possibility is to divide the population into subgroups, also called strata, and draw an independent random sample in each stratum. For example, we may draw an independent random sample of women and and one of men. This procedure fixes the proportion of women and men in the sample, thereby avoiding “bad” samples and improving the precision of our estimates. Being able to ensure that the sample contains enough units from a particular sub-population is also helpful if estimates among the sub-population are of independent interest. If we would like to estimate the gender pay gap, for example, we need a sample that contains enough men and women to obtain sufficiently precise estimates of each group’s average income. This latter use of stratification is especially important for learning about rare subgroups."
  },
  {
    "objectID": "guides/data-strategies/sampling_en.html#clustering",
    "href": "guides/data-strategies/sampling_en.html#clustering",
    "title": "10 Things to Know About Sampling",
    "section": "3.3 Clustering",
    "text": "3.3 Clustering\nSuppose we would like to estimate the average income among residents of a city. Our sample frame may not identify individual residents but we may have access to a list of households. Instead of directly sampling residents, we can randomly select households and interview all members of the selected households. In this case, the primary sampling units — the units that can be selected — differ from the observation units on which measurements are taken. Households serve as primary sampling units or clusters, while household members serve as observation units. The core downside of cluster sampling is that it typically leads to a loss in precision. For an analogous problem with cluster random assignment and for more on clustering and information reduction see “10 Things to Know About Cluster Randomization”. This loss in precision will be greater when units within the same cluster are more similar to each other (for example, members of the same household may have similar incomes or views). This problem of similarity or dependence within clusters can be severe in studies of politics where all members of a place have the same representative or share similar attitudes (see Stoker and Bowers (2002)). Nonetheless, cluster sampling may be necessary if sampling frames of individual observation units cannot be obtained. Cluster sampling may also save survey costs. For example, suppose we would like to estimate household-level income. Using households as the primary sampling unit may lead to a sample of households that is dispersed throughout the city, which increases transportation costs. Instead, we may select entire city blocks and interview all households within the selected blocks. Doing so may make it possible to interview more households with a smaller budget. Whether or not the gain in precision from a bigger sample size outweighs the loss in precision from clustering will depend on the degree to which households within the same city block have similar incomes."
  },
  {
    "objectID": "guides/data-strategies/sampling_en.html#multi-stage",
    "href": "guides/data-strategies/sampling_en.html#multi-stage",
    "title": "10 Things to Know About Sampling",
    "section": "3.4 Multi-stage",
    "text": "3.4 Multi-stage\nInstead of sampling all units in a cluster, one may draw a sub-sample of units. For example, instead of interviewing all members, one could sample two members in each sampled household. Additional stages can be added to this approach. For instance, we could first draw a sample of city blocks, then a sub-sample of households within each sampled city block and finally a sub-sample of household members within each sampled household. In this example, households would be referred to as secondary sampling units and household members as tertiary sampling units. One advantage of multi-stage sampling is that it allows researchers to navigate the trade-off between precision and cost-effectiveness. Increasing the number of clusters and sub-sampling fewer units per cluster can yield a more diverse sample and hence reduce sampling variability. Yet, doing so may also increase survey costs."
  },
  {
    "objectID": "guides/data-strategies/sampling_en.html#random-walk",
    "href": "guides/data-strategies/sampling_en.html#random-walk",
    "title": "10 Things to Know About Sampling",
    "section": "4.1 Random walk",
    "text": "4.1 Random walk\nEnumerators are given (possibly randomly selected) starting points and are instructed to follow a walk pattern, e.g., “walk to the right and conduct an interview in every fifth dwelling.” The advantage of this approach is its cost effectiveness. No listing of households beyond those that enter the sample is required. Yet, a random walk procedure may not yield a particularly good approximation to a random sample. First, even though it is often treated as a way to select a simple random sample, a random walk procedure in fact introduces geographic clustering. The walk pattern above, for example, implies that neighboring houses can never both be sampled and that, once a particular household is sampled, the one five houses down the road will be sampled as well. See Lohr (2009, chap. 5.5) for more on the consequences of this kind of clustering. Second, even if starting points are, say, randomly chosen geo-coordinates, the probability of being sampled may vary across households due to geographic particularities (e.g., street layout, density of dwellings). If selection probabilities are unknown to the researcher but correlated with the characteristic of interest, bias will result. Finally, a random walk procedure is difficult to implement in practice. That enumerators select households and conduct interviews in one step increases their incentives to informally replace households that are reluctant or unavailable — a practice that can only be detected through intensive monitoring."
  },
  {
    "objectID": "guides/data-strategies/sampling_en.html#household-listing",
    "href": "guides/data-strategies/sampling_en.html#household-listing",
    "title": "10 Things to Know About Sampling",
    "section": "4.2 Household listing",
    "text": "4.2 Household listing\nThe more principled option is to create a list of all households in the sampling unit. Households can then be randomly selected from this list prior to the start of the survey. A listing exercise can be expensive, but is also more likely to allow the researcher to approximate a random sample. Sampling from a full list of households avoids problems of geographic clustering and unknown sampling probabilities. Moreover, separating the listing from the enumeration step makes it easier to guard against unwarranted replacements. At the listing stage, enumerators have fewer incentives to skip houses, since it is not yet clear whether a household will cooperate at the survey stage. That a record of all households within and outside the sample exists prior to the start of survey activities can also make it easier to detect instances in which enumerators interviewed the incorrect household. Finally, it is sometimes possible to re-use household lists to draw additional random samples in the future, at least in places where mobility is low. This feature opens up the possibility of sharing the costs of a listing exercise across multiple studies."
  },
  {
    "objectID": "guides/data-strategies/sampling_en.html#footnotes",
    "href": "guides/data-strategies/sampling_en.html#footnotes",
    "title": "10 Things to Know About Sampling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhile only random sampling allows for the characterization of sampling uncertainty, random sampling does not guarantee the ability to obtain uncertainty estimates (see Kish (1965, 24) for a discussion). A design that samples only one cluster at random, for example, would not allow for the construction of standard errors and confidence intervals.↩︎\nNote that when talking about random assignment, we refer to the analogous procedure that assigns a fixed number of units to treatment with a a given probability as complete random assignment. The term simple random assignment refers to a procedure where the researcher, say, flips a coin for every unit to decide whether it should be assigned to treatment or control. The number of units assigned to treatment under simple random assignment thus remains a random variable. ↩︎\nSee Lohr (2009, chap. 14) for an overview of other approaches and Tran et al. (2015) for a comparison of RDS to time-location sampling as a major alternative.↩︎\nOn a deeper level, however, random assignment has much in common of with random sampling. The process of randomly assigning units to a treatment group can be thought of as a process of drawing a random sample of treated potential outcomes in order to estimate the average treated potential outcome in the entire experimental subject pool (see Neyman (1990)). ↩︎"
  },
  {
    "objectID": "guides/data-strategies/measurement.fr.html",
    "href": "guides/data-strategies/measurement.fr.html",
    "title": "10 choses à savoir sur la mesure dans les expériences",
    "section": "",
    "text": "1 La validité des inférences que nous tirons d’une expérience dépend de la validité des mesures utilisées\nNous expérimentons généralement afin d’estimer l’effet causal d’un traitement \\(Z\\) sur un résultat \\(Y\\). Pourtant, la raison pour laquelle nous nous soucions d’estimer cet effet causal est, en principe, de comprendre les caractéristiques de la relation entre deux concepts théoriques non observés mesurés par les variables observées \\(Z\\) et \\(Y\\).\nÀ la suite d’Adcock et Collier (2001), considérons le processus de mesure représenté graphiquement dans la figure 1 en trois étapes. Premièrement, les chercheurs commencent par un concept systématisé, une construction théorique clairement définie. À partir de ce concept, le chercheur développe un indicateur matérialisant le concept sur une échelle ou un ensemble de catégories. Enfin, les unités ou les cas sont notés en fonction de l’indicateur, ce qui donne une mesure du traitement \\(Z\\) et un résultat \\(Y\\). Une mesure est valide si la variation de l’indicateur se rapproche étroitement de la variation du concept d’intérêt sous-jacent.\nUne conception de recherche expérimentale devrait permettre à un chercheur d’estimer l’effet causal de \\(Z\\) sur \\(Y\\) sous des hypothèses standard. Mais si le but ultime est de faire une inférence sur l’effet causal du concept que \\(Z\\) mesure sur le concept que \\(Y\\) mesure, les inférences que nous pouvons espérer faire sur la base de nos preuves expérimentales sont valides si et seulement si les deux mesures sont valides.\n\n\n\n\n\nLe processus de mesure dans le contexte d’une conception de recherche expérimentale.\n\n\n\n\n\n\n2 La mesure est le lien entre l’argument substantif et/ou théorique d’un chercheur et une conception de recherche (expérimentale).\nLorsque nous considérons la conception d’une expérience, nous avons tendance à nous concentrer sur le processus par lequel le traitement \\(Z\\) est assigné de manière aléatoire et sur la distribution conjointe de \\(Z\\) et du résultat \\(Y\\). En d’autres termes, nous avons tendance à séparer les scores de \\(Z\\) et \\(Y\\) des concepts plus larges lorsque nous considérons les propriétés statistiques d’une conception de recherche. Dans ce cas, deux expériences complètement différentes avec la même distribution de \\(Z\\) et \\(Y\\) pourraient avoir des propriétés identiques.\nPar exemple, un essai clinique sur l’efficacité de l’aspirine sur les maux de tête et une expérience qui fournit des informations sur le niveau de corruption d’un politicien sortant pour demander ensuite à la personne interrogée si elle votera pour le candidat sortant pourraient avoir des échantillons de mêmes taille et distribution, des assignations, paramètres et réalisations des résultats (données) identiques. Pourtant, caractériser comme “équivalents” ces deux projets de recherche complètement distincts qui cherchent à faire des inférences complètement distinctes peut nous sembler assez étrange ou même troublant.\nCependant, lorsque nous considérons la mesure comme une composante fondamentale de la conception de recherche, ces expériences sont clairement distinctes. Nous observons des mesures de différents concepts dans les données des deux expériences. En considérant les indicateurs et les concepts plus larges qui sous-tendent les traitements et les résultats, nous sommes obligés d’examiner les théories ou arguments respectifs des chercheurs. Ce faisant, nous pouvons soulever des questions sur la validité des mesures et la relation entre la validité des mesures et la validité des inférences finales et substantielles.\n\n\n3 La mesure des traitements comprend l’opérationnalisation du traitement ainsi que la conformité de l’assignation au traitement.\nDans une expérience, les traitements sont généralement conçus ou, au minimum, décrits par le chercheur. Les consommateurs de recherche expérimentale devraient s’intéresser aux caractéristiques du traitement et à la façon dont il manipule un concept d’intérêt. La plupart des traitements en sciences sociales sont composés ou comprennent un ensemble d’attributs. Nous pouvons être intéressés par l’effet de fournir aux électeurs des informations sur la performance de leurs élus. Pourtant, fournir des informations inclut également le mode de transmission et qui a fourni les informations. Pour comprendre dans quelle mesure le traitement manipule un concept, nous devons également comprendre ce que le traitement pourrait manipuler d’autre.\nCependant, malgré tous les efforts pour opérationnaliser un traitement, dans la recherche expérimentale, le lien entre l’opérationnalisation et l’indicateur de traitement est fondamentalement distinct de la mesure des covariables ou des résultats pour deux raisons. Premièrement, en assignant un traitement, les expérimentalistes visent à contrôler les valeurs que prend une unité donnée. Deuxièmement, pour l’indicateur de traitement, le score provient de l’assignation de traitement, qui est un produit de la randomisation. Un sujet peut ou non avoir reçu le traitement, mais son score sur l’indicateur de traitement est simplement le traitement auquel il a été assigné, et non le traitement qu’il a reçu.\nLorsque les sujets reçoivent des traitements autres que ceux auxquels ils sont assignés, nous cherchons généralement à mesurer la conformité — si les traitements ont été délivrés et dans quelle mesure. Pour ce faire, nous définissons ce qui constitue la conformité de l’assignation de traitement. Pour déterminer ce qui constitue la conformité, les chercheurs doivent tenir compte de l’aspect central de la façon dont le traitement manipule le concept d’intérêt. A quel moment de l’administration du traitement se produit cette manipulation ? Une fois la conformité opérationnalisée, nous cherchons à coder l’indicateur de conformité de manière fidèle à cette définition.\nPar exemple, considérons une campagne de démarchage en porte-à-porte qui diffuse des informations sur les performances d’un politicien sortant. Les ménages sont assignés à recevoir une visite d’un solliciteur qui partage l’information (traitement) ou non (contrôle). L’indicateur de traitement est simplement de savoir si un ménage a été assigné au traitement ou non. Cependant, si les résidents d’un ménage ne sont pas à la maison lors de la visite du solliciteur, ils ne reçoivent pas l’information. Notre définition de la conformité devrait déterminer ce qui constitue un “traité” sur notre mesure (endogène) : “le fait qu’un ménage ait reçu ou non le traitement”, ici l’information. Certaines définitions courantes de la conformité peuvent être (a) qu’un membre du ménage a ouvert la porte ; ou (b) qu’un membre du ménage a écouté le script d’information complet.\n\n\n4 La plupart des résultats d’intérêt en sciences sociales sont latents.\nContrairement à la mesure des indicateurs de traitement et de conformité, la mesure des résultats dans la recherche expérimentale suit de beaucoup plus près le processus décrit dans la figure ci-dessus. Nous théorisons comment le traitement peut influencer un concept de résultat. Nous opérationnalisons ensuite le concept et enregistrons des scores ou des valeurs pour compléter notre mesure du résultat.\nUn défi particulier dans la mesure des résultats est que bon nombre des résultats d’intérêt les plus courants en sciences sociales sont latents. Cela signifie que nous ne pouvons pas observer directement la vraie valeur du concept de résultat. En fait, la nature de la vraie valeur peut elle-même être débattue (par exemple, le débat sur la mesure de la “démocratie” est un cas classique où la définition du concept lui-même est contestée). Les résultats, y compris les connaissances, les préférences ou les attitudes, sont latents. Nous enregistrons ou notons ainsi des indicateurs observables supposés être liés au résultat latent en vue de déduire les caractéristiques de la variable latente. Même les résultats comportementaux sont souvent utilisés comme des manifestations de concepts latents plus larges (i.e. le comportement électoral estimé est utilisé pour faire des déductions sur la “responsabilité électorale” à un endroit).\nÉtant donné que ces variables sont latentes, il est difficile de concevoir des indicateurs appropriés. Une mauvaise opérationnalisation a des conséquences assez drastiques sur la validité de nos inférences sur le concept d’intérêt pour deux raisons. Comme dans la section 1 ci-dessus, si ces indicateurs ne mesurent pas conceptuellement le concept d’intérêt, alors les inférences que nous faisons sur la relation entre \\(Z\\) et \\(Y\\) (même avec une conception “parfaite” en termes de puissance statistique et de données manquantes, etc.) peuvent ne pas nous renseigner sur “l’inférence ultime” que nous cherchons. De plus, l’erreur de mesure peut miner notre capacité à estimer l’effet de \\(Z\\) et \\(Y\\) conduisant à des inférences incorrectes. Le reste de ce guide se concentre sur ce dernier problème.\n\n\n5 Il existe deux types d’erreurs de mesure que nous devrions considérer.\nOn peut formaliser assez simplement les enjeux de mesure. Supposons qu’un traitement \\(Z_i\\) soit supposé changer les préférences pour les normes démocratiques \\(\\nu_i\\). En principe, la quantité que nous aimerions estimer est \\(E[\\nu_i|Z_i = 1] - E[\\nu_i|Z_i =0]\\), l’ATE sur les préférences pour les normes démocratiques. Cependant, \\(\\nu_i\\) est une variable latente : on ne peut pas la mesurer directement. Au lieu de cela, nous demandons si les unités soutiennent divers comportements pensés correspondre à ces normes. Cet indicateur, \\(Y_i\\), peut être décomposé en la variable latente, \\(\\nu_i\\) et deux formes d’erreur de mesure :\n\nErreur de mesure non systématique, \\(\\delta_i\\) : cette erreur est indépendante de l’assignation au traitement, \\(\\delta_i \\perp Z_i\\).\nErreur de mesure systématique, \\(\\kappa_i\\) : cette erreur n’est pas indépendante de l’assignation au traitement, \\(\\kappa_i \\not\\perp Z_i\\).\n\n\\[Y_i = \\underbrace{\\nu_i}_{\\text{Résultat latent}} + \\underbrace{\\delta_i}_{\\substack{\\text{Erreur de mesure} \\\\ \\text{non-systématique}}} + \\underbrace{\\kappa_i}_{\\substack{\\text{Erreur de mesure} \\\\ \\text{systématique}}}\\]\n\n\n6 L’erreur de mesure réduit la puissance statistique de votre expérience.\nL’erreur de mesure non systématique, représentée par \\(\\delta_i\\) ci-dessus, fait référence au bruit avec lequel nous mesurons la variable latente. En l’absence d’erreur de mesure systématique, on mesure :\n\\[Y_i = \\underbrace{\\nu_i}_{\\text{Résultat latent}} + \\underbrace{\\delta_i}_{\\substack{\\text{Erreur de mesure} \\\\ \\text{non-systématique}}}\\]\nMaintenant, considérons la formule de puissance analytique pour une expérience à deux bras. Nous pouvons exprimer \\(\\sigma\\), ou l’écart type du résultat sous la forme \\(\\sqrt{Var(Y_i)}\\). Notez que dans la formule ci-dessous, ce terme apparaît au dénominateur du premier terme. À mesure que \\(\\sqrt{Var(Y_i)}\\) augmente, la puissance statistique diminue.\n\\[\\beta = \\Phi \\left(\\frac{|\\mu_t− \\mu_c| \\sqrt{N}}{2 \\color{red}{\\sqrt{Var(Y_i)}}} − \\Phi^{−1}\\left(1 − \\frac{\\alpha}{2}\\right)\\right)\\]\nEn quoi l’erreur de mesure non systématique \\(\\delta_i\\) impacte-t-elle la puissance ? Nous pouvons décomposer \\(\\sqrt{Var(Y_i)}\\) comme suit :\n\\[\\sqrt{Var(Y_i)} = \\sqrt{Var(\\nu_i) + Var(\\delta_i) + 2 Cov(\\nu_i, \\delta_i)}\\]\nTant que \\(Cov(\\nu_i, \\delta_i)\\geq 0\\) (nous supposons souvent que \\(Cov(\\nu_i, \\delta_i)= 0\\)), \\(Var(Y_i)\\) augmente quand l’erreur de mesure \\(Var(\\delta_i)\\) augmente. Cela implique que la puissance diminue à mesure que l’erreur de mesure non systématique augmente. En d’autres termes, plus nos mesures d’une variable latente sont bruyantes, plus notre capacité à détecter l’effet du traitement sur une variable latente est faible.\nQu’en est-il du cas où \\(Cov(\\nu_i, \\delta_i) &lt; 0\\) ? Bien que cela réduise \\(Var(Y_i)\\) (en maintenant \\(Var(\\nu_i)\\) et \\(Var(\\delta_i)\\) constants), cela atténue également la variation que nous mesurons dans \\(Y_i\\). En principe, cela devrait atténuer le numérateur \\(|\\mu_t-\\mu_c|\\), qui réduira également la puissance, si cela suffit par rapport à la réduction de la variance.\n\n\n7 L’erreur de mesure systématique biaise les estimations de l’effet causal d’intérêt.\nSi nous estimons l’effet moyen de traitement (ATE) de notre traitement \\(Z_i\\), sur les préférences pour les normes démocratiques, \\(\\nu_i\\), nous essayons de calculer \\(E[\\nu_i|Z_i = 1] - E[\\nu_i|Z_i =0]\\). Cependant, en présence d’une erreur de mesure systématique, où l’erreur de mesure est liée à l’assignation du traitement lui-même (disons, le résultat est mesuré différemment dans les groupes de traitement et de contrôle), un estimateur de la différence des moyennes sur le résultat observé, \\(Y_i\\), renvoie une estimation biaisée de l’ATE. L’effet du traitement comprend désormais la différence de mesure ainsi que la différence entre les groupes de traitement et de contrôle :\n\\[E[Y_i|Z_i = 1]−E[Y_i|Z_i = 0] =  E[\\nu_i + \\delta_i + \\kappa_i |Z_i = 1] − E[\\nu_i + \\delta_i + \\kappa_i|Z_i =0]\\]\nEn raison de l’erreur de mesure non systématique, \\(\\delta_i\\) est indépendant de l’assignation du traitement, \\(E[\\delta_i|Z_i = 1] = E[\\delta_i |Z_i = 0]\\). En simplifiant et en réarrangeant, on peut écrire :\n\\[E[Y_i|Z_i = 1]−E[Y_i|Z_i = 0] = \\underbrace{E[\\nu_i|Z_i = 1] − E[\\nu_i|Z_i =0]}_{ATE} +\n\\underbrace{E[\\kappa_i|Z_i = 1] - E[\\kappa_i|Z_i =0]}_{\\text{Biais}}\\]\nIl existe diverses sources d’erreur de mesure non systématique dans les expériences. L’effet de demande et l’effet Hawthorne peuvent être sources d’erreur de mesure systématique. De plus, les conceptions qui mesurent les résultats de manière asymétrique dans les groupes de traitement et de contrôle peuvent être sujettes à une erreur de mesure systématique. Dans tous les cas, il existe une asymétrie entre les conditions de traitement dans (a) la manière dont les sujets réagissent à l’observation ; ou (b) la façon dont nous observons les résultats qui est distincte de tout effet du traitement sur la variable latente d’intérêt. L’estimation biaisée de l’ATE devient le net de tout effet sur les variables latentes (l’ATE) et l’erreur de mesure non systématique.\n\n\n8 Tirez parti de plusieurs indicateurs pour évaluer la validité d’une mesure, mais soyez conscient des limites de ces tests.\nAu-delà de la considération de la qualité de la correspondance entre un concept et une mesure, nous pouvons souvent évaluer la qualité de la mesure en la comparant à des mesures provenant d’opérationnalisations alternatives du même concept, de concepts étroitement liés ou de concepts distincts. Dans les tests convergents de la validité d’une mesure, nous évaluons la corrélation entre les mesures alternatives d’un concept. Si elles sont codées dans le même sens, nous nous attendons à ce que la corrélation soit positive et que la validité des deux mesures augmente à mesure que l’amplitude de la corrélation augmente. Une limitation des tests de validité convergents est que si deux mesures sont faiblement corrélées, en l’absence d’informations supplémentaires, nous ne savons pas si une mesure est valide (et laquelle) ou si les deux mesures sont invalides.\nLa collecte de plusieurs indicateurs peut également permettre aux chercheurs d’évaluer la validité prédictive d’une mesure. Dans quelle mesure la mesure d’un concept latent prédit-elle un comportement que l’on pense être façonné par le concept ? Par exemple, l’idéologie politique (la variable latente) prédit-elle le choix de vote déclaré pour les partis de gauche ? Cela fournit un moyen supplémentaire de valider une mesure. Ici, plus la capacité d’un indicateur à prédire le comportement (ou d’autres résultats) est élevée, plus la validité prédictive de l’indicateur est forte. Pourtant, nous pensons que la plupart des comportements sont le résultat d’un ensemble complexe de causes. Déterminer si une mesure est un prédicteur “assez bon” est une détermination quelque peu arbitraire.\nEnfin, nous pouvons vouloir déterminer si nous mesurons le concept d’intérêt isolément plutôt qu’un ensemble de concepts. Les tests de validité discriminante examinent les indicateurs d’un concept et d’un concept lié mais distinct. En principe, nous recherchons des corrélations faibles (corrélations proches de 0) entre les deux indicateurs. Une limitation des tests de validité discriminante est que nous ne savons pas comment les concepts distincts sous-jacents varient ensemble. Il se peut que nous ayons des indicateurs valides des deux concepts, mais ils présentent une forte corrélation (positive ou négative) car les unités avec des niveaux élevés de \\(A\\) ont tendance à avoir des niveaux plus élevés (resp. faibles) de \\(B\\).\nEn somme, l’ajout de plus de mesures peut aider à valider un indicateur, mais ces tests de validation sont limités dans ce qu’ils nous disent lorsqu’ils échouent. Dans cette mesure, nous devons rester conscients des limites en plus de l’utilité de collecter des mesures supplémentaires pour simplement valider un indicateur.\n\n\n9 L’utilisation de plusieurs indicateurs améliore souvent la puissance de votre expérience, mais peut introduire un compromis biais-efficacité.\nRegrouper plusieurs indicateurs d’un concept ou d’un résultat peut également améliorer la puissance de votre expérience. Si plusieurs indicateurs mesurent le même concept mais sont mesurés avec une erreur (non systématique), nous pouvons améliorer la précision avec laquelle nous mesurons la variable latente en tirant parti de plusieurs mesures.\nIl existe plusieurs façons d’agréger plusieurs résultats dans un indice. “10 choses à savoir sur les comparaisons multiples” décrit des indices construits à partir de la score-\\(z\\) et d’une pondération inverse de covariance pour des résultats multiples. Il existe également de nombreux autres modèles structurels pour estimer les variables latentes à partir de plusieurs mesures.\nCi-dessous, nous examinons un simple indice de score-\\(z\\) pour deux mesures bruitées d’une variable latente. Nous supposons que les variables latentes et les deux indicateurs “Mesure 1” et “Mesure 2” sont tirés d’une distribution normale multivariée et sont positivement corrélés avec la variable latente et entre eux. Pour les besoins de la simulation, nous supposons que nous connaissons la variable latente, bien qu’en pratique cela ne soit pas possible. Premièrement, nous pouvons montrer qu’à travers de nombreuses simulations des données, la corrélation entre l’indice de score-\\(z\\) des deux mesures et la variable latente est, en moyenne, plus élevée que la corrélation entre l’un ou l’autre des indicateurs et la variable latente. Lors de la représentation graphique de la corrélation entre mesures individuelles et variable latente (sur l’axe des \\(x\\)) par rapport à la corrélation entre indice et variable latente (sur l’axe des \\(y\\)), presque tous les points sont au-dessus de la ligne à 45 degrés. Cela montre que l’indice se rapproche de la variable latente avec une plus grande précision.\n\nlibrary(mvtnorm)\nlibrary(randomizr)\nlibrary(dplyr)\nlibrary(estimatr)\nmake_Z_score &lt;- function(data, outcome){\n  ctrl &lt;- filter(data, Z == 0)\n  return(with(data, (data[,outcome] - mean(ctrl[,outcome]))/sd(ctrl[,outcome])))\n}\npull_estimates &lt;- function(model){\n  est &lt;- unlist(model)$coefficients.Z\n  se &lt;- unlist(model)$std.error.Z\n  return(c(est, se))\n}\ndo_sim &lt;- function(N, rhos, taus, var = c(1, 1, 1)){\n   measures &lt;- rmvnorm(n = N, \n                       sigma = matrix(c(var[1], rhos[1], rhos[2], \n                                        rhos[1], var[2], rhos[3], \n                                        rhos[2], rhos[3], var[3]), nrow = 3))\n   df &lt;- data.frame(Z = complete_ra(N = N),\n                      latent = measures[,1],\n                      Y0_1 = measures[,2],\n                      Y0_2 = measures[,3]) %&gt;%\n            mutate(Yobs_1 = Y0_1 + Z * taus[1],\n                   Yobs_2 = Y0_2 + Z * taus[2])\n   df$Ystd_1 = make_Z_score(data = df, outcome = \"Yobs_1\")\n   df$Ystd_2 = make_Z_score(data = df, outcome = \"Yobs_2\")\n   df$index = (df$Ystd_1 + df$Ystd_2)/2\n   cors &lt;- c(cor(df$index, df$latent), cor(df$Ystd_1, df$latent), cor(df$Ystd_2, df$latent))\n   ests &lt;- c(pull_estimates(lm_robust(Ystd_1 ~ Z, data = df)),\n             pull_estimates(lm_robust(Ystd_2 ~ Z, data = df)),\n             pull_estimates(lm_robust(index ~ Z, data = df)))\n   output &lt;- c(cors, ests)\n   names(output) &lt;- c(\"cor_index\", \"cor_Y1\", \"cor_Y2\", \"est_Y1\", \"se_Y1\",\n                      \"est_Y2\", \"se_Y2\", \"est_index\", \"se_index\")\n   return(output)\n}\nsims &lt;- replicate(n = 500, expr = do_sim(N = 200, \n                                         rhos = c(.6, .6, .6), \n                                         taus = c(.4, .4),\n                                         var = c(1, 3, 3)))\ndata.frame(measures = c(sims[\"cor_Y1\",], sims[\"cor_Y2\",]),\n           index = rep(sims[\"cor_index\",], 2),\n           variable = rep(c(\"Mesure 1\", \"Mesure 2\"), each = 500)) %&gt;%\n  ggplot(aes(x = measures, y = index)) + geom_point() + \n  facet_wrap(~variable) + \n  geom_abline(a = 0, b = 1, col = \"red\", lwd = 1.25) + \n  scale_x_continuous(\"Corrélation entre mesure et variable latente\", limits = c(0.1, .6)) +\n  scale_y_continuous(\"Corrélation entre indice et variable latente\", limits = c(0.1, .6)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nMaintenant, considérons les implications pour la puissance statistique. Dans les simulations, nous estimons l’ATE d’un traitement sur la Mesure 1, la Mesure 2 et l’indice. Le graphique suivant visualise les estimations. Les lignes bleues montrent des intervalles de confiance à 95 %. Les intervalles de confiance plus petits autour l’indice visualisent les gains de précision résultant de l’utilisation des deux mesures. Nous voyons que cela se manifeste par une puissance statistique plus élevée pour l’expérience.\n\ndata.frame(est = c(sims[\"est_index\",], sims[\"est_Y1\",], sims[\"est_Y2\",]),\n           se = c(sims[\"se_index\",], sims[\"se_Y1\",], sims[\"se_Y2\",]),\n           outcome = rep(c(\"Indice\", \"Mesure 1\", \"Mesure 2\"), each = 500)) %&gt;%\n  mutate(T = est/se, \n         sig = 1 * (abs(T) &gt; 1.96)) %&gt;%\n  group_by(outcome) %&gt;%\n  mutate(power = sum(sig)/n(),\n          lab = paste0(\"Puissance = \", round(power, 2))) %&gt;%\n  arrange(est) %&gt;%\n  mutate(order = 1:500/500) %&gt;%\n  ggplot(aes(x = order, y = est)) + \n  geom_errorbar(aes(ymin = est - 1.96 * se, ymax = est + 1.96 * se), width = 0,\n                col = \"light blue\", alpha = .25) +\n  geom_point() +\n  facet_wrap(~outcome) +\n  geom_text(x = 0.5, y = -.4, aes(label = lab), cex = 4) +\n  geom_hline(yintercept = 0, col = \"red\", lty = 3) + \n  theme_minimal() + xlab(\"Percentile de l'estimation dans 500 simulations\") +\n  ylab(\"ATE\")\n\n\n\n\n\n\n\n\nNous avons examiné un indice composé de seulement deux indicateurs. En principe, il y a d’autres gains d’efficacité à faire en intégrant plus d’indicateurs dans votre indice. Pourtant, à mesure que nous augmentons le nombre d’indicateurs, nous devrions considérer dans quelle mesure la fusion des indicateurs adhère au concept original. En ajoutant des mesures pour tirer parti des gains d’efficacité, nous pouvons introduire un biais dans la mesure du concept latent. Les chercheurs doivent louvoyer avec ce compromis. Le pré-enregistrement des composants d’un indice fournit une manière raisonnée de naviguer pour un problème qui oblige un examen approfondi du concept en l’absence de données. Cela évite également les questions ex post sur le choix des indicateurs pour un indice.\n\n\n10 Alors que les concepts peuvent être globaux, de nombreux indicateurs sont spécifiques au contexte.\nDe nombreuses études en sciences sociales se concentrent sur des concepts qui sont généralement supposés être latents, notamment les préférences, les connaissances ou les attitudes. Dans la mesure où nous travaillons sur des concepts communs, il existe une tendance à s’inspirer des opérationnalisations existantes à partir d’études sur des concepts connexes dans des contextes différents. Dans des études menées dans de multiples contextes, comme dans l’initiative Metaketa de EGAP, les chercheurs visent à étudier la même relation causale dans plusieurs contextes nationaux. Mais le désir d’étudier des concepts communs n’implique pas nécessairement que les mêmes indicateurs doivent être utilisés dans tous les contextes.\nPar exemple, considérons un ensemble d’études qui cherchent à mesurer la variation du concept de connaissance ou de sophistication politique. Les connaissances politiques peuvent être évaluées par des questions qui demandent aux sujets de se rappeler d’un fait politique. Une question peut demander aux sujets de se rappeler le nom de l’exécutif actuel (président, premier ministre, etc.), en notant les réponses comme “correctes” ou “incorrectes”. Dans le pays \\(A\\), 50% des personnes interrogées répondent correctement à la question. Dans le pays \\(B\\), 100 % des personnes interrogées répondent correctement à la question. Dans le pays \\(B\\), nous ne pouvons identifier aucune variation dans l’indicateur car tout le monde pouvait répondre à la question. Cela ne signifie pas qu’il n’y a pas de variation dans les connaissances politiques dans le pays \\(B\\), juste que cet indicateur est une mauvaise mesure de la variation qui existe. Dans le pays \\(A\\), cependant, cette question peut être un indicateur tout à fait approprié pour la connaissance politique. Si la connaissance politique était le résultat d’une expérience, l’absence de variation dans le résultat dans le pays \\(B\\) ne nous permet pas d’identifier une différence dans la connaissance politique entre les groupes de traitement et de contrôle.\nPour cette raison, s’il peut être utile de développer des indicateurs basés sur des travaux existants ou des instruments provenant d’autres contextes, ce n’est pas nécessairement la meilleure façon de développer des mesures dans un nouveau contexte. Les pré-tests peuvent fournir des indications pour savoir si les indicateurs sont appropriés dans un contexte donné. En somme, la correspondance entre les concepts et les indicateurs est spécifique au site dans de nombreux cas. Les chercheurs devraient tenir compte de ces limites lorsqu’ils opérationnalisent des concepts communs dans des contextes distincts.\n\n\n11 Bibliographie\nAdcock, Robert et David Collier. “Measurement Validity: A Shared Standard for Qualitative and Quantitative Research.” American Political Science Review. 95 (3): 529-546.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "guides/data-strategies/randomization.fr.html",
    "href": "guides/data-strategies/randomization.fr.html",
    "title": "10 choses à savoir sur la randomisation",
    "section": "",
    "text": "Il existe de nombreuses façons de randomiser. Le plus simple est de lancer une pièce à chaque fois que vous voulez déterminer si un sujet donné reçoit un traitement ou non. Cela garantit que chaque sujet a une probabilité de 0,5 de recevoir le traitement et une probabilité de 0,5 de ne pas le recevoir. De cette façon, le fait qu’un sujet reçoive le traitement n’affecte en rien le fait que le sujet suivant reçoive le traitement, chaque sujet a une chance égale de recevoir le traitement, et le traitement ne sera pas corrélé avec tous les facteurs de confusion - du moins en espérance.\nCe n’est pas une mauvaise approche, mais elle présente des lacunes. Premièrement, en utilisant cette méthode, vous ne pouvez pas savoir à l’avance combien d’unités seront sous traitement et combien sous contrôle. Si vous voulez le savoir à l’avance, vous avez besoin d’un moyen de faire des sélections afin que les différents tirages ne soient pas statistiquement indépendants les uns des autres (comme tirer des noms d’un chapeau). Deuxièmement, vous voudrez peut-être avoir le contrôle sur la proportion exacte des unités assignées au traitement et au contrôle. C’est difficile à faire avec une pièce de monnaie. Troisièmement, vous voudrez peut-être pouvoir reproduire votre randomisation pour montrer qu’il n’y avait pas d’erreurs. C’est difficile à faire avec des pièces de monnaie ou des chapeaux. Enfin, comme nous le montrons ci-dessous, il existe toutes sortes de façons de faire de la randomisation pour améliorer la puissance statistique et assurer l’équilibre. C’est plus difficile en utilisant des pièces de monnaie ou des chapeaux.\nHeureusement cependant, une randomisation réplicable et flexible est possible avec des logiciels disponibles gratuitement. Le code suivant en R peut, par exemple, être utilisé pour générer une assignation aléatoire, spécifiant le nombre d’unités à traiter. Ici, N (100) est le nombre d’unités dont vous disposez et m (34) est le nombre que vous souhaitez traiter. La “graine” permet de reproduire le même tirage à chaque fois que vous exécutez le code (ou vous pouvez changer la graine pour un tirage différent).1 2\n\nlibrary(randomizr)\n# définir une \"graine\" aléatoire pour rendre cela reproductible\nset.seed(343)\ncomplete_ra(100, 34)"
  },
  {
    "objectID": "guides/data-strategies/randomization.fr.html#footnotes",
    "href": "guides/data-strategies/randomization.fr.html#footnotes",
    "title": "10 choses à savoir sur la randomisation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLes générateurs de nombres aléatoires sont en fait pseudo-aléatoires car ils génèrent un vecteur de nombres aléatoires basé sur un petit ensemble de valeurs initiales, appelé état de départ. Les générateurs de nombres aléatoires fonctionnent de cette manière afin d’améliorer la vitesse de calcul. Cependant, la série de nombres aléatoires générés est aussi aléatoire que nécessaire pour une assignation aléatoire, car elle n’a aucun rapport avec les résultats potentiels de vos sujets.↩︎\nLe code a été mis à jour par Alex Coppock le 25 novembre 2020.↩︎\nVoir Moore, Ryan T. et Sally A. Moore. “Blocking for sequential political experiments.” Political Analysis 21.4 (2013): 507-523.↩︎\nPour une présentation plus détaillée des procédures de randomisation disponibles dans le package R randomizr, voir https://declaredesign.org/r/randomizr/articles/randomizr_vignette.html↩︎\nMais si nous sommes certains des effets du prêt, alors pourquoi mener une expérience pour le tester ? Dans la recherche médicale, les essais contrôlés randomisés s’arrêtent souvent s’il devient clair dès le début qu’un médicament guérit sans aucun doute des maladies potentiellement mortelles, et donc le refuser aux sujets témoins est dangereux. (De même, un essai s’arrêterait également s’il était clair dès le début qu’un médicament provoque sans aucun doute des effets négatifs et nocifs.)↩︎"
  },
  {
    "objectID": "guides/data-strategies/missing-data_en.html",
    "href": "guides/data-strategies/missing-data_en.html",
    "title": "10 Things to Know About Missing Data",
    "section": "",
    "text": "1 What is missing data?\nWhen variables are missing some data values, we say that there is “missing data.” Depending on your software and the coding of the dataset, missing values may be coded as NA, ., an empty cell (\"\"), or a common numeric code (often -99 or 99).\nThe consequences of missing data for estimation and interpretation depend on the type of variable missing the data. For our purposes, we will consider three types of variables: pretreatment covariates, treatment indicator(s), and outcome (or dependent) variables. Pretreatment covariates, often known simply as “covariates,” are variables that we observe and measure before treatment is assigned. Outcome (or dependent) variables refer to outcomes that are measured after the assignment of treatment.\nMissing data emerges for different reasons. In survey data, a respondent could decline to answer a question or quit the survey without completing all questions. In a panel survey, some subjects may skip the second or later waves. With administrative data, records may be lost at some point in the process of collecting or recording data. To the extent that we can know the process by which data becomes missing, we can better understand the consequences of missing data for our analysis and inferences.\n\n\n2 Missing treatment or outcome data can bias our ability to describe empirical patterns and estimate causal effects.\nMissing data can induce bias in our estimates of descriptive patterns and causal effects. Consider a researcher trying to describe the income distribution in a country with survey data. Some individuals’ incomes are missing but the researcher describes the non-missing data at hand. Suppose low-income individuals are less likely to report their income than high-income individuals, thus missingness concentrates in the lower portion of the distribution. Then, the researcher’s characterization of the income distribution is apt to be biased. For example, the researcher’s estimate of median income is bound to be higher than the true (unknown) median income because more data is missing from the lower portion of the distribution. Since missingness is correlated with the variable that we are trying to describe, our characterization of the median of the distribution is biased.\nWe can illustrate two types of missingness by considering a variable \\(x\\) – for example, income, as above. The left panel shows the full distribution of the variable \\(x\\) without missing data. The middle panel depicts a scenario in which simulated missingness in \\(x\\) is not independent of \\(x\\) – low values of \\(x\\) are much more likely to be missing. The right panel depicts a scenario in which simulated missingness in \\(x\\) is independent of \\(x\\) (often called “missing at random”). The red line represents the median of the distribution without missingness. The blue lines represent the median of the observed data in each simulation. Where missingness is not independent of \\(x\\), we observe that the median of the nonmissing data is, in this instance, higher than the true median. This illustrates that missing data can bias our descriptions of single variables.\n\nlibrary(rmutil)\nlibrary(dplyr)\nlibrary(ggplot2)\ndata.frame(x = rep(rchisq(n = 1000, df = 5), 3),\n           panel = rep(c(\"No missing data\", \"Missingness is not\\nindependent of x\", \"Missingness is\\nindependent of x\"), each = 1000)) %&gt;%\n  mutate(panel = factor(panel, levels  = unique(panel)),\n         missing_cor = rbinom(n = 3000, size = 1,  prob = (1 - pchisq(x, df = 5))),\n         missing_ind = rbinom(n = 3000, size = 1, prob = .5),\n         obs = ifelse(missing_cor == 1 & panel == \"Missingness is not\\nindependent of x\", NA, \n                      ifelse(missing_ind == 1 & panel == \"Missingness is\\nindependent of x\", NA, x))) %&gt;%\n  group_by(panel) %&gt;%\n  mutate(med_x = median(x),\n         med_obs = median(obs, na.rm = T),\n         med_obs = ifelse(panel == \"No missing data\", NA, med_obs)) %&gt;%\n  ggplot(aes(x = obs)) +\n  geom_histogram(bins = 50) +\n  facet_grid(~panel) +\n  geom_vline(aes(xintercept = med_x), col = \"red\", lwd = 1) +\n  geom_vline(aes(xintercept = med_obs), col = \"blue\", lty = 2, lwd = 1) +\n  scale_x_continuous(\"x\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSimilarly, when we seek to estimate causal effects, some patterns of missing data can lead to biased estimates of causal effects. In particular, missingness of the treatment indicator or the outcome variable of interest can induce bias in estimates of the ATE. First, consider missingness of an outcome variable \\(Y_i(Z)\\). Adopting some notation from Gerber and Green (2012), define “reporting” as a potential outcome of a treatment, \\(Z\\), as \\(R_i(Z) \\in \\{0, 1\\}\\). In this notation, \\(R_i(Z) = 0\\) indicates that \\(Y_i(Z)\\) is missing and \\(R_i(Z)=1\\) indicates that the outcome is non-missing. Using this notation, we can express the ATE as:\n\\[\\begin{align}\n\\underbrace{E[Y_i(1)]-E[Y_i(0)]}_{ATE} =& \\underbrace{E[R_i(1)]E[Y_i(1)|R_i(1) = 1]}_{Z = 1\\text{ and }Y_i \\text{ is not missing}} + \\underbrace{(1-E[R_i(1)])(E[Y_i(1)|R_i(1) = 0])}_{Z = 1 \\text{ and } Y_i \\text{ is missing}} - \\\\\n&\\underbrace{E[R_i(0)]E[Y_i(0)|R_i(0) = 1]}_{Z = 0 \\text { and } Y_i \\text{ is not missing }} - \\underbrace{(1-E[R_i(0)])E[Y_i(0)|R_i(0) = 0]}_{Z = 0 \\text { and } Y_i \\text{ is missing }}\n\\end{align}\\]\nIf we condition our analysis on on non-missing outcomes, our estimator of the ATE is:\n\\[E[Y_i(1)|R_i(1)=1] - E[Y_i(0) |R_i(0)=1]\\]\nExamining the preceding equations, the estimator using only only the non-missing outcomes is only (necessarily) equivalent to the ATE if:\n\nThere is no missingness, \\(E[R_i(1)] = 1\\) and \\(E[R_i(0)] = 1\\).\nMissingness is independent of potential outcomes:\n\n\\[\n\\begin{align}\n\\underbrace{E[Y_i(1)|R_i(1) = 1]}_{E[Y_i(1)] \\text{ if } Y_i(1) \\text{ is not missing }} &=  \\underbrace{E[Y_i(1)|R_i(1) = 0]}_{E[Y_i(1)] \\text{ if } Y_i(1) \\text{ is missing }}\\\\\n&\\text{and } \\\\\n\\underbrace{E[Y_i(0)|R_i(0) = 1]}_{E[Y_i(0)] \\text{ if } Y_i(0) \\text{ is not missing }} &= \\underbrace{E[Y_i(0)|R_i(0) = 0]}_{E[Y_i(0)] \\text{ if } Y_i(0) \\text{ is missing }}\n\\end{align}\n\\]\nOtherwise, the analysis conditional upon observing \\(Y_i(Z)\\) can induce an unknowable (but boundable) amount of bias in our estimate of the ATE.\nWe often do not think of missingness of the treatment indicator in experiments. Indeed, competent administration of an experiment generally ensures against missing treatment values. Nevertheless, it is important to note that missingness of the treatment indicator can also produce bias if missingness is not independent of potential outcomes.\nThe following simulation shows the consequences of two types of missingness for estimation of the ATE. We set the true ATE to 0.5 (the red vertical lines) in all cases. We simulate missingness through two types of data generating processes. In both cases, all missingness occurs among subjects in treatment (\\(Z = 1\\)). In the top panel, missingness is most likely among subjects in treatment with higher values of the outcome \\(Y_i(1)\\). In the bottom pannel, missingness is independent of the value of \\(Y_i(1)\\). If missingness is correlated with potential outcomes, the estimator of the ATE is biased (top row). This occurs whether we are missing values of the outcome (left column) or the treatment indicator (right column). In contrast, when missing data is independent of potential outcomes, the estimator is unbiased (bottom row).\n\nlibrary(randomizr)\nlibrary(estimatr)\n  \nextract_ests &lt;- function(estimatr_model){summary(estimatr_model)$coef[2,1]}\nsimulation &lt;- function(){\n  Y0 &lt;- rnorm(2000)\n  Z &lt;- complete_ra(2000)\n  Yobs &lt;- Y0 + Z * .5 # .5 standard deviation treatment effect\n  Z_miss &lt;- rbinom(n = 2000, prob = Z * pnorm(Yobs), size = 1)\n  Z_miss_ind &lt;- rbinom(n = 2000, prob = sum(Z_miss)/2000, size = 1)\n  Y_miss &lt;- rbinom(n = 2000, prob = Z * pnorm(Yobs), size = 1)\n  Y_miss_ind &lt;- rbinom(n = 2000, prob = sum(Y_miss)/2000, size = 1)\n  \n  m1 &lt;- lm_robust(Yobs ~ Z, subset = Z_miss == 0)\n  m2 &lt;- lm_robust(Yobs ~ Z, subset = Z_miss_ind == 0)\n  m3 &lt;- lm_robust(Yobs ~ Z, subset = Y_miss == 0)\n  m4 &lt;- lm_robust(Yobs ~ Z, subset = Y_miss_ind == 0)\n  return(sapply(list(m1, m2, m3, m4), FUN = extract_ests))\n}\nreps &lt;- replicate(n = 500, expr = simulation())\ndata.frame(ests = as.vector(t(reps)),\n           missing = rep(c(\"Missing Treatment Indicator\", \"Missing Outcome\"), each = 1000),\n           pos = rep(c(\"Missing is\\nNot Independent of POs\", \"Missingness is\\nIndependent of POs\"), each = 500)) %&gt;%\n  ggplot(aes(x = ests)) + \n  facet_grid(pos ~ missing) +\n  geom_histogram(bins = 100) +\n  geom_vline(xintercept = .5, col = \"red\") +\n  theme_minimal() +\n  xlab(\"ATE Estimates\")\n\n\n\n\n\n\n\n\n\n\n3 The potential for bias increases in the proportion of treatment or outcome data that is missing.\nReturning to the equations in #2, it is useful to identify the terms that we can estimate (know) as analysts of a dataset. Rewriting the expression for the ATE in the presence of missing data, we can estimate:\n\nThe proportion of missingness in treatment and control: \\(\\color{red}{E[R_i(1)]}\\) and \\(\\color{red}{E[R_i(0)]}\\).\nThe expectation of the outcome variable among reporters (non-missing data) in treatment and control: \\(\\color{red}{E[Y_i(1)|R_i(1) = 1]}\\) and \\(\\color{red}{E[Y_i(0)|R_i(0)=1]}\\)\n\nThese expressions are colored in the following equation.\n\\[\\begin{align}\n\\underbrace{E[Y_i(1)]-E[Y_i(0)]}_{ATE} =& \\underbrace{\\color{red}{E[R_i(1)]E[Y_i(1)|R_i(1) = 1]}}_{Z = 1\\text{ and }Y_i \\text{ is not missing}} + \\underbrace{(1-E[R_i(1)])(E[Y_i(1)|R_i(1) = 0])}_{Z = 1 \\text{ and } Y_i \\text{ is missing}} - \\\\\n&\\underbrace{\\color{red}{E[R_i(0)]E[Y_i(0)|R_i(0) = 1]}}_{Z = 0 \\text { and } Y_i \\text{ is not missing }} - \\underbrace{(1-E[R_i(0)])E[Y_i(0)|R_i(0) = 0]}_{Z = 0 \\text { and } Y_i \\text{ is missing }}\n\\end{align}\\]\nWe are ultimately interested in estimating the ATE, \\(E[Y_i(1)]-E[Y_i(0)]\\). One straightforward implication of this expression is that the magnitude of bias possible for an estimator of the ATE on non-missing observations increases in the amount of missingness. As \\(E[R_i(1)] \\rightarrow 1\\) and \\(E[R_i(0)]\\rightarrow 1\\), bias stemming from missing outcome data converges to 0. In contrast, when we are missing a large proportion of the data, the magnitude of possible bias increases. Thus, our concerns about missing data should increase as the amount (proportion) of missingness increases.\n\n\n4 The consequences of missing data for bias in estimates of causal effects depend on the type of variable that is missing.\nMissingness of pretreatment covariates need not induce bias in our estimates of the ATE. However, researchers can actually induce bias through improper treatment of missing pretreatment covariate data. If treatment is randomly assigned, treatment assignment should be orthogonal to pre-treatment missingness. In other words, pre-treatment missingness should be balanced across treatment assignment conditions.\nHowever, we should avoid “dropping” (excluding) observations based on pretreatment missingness for two reasons. First, it is possible to induce bias in our estimate of an ATE by dropping observations with pre-treatment missingness. After dropping these observations, we can estimate an unbiased estimate the local average treatment effect (LATE) among observations with no missing pretreatment data. However, if treatment effects vary with missingness of pretreatment variables, this LATE may be quite different than the ATE. Second, as we drop observations the number of observations decreases, reducing our power to detect a given ATE. In sum, we should refrain from dropping observations based on pre-treatment covariates to avoid inducing bias or efficiency loss in our estimates of the ATE.\nIn contrast, missingness of the treatment indicator or outcome variable(s) can induce bias in our estimates of causal effects, as demonstrated in #2. This categorization informs the strategies that we adopt to address the consequences of missing data.\n\n\n5 What assumptions do we invoke when we “ignore” treatment or outcome missingness in estimation?\nSuppose that we analyze a dataset with missing treatment or outcome data while ignoring the missingness. If we “ignore” the missingness, we drop observations for which we lack a value for one of these variables. If we proceed to estimate the ATE on the subsample of data for which we have full data for both treatment and outcomes, we estimate:\n\\[E[Y_i(1)|R_i(1)]- E[Y_i(0)|R_i(1)]\\]\nThis is necessarily equivalent to the ATE estimand \\(E[Y_i(1)]-E[Y_i(0)]\\) if missingness is independent of potential outcomes (MIPO), i.e. \\(Y_i(Z) \\perp R_i(Z)\\) (Gerber and Green 2012). Thus, to interpret \\(E[Y_i(1)|R_i(1)]- E[Y_i(0)|R_i(1)]\\) as an unbiased estimator of the ATE, we must impose an assumption that MIPO.\nThe assumption that MIPO is most plausible when missingness occurs by some random process. Under what conditions might this assumption be plausible? Perhaps we were only able to gather a random subset of administrative outcome data. In this case, missingness would be independent of both potential outcomes and pretreatment covariates. Alternatively, idiosyncratic behavior in data collection (enumerator absence or error) that is plausibly unrelated to potential outcomes may also be consistent with MIPO. On the other hand, survey non-response or other forms of outcome measurement dependent on subject reporting are often much harder to justify under MIPO. Because we cannot validate MIPO, we depend on researchers’ consideration of whether the assumption is plausible under a given data generating process. Where MIPO is not plausible, we should not simply “ignore” missing data in estimation. In these cases, researchers should consider the methods described in #6-#10.\n\n\n6 Why should we assess whether missingness of outcomes is related to treatment assignment?\nWhen researchers encounter missingness in an experiment, they often examine the relationship between missingness of outcomes and treatment assignment. We have denoted reporting (or non-missingness) as a potential outcome of treament assignment, \\(R_i(Z)\\). For a binary treatment, we can denote four “types” of subjects in the experiment, as denoted in the following table.\n\n\n\nType\nProportion\n\\(R_i(1)\\)\n\\(R_i(0)\\)\n\n\n\n\nAlways Reporter\n\\(\\pi_A\\)\n1\n1\n\n\nIf Treated Reporter\n\\(\\pi_T\\)\n1\n0\n\n\nIf Untreated Reporter\n\\(\\pi_U\\)\n0\n1\n\n\nNever Reporter\n\\(\\pi_N\\)\n0\n0\n\n\n\nIn the case in which missingness is related to potential outcomes but not to treatment assignment, we are generally not able to identify an unbiased estimate of the ATE. However, we may be interested in identifying the LATE among always reporters – those subjects for which we observe the outcome regardless of treatment assignment. This effect can be a main estimand of interest and even the most policy-relevant estimand in certain settings. However, to estimate this LATE, we want to be sure that the outcomes that we observe are those of always reporters, not if-treated or if-untreated reporters.\nTo assess the plausibility of ths conjecture, we often examine the relationship between treatment assignment and reporting (non-missingness). Using the notation from the table, we can estimate:\n\\[\\begin{align}\nE[R_i(1)-R_i(0)] &= \\pi_A + \\pi_T - (\\pi_A + \\pi_U)\\\\\n&=\\pi_T - \\pi_U\n\\end{align}\\]\nIf we find no difference in reporting across treatment groups, this test provides no evidence that \\(\\pi_T \\neq \\pi_U\\). However, to further justify that the complete observations in the data are those of “always reporters,” we further must know that \\(\\pi_T = \\pi_U = 0\\). We cannot test this by examining the relationship between reporting and treatment assignment. As such, we generally complement this test with a justification of why reporting should not be endogenous to treatment assignment: for example by assessing covariate balance between treated and control units who are not missing, or by assessing covariate balance between units with missing vs non-missing outcomes. If reporting is not endogenous, then in principle the assumption that \\(\\pi_T = \\pi_U = 0\\) holds. As such, if we find no evidence of differential levels of reporting and see no way that reporting should be endogenous to treatment, we can justify that the \\(E[Y_i(1)|R_i(1) = 1] - E[Y_i(0)|R_i(0) = 1]\\) estimates the LATE among always reporters.\n\n\n7 What is imputation?\nThe most common approaches used to deal with missing data involve the imputation or “filling in” of missing values. In the following dataset with missingness, imputation procedures “fill in” the missing data – the NAs.\n\n\n\n\n\nXobs\nZ\nYobs\n\n\n\n\n-1.2070\n0\n3\n\n\nNA\n0\n2\n\n\n1.0840\n1\n3\n\n\n-2.3460\n1\n4\n\n\n0.4291\n1\n2\n\n\n0.5061\n0\nNA\n\n\nNA\n1\nNA\n\n\nNA\n1\n3\n\n\n-0.5645\n0\n2\n\n\nNA\n0\n1\n\n\n-0.4772\n0\n2\n\n\n-0.9984\n1\n4\n\n\n\n\n\nJust as the consequences of missingness vary by the type of variable that is missing, the imputation methods advocated to address missingness also vary. Importantly, these methods vary in the strength of the assumptions about missingness that are invoked to identify causal effects in the presence of missingness. We review three common approaches to imputation in #8-#10.\n\n\n8 How do we address missingness of pre-treatment covariates and why does this matter?\nAs mentioned in #4, we should never “drop” observations on account of missing pre-treatment data. In order to estimate a model with covariate adjustment, thus, we need to “fill in” missing values to avoid dropping observations. We outline two forms of imputation advocated for missing pre-treatment covariates. The most common approach to address missingness of pre-treatment covariates is to create indicators for missingness and include these as covariates. To do this form of imputation:\n\nSubstitute a numerical value for the NA (as necessary). In the dataset below, we impute a 0 for all values of Xobs that are NAs. The new variable is named Ximputed.\nCreate an indicator for the missingness in each pretreatment variable. This indicator, Xmissing takes the value 1 whenever Xobs is NA, and a 1 otherwise.\n\nOur imputed dataset is thus:\n\n\n\n\n\nXobs\nXimputed\nXmissing\nZ\nYobs\n\n\n\n\n-1.2070\n-1.2070\n0\n0\n3\n\n\nNA\n0.0000\n1\n0\n2\n\n\n1.0840\n1.0840\n0\n1\n3\n\n\n-2.3460\n-2.3460\n0\n1\n4\n\n\n0.4291\n0.4291\n0\n1\n2\n\n\n0.5061\n0.5061\n0\n0\nNA\n\n\nNA\n0.0000\n1\n1\nNA\n\n\nNA\n0.0000\n1\n1\n3\n\n\n-0.5645\n-0.5645\n0\n0\n2\n\n\nNA\n0.0000\n1\n0\n1\n\n\n-0.4772\n-0.4772\n0\n0\n2\n\n\n-0.9984\n-0.9984\n0\n1\n4\n\n\n\n\n\nConsider how this imputation changes the specification of a regression model. Instead of estimating:\n\nmodel1 &lt;- lm(Yobs ~ Z + Xobs)\n\nwe estimate:\n\nmodel2 &lt;- lm(Yobs ~ Z + Ximputed + Xmissing)\n\nThe estimator model2 does not drop observations on the basis of the missing pre-treatment variable Xobs like model1 does.\nAlternatively, if pre-treatment missingness is minimal (less than 10% of observations), or when working with very small datasets with few observations relative to the number of covariates and missingness indicators, Lin, Green, and Coppock (2016) advocate imputing the (unconditional) mean of the observed pre-treatment covariate. Call this variable Ximputed_mean.\n\n\n\n\n\nXobs\nXimputed_mean\nZ\nYobs\n\n\n\n\n-1.2070\n-1.2070000\n0\n3\n\n\nNA\n-0.4467375\n0\n2\n\n\n1.0840\n1.0840000\n1\n3\n\n\n-2.3460\n-2.3460000\n1\n4\n\n\n0.4291\n0.4291000\n1\n2\n\n\n0.5061\n0.5061000\n0\nNA\n\n\nNA\n-0.4467375\n1\nNA\n\n\nNA\n-0.4467375\n1\n3\n\n\n-0.5645\n-0.5645000\n0\n2\n\n\nNA\n-0.4467375\n0\n1\n\n\n-0.4772\n-0.4772000\n0\n2\n\n\n-0.9984\n-0.9984000\n1\n4\n\n\n\n\n\nIf we are simply imputing the pretreatment mean, then instead of estimating model1 (as above) our regression model should be:\n\nmodel3 &lt;- lm(Yobs ~ Z + Ximputed_mean)\n\nSince we have imputed missing values in Xobs when constructing Ximputed_mean, we will not lose observations on the basis of the missing pretreatment covariate when estimating model3.\n\n\n9 We can bound ATEs to account for missing outcome data without making assumptions about the distribution of missing outcomes.\nIf we know the maximum and minimum values of the outcome variable, we can construct bounds on the ATE even in the presence of missing values of the dependent variable. For example, the range of the variable Yobs is 1 to 4. While we do not know what values missing values of Yobs would have been, we can take advantage of the fact that we know the maximum and minimum possible value to construct bounds on the ATE. This allows us to construct an interval estimate of the ATE instead of the point estimate we would construct if we had no missing data. These bounds are known as “extreme value bounds” or “Manski bounds” and do not invoke any additional assumptions about the value of Yobs.\nManski bounds consist of a maximum and minimum bound (estimate) of the possible ATE. To create these bounds, we impute the maximum or minimum value of the outcome variable. as a function of treatment assignment. Thus we construct:\n\nA maximum bound by imputing the maximum value of the outcome for missing values in treatment (\\(Z=1\\)) and imputing the minimum value of the outcome for missing values in control (\\(Z=0\\)).\nA minimum bound by imputing the minimum value of the outcome for missing values in treatment (\\(Z=1\\)) and imputing the maximum value of the outcome for missing values in control (\\(Z=0\\)).\n\nUsing the above dataset, we can construct two variables Y_maxbound and Y_minbound as follows:\n\n\n\n\n\nXobs\nZ\nYobs\nY_maxbound\nY_minbound\n\n\n\n\n-1.2070\n0\n3\n3\n3\n\n\nNA\n0\n2\n2\n2\n\n\n1.0840\n1\n3\n3\n3\n\n\n-2.3460\n1\n4\n4\n4\n\n\n0.4291\n1\n2\n2\n2\n\n\n0.5061\n0\nNA\n1\n4\n\n\nNA\n1\nNA\n4\n1\n\n\nNA\n1\n3\n3\n3\n\n\n-0.5645\n0\n2\n2\n2\n\n\nNA\n0\n1\n1\n1\n\n\n-0.4772\n0\n2\n2\n2\n\n\n-0.9984\n1\n4\n4\n4\n\n\n\n\n\nWithout covariate adjustment, we can obtain our interval estimate of the ATE by estimating:\n\nupper &lt;- lm(Y_maxbound ~ Z)\nlower &lt;- lm(Y_minbound ~ Z)\ncoef(upper)[2]\n\n  Z \n1.5 \n\ncoef(lower)[2]\n\n  Z \n0.5 \n\n\nOur interval estimate of the ATE using Manski bounds is thus [0.5, 1.5].\n\n\n10 Multiple imputation for missing outcomes allows for point estimation of ATEs, but relies on stronger assumptions than bounding.\nThe methods in #8 and #9 describe methods of single imputation, where a single value is substituted for missing values. In multiple imputation, we impute missing values of the dataset multiple times according to an assumed stochastic data generating process. Different methods for multiple imputation impose different structures and assumptions about the probability distributions governing the data generating processes used to impute missing values. In general, multiple imputation proceeds via three stages:\n\nImputation: Missing values are imputed via a random draw of plausible values under the specified data generating process. This creates a full dataset without missing values. Typically, researchers will impute at least five complete datasets. The only differences across these imputed datasets are the values that were missing in the original data.\nEstimation: Estimate the ATE (or other estimand of interest) using each imputed dataset. This generates as many estimates and standard errors as there are imputed datasets.\nPooling estimates: Finally, researchers combine estimates from the different imputed datasets to generate a point estimate of the ATE and its stanard error. Typically, this point estimate can be calculated using rules laid out by Rubin (2004).\n\nMultiple imputation is implemented in many software packages and relatively straightforward to implement. However, the specification of a data generating process from which datasets are imputed relies on additional assumptions about the correctness of the model of missingness (the data generating process). These assumptions are generally untestable and stronger than the standard experimental assumptions invoked to identify an interval estimate of the ATE using Manski bounds.\n\n\n\n\n\n\n\n\n Back to top11 References\n\nGerber, Alan S., and Donald P. Green. 2012. Field Experiments: Design, Analysis, and Interpretation. W.W. Norton.\n\n\nLin, Winston, Donald P. Green, and Alexander Coppock. 2016. “Standard Operating Procedures for Don Green’s Lab at Columbia.”\n\n\nRubin, Donald B. 2004. Multiple Imputation for Nonresponse in Surveys. New York: John Wiley; Sons."
  },
  {
    "objectID": "guides/implementation/workflow_en.html",
    "href": "guides/implementation/workflow_en.html",
    "title": "10 Things You Need to Know About Project Workflow",
    "section": "",
    "text": "There are limits to human memory. Since most experimental research requires at least months if not years of design, monitoring, analysis, and reporting, few researchers can maintain mental oversight of all of a project’s moving pieces over time. Introduce additional investigators into the mix, and the questions of who did what, when, and why (if not how) multiply and become harder to answer. As replication becomes more important (by the original project team or outside researchers), maintaining a written record of decisions, actions, and questions becomes essential. Bowers and Voors (2016) provide a framework and steps for improving project’s workflow; this guide draws upon their paper and upon additional tools aimed at documenting the important choices made by researchers and effectively communicating those choices to the project team."
  },
  {
    "objectID": "guides/implementation/workflow_en.html#footnotes",
    "href": "guides/implementation/workflow_en.html#footnotes",
    "title": "10 Things You Need to Know About Project Workflow",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCalled, anachronistically, Sweave↩︎\nNote that all of the EGAP methods guides that produce figures or tables are produced this way (see https://github.com/egap/methods-guides)↩︎\nThere are many ways to type text that includes code such that parenthesis are automatically closed, spelling is checked, or even help is provided about the code itself. RStudio is an integrated development environment (IDE) that includes a nice editor. Some of us use older tools that are more widespread such as vim, or eMacs. Others any of the contemporary improvements on those old tools such as Atom.↩︎\nhttps://docs.google.com↩︎\nhttps://www.dropbox.com↩︎\nSee King (1995).↩︎"
  },
  {
    "objectID": "guides/implementation/evaluation-conversations_en.html",
    "href": "guides/implementation/evaluation-conversations_en.html",
    "title": "10 Conversations that Implementers and Evaluators Need to Have",
    "section": "",
    "text": "This may seem blatantly straightforward–the evaluation is to see if the program worked. Yet most policy evaluation efforts involve multiple goals. The program may be in a pilot stage, and you want to gather preliminary evidence before expanding the program. The program may have worked in one context (country, community), and you would like to know how generalizable it is. You may want to know who the program works for. Does it work differently for men or women? Marginalized communities or wealthy and powerful groups? You may want to know how part of your intervention affects other aspects of your existing interventions (e.g., cash vs. cash + training) or what combination of activities are most cost effective. You may want to know if you should try to scale the program. Or you may want to know not so much whether or not the program works, but why it works.\nAdditionally, many programs have multiple components. The program may not only be delivering a service (e.g., distributing information on how to access health services), but it may also be trying to build the capacity of a local agency; (e.g., help a public health agency to identify households for additional forms of support ). As a result, there are potentially both individual-level (i.e., micro) questions–the effect of school lunches on children–and more macro questions–how are local governments better able to identify vulnerable children?–that need answers. While both may be asked in one evaluation, the evaluators need to understand how the implementer prioritizes the two questions. This prioritization will affect the evaluation design (see the next Conversation).\nAs implementers, it's important to be clear on your goal of what you want to learn among yourselves and be clear about those goals with the evaluation team. The evaluation team or person, particularly if they are an academic or publish their work more widely, will also have goals about what they want to learn. Without being clear about what you and your organization need to learn, the evaluator may not design the most appropriate evaluation method. This takes us to the next question."
  },
  {
    "objectID": "guides/implementation/evaluation-conversations_en.html#footnotes",
    "href": "guides/implementation/evaluation-conversations_en.html#footnotes",
    "title": "10 Conversations that Implementers and Evaluators Need to Have",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n[^1]: Much of this document is inspired by the Project Process of the OES as well as discussions hosted by the Causal Inference for Social Impact project at CASBS and the Evidence in Governance and Politics network. See the MIT Gov Lab Guide to Difficult Conversations for more guidance about academic-practitioner collaborations as well as the Research4Impact findings about cross-sector collaborations. We anticipate that this document will be open source and revised over time based on your comments and suggestions. Thanks much to Carrie Cihak, Matt Lisiecki, Ruth Ann Moss, Betsy Rajala, Cyrus Samii, Rebecca Thornton,, and folks at the organizations listed above for helpful comments.↩︎\nWe do recognize that some donors may not allow this (though many are becoming proponents of it), and the competitive nature of fundraising may make sharing data seem risky, especially as a first mover.↩︎"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching impact evaluation",
    "section": "",
    "text": "EGAP seeks to improve methods for rigorous impact evaluation throughout the social and behavioral science research community, as well as to make existing methods accessible to a wider audience. EGAP members and staff have developed teaching materials for three audiences: learners, who wish to work at their own pace using course materials; teachers of academic courses; and teachers of EGAP’s Learning Days short courses on impact evaluation. These resources are assembled below.\n\n\n\nThe Theory and Practice of Field Experiments Book\n\n\n\n\n\n\n\n\nThe Theory and Practice and of Field Experiments book is a comprehensive overview of causal inference methods for researchers developing an experimental research design. It is organized in modules and covers topics such as causal inference, randomization, hypothesis testing, estimands, estimators, statistical power, measurement, threats to internal validity, and the ethics of experimentation. The modules appear in the order the Learning Days instructors have found most useful. However, the modules are linked to one another and can be reordered to suit your needs as an instructor. In the appendix, we include some course preliminaries including a glossary of terms and an introduction to R and RStudio.\n\n\n\n\n\n\n\n\n Back to top"
  }
]