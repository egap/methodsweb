[
  {
    "objectID": "guides.html",
    "href": "guides.html",
    "title": "Methods guides",
    "section": "",
    "text": "EGAP seeks to improve methods for rigorous impact evaluation throughout the social and behavioral science research community, as well as to make existing methods accessible to a wider audience. The methods guides are primers on a range of methodological and research topics. Each guide takes one issue and outlines the ten most important points to understand about it or the ten essential steps necessary to addressing that issue in your work."
  },
  {
    "objectID": "guides.html#getting-started",
    "href": "guides.html#getting-started",
    "title": "Methods guides",
    "section": "Getting started",
    "text": "Getting started\n\n\n\n\n\n\n \n\n\n\n10 Conversations that Implementers and Evaluators Need to Have\n\n\n\nJake Bowers, Rebecca Wolfe\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n10 Randomization Inference Procedures with ri2\n\n\n\nAlexander Coppock\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Strategies for Figuring out if X Causes Y\n\n\n\nMacartan Humphreys\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About Causal Inference\n\n\n\nMacartan Humphreys\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About External Validity\n\n\n\nRenard Sexton\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About Multiple Comparisons\n\n\n\nAlexander Coppock\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n10 Things You Need to Know About Project Workflow\n\n\n\nMatthew Lisiecki\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About Randomization\n\n\n\nLindsay Dolan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About Statistical Power\n\n\n\nAlexander Coppock\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About the Local Average Treatment Effect\n\n\n\nPeter van der Windt\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n10 Things Your Null Result Might Mean\n\n\n\nRekha Balu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Adaptive Experimental Design\n\n\n\nDonald Green, Molly Offer-Westort\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Cluster Randomization\n\n\n\nJake Bowers\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n10 Things to Know About Conducting a Meta-Analysis\n\n\n\nDonald Green\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Covariate Adjustment\n\n\n\nLindsay Dolan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Heterogeneous Treatment Effects\n\n\n\nAlbert Fang\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n10 Things to Know About Measurement in Experiments\n\n\n\nTara Slough\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Mechanisms\n\n\n\nLindsay Dolan\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n10 Things to Know About Missing Data\n\n\n\nTara Slough\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n10 Things to Know About Multisite or Block-Randomized Trials\n\n\n\nKristen Hunter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Pilot Studies\n\n\n\nKaylyn Jackson Schiff, Daniel S. Schiff, Natália S. Bueno\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n10 Things to Know About Pre-Analysis Plans\n\n\n\nNuole (Lula) Chen, Christopher Grady\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Randomization Inference\n\n\n\nDonald Green\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Reading a Regression Table\n\n\n\nAbby Long\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n10 Things to Know About Sampling\n\n\n\nAnna M. Wilke\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Spillovers\n\n\n\nAlexander Coppock\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Survey Design\n\n\n\nGabriella Sacramone-Lutz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Survey Experiments\n\n\n\nChristopher Grady\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Survey Implementation\n\n\n\nGabriella Sacramone-Lutz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Types of Treatment Effect You Should Know About\n\n\n\nPaul Testa\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides.html#planning-research-and-evaluations",
    "href": "guides.html#planning-research-and-evaluations",
    "title": "Methods guides",
    "section": "Planning research and evaluations",
    "text": "Planning research and evaluations\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About Statistical Power\n\n\n\nAlexander Coppock\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "coursebook/statisticalpower.html",
    "href": "coursebook/statisticalpower.html",
    "title": "Methods for Impact Evaluations",
    "section": "",
    "text": "Before we run a study, we would like to know whether a particular design has the statistical power to detect an effect if it exists. It is difficult to learn from an under-powered study, since it would be unclear whether a null result indicates that there was no effect or just that we failed to detect a non-zero effect that exists. A power analysis can help you improve your design and allocate your resources better; it may even help you decide against conducting the study.\nIn this module, we introduce statistical power, core approaches to calculating power through analytical calculations and through simulation, and how design features such as blocking, covariate adjustment, and clustering impact power.\n\n\n\nStatistical power is the ability of a study to detect an effect given that it exists.\nPower analysis is something we do before a study. It helps you figure out the sample you need or what effects you can detect. It is an essential step in research design and helps you communicate your design.\nCommon approaches to power calculation:\n\nAnalytical power calculations (using a formula)\nSimulations (for example, using DeclareDesign)\n\nCovariate adjustment and blocking can increase power.\nFor clustered designs you need to take account of the intra-cluster correlation (the within-cluster variance relative to the overall variance).\nPower is closely liked to study design, hypothesis testing and estimation.\n\n\n\n\nBelow are slides with the core content that we cover in our lecture on power. You can directly use these slides or make your local copy and edit.\n\nR Markdown Source\nPDF version\nHTML version\n\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe power presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019\nThe power presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019\nThe power presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nThe power presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe power presentation from EGAP Learning Days in Salima, Malawi, February 2017\nThe power presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\n\n\n\n\n\n\n\nEGAP Methods Guide 10 Things to Know about Statistical Power\nEGAP Methods Guide 10 Things to Know about Covariate Adjustment\nEGAP Methods Guide 10 Things Your Null Results Might Mean\n\n\n\n\nSome examples of power analysis in designs:\n\nPre-Analysis Plan. Accountability Can Transform (ACT) Health: A Replication and Extension of Bjorkman and Svensson (2009)\nEGAP Policy Brief 58: Can bottom-up accountability generate improvements in health outcomes?\n\n\n\n\n\nInteractive power analysis\n\nEGAP Power Calculator\nrpsychologist\n\nR packages for power analysis\n\npwr\nDeclareDesign, see also https://declaredesign.org/"
  },
  {
    "objectID": "coursebook/measurement.html",
    "href": "coursebook/measurement.html",
    "title": "Methods for Impact Evaluations",
    "section": "",
    "text": "To estimate effects and test hypotheses, we often use an outcome of interest measured with quantitative data from surveys, behavioral games, or administrative records. For causal questions, we typically use data on immediate and final outcomes and core mechanisms. We use baseline data to identify relevant subgroups, adjust our estimates, or help block-randomize our treatment. Measurements should be valid and reliable. Be aware that data can be noisy (random error) and/or biased (systematic error).\nThis module discusses what to measure and how to measure. It shows how good measurement is closely linked to your research design and statistical power.\n\n\n\nWhen we represent some attribute of a unit by some number, letter, word, or symbol in some systematic way (perhaps in a cell in a simple dataset), we are measuring.\nA valid measure of a concept or phenomenon of interest should clearly represent that underlying and often abstract entity.\nA reliable measure of a concept would provide the same score for the unit of measurement (for example, a person or a village) if conditions were not changed.\nWe can assess our theories of measurement using multiple approaches to measuring outcomes, covariates, or differences between units implied by different accounts of causal mechanisms.\nInvalid measurement can make it hard for your research design to effectively distinguish between alternative explanations for the relationship between treatment and outcome.\nUnreliable measurement can diminish statistical power.\nDifficult measurement may call for a pilot study focused on measurement itself.\n\n\n\n\nBelow are slides with the core content that we cover in our lecture on measurement. You can directly use these slides or make your own local copy and edit.\n\nR Markdown Source\nPDF Version\nHTML Version\n\n\n\n\n\n\n\nEGAP Methods Guide 10 Things to Know about Measurement in Experiments\nEGAP Methods Guide 10 Things to Know about Survey Design\nEGAP Methods Guide 10 Things to Know about Survey Implementation\n\n\n\n\n\n[@adcocoll:2001]\n[@scacco_can_2018]\n[@shadish2002experimental]\n[@vicente_is_2014]\n\n\n\n\nUsing survey data at multiple levels\n\nEGAP Policy Brief 58: Does Bottom-Up Accountability Work?\n\nUsing text messages\n\nEGAP Policy Brief 27: ICT and Politicians in Uganda\nEGAP Policy Brief 56: Reporting Corruption in Nigeria\n\nUsing administrative data\n\nEGAP Policy Brief 16: Spillover Effects of Observers in Ghana\nEGAP Policy Brief 67: Electoral Administration in Kenya"
  },
  {
    "objectID": "coursebook/references.html",
    "href": "coursebook/references.html",
    "title": "Methods for Impact Evaluations",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "coursebook/estimation.html",
    "href": "coursebook/estimation.html",
    "title": "Methods for Impact Evaluations",
    "section": "",
    "text": "Randomized experiments generate good guesses about the average outcome under treatment and the average outcome under control. This allows us to write down unbiased estimators of average treatment effects. We can also use the randomization to describe how estimates generated by an estimator can vary from experiment to experiment in the form of standard errors and confidence intervals.\nIn this module, we introduce several types of estimands, the target quantity to be estimated. The choice of estimand is a scientific and policy-informed decision – what quantity is useful for us to learn about? In addition, we want to select an appropriate estimator for this estimand as part of the research design. We discuss how estimators are applied to data to generate an estimate of our estimand and how to characterize the variability of this estimate.\n\n\n\nA causal effect, \\(\\tau_i\\), is a comparison of unobserved potential outcomes for each unit \\(i\\). For example, this can be a difference or a ratio of unobserved potential outcomes.\nTo learn about \\(\\tau_{i}\\), we can treat \\(\\tau_{i}\\) as an estimand or target quantity to be estimated (this module) or as a target quantity to be hypothesized about (hypothesis testing module).\nMany focus on the average treatment effect (ATE), \\(\\bar{\\tau}=\\sum_{i=1}^n  \\tau_{i}\\), in part, because it allows for easy estimation.\nAn estimator is a recipe for calculating a guess about the value of an estimand. For example, the difference between the mean of observed outcomes for \\(m\\) treated units and the mean of observed outcomes for \\(N-m\\) untreated units is one estimator of \\(\\bar{\\tau}\\).\nDifferent randomizations will produce different values of the same estimator targeting the same estimand. A standard error summarizes this variability in an estimator.\nA \\(100(1-\\alpha)\\)% confidence interval is a collection of hypotheses that cannot be rejected at the \\(\\alpha\\) level. We tend to report confidence intervals containing hypotheses about values of our estimand and use our estimator as a test statistic.\nEstimators should (1) avoid systematic error in their guessing of the estimand (be unbiased); (2) vary little in their guesses from experiment to experiment (be precise or efficient); and perhaps ideally (3) converge to the estimand as they use more and more information (be consistent).\nAnalyze as you randomize in the context of estimation means that (1) our standard errors should measure the variability from randomization and (2) our estimators should target estimands defined in terms of potential outcomes.\nWe do not control for background covariates when we analyze data from randomized experiments. But covariates can make our estimation more precise. This is called covariance adjustment. Covariance adjustment in randomized experiments differs from controlling for variables in observational studies.\nA policy intervention (like a letter that encourages exercise) may intend to change behavior via an active dose (actual exercise). We can learn about the causal effect of the intention by randomly assigning letters; this is the intent to treat effect, ITT.\nWe can learn about the causal effect of actual exercise by using the random assignment of letters as an instrument for the active dose (exercise itself) in order to learn about the causal effect of exercise among those who would change their behavior after receiving the letter. The average causal effect versions of these effects are often known as the complier average causal effect or the local average treatment effect.\n\n\n\n\nBelow are slides with the core content that we cover in this session.\n\nR Markdown Source\nPDF Version\nHTML Version\n\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe estimation presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019\nThe estimation presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019\nThe estimation presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nThe estimation presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\n\nYou can also see discussion of the problems of estimating the effect of the active dose of a treatment in these slides (as well as discussion of the problems that missing data on outcomes cause for estimation of average causal effects):\n\nThe design issues presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019 (first section reviews randomization designs)\nThe spillovers and attrition presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe threats presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe complications presentation from EGAP Learning Days in Salima, Malawi, February 2017\nThe threats presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016 (the middle section reviews ITT and non-compliance )\n\n\n\n\n\n\n\nEGAP Methods Guide 10 Types of Treatment Effect You Should Know About\nEGAP Methods Guide 10 Things to Know about Covariate Adjustment\nEGAP Methods Guide 10 Things to Know about Missing Data\nEGAP Methods Guide 10 Things to Know about the Local Average Treatment Effect\nEGAP Methods Guide 10 Things to Know about Spillovers\n\n\n\n\n\n[@gerber_field_2012]. Chapter 2.7 on excludability and non-interference, Chapter 3, Chapter 5 on one-sided noncompliance, Chapter 6 on two-sided noncompliance, Chapter 7 on attrition, Chapter 8 on interference between experimental units.\n[@bowers2020causality].\n\n\n\n\n\nDeclareDesign\nestimatr package for R"
  },
  {
    "objectID": "coursebook/threats.html",
    "href": "coursebook/threats.html",
    "title": "Methods for Impact Evaluations",
    "section": "",
    "text": "Randomized experiments can run into issues that undermine their ability to demonstrate causal effects – that is, threaten the internal validity of randomized experiments. Some units might be missing outcome data and that missingness may be due to the treatment. They may not take the treatment status assigned to them or be subject to spillover effects from a treated neighbor.\nIn this module, we cover some common threats and some best practices to avoid or work around them.\n\n\n\nReview the three core assumptions discussed in the causal inference module.\nWe have said “Analyze as you randomize” in the module on estimands and estimators. Remember that you randomized treatment assignment, not whether the treatment is received or whether a unit participates in data collection.\nMissing data on the outcome (attrition) is especially a problem if the patterns of missingness are caused by the treatment itself. This is a very common problem.\n\nDo not drop observations that are missing outcome data from your analysis.\nYou may be able to bound estimates of treatment effects.\n\nNon-compliance. The effect of treatment assignment is not the same as the effect of receiving the treatment. Sometimes units will not comply with their assigned treatment status.\n\nOne-sided compliance occurs when some units assigned to treatment fail to take the treatment, but all units assigned to control do not take the treatment.\nThe local average treatment effect (LATE, also known as the complier average causal effect, CACE) is the average effect for the units that take the treatment when assigned, but not otherwise. If the monotonicity assumption and the exclusion restriction hold, we may be able to estimate LATE when we have non-compliance.\n\n“Spillover effects” or interference between units is a violation of one of the core assumptions for causal inference (causal inference).\n\nHowever, this may not be a problem if you are interested in spillover effects and/or have designed your research to account for it.\n\nHawthorne effects are when subjects behave differently because they are being observed.\nNon-excludability. Treating treatment and control units differently, such as with different data collection processes or extra attention to the treated units, can confuse interpretation of experimental results.\n\nIf Hawthorne effects are present for treated units but not control units, then we have a violation of the excludability assumption.\n\n\n\n\n\nBelow are slides with the core content that we cover in our lecture on threats to the internal validity of randomized experiments. You can directly use these slides or make your local copy and edit.\n\nR Markdown Source\nPDF version\nHTML version\n\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe threats presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019 (first section reviews randomization designs)\nThe attrition and missing data presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\n\n\n\n\n\n\n\nEGAP Methods Guide 10 Things to Know about Missing Data\nEGAP Methods Guide 10 Types of Treatment Effect You Should Know About\nEGAP Methods Guide 10 Things to Know about the Local Average Treatment Effect\n\n\n\n\n\nStandard operating procedures for Don Green’s lab at Columbia University. A comprehensive set of procedures and rules of thumb for conducting experimental studies.\n[@gerber_field_2012]. Chapters 5–8 address non-compliance, attrition, and interference.\n\n\n\n\n\nEGAP Policy Brief 11: Election Observers and Fraud in Ghana\nEGAP Policy Brief 16: Spillover Effects of Observers in Ghana"
  },
  {
    "objectID": "coursebook/glossary.html#key-concepts",
    "href": "coursebook/glossary.html#key-concepts",
    "title": "Methods for Impact Evaluations",
    "section": "Key Concepts",
    "text": "Key Concepts\nSee the module on causal inference, estimands and estimators.\n\nPotential outcome \\(Y_i(T)\\) The outcome \\(Y\\) that unit \\(i\\) would have under treatment condition \\(T\\). We think of these as fixed quantities for a specific point in time. \\(T\\) can be 0 for control or 1 for treatment if there is only one type of treatment. See the module on causal inference.\nTreatment effect \\(\\tau_i\\) for unit \\(i\\) The contrast between potential outcomes under two treatment conditions for unit \\(i\\). We typically define the treatment effect as the difference in potential outcomes under treatment and control, \\(Y_i(1)-Y_i(0)\\). See the module on causal inference.\nFundamental problem of causal inference in the counterfactual framework. We can’t observe both \\(Y_i(1)\\) and \\(Y_i(0)\\) for a given unit, so we can’t get \\(\\tau_i\\) directly. See the module on causal inference.\nEstimand The thing you want to estimate. An example of an estimand is the average treatment effect. In counterfactual causal inference, this is a function of potential outcomes, not fully observed outcomes. See the module on estimands and estimators.\nEstimator How you make a guess about the value of your estimand from the data you have (i.e., observed). An example of an estimator is the difference-in-means. See the module on estimands and estimators.\n\nAverage treatment effect, ATE The average of the treatment effect for all individuals in your subject pool. This is a type of estimand. If we define \\(\\tau_i\\) to be \\(Y_i(1)-Y_i(0)\\), then the ATE is \\(\\overline{Y_i(1)-Y_i(0)}\\), which is also equivalent to \\(\\overline{{Y}_i(1)}-\\overline{{Y}_i(0)}\\). Notice that we do not use the \\(E[Y_i (1)]\\) style of notation here because \\(E[]\\) means “average over repeated operations,” but \\(\\overline{Y}\\) means “average over a set of observations”. See the module on causal inference and the module on estimands and estimators.\n\nRandom sampling Selecting subjects from a population with known probabilities strictly between 0 and 1.\n\\(k\\)-arm experiment An experiment that has \\(k\\) treatment conditions (including control). See the module on randomization.\nRandom assignment Assigning subjects to experimental conditions with known probabilities strictly between 0 and 1. This is equivalent to random sampling without replacement from the potential outcomes. There are several strategies for random assignment: simple, complete, cluster, block, blocked-cluster. See the module on randomization.\nExternal validity Findings from your study teach you about contexts outside of your sample — in other locations or for other interventions."
  },
  {
    "objectID": "coursebook/glossary.html#statistical-inference",
    "href": "coursebook/glossary.html#statistical-inference",
    "title": "Methods for Impact Evaluations",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nSee modules on hypothesis testing and statistical power.\n\nHypothesis A simple, clear, falsifiable claim about the world. In counterfactual causal inference, this is a statement about a relationship among potential outcomes, like \\(H_0: Y_i(T_i=0) = Y_i(T_i=1) + \\tau_i\\) for the hypothesis that the potential outcome under treatment is the potential outcome under control plus some effect for each unit \\(i\\). See the module on hypothesis testing.\nNull hypothesis A conjecture about the world that you may reject after seeing the data. See the module on hypothesis testing.\nSharp null hypothesis of no effect The null hypothesis that there is no treatment effect for any subject. This means \\(Y_i(1)=Y_i(0)\\) for all \\(i\\). We might write this as \\(H_0: Y_i(T_i=0) = Y_i(T_i=1)\\). See the module on hypothesis testing.\n\\(p\\)-value The probability of seeing a test statistic as large (in absolute value) as or larger than the test statistic calculated from observed data. See the module on hypothesis testing.\nOne-sided vs. two-sided test When you have a strong expectation that the effect is either positive or negative, you can conduct a one-sided test. When you do not have such a strong expectation, conduct a two-sided test. A one-sided test has more power than a two-sided test for the same experiment. See the module on hypothesis testing.\nStandard deviation Square root of the mean-square deviation from the average of a variable. It is a measure of the dispersion or spread of a statistic. \\(SD_x=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_i-\\bar{x})^2}\\)\nFalse Positive Rate/Type I Error of a Test A well-operating hypothesis test rejects a hypothesis about a true causal effect no more than \\(\\alpha\\) % of the time. The false positive rate is the rate at which a test will cast doubt on a true hypothesis. It is the rate at which the test will encourage the analyst to say “statistically significant” when, in fact, there is no causal relationship. See the module on hypothesis testing.\nSampling distribution The distribution of estimates (e.g., estimates of the ATE) for all possible treatment assignments. In design-based statistical inference for randomized experiments, the distribution of estimates from an estimator is generated from randomizations. Many call this a “sampling distribution” because textbooks often use the idea of repeated samples from a population rather than repeated randomizations to describe this kind of variation.\nStandard error The standard deviation of the sampling distribution. A bigger standard error means that our estimates are more susceptible to sampling variation. See the module on estimands and estimators.\nCoverage of a confidence interval A well-operating confidence interval contains the true causal effect \\(100 ( 1 - \\alpha)\\) % of the time. A confidence interval has incorrect coverage when it excludes the true parameter less than \\(100 (1 - \\alpha)\\)% of the time. For example, a 95% confidence interval is supposed to only exclude the true parameter less than 5% of the time.\nStatistical power of a test Probability that a test of causal effects will detect a statistically significant treatment effect if the effect exists. See the module on statistical power. This depends on:\n\nThe number of observations in each arm of the experiment\nEffect size (usually measured in standardized units)\nNoisiness of the outcome variable\nSignificance level (\\(\\alpha\\), which is fixed by convention)\nOther factors including what proportion of your units are assigned to different treatment conditions.\n\nIntra-cluster correlation How correlated the potential outcomes of units are within clusters compared to across clusters. Higher intra-cluster correlation hurts power.\nUnbiased An estimator is unbiased if you expect that it will return the right outcome. That means that if you were to run the experiment many times, the estimate might be too high or to low sometimes but it will be right on average. See the module on estimands and estimators.\nBias Bias is the difference between the average value of the estimator across its sampling distribution and the single, fixed value of the estimand. See the module on estimands and estimators.\nConsistency of an estimator An estimator that produces answers that become ever nearer to the true value of the estimand as the sample size increases is a consistent estimator of that estimand. A consistent estimator may or may not be unbiased. See the module on estimands and estimators.\nPrecision/Efficiency of an estimator The variation in or width of the sampling distribution of an estimator. See the module on estimands and estimators."
  },
  {
    "objectID": "coursebook/glossary.html#randomization-strategies",
    "href": "coursebook/glossary.html#randomization-strategies",
    "title": "Methods for Impact Evaluations",
    "section": "Randomization Strategies",
    "text": "Randomization Strategies\nSee the module on randomization.\n\nSimple An independent coin flip for each unit. You are not guaranteed that your experiment will have a specific number of treated units.\nComplete Assign \\(m\\) out of \\(N\\) units to treatment. You know how many units will be treated in your experiment and each unit has a \\(m/N\\) probability of being treated. The number of ways treatment can be assigned (number of permutations of treatment assignment) is \\(\\frac{N!}{m!(N-m)!}\\).\nBlock First divide the sample into blocks, then do complete randomization separately in each block. A block is a set of units within which you conduct random assignment.\nCluster Clusters of units are randomly assigned to treatment conditions. A cluster is a set of units that will always be assigned to the same treatment status.\nBlocked-Cluster First form blocks of clusters. Then in each block, randomly assign the clusters to treatment conditions using complete randomization."
  },
  {
    "objectID": "coursebook/glossary.html#factorial-designs",
    "href": "coursebook/glossary.html#factorial-designs",
    "title": "Methods for Impact Evaluations",
    "section": "Factorial Designs",
    "text": "Factorial Designs\nSee the module on randomization.\n\nFactorial design A design with more than one treatment, with each treatment assigned independently. The simplest factorial design is a 2 by 2.\nConditional marginal effect The effect of one treatment, conditional on the other being held at a fixed value. For example: \\(Y_i(T_1=1|T_2=0)-Y_i(T_1=0|T_2=0)\\) is the marginal effect of \\(T_1\\) conditional on \\(T_2=0\\).\nAverage marginal effect Main effect of each treatment in a factorial design. It is the average of the conditional marginal effects for all the conditions of the other treatment, weighted by the proportion of the sample that was assigned to each condition.\nInteraction effect In a factorial design, we may also estimate interaction effects.\n\nNo interaction effect: one treatment does not amplify or reduce the effect of the other treatment.\nMultiplicative interaction effect: the effect of one treatment depends on which condition a unit was assigned for the other treatment. This means one treatment does amplify or reduce the effect of the other. The effect of two treatments together is not the sum of the effect of each treatment."
  },
  {
    "objectID": "coursebook/glossary.html#threats",
    "href": "coursebook/glossary.html#threats",
    "title": "Methods for Impact Evaluations",
    "section": "Threats",
    "text": "Threats\nSee the module on threats.\n\nHawthorne effect When a subject responds to being observed.\nSpillover When a subject responds to another subject’s treatment status. Example: my health depends on whether my neighbor is vaccinated, as well as whether I am vaccinated.\nAttrition When outcomes for some subjects are not measured. This might be caused, for example, by people migrating, refusing to respond to endline surveys, or dying. This is especially problematic for inference when it is correlated with treatment status.\nCompliance A unit’s treatment status matches its assigned treatment condition. Example of non-compliance: a unit assigned to treatment doesn’t take it. Example of compliance: a unit assigned to control does not take treatment.\nCompliance types There are four types of units in terms of compliance:\n\nCompliers Units that would take treatment if assigned to treatment and would be untreated if assigned to control.\nAlways-takers Units that would take treatment if assigned to treatment and if assigned to control.\nNever-takers Units that would be untreated if assigned to treatment and if assigned to control.\nDefiers Units that would be untreated if assigned to treatment and would take treatment if assigned to control.\n\nOne-sided non-compliance The experiment has only compliers and either always-takers or never-takers. Usually, we think of one-sided non-compliance as having only never-takers and compliers, meaning that that local average treatment effect is the effect of treatment on the treated.\nTwo-sided non-compliance The experiment may have all four latent groups.\nEncouragement design An experiment that randomizes \\(T\\) (treatment assignment), and we measure \\(D\\) (whether the unit takes treatment) and \\(Y\\) (outcome). We can estimate the ITT and the LATE (local average treatment effect, aka CACE—complier average causal effect). It requires three assumptions.\n\nMonotonicity Assumption of either no defiers or no compliers. Usually we assume no defiers which means that the effect of assignment on take up of treatment is either positive or zero but not negative.\nFirst stage Assumption that there is an effect of \\(T\\) on \\(D\\).\nExclusion restriction Assumption that \\(T\\) affects \\(Y\\) only through \\(D\\). This is usually the most problematic assumption.\n\nIntention-to-treat effect (ITT) The effect of \\(T\\) (treatment assignment) on \\(Y\\).\nLocal average treatment effect (LATE) The effect of \\(D\\) (taking treatment) on \\(Y\\) for compliers. Also known as the complier average causal effect (CACE). Under the exclusion restriction and monotonicity, the LATE is equal to ITT divided by the proportion of your sample who are compliers.\nDownstream experiment An encouragement design study that takes advantage of the randomization of \\(T\\) by a previous study. The outcome from that previous study is the \\(D\\) in the downstream experiment."
  },
  {
    "objectID": "coursebook/researchdesignform.html",
    "href": "coursebook/researchdesignform.html",
    "title": "Methods for Impact Evaluations",
    "section": "",
    "text": "This book aims to help you understand and design randomized field experiments. But before we dive into the details of research design, we need a good research question – a question that will advance knowledge or help make a policy decision or both. There is no simple recipe for finding or developing a good scientific or policy question, but our theories are important for articulating good questions that underlie impactful research. After formulating our question, we develop the best design possible within our resource constraints, using our knowledge of causal inference and statistics from the modules that follow.\nThis module introduces the EGAP Research Design Form, a checklist to guide you through the many stages of the research process. The Learning Days workshops are organized around the Research Design Form. We also point you towards the DeclareDesign software package to explore the implications of different choices we could make for our research designs. Finally, this module discusses pre-analysis plans and pre-registration. When plan our analyses and make these plans public, we improve our chances of persuading others with our results.\n\n\n\nA good research question advances science and/or is a question the answer to which will inform a policy decision.\nCertain research designs are better able to address certain questions. We want to choose the design that best answers our key questions within our constraints.\nThe questions we ask arise — often implicitly — from our values and from our understanding about how the world works. These theories make our questions relevant. And the experiments that we execute teach us about the theory. That is, we hope that the evidence and data arising from these research designs improve our understanding.\nCore components of a research design\nIntroduce core components of the EGAP Research Design Form.\nIntroduce a research design software package, DeclareDesign.\nThe move in social science towards the review of designs, rather than outcomes.\nPre-registration – what it is, and why and how we should do it.\n\n\n\n\nBelow are slides with the core content that we cover in our lecture on research design. You can directly use these slides or make your own local copy and edit.\n\nR Markdown Source\nPDF Version\nHTML Version\n\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe DeclareDesign presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nThe DeclareDesign presentation from EGAP Learning Days in Salima, Malawi, February 2017\nThe DeclareDesign presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\n\nYou can also see slides for Design Talks in previous EGAP Learning Days, where presenters focus on issues that come up in designing the research, rather than the results:\n\nDesign Talk from EGAP Learning Days at the African School of Economics, Benin, March 2018\nDesign Talk 1 from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nDesign Talk 2 from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nDesign Talk 3 from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nDesign Talk 1 from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\nDesign Talk 2 from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\nDesign Talk 3 from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\nDesign Talk from EGAP Learning Days in Guatemala City, Guatemala, August 2017\n\n\n\n\n\nEGAP Research Design Form. A checklist we created for the Learning Days to guide you through the stages of the research process.\n\nDocx Version\nPDF Version\nHTML Version\n\nLinks to repositories for pre-registration/pre-analysis plans:\n\nEGAP registry, hosted by OSF (https://egap.org/registry/)\nAEA RCT registry (https://www.socialscienceregistry.org/)\nOSF (https://osf.io/registries)\n\nExamples of other pre-registrations/pre-analysis plans:\n\nSMS Messages in Mozambique from the US Federal Government\nPolice Body-Cameras from the Lab @ DC\n\n\n\n\n\n\n\n\nEGAP Methods Guide 10 Things to Know about Pre-Analysis Plans\nEGAP Methods Guide 10 Things to Know about Measurement in Experiments\n\n\n\n\n\nPreregistration as a Tool for Strengthening Federal Evaluation. A white paper from the US Government’s Office of Evaluation Sciences. You can also see examples of their pre-analysis plans on all of their field experiment pages.\n[@christensen_transparent_2019]. The book summarizes new approaches in social science research on transparency and reproducibility.\n[@gerber_field_2012]. Chapter 12 includes some examples of experimental research designs.\n\n\n\n\n\nDeclareDesign, an exciting and comprehensive set of software tools for describing, assessing, and conducting empirical research."
  },
  {
    "objectID": "coursebook/ethics.html",
    "href": "coursebook/ethics.html",
    "title": "Methods for Impact Evaluations",
    "section": "",
    "text": "A randomized experiment involves one group of humans changing the lives of another group humans. Those working in government do this as a matter of course — their very job is to provide food, shelter, safety, justice, etc., to their people. Academics, whose work does not usually have immediate impacts on the public, must remember to also carefully consider how their research might change the lives of those exposed to the intervention, as well as those not exposed. When one person influences the life of another, the influencer has responsibilities not to harm the person being influenced.\nThis module discusses the core topics on research ethics, such as privacy and autonomy; the basic principles relating to respect for persons, beneficence, and justice; and how informed consent helps communicate these principles to study participants.\n\n\n\nResearch must weigh the potential benefits of the knowledge to be gained from the research against the potential harms it may do to human subjects.\nHow would you feel if you were a research subject in your study? In the control group? In the treatment group? A relatively high-status member of the community? A relatively low-status member of the community?\nKey tenets: privacy and autonomy.\nBasic principles in the Belmont Report: respect for persons, beneficence, justice.\nInformed consent: Can you ensure that research subjects have the freedom to refuse to participate and/or drop out of the study if they want to? Can you ensure that research subjects can report problems that might arise?\nChallenges for social science experimental research in general:\n\nMany more people may benefit (or suffer from) your intervention than directly participate in your study.\nChanging election results or corruption can produce large societal changes. Is this beyond the remit of research?\n\n\n\n\n\nBelow are slides with the core content that we cover in this session.\n\nR Markdown Source\nPDF Version\nHTML Version\n\n\n\n\n\nEGAP Research Principles and Work on Ethics\nAPSA Principles and Guidance on Human Subjects Research\nBelmont Report\nInstitutional Review Boards in the US\nExample: Research Ethics at Oxford University in the UK\nExample: Ethics for Researchers in the EU\nExample: Research Ethics at the Universidad Catolica de Chile\n\n\n\n\n[@Asieduetal2021ethics]\n[@Evans2021ethics] \n\n\n\n\nExamples of PAPs and papers that discuss ethical issues:\n\nPre Analysis Plan: The Effects of Non-Food Item Vouchers in a Humanitarian Context The Case of the Rapid Response to Movements of Population Program in Congo\nPaper: Appendix E.1 in Countering violence against women by encouraging disclosure: A mass media experiment in rural Uganda"
  },
  {
    "objectID": "coursebook/index.html",
    "href": "coursebook/index.html",
    "title": "Theory and Practice of Field Experiments",
    "section": "",
    "text": "Over the past decade, Evidence in Governance and Politics (EGAP) has organized Learning Days workshops with the aim of building experimental social-science research capacity among principal investigators (PIs) – both researchers and practitioners – in Africa and Latin America. By sharing the practical and statistical methods of randomized field experiments with workshop participants, the Learning Days effort hopes to identify and nurture researcher networks around the world and to create strong, productive connections between these researchers and EGAP members.\nThe Learning Days workshops are a combination of design clinics, research presentations, guided work with statistical software, and topical lectures by a small group of instructors, largely professors and PhD students from the EGAP network. The workshops focus on methods for the design and analysis of randomized experiments in the field rather than on randomized experiments in the lab or non-randomized studies.\nThis book grew out of a desire to share the materials we developed for the Learning Days. The current version is written primarily for instructors and organizers of similar workshops and courses aimed at PIs — i.e., professors, post-doctoral fellows, PhD students, and NGO/government agency evaluators — who will implement randomized studies of programs related to institutions, governance, and development. Much of the material will also be useful as a refresher for past participants of the Learning Days workshops.\nThis book is a comprehensive overview of causal inference methods for researchers developing an experimental research design. It is organized in modules and covers topics such as causal inference, randomization, hypothesis testing, estimands, estimators, statistical power, measurement, threats to internal validity, and the ethics of experimentation. The modules appear in the order the Learning Days instructors have found most useful. However, the modules are linked to one another and can be reordered to suit your needs as an instructor. In the appendix, we include some course preliminaries including a glossary of terms and an introduction to R and RStudio.\nThe book includes slides on the core content, the EGAP Research Design Form, and references to research examples and slides used in previous Learning Days. This material builds significantly on and links to EGAP’s work on methodology, summarized in the EGAP Methods Guides. We have made significant extensions to the past Learning Days’ materials on hypothesis testing, estimation, and statistical power and added new modules on the research design process, measurement, and ethical considerations. The slides and modules presented here contain too much information to be covered in a single week, the usual length of a Learning Days workshop. We have chosen to present more rather than less information, however, to help instructors tailor their courses to their specific audiences.\n\n\nTo gain the most benefit from this book, please have R and RStudio installed on your machine. In fact, the slides assume that you will use Rmarkdown to adapt them for your own purposes.\nTo get going with R, see the module Introduction to R and RStudio.\nYou can copy this book or parts thereof (e.g., slides, etc.) either by using the Download button on the front page of http://github.com/egap/theory_and_practice_of_field_experiments or by using github directly (by forking this repository).\nWe are happy for anyone to use the materials as long as EGAP is attributed. See Creative Commons Attribution-ShareAlike 4.0 International License for the precise terms.\n\n\n\nIf you have any questions, feedback or have organized your own event, please get in touch! Simply post an issue on Github or make comments using hypothes.is in your browser and let us know via email, admin@egap.org. We’ll periodically go through the comments.\n\n\n\nThe materials included in this book have been developed over the past years by various Learning Days instructors. These include (in alphabetical order) Jake Bowers, Jasper Cooper, Ana De la O, Lindsay Dolan, Natalia Garbiras Díaz, Macartan Humphreys, Nahomi Ichino, Salif Jaiteh, Gareth Nellis, Dan Nielson, Rafael Piñeiro, Fernando Rosenblatt, Tara Slough, Peter van der Windt and Maarten Voors. We thank Natalia Garbiras Díaz, Macartan Humphreys, Anghella Brigeth Rosero Rodriguez, and Tara Slough in particular for comments on an early draft of the book. We also thank Brice Bado, Simon Chauchard, Jasper Cooper, Simone Dietrich, Thad Dunning, Jessica Gottlieb, Macartan Humphreys, Julien Labonne, Ambaliou Olounlade, Daniel Rubenson, and Saloua Zerhouni for their help in reviewing the French translation and Rosario Aguilar, Ana De la O, Pablo Egaña del Sol, Omar Garcia Ponce, Paul Lagunes, Luis Maldonado, Fernando Martel Garcia, Paula Muñoz, Raul Pacheco-Vega, Rafael Piñiero, Pablo Querubin, Mauricio Romero, Fernando Rosenblatt, Santiago Saavedra, Lucía Tiscornia, Santiago Tobón, and Juan Vargas for their help in reviewing the Spanish translation of these materials.\nAt EGAP, Matt Lisiecki, Ingrid Lee, Goldie Negelev, Max Mendez-Back and others have provided wonderful support. Learning Days have been generously funded by the Hewlett Foundation and supported by institutions around the world including the African School of Economics (Benin), Universidad Diego Portales (Chile), Universidad de los Andes (Colombia), Ghana Center for Democratic Development (Ghana), Mercy Corps (Guatemala), Invest in Knowledge (Malawi), NYU Abu Dhabi (UAE), Universidad Católica del Uruguay (Uruguay)."
  },
  {
    "objectID": "coursebook/randomization.html",
    "href": "coursebook/randomization.html",
    "title": "Methods for Impact Evaluations",
    "section": "",
    "text": "The module on causal inference discussed the crucial role of randomization for drawing valid inferences from a comparison of treated and untreated groups. In this module, we move from theory to the first of many concrete choices for your research design.\nWe introduce four common ways to randomize treatment – simple, complete, block, and clustered – and when these different types of randomization may be available and appropriate. We also cover several popular designs including factorial designs and encouragement designs. The module provides some guidance on implementation, including best practices for checking for balance and ensuring replicability.\n\n\n\nWhat is randomization? Random assignment is not the same as random sampling.\nFour common ways to randomize treatment:\n\nSimple: randomly assign units to treatment (like a coin flip).\nComplete: within a list of eligible units, a assign a fixed number to receive a treatment (like drawing from a urn).\nBlock (or stratified): assign treatment within specific strata or blocks, as if you are running an experiment within each block.\nCluster: assign groups (clusters) of observations to the same treatment condition.\n\nSome popular designs:\n\nRandomized access: randomization to availability of a treatment.\nRandomized delayed access: randomize the timing of access.\nFactorial: randomize units to combinations of treatment arms.\nEncouragement: randomize the invitation to receive treatment.\n\nHow do you check whether your randomization produced balance on observables? Typically we conduct randomization tests also known as balance tests using the \\(d^2\\) omnibus test from xBalance in the RItools package (because it is randomization inference) or approximate this result with an \\(F\\)-test.\nThere are, of course, limits to randomization. We discuss some here and direct you to the module on threats for more.\n\n\n\n\nBelow are slides with the core content that we cover in our lecture on randomization. You can directly use these slides or make your own local copy and edit.\n\nR Markdown Source\nPDF version\nHTML version\n\nThe linked files shows how to do replicable randomization in R. You can also see more examples of randomization in R at 10 Things You Need to Know About Randomization.\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe design issues presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019 (first section reviews randomization designs)\nThe randomization presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019\nThe randomization presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nThe randomization presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe randomization presentation from EGAP Learning Days in Salima, Malawi, February 2017\nThe randomization presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\n\n\n\n\n\n\n\nEGAP Methods Guide 10 Things You Need to Know About Randomization\nEGAP Methods Guide 10 Things You Need to Know About Cluster Randomization\n\n\n\n\n\nStandard operating procedures for Don Green’s lab at Columbia University. A comprehensive set of procedures and rules of thumb for conducting experimental studies.\n[@glennerster_running_2013]. Chapter 2 on randomization.\n[@gerber_field_2012]. Chapter 2: Causal Inference and Experimentation\n\n\n\n\nFactorial designs\n\nEGAP Policy Brief 57: How Media Influence Social Norms: Evidence from Mexico\nEGAP Policy Brief 58: Does Bottom-Up Accountability Work?\n\nRandomizing access\n\nEGAP Policy Brief 24: Reducing Elite Capture in the Solomon Islands\n\nRandomizing delayed access\n\nEGAP Policy Brief 35: Reducing Reconvictions Among Released Prisoners\nEGAP Policy Brief 60: Reducing Youth Support for Violence through Training and Cash Transfers in Afghanistan\n\nCluster randomization\n\nEGAP Policy Brief 22: Getting Out the Vote\n\nBlocked cluster randomization\n\nEGAP Policy Brief 54: Incumbent Malfeasance Revelations\nEGAP Policy Brief 56: Reporting Corruption\n\n\n\n\n\nRItools, a set of tools for randomization-based inference including balance testing.\n\n\n\n\n\nRandomization vs. Random Sampling\nCluster vs. Block Randomization"
  },
  {
    "objectID": "coursebook/causalinference.html",
    "href": "coursebook/causalinference.html",
    "title": "Methods for Impact Evaluations",
    "section": "",
    "text": "Much of social science is about causality. We might ask questions like whether voter registration increases political participation, whether bottom-up accountability can improve health outcomes, or whether personal narratives of immigrants help reduce prejudicial attitudes towards them.\nOver the past decade, social science has become much more serious about how causal claims are made, building on a long history of work on causality dating back to the classic writings of Fisher and Rubin. We make greater use of experiments, and randomization has become the gold standard for addressing causal questions.\nIn this module, we introduce the counterfactual approach to causal inference and how statements with causal claims can be interpreted. We introduce the potential outcome framework and how random assignment helps us make claims about what would have happened in the absence of the policy, action or program we study. We discuss the three core assumptions for causal inference: random assignment of subjects to treatment, non-interference, and excludability.\n\n\n\nWhat do we mean when we say “cause”? And why does it matter to be clear about the meaning of causal claims?\nAn introduction to potential outcomes as a way to think about alternative states of the world.\nRandomization helps us learn about counterfactual causal claims in a particularly useful way.\nThe three key core assumptions for causal inference: random assignment of subjects to treatment, non-interference, and excludability.\nComparison of randomized studies with observational studies.\nRandomization brings high internal validity, but it can’t promise external validity.\nYour causal question is closely linked so your research design.\n\n\n\n\nBelow are slides with the core content that we cover in our lecture on causality. You can directly use these slides or make your own local copy and edit.\n\nR Markdown Source\nPDF Version\nHTML Version\n\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe causal inference presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019\nThe causal inference presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019\nThe causal inference presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nThe causal inference presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe introduction to experiments presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe causal inference presentation from EGAP Learning Days in Salima, Malawi, February 2017\nThe causal inference presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\n\n\n\n\n\n\n\nEGAP Methods Guide 10 Things You Need to Know about Causal Inference\nEGAP Methods Guide 10 Strategies for Figuring Out If X Caused Y\nEGAP Methods Guide 10 Things You Need to Know about Mechanisms\nEGAP Methods Guide 10 Things to Know About External Validity\n\n\n\n\n\n\n\n[@fisher_design_1935]. Fisher introduces the idea of randomization and hypothesis testing as a way to learn about causal inference.\n[@rubin:1974]. Rubin introduces the idea of potential outcomes and links counterfactual conceptualizations of causality to statistical inference.\n\n\n\n\n\n[@brady2008causation].\n[@gerber_field_2012, Chapter 1]. This book is a great resource for many topics in experimental design.\n[@morgan_counterfactuals_2007, Chapter 1]. This book includes nice examples of thinking through making causal claims from observational data.\n[@glennerster_running_2013]. This is a great introduction to running field experiments and discusses many examples.\n\n\n\n\n\nSome examples of causal questions:\n\nEGAP Policy Brief 38: Are radio voter education campaigns effective in discouraging voters from voting for parties/candidates that engage in vote-buying?\nEGAP Policy Brief 51: Can free and anonymous information communication technology strengthen local accountability and improve the delivery of public services?\nEGAP Policy Brief 58: Can bottom-up accountability generate improvements in health outcomes?\nEGAP Policy Brief 69: Does bottom-up, citizen-based monitoring improve public service delivery?"
  },
  {
    "objectID": "coursebook/hypothesistesting.html",
    "href": "coursebook/hypothesistesting.html",
    "title": "Methods for Impact Evaluations",
    "section": "",
    "text": "We cannot directly observe causal effects because of the fundamental problem of counterfactual causal inference (causal inference module). So how can we learn about these unobserved causal effects using what we do observe? In a randomized experiment, we can assess guesses or hypotheses about the unobserved causal effects by comparing what we observe in a given experiment to what we would observe if we were able to repeat the experimental manipulation and the guess or hypothesis were true.\nIn this module we introduce hypothesis testing, how it relates to causal inference, \\(p\\)-values, and what to do when we have multiple hypotheses to test.\n\n\n\nWhat is a good hypothesis?\nThe relationship between hypothesis testing and causal inference.\nHypothesis tests.\n\nNull hypotheses.\nEstimators versus test statistics.\nIn an experiment, a reference distribution for a hypothesis test comes from the experimental design and the randomization.\n\\(p\\)-values and how to interpret the results of hypothesis tests.\n\nA good hypothesis test should (1) cast doubt on the truth rarely (i.e., have a controlled and low false positive rate), and (2) easily distinguish signal from noise (i.e., cast doubt on falsehoods often; have high statistical power).\nHow would we know when our hypothesis test is doing a good job? (Power analysis is its own module).\n\nFalse positive rates.\nCorrect coverage of a confidence interval.\nAssessing the false positive rate of a hypothesis test for a given design and choice of test statistic; the case of cluster-randomized trials and robust cluster standard errors.\n\nBe careful when testing many hypotheses, such as when you have more than two treatment arms or you are assessing the effects of a treatment on multiple outcomes. We should be careful to adjust the \\(p\\)-values or confidence intervals to reflect the number of tests/intervals produced.\n\n\n\n\nBelow are slides with the core content that we cover in our lecture on hypothesis testing. You can directly use these slides or make your own local copy and edit.\n\nR Markdown Source\nPDF Version\nHTML Version\n\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe hypothesis testing presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019\nThe hypothesis testing presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019\nThe hypothesis testing presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nThe hypothesis testing presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe hypothesis testing presentation from EGAP Learning Days in Salima, Malawi, February 2017\nThe hypothesis testing presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\n\n\n\n\n\n\n\nEGAP Methods Guide 10 Things to Know about Hypothesis Testing\nEGAP Methods Guide 10 Things You Need to Know about Multiple Comparisons\n\n\n\n\n\n[@gerber_field_2012]. Chapter 3: Sampling Distributions, Statistical Inference, and Hypothesis Testing.\n[@rosenbaum2010design]. Chapter 2: Causal Inference in Randomized Experiments.\n[@rosenbaum2017observation]. Part I: Randomized Experiments."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Methods for Impact Evaluations",
    "section": "",
    "text": "Evidence in Governance and Politics (EGAP) is a global research, evaluation, and learning network that promotes rigorous knowledge accumulation, innovation, and evidence-based policy in various governance and accountability domains.\nOur methods resources are aimed at learners and teachers of all levels. Guides are for self-learning, and our coursebook is for teachers of impact evaluation methods.\n\nMethods guides\nLearn methods for impact evaluations just-in-time to use them with guides for impact evaluation methods – for researchers, evidence users, and students\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About Causal Inference\n\n\n\nMacartan Humphreys\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About External Validity\n\n\n\nRenard Sexton\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Covariate Adjustment\n\n\n\nLindsay Dolan\n\n\n\n\n\n\n\n\nNo matching items\n\n\nMore…\n\n\nTeaching coursebook\n\n\n\nLearning days host cities (red) and participant home countries (blue).\n\n\nOver the past decade, Evidence in Governance and Politics (EGAP) has organized Learning Days workshops with the aim of building experimental social-science research capacity among principal investigators (PIs) – both researchers and practitioners – in Africa and Latin America. By sharing the practical and statistical methods of randomized field experiments with workshop participants, the Learning Days effort hopes to identify and nurture researcher networks around the world and to create strong, productive connections between these researchers and EGAP members. The teaching coursebook, which grew out of a desire to share the materials we developed for the Learning Days, is a comprehensive overview of causal inference methods for researchers developing an experimental research design."
  },
  {
    "objectID": "guides/planning/power_en.html",
    "href": "guides/planning/power_en.html",
    "title": "10 Things You Need to Know About Statistical Power",
    "section": "",
    "text": "Abstract\nThis guide1 will help you assess and improve the power of your experiments. We focus on the big ideas and provide examples and tools that you can use in R and Google Spreadsheets.\n\n\n1 What Power Is\nPower is the ability to distinguish signal from noise.\nThe signal that we are interested in is the impact of a treatment on some outcome. Does education increase incomes? Do public health campaigns decrease the incidence of disease? Can international monitoring decrease government corruption?\nThe noise that we are concerned about comes from the complexity of the world. Outcomes vary across people and places for myriad reasons. In statistical terms, you can think of this variation as the standard deviation of the outcome variable. For example, suppose an experiment uses rates of a rare disease as an outcome. The total number of affected people isn’t likely to fluctuate wildly day to day, meaning that the background noise in this environment will be low. When noise is low, experiments can detect even small changes in average outcomes. A treatment that decreased the incidence of the disease by 1% percentage points would be easily detected, because the baseline rates are so constant.\nNow suppose an experiment instead used subjects’ income as an outcome variable. Incomes can vary pretty widely – in some places, it is not uncommon for people to have neighbors that earn two, ten, or one hundred times their daily wages. When noise is high, experiments have more trouble. A treatment that increased workers’ incomes by 1% would be difficult to detect, because incomes differ by so much in the first place.\nA major concern before embarking on an experiment is the danger of a false negative. Suppose the treatment really does have a causal impact on outcomes. It would be a shame to go to all the trouble and expense of randomizing the treatment, collecting data on both treatment and control groups, and analyzing the results, just to have the effect be overwhelmed by background noise.\nIf our experiments are highly-powered, we can be confident that if there truly is a treatment effect, we’ll be able to see it.\n\n\n2 Why You Need It\nExperimenters often guard against false positives with statistical significance tests. After an experiment has been run, we are concerned about falsely concluding that there is an effect when there really isn’t.\nPower analysis asks the opposite question: supposing there truly is a treatment effect and you were to run your experiment a huge number of times, how often will you get a statistically significant result?\nAnswering this question requires informed guesswork. You’ll have to supply guesses as to how big your treatment effect can reasonably be, how many subjects will answer your survey, how many subjects your organization can realistically afford to treat.\nWhere do these guesses come from? Before an experiment is run, there is often a wealth of baseline data that are available. How old/rich/educated are subjects like yours going to be? How big was the biggest treatment effect ever established for your dependent variable? With power analysis, you can see how sensitive the probability of getting significant results is to changes in your assumptions.\nMany disciplines have settled on a target power value of 0.80. Researchers will tweak their designs and assumptions until they can be confident that their experiments will return statistically significant results 80% of the time. While this convention is a useful benchmark, be sure that you are comfortable with the risks associated with an 80% expected success rate.\nA note of caution: power matters a lot. Negative results from underpowered studies can be hard to interpret: Is there really no effect? Or is the study just not able to figure it out? Positive results from an underpowered study can also be misleading: conditional upon being statistically significant, an estimate from an underpowered study probably overestimates treatment effects. Under powered studies are sometimes based on overly optimistic assumptions; a convincing power analysis makes these assumptions explicit and should protect you from implementing designs that realistically have no chance of answering the questions you want to answer.\n\n\n3 The Three Ingredients of Statistical Power\nThere are three big categories of things that determine how highly powered your experiment will be. The first two (the strength of the treatment and background noise) are things that you can’t really control – these are the realities of your experimental environment. The last, the experimental design, is the only thing that you have power over – use it!\n\nStrength of the treatment. As the strength of your treatment increases, the power of your experiment increases. This makes sense: if your treatment were giving every subject $1,000,000, there is little doubt that we could discern differences in behavior between the treatment and control groups. Many times, however, we are not in control of the strength of our treatments. For example, researchers involved in program evaluation don’t get to decide what the treatment should be, they are supposed to evaluate the program as it is.\nBackground noise. As the background noise of your outcome variables increases, the power of your experiment decreases. To the extent that it is possible, try to select outcome variables that have low variability. In practical terms, this means comparing the standard deviation of the outcome variable to the expected treatment effect size — there is no magic ratio that you should be shooting for, but the closer the two are, the better off your experiment will be. By and large, researchers are not in control of background noise, and picking lower-noise outcome variables is easier said than done. Furthermore, many outcomes we would like to study are inherently quite variable. From this perspective, background noise is something you just have to deal with as best you can.\nExperimental Design. Traditional power analysis focuses on one (albeit very important) element of experimental design: the number of subjects in each experimental group. Put simply, a larger number of subjects increases power. However, there are other elements of the experimental design that can increase power: how is the randomization conducted? Will other factors be statistically controlled for? How many treatment groups will there be, and can they be combined in some analyses?\n\n\n\n4 Key Formulas for Calculating Power\nStatisticians have derived formulas for calculating the power of many experimental designs. They can be useful as a back of the envelope calculation of how large a sample you’ll need. Be careful, though, because the assumptions behind the formulas can sometimes be obscure, and worse, they can be wrong.\nHere is a common formula used to calculate power2\n\\[\\beta = \\Phi \\left(\\frac{|\\mu_t-\\mu_c|\\sqrt{N}}{2\\sigma}-\\Phi^{-1} \\left(1-\\frac{\\alpha}{2}\\right) \\right)\\]\n\n\\(\\beta\\) is our measure of power. Because it’s the probability of getting a statistically significant result, β will be a number between 0 and 1.\n\\(\\Phi\\) is the CDF of the normal distribution, and \\(\\Phi^{-1}\\) is its inverse. Everything else in this formula, we have to plug in:\n\\(\\mu_t\\) is the average outcome in the treatment group. Suppose it’s 65.\n\\(\\mu_c\\) is the average outcome in the control group. Suppose it’s 60.\nTogether, assumptions about μt and μc define our assumption about the size of the treatment effect: 65-60= 5.\n\\(\\sigma\\) is the standard deviation of outcomes. This is how we make assumptions about how noisy our experiment will be — one of the assumptions we’re making is that sigma is the same for both the treatment and control groups. Suppose \\(\\sigma=20\\)\n\\(\\alpha\\) is our significance level – the convention in many disciplines is that α should be equal to 0.05. \\(N\\) is the total number of subjects. This is the only variable that is under the direct control of the researcher. This formula assumes that every subject had a 50/50 chance of being in control. Suppose that \\(N=500\\).\n\nWorking through the formula, we find that under this set of assumptions, \\(β = 0.80\\), meaning that we have an 80% chance of recovering a statistically significant result with this design. Click here for a google spreadsheet that includes this formula. You can copy these formulas directly into Excel. If you’re comfortable in R, here is code that will accomplish the same calculation.\n\npower_calculator <- function(mu_t, mu_c, sigma, alpha=0.05, N){ \n  lowertail <- (abs(mu_t - mu_c)*sqrt(N))/(2*sigma) \n  uppertail <- -1*lowertail \n  beta <- pnorm(lowertail- qnorm(1-alpha/2), lower.tail=TRUE) + 1- pnorm(uppertail- qnorm(1-alpha/2), lower.tail=FALSE) \n  return(beta) \n  } \n\n\n\n5 When to Believe Your Power Analysis\nFrom some perspectives the whole idea of power analysis makes no sense. You want to figure out the size of some treatment effect but first you need to do a power analysis which requires that you already know your treatment effect and a lot more besides.\nSo in most power analyses you are in fact seeing what happens with numbers that are to some extent made up. The good news is that it is easy to find out how much your conclusions depend on your assumptions: simply vary your assumptions and see how the conclusions on power vary.\nThis is most easily seen by thinking about how power varies with the number of subjects. A power analysis that looks at power for different study sizes simply plugs in a range of values in for N and seeing how β changes.\nUsing the formula in section 4, you can see how sensitive power is to all of the assumptions: Power will be higher if you assume the treatment effect will be larger, or if you’re willing to accept a higher alpha level, or if you have more or less confidence in the noisiness of your measures.3\n\n\n\n\n6 How to Use Simulation to Estimate Power\nPower is a measure of how often, given assumptions, we would obtain statistically significant results, if we were to conduct our experiment thousands of times. The power calculation formula takes assumptions and return an analytic solution. However, due to advances in modern computing, we don’t have to rely on analytic solutions for power analysis. We can tell our computers to literally run the experiment thousands of times and simply count how frequently our experiment comes up significant.\nThe code block below shows how to conduct this simulation in R.\n\npossible.ns <- seq(from=100, to=2000, by=40) # The sample sizes we'll be considering\nstopifnot(all( (possible.ns %% 2)==0 )) ## require even number of experimental pool\npowers <- rep(NA, length(possible.ns)) # Empty object to collect simulation estimates \nalpha <- 0.05 # Standard significance level \nsims <- 500 # Number of simulations to conduct for each N \n#### Outer loop to vary the number of subjects #### \nfor (j in 1:length(possible.ns)){ N <- possible.ns[j] # Pick the jth value for N \n  Y0 <- rnorm(n=N, mean=60, sd=20) # control potential outcome \n  tau <- 5 # Hypothesize treatment effect \n  Y1 <- Y0 + tau # treatment potential outcome                                   \n  significant.experiments <- rep(NA, sims) # Empty object to count significant experiments \n                                  \n  #### Inner loop to conduct experiments \"sims\" times over for each N #### \n        Y0 <- rnorm(n=N, mean=60, sd=20) # control potential outcome \n        tau <- 5 # Hypothesize treatment effect \n        Y1 <- Y0 + tau # treatment potential outcome \n  for (i in 1:sims){ \n        ## Z.sim <- rbinom(n=N, size=1, prob=.5) # Do a random assignment  by coin flip\n        Z.sim <- sample(rep(c(0,1),N/2)) ## Do a random assignment ensuring equal sized groups\n        Y.sim <- Y1*Z.sim + Y0*(1-Z.sim) # Reveal outcomes according to assignment \n        fit.sim <- lm(Y.sim ~ Z.sim) # Do analysis (Simple regression) \n        p.value <- summary(fit.sim)$coefficients[2,4] # Extract p-values \n        significant.experiments[i] <- (p.value <= alpha) # Determine significance according to p <= 0.05\n        }\n  powers[j] <- mean(significant.experiments) # store average success rate (power) for each N \n  } \npowers \n\n [1] 0.244 0.254 0.450 0.538 0.562 0.518 0.634 0.710 0.722 0.770 0.774 0.824\n[13] 0.876 0.864 0.918 0.870 0.922 0.954 0.948 0.946 0.942 0.966 0.970 0.980\n[25] 0.990 0.984 0.990 0.988 0.996 0.996 0.998 0.994 0.996 0.998 1.000 0.998\n[37] 1.000 1.000 0.998 0.996 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000\n\n\nThe code for this simulation and others is available here. Simulation is a far more flexible, and far more intuitive way to think about power analysis. Even the smallest tweaks to an experimental design are difficult to capture in a formula (adding a second treatment group, for example), but are relatively straightforward to include in a simulation.\nIn addition to counting up how often your experiments come up statistically significant, you can directly observe the distribution of p-values you’re likely to get. The graph below shows that under these assumptions, you can get expect to get quite a few p-values in the 0.01 range, but that 80% will be below 0.05.\n\n\n\n7 How to Change your Design to Improve Your Power\nWhen it comes to statistical power, the only thing that that’s under your control is the design of the experiment. As we’ve seen above, an obvious design choice is the number of subjects to include in the experiment. The more subjects, the higher the power.\nHowever, the number of subjects is not the only design choice that has consequences for power. There are two broad classes of design choices that are especially important in this regard.\n\nChoice of estimator. Are you using difference-in-means? Will you be doing some transformation, such as a logit or a probit? Will you be controlling for covariates? Will you be using some kind of robust standard error estimator? All of these choices will make a difference for the statistical significance of your results, and therefore for the power of your experiment. One easy way to think about this is to imagine what command you’ll be running in R or Stata after the experiment has come back; that’s your estimator!\nRandomization Protocol. What kind of randomization will you be employing? Simple randomization gives all subjects an equal probability of being in the treatment group, and then performs a (possibly weighted) coin flip for each. Complete randomization is similar, but it ensures that exactly a certain number will be assigned to treatment. Block randomization is even more powerful — it ensures that a certain number within a subgroup will be assigned to treatment. A restricted random assignment rejects some random assignments based on some set of criteria — lack of balance perhaps. These various types of random assignment can dramatically increase the power of an experiment at no extra cost. Read up on randomization protocols here.\n\nThere are too many choices to cover in this short article, but check out the Simulation for Power Analysis code page for some ways to get started. But to give a flavor of the simulation approach, consider how you would conduct a power analysis if you wanted to include covariates in your analysis.\nIf the covariates you include as control variables are strongly related to the outcome, then you’ve dramatically increased the power of your experiment.Unfortunately, the extra power that comes with including control variables is very hard to capture in a compact formula. Almost none of the power formulas found in textbooks or floating around on the internet can provide guidance on what the inclusion of covariates will do for your power.\nThe answer is simulation.\n\nSuppose we’re studying the effect of an educational intervention on income\nSuppose we have good data on the relationship between two covariates and income: age and gender. In this economy, men earn more than women, and older people earn more than younger people.\nRun a regression of income on age and gender and record the coefficients, using pre-existing survey data (better yet: use baseline data from future participants in your experiment!) *Generate fake covariate data — N total subjects, but broken up by age and gender in a way that reflects your experimental subject pool.\nGenerate fake control data — where the outcome is a function of age and gender according to your regression estimates\nHypothesize a treatment effect to generate fake treatment data\nRun the experiment 10,000 times, and record how often, using a regression with controls, your experiment turns up significant.\n\nHere’s a graph that compares the power of an experiment that does control for background attributes to one that does not. The R-square of the regression relating income to age and gender is pretty high — around .66 — meaning that the covariates that we have gathered (generated) are highly predictive. For a rough comparison, sigma, the level of background noise that the unadjusted model is dealing with, is around 33. This graph shows that at any N, the covariate-adjusted model has more power — so much so that the unadjusted model would need 1500 subjects to achieve what the covariate-adjusted model can do with 500.\n\nThis approach doesn’t rely on a formula to come up with the probability of getting a statistically significant result: it relies on brute force! And because simulation lets you specify every step of the experimental design, you do a far more nuanced power analysis than simply considering the number of subjects.\n\n\n8 Power Analysis for Multiple Treatments\nMany experiments employ multiple treatments which are compared both to each other and to a control group. This added complication changes what we mean when we say the “power” of an experiment. In the single treatment group case, power is just the probability of getting a statistically significant result. In the multiple treatment case, it can mean a variety of things: A) the probability of at least one of the treatments turning up significant, B) the probability of all the treatments turning up significant (versus control) or C) the probability that the treatments will be ranked in the hypothesized order, and that those ranks will be statistically significant.\nThis question of multiple treatment arms is related to the problem of multiple comparisons. (See our guide on this topic for more details.) Standard significance testing is based on the premise that you’re conducting a single test for statistical significance, and the p-values derived from these tests reflect the probability under the null of seeing such a larger (or larger) treatment effect. If, however, you are conducting multiple tests, this probability is no longer correct. Within a suite of tests, the probability that at least one of the tests will turn up significant even when the true effect is zero is higher, essentially because you have more attempts. A commonly cited (if not commonly used) solution is to use the Bonferroni correction: specify the number of comparisons you will be making in advance, then divide your significance level (alpha) by that number.\nIf you are going to be using a Bonferroni correction, then standard power calculators will be more complicated to use: you’ll have to specify your Bonferroni-corrected alpha levels and calculate the power of each separate comparison. To calculate the probability that all the tests are significant, multiply all the separate powers together. To calculate the probability that at least one of the tests is significant, calculate the probability that none are, then subtract from one.\nOr you can use simulation. An example of a power calculation done in R is available on the simulations page.\n\n\n9 How to Think About Power for Clustered Designs\nWhen an experiment has to assign whole groups of people to treatment rather than individually, we say that the experiment is clustered. This is common in educational experiments, where whole classrooms of children are assigned to treatment or control, or in development economics, where whole villages of individuals are assigned to treatment or control. (See our guide on cluster randomization for more details.)\nAs a general rule, clustering decreases your power. If you can avoid clustering your treatments, that is preferable for power. Unless you face concerns related to spillover, logistics, or ethics, take the variation down to the lowest level that you can.\nThe best case scenario for a cluster-level design is when which cluster a subject is in provides very little information about their outcomes. Suppose subjects were randomly assigned to clusters — the cluster wouldn’t help to predict outcomes at all. If the cluster is not predictive of the outcome, then we haven’t lost too much power to clustering.\nWhere clustering really causes trouble is when there is a strong relationship between the cluster and the outcome. To take the villages example, suppose that some villages are, as a whole, much richer than others. Then the clusters might be quite predictive of educational attainment. Clustering can reduce your effective sample size from the total number of individuals to the total number of clusters.\nThere are formulas that can help you understand the consequences of clustering — see Gelman/Hill page 447-449 for an extended discussion. While these formulas can be useful, they can also be quite cumbersome to work with. The core insight however is a simple one: you generally get more power from increasing the number of clusters than you do from increasing the number of subjects within clusters. Better to have 100 clusters with 10 subjects in each than 10 clusters with 100 subjects in each.\nAgain, a more flexible approach to power analysis when dealing with clusters is simulation. See the (Declare Design library for block and cluster randomized experiments)[https://declaredesign.org/r/designlibrary/reference/block_cluster_two_arm_designer.html] for some starter code. The (DeclareDesign)[https://declaredesign.org] software aims to make simulations for power analysis (among many other tasks) easier. See also Gelman/Hill page 450-453 for another simulation approach.\n\n\n10 Good Power Analysis Makes Preregistration Easy\nWhen you deal with power you focus on what you cannot control (noise) and what you can control (design). If you use the simulation approach to power analysis then you will be forced to imagine how your data will look and how you will handle it when it comes in. You will get a chance to specify all of your hunches and best guesses in advance, so that you can launch your experiments with clear expectations of what they can and cannot show. That’s some work but the good news is that if you really do it you are most of the way to putting together a comprehensive and registerable pre-analysis plan.\n\n\n\n\n\nFootnotes\n\n\nOriginating author: Alex Coppock, 20 Nov 2013. The guide is a live document and subject to updating by EGAP members at any time. Coppock is not responsible for subsequent edits to this guide↩︎\nReproduced from Gerber and Green 2012, page 93↩︎\nFor an additional online power visualization tool, see Kristoffer Magnusson’s R Psychologist blog.↩︎"
  },
  {
    "objectID": "guides/getting-started/survey-experiments_en.html",
    "href": "guides/getting-started/survey-experiments_en.html",
    "title": "10 Things to Know About Survey Experiments",
    "section": "",
    "text": "Abstract\nThis guide discusses techniques for using randomization to create experiments within the text of a survey (i.e. survey experiments). These survey experiments are distinct from studies that use surveys to gather information related to an experiment that occurs outside of the survey. The guide distinguishes between survey experiments that are used mainly for measuring sensitive attitudes, like list experiments, and those that are mainly used to learn about causal effects, like conjoint experiments. Survey experiments for measurement attempt to ensure honest responses to sensitive questions by providing anonymity to respondents. Survey experiments for causal identification randomize images and text to learn how the image or text influences respondents. Both types of survey experiments face challenges, such as respondents not perceiving anonymity or not interpreting images and text as the researcher intended. New experimental techniques seek to address these challenges.\n\n\n1 What is a Survey Experiment\nA survey experiment is an experiment conducted within a survey. In an experiment, a researcher randomly assigns participants to at least two experimental conditions. The researcher then treats each condition differently. Due to random assignment, the researcher can assume that the only difference between conditions is the difference in treatment. For example, a medical experiment may learn about the effect of a pill by creating two experimental conditions and giving the pill to participants in only one condition. In a survey experiment, the randomization and treatment occur within a survey questionnaire.\nThere are two types of survey experiments. One type is used to measure sensitive attitudes or behaviors and the other is used to learn about causal relationships. By sensitive attitudes and behaviors, we mean any attitude or behavior that the respondent does not want to be publicly associated with. Many respondents, for example, do not want to be associated with racism or illegal behaviors.\nSurvey experiments for measurement attempt to provide respondents with anonymity so that they can express potentially sensitive attitudes without being identified as holding the sensitive attitude. These measurement survey experiments are alternatives to asking direct questions when direct questions are likely subject to response biases (i.e. when the respondents are likely to lie). These indirect measures are especially useful in contexts where direct questions may be dangerous for survey respondents and enumerators (Bullock, Imai, and Shapiro 2011).\nSurvey experiments to measure causal relationships are just like any other experiment, except the experimental intervention and outcome measurement occur within the context of a survey. Researchers randomly assign respondents to one or more experimental conditions or a control group, administer treatments, and measure outcomes. As their experimental manipulation, survey experiments tend to present a randomized stimulus (such as text, short videos, audio clips, or other stimuli that can easily fit into a survey format) in the context of a survey question or use variation in question wording and question ordering. They often administer these randomized manipulations by using survey questionnaire software or by providing different physical survey forms to enumerators.\n\n\n2 Survey Experiments to Measure Sensitive Topics: List Experiment\nList experiments (also known as the item-count technique) measure a sensitive attitude or behavior when the researcher expects the respondent to lie if asked a direct question about the attitude or behavior. For example, respondents may be reluctant to admit racial animus (Kuklinski et al. 1997) or illegal behavior (Garcia-Sanchez and Quierolo, n.d.), even after being assured of the survey’s anonymity.\nIn a list experiment the researcher randomly assigns respondents to a control or treatment condition. The control condition presents respondents with a list of items; the treatment condition presents respondents with the same list plus a treatment item measuring the attitude or behavior of interest. Respondents are then asked how many of those items apply to them. The average difference between the treatment and control condition represents the percentage of respondents for whom the treatment item applies. A list experiment does not tell the researcher about the attitude of any individual respondent, but it tells her about the prevalence of the sensitive attitude in her survey population. Answers to this question are anonymous because the respondent’s attitude towards each item cannot be determined unless the respondent answers that all or none of the items apply to them. Respondents for whom the treatment item applies can increase their response by one without revealing their attitude.\nFor example, Kuklinski et al. (1997) studied racial animus with a survey list experiment. They told respondents:\n\n\nNow I am going to read you three things that sometimes make people angry or upset. After I read all three, just tell me HOW MANY of them upset you. I don’t want to know which ones, just HOW MANY.  (1) the federal government increasing the tax on gasoline  (2) professional athletes getting million-dollar contracts (3) large corporations polluting the environment (4) a black family moving in next door  The 4th item was withheld from the control condition.\n\nFor that paper’s population of interest, the mean number of items chosen in the treatment group was 2.37, compared to 1.95 in the control. The difference of 0.42 between treatment and control indicates that 42% of respondents would be upset by a black family moving in next door.\n\nPitfalls of list experiments\nList experiments are vulnerable to satisficing. Satisficing occurs when respondents put in minimal effort to understand and answer a survey question (Krosnick 1991; Simon and March 2006). In a list experiment, satisficing manifests when respondents do not count the number of items that apply to them, instead answering with a number of items that seems reasonable (Kramon and Weghorst 2012; Schwarz 1999).\nRespondents may perceive a lack of anonymity. Despite the anonymity provided by a list experiment, respondents may still worry that their response reflects their attitudes about the sensitive item. When respondents worry about a lack of anonymity, they may increase or decrease their response to portray themselves in the best light possible, rather than answer honestly (Leary and Kowalski 1990). For example, the addition of a treatment item about race can decrease the number of items that respondents report because being associated with “three of the four [list items] may be interpreted as a 75% chance that they are racist” (Zigerell 2011, 544).\nThe lack of anonymity is most obvious when all or none of the list items apply to the respondent. Researchers can reduce this possibility by using uncorrelated or negatively correlated control items that are unlikely to apply to one respondent. In the Kuklinski et al. (1997) example above, the type of person who is upset by pollution is unlikely to also be upset by a gasoline tax. Negatively correlated items also reduce likelihood that respondent will satisfice because negatively correlated items are unlikely to be interpreted as a scale measuring one concept. The control items should also fit with the treatment item in some way so that the treatment item does not jump out to respondents as the real item of interest to researchers. \n\n\nVariants/Modifications\nDouble list experiments help overcome some pitfalls of single list experiments (Glynn 2013; Droitcour et al. 2004). In a double list experiment, the treatment item is randomly selected to appear on either the first or the second control list, so that some respondents see it on the first list and some respondents see it on the second. If researchers observe the same treatment effect on both lists, there is less risk that the effect depends on a particular control list or on how respondents interpret the list. The double list experiment is also more statistically efficient than a single list experiment (Glynn 2013).\nPlacebo-controlled list experiments ensure that the difference in responses to the treatment and control lists is due to the treatment item and not due to the treatment list having more items than the control list. A placebo-controlled list experiment uses an additional item as a placebo on the control list; unlike the additional item on the treatment list, the additional item on the control list is something innocuous that would not apply to any respondent. The placebo item ensures that the difference between the two lists is due to the treatment item, not the presence of an additional item (Riambau and Ostwald 2019).\nVisual aids also help reduce satisficing and ensure that respondents follow the instruction to count list items instead of satisfice. If enumerators can carry a laminated copy of the list and a dry erase marker, respondents can check off items on the list to get an exact count and erase it before handing it back to the enumerator (Kramon and Weghorst 2012, 2019).\n\n\n\n\n3 Survey Experiments to Measure Sensitive Topics: Randomized Response\nThe randomized response technique is also used to measure a sensitive attitude or behavior when the researcher expects the respondent to lie if asked a direct question (Warner 1965; Boruch 1971; D. Gingerich 2015; D. W. Gingerich 2010).\nIn the most common version of the randomized response technique, respondents are directly asked a yes or no question about a sensitive topic. The respondent is also given some randomization device, like a coin or die. The respondent is told to answer the direct question when the randomization device takes on a certain value (tails) or to say “yes” when the randomization device takes a different value (heads). Researchers assume that respondents will believe their anonymity is protected because the researcher cannot know whether a “yes” resulted from agreement with the sensitive item or the randomization device.\nFor example, Blair, Imai, and Zhou (2015) studied support for militants in Nigeria with the randomized response technique. They gave respondents a die and had the respondent practice throwing it. They then told respondents:\n\n\nFor this question, I want you to answer yes or no. But I want you to consider the number of your dice throw. If 1 shows on the dice, tell me no. If 6 shows, tell me yes. But if another number, like 2 or 3 or 4 or 5 shows, tell me your opinion about the question that I will ask you after you throw the dice.  [ENUMERATOR TURN AWAY FROM THE RESPONDENT] Now throw the dice so that I cannot see what comes out. Please do not forget the number that comes out. [ENUMERATOR WAIT TO TURN AROUND UNTIL RESPONDENT SAYS YES TO]: Have you thrown the dice? Have you picked it up? Now, during the height of the conflict in 2007 and 2008, did you know any militants, like a family member, a friend, or someone you talked to on a regular basis? Please, before you answer, take note of the number you rolled on the dice.  In expectation, 1/6th of respondents answer “yes” due to the die throw. The researcher can thus determine what percentage of respondents engaged in the sensitive behavior. \n\n\nPitfalls of the randomized response technique\nSome versions are complicated. Even the common version described above, valued in part for its simplicity, requires respondents to use some randomization device and remember the outcome of the randomization device. Other versions use more complicated techniques to ensure anonymity; these versions may be difficult both for the respondent and the enumerator (Blair, Imai, and Zhou 2015; D. W. Gingerich 2010). It is possible that some respondents do not understand the instructions and some enumerators do not implement the randomized response technique properly.\nRespondents may perceive a lack of anonymity. As was true for list experiments, respondents may not feel that their answers to randomized response questions are truly anonymous. If a respondent answers “yes”, the answer could have been dictated by the randomization device, but it could also signal agreement with the sensitive item (Edgell, Himmelfarb, and Duchan 1982; Yu, Tian, and Tang 2008). Thus, answering “yes” is not unequivocally protected by the design. Edgell, Himmelfarb, and Duchan (1982) surreptitiously set the randomization device to always dictate “yes” or “no” for specific questions and observed as high as 26% of respondents say “no” even when the randomization device dictated they say “yes”.\n\n\nVariants/modifications\nThe repeated randomized response technique helps researchers identify respondents who lie on randomized response questions (Azfar and Murrell 2009). The repeated technique asks a series of randomized response questions with sensitive and non-sensitive items. The probability of the randomization device dictating that the respondent should answer “no” for all of the sensitive items is very low. The technique thus allows researchers to identify and remove from analysis the respondents who are likely saying “no” even when their coin flip dictates they say “yes”. Researchers can also determine if certain questions induce widespread lying if the “yes” rate for that question is lower than the randomization device would dictate. The repeated randomized response technique, however, may be impractical to include on a large survey.\nThe Crosswise model modifies the randomized response technique so that respondents have no incentive to answer “yes” or “no” (Yu, Tian, and Tang 2008; Jann, Jerke, and Krumpal 2011). In the Crosswise model, respondents are presented with two statements, one sensitive statement and one non-sensitive statement for which the population mean is known. The respondent is asked to say if (a) neither or both statements are true, or (b) one statement is true. Unlike a typical randomized response question, where individuals who agree with the sensitive statement only occupy the “yes” group, people who agree with the sensitive statement could occupy either group using the Crosswise model. Since being in category (a) and (b) are equally uninformative about the respondent’s agreement with the sensitive statement, the Crosswise model removes a respondent’s incentive to lie. The Crosswise model can be used any time researchers know the population mean of a non-sensitive statement, such as “My mother was born in April.”\n\n\n\n\n4 Survey Experiments to Measure Sensitive Topics: Priming Experiment\nList experiments and randomized response techniques do not uncover implicit attitudes, but many sensitive topics appear so sensitive that an individual’s conscious, explicit attitudes may differ from their implicit attitudes (Greenwald and Banaji 1995). Even many nonsensitive attitudes seem to be beyond an individual’s conscious awareness (Nisbett and Wilson 1977). Whereas techniques to measure explicit attitudes seek to provide respondents with anonymity, techniques to measure implicit attitudes seek to keep the respondent consciously unaware of the implicit attitude being measured. To do so, researchers often use priming experiments.\nIn a priming experiment, researchers expose respondents to a stimulus representing topic X in order to influence their response to a survey question about topic Y, without the respondent realizing that the researchers are interested in topic X. A control group is not exposed to the stimuli representing topic X, so the difference between the treatment group and control group is due to exposure to the treatment stimuli. Priming experiments work by directing respondents’ consciousness away from topic X and towards topic Y so that respondents do not consciously censor their feelings about topic X (Macrae et al. 1994; Schwarz and Clore 1983).\nPriming experiments are a broad class and include any experiment that makes a sensitive topic salient in the mind of the respondent. One common method of priming is the use of images. For example, Brader, Valentino, and Suhay (2008) use images in a priming experiment to estimate the effect that race plays in opposition to immigration. The researchers show subjects a positive or negative news article about immigration paired with a picture of a European immigrant or an Hispanic immigrant. Subjects expressed negative attitudes about immigration when the negative news article is paired with the Hispanic immigrant picture but not in other conditions. The picture primes people to think about Hispanic immigrants, and thinking about Hispanic immigrants reduces support for immigration compared to thinking about European immigrants even though subjects do not consciously admit to bias.\nSurvey experiments for measurement and for causal identification overlap in priming experiments. Researchers can use them to measure implicit attitudes or to assess how the activation of implicit attitudes affects another outcome, like attitudes towards immigration.\n\nPitfalls of priming experiments\nPriming experiments are difficult. Priming attitudes experimentally is difficult because the researcher cannot be certain that the prime affects subjects as the researcher intended. A prime intended to induce fear, for example, may induce fear in some subjects and excitement in others. Priming sensitive attitudes is especially difficult because the researcher must prime a sensitive attitude without the respondent becoming aware that the researcher is interested in the sensitive attitude. If respondents realize what the priming experiment is about, the experiment fails because respondents will consciously censor their attitude, rather than passively allow their implicit attitude to influence their response (Macrae et al. 1994; Schwarz and Clore 1983). To prevent subjects from ascertaining the goal of the study, researchers try to hide the prime amid other, ostensibly more important, information.\nPriming experiments can suffer from confounding and lack of “information equivalence” between treatment groups (Dafoe, Zhang, and Caughey 2018). The researchers may prime topic \\(X\\) with the intent of learning about respondents’ implicit attitudes towards topic \\(X\\), but if topic \\(X\\) is strongly linked with topic \\(Y\\) then the researcher will estimate the effect of \\(X\\) and \\(Y\\), not just \\(X\\). For example, priming a partisan group may also prime ideological and policy views associated with the partisan group (Nicholson 2011). A basic priming experiment cannot differentiate the effect of priming the partisan group from the effect of priming the ideological and policy views associated with the partisan group.\nRespondents may be pretreated before the experiment. Individuals are exposed to stimuli that prime attitudes during their daily lives. News broadcasts prime people to think about issues covered on the news, and anti-racism protests prime people to think about racial issues. Even words seen immediately before answering survey questions influences responses to those survey questions (Norenzayan and Schwarz 1999). If subjects, before participating in the experiment, encounter the stimuli that the researcher wants to prime, there may be no difference between treatment and control groups because all subjects were “pretreated” with the prime, even subjects in the control group (Gaines, Kuklinski, and Quirk 2007; Druckman and Leeper 2012). If the issue being primed is already salient in the mind of the respondent, priming experiments fail.\n\n\nVariants/modifications\nTo ensure information equivalence and to reduce confounding the prime with an associated factor, researchers utilize priming experiments as part of factorial experiments. Factorial experiments vary multiple factors that may be linked in the minds of respondents. Nicholson (2011), for example, asked respondents about support for a policy. He varied both partisan endorsement and policy details to learn how partisan bias influenced respondents’ attitudes beyond any assumptions about the party’s policy positions. Factorial experiments are mainly used to determine causal relationships and are discussed in section 7.\n\n\n\n\n5 Survey Experiments to Measure Sensitive Topics: Endorsement Experiments\nEndorsement experiments measure sensitive attitudes towards an attitude object, like a political actor or a policy. They were first developed to study partisan bias (Cohen 2003; Kam 2005) but have since been used to measure support for militant groups (Bullock, Imai, and Shapiro 2011; Lyall, Blair, and Imai 2013). They have also been inverted to measure support for a policy rather than a political actor (Rosenfeld, Imai, and Shapiro 2016).\nIn a typical endorsement experiment, respondents are asked how much they support a policy. In the treatment condition, the policy is “endorsed” by a group that respondents would not consciously admit to influencing their opinion. In the control condition, the policy is not endorsed by any group. The average difference in support between the endorsed and unendorsed policy represents the change in support for the policy because of the endorsement.\nEndorsement experiments can measure implicit attitudes or explicit attitudes. They measure implicit attitudes like a priming experiment if respondents do not realize the group’s endorsement is what the researcher is interested in. They measure explicit attitudes like a list experiment if respondents realize the group’s endorsement is what the researcher is interested in. Whereas list experiments hide the respondent’s opinion by pairing the sensitive item with non-sensitive control items, endorsement experiments hide the respondent’s opinion by pairing the sensitive item with a policy that could feasibly be responsible for the respondent’s attitude.\n\n\nFor example, Nicholson (2012) used an endorsement experiment to study partisan bias in the United States during the 2008 Presidential campaign. The researchers asked respondents about policies, varying whether the policy was unendorsed or endorsed by the Presidential candidates of the two main political parties, Barack Obama (Democrat) and John McCain (Republican). Respondents were told:\n\n\nAs you know, there has been a lot of talk about immigration reform policy in the news. One proposal [backed by Barack Obama/backed by John McCain] provided legal status and a path to legal citizenship for the approximately 12 million illegal immigrants currently residing in the United States. What is your view of this immigration reform policy?  The difference between the control condition and the Obama (McCain) condition for Democrats (Republicans) shows in-party bias. The difference between the control condition and the Obama (McCain) condition for Republicans (Democrats) shows out-party bias.\n\n\nPitfalls of endorsement experiments\nAs with priming experiments, endorsement experiments suffer from confounding and a lack of information equivalence. Researchers cannot be certain if differential support for the policy is due to the endorsement or due to different substantive assumptions about the policy that respondents make as a result of the endorsement.\nChoosing a policy is difficult. The value of the endorsement experiments depends largely on the characteristics of the policy being (or not being) endorsed. The chosen policy must not possess too much or too little support in the survey population, otherwise attitudes towards the policy will wipe out the effect of the group’s endorsement. Too much or too little support could also reduce perceived anonymity if respondents think that no one would support/oppose the policy unless they liked/disliked the endorsing group.\nEndorsement experiments can have low power to detect effects, even relative to other survey experiments (Bullock, Imai, and Shapiro 2011). Some subset of subjects will be unaffected by the endorsement because they feel strongly about the policy, and that subset adds substantial noise to endorsement experiments.\n\n\nVariants/modifications\nTo overcome low power, Bullock, Imai, and Shapiro (2011) recommend using multiple policy questions that are on the same one-dimensional policy space. Multiple questions on one policy space allows the researcher to predict each respondent’s level of support for the policy if it were not endorsed by the group of interest. The researcher can thus model the noise caused by strong feelings towards the policy.\nAs with priming experiments, to ensure information equivalence and to reduce confounding factors, researchers use endorsement experiments as part of factorial experiments that vary the multiple factors that may be linked in the mind of respondents. Factorial experiments are mainly used to determine causal relationships and are discussed in section 7.\n\n\n\n\n6 Limitations of Survey Experiments as a Measurement Technique\nSurvey experiments induce less bias than direct questions when measuring sensitive attitudes (Blair, Imai, and Zhou 2015; Rosenfeld, Imai, and Shapiro 2016; Lensvelt-Mulders et al. 2005). They are not a panacea, however, and researchers must still ask themselves several questions when using survey experiments to measure sensitive outcomes.\nThe first question is whether the researcher is interested in an explicit or implicit attitude. An explicit attitude is one the respondent is consciously aware of and can report; an implicit attitude is an automatic positive or negative evaluation of an attitude object that the respondent may not be aware of (see Nosek 2007 for a more thorough discussion). A list experiment, for example, may help uncover explicit racial animus, but it will not reveal implicit racial bias.\nThe next question is what conditions are necessary for a survey respondent to reveal their explicit attitudes. Survey experimental methods for sensitive explicit attitudes focus on ensuring anonymity. But is ensuring anonymity a sufficient condition to obtain honest answers to sensitive questions? In addition to anonymity, a further assumption must be made: respondents want to express their socially undesirable opinion in a way that evades social sanctions. If that assumption is not true, then anonymity is worth little (Diaz, Grady, and Kuklinski 2020).\nResearchers also need to think about the numerous pitfalls of survey questions that measurement survey experiments do not solve. Survey experiments do not help researchers avoid question ordering effects or contamination from earlier questions in the survey. Nor do they do expose how respondents interpret the survey question or ensure information equivalence. All survey questions assume that the respondent interprets the question in the way intended by researchers; techniques to ensure anonymity may make that interpretation less likely by obfuscating the question’s purpose (Diaz, Grady, and Kuklinski 2020).\nLastly, researchers must also ask about measurement validity: how does one verify that a measure accurately represents a concept of interest? For some outcomes, such as voter turnout, researchers can compare their measure with population estimates (Rosenfeld, Imai, and Shapiro 2016). But for other outcomes, such as racism or the effect that political parties have on citizens’ policy preferences, there exists no population estimate with which to validate measures.\n\n\n7 Survey Experiments to Determine a Causal Relationship: Vignette and Factorial Designs\nNot all survey experiments share the goal of accurately measuring one concept of interest. Some survey experiments, like lab experiments, are interested in how an experimental manipulation impacts outcomes of interest. These survey experiments for causal inference randomize a treatment and then measure outcomes. When measuring outcomes, they may use techniques like list experiments.\nOne of the most common designs of survey experiments for causal inference are vignette and factorial designs (Auspurg and Hinz 2014; Sniderman et al. 1991). In a vignette/factorial experiment, the researcher provides the respondent with a hypothetical scenario to read, varying key components of the scenario. In a typical vignette, the researcher varies only one component of the scenario. In a typical factorial experiment, the researcher varies several components of the scenario.\nBoth vignette and factorial designs benefit from embedding the survey question in a concrete scenario so that they require little abstraction from the survey respondent. Their concrete nature can make them more interesting and easier to answer than typical survey questions, decreasing survey fatigue. They can also function as priming experiments if the concept of interest is embedded in other concepts.\nAs an example, Winters and Weitz-Shapiro (2013) uses factorial vignettes to learn if voters sanction corrupt politicians in Brazil. They posit that corruption could interact with competence, so the authors varied a Brazilian mayor’s corruption, competence, and political affiliation in the vignette. They tell respondents:\n\n\nImagine a person named Gabriel (Gabriela for female respondents), who is a person like you, living in a neighborhood like yours, but in a different city in Brazil. The mayor of Gabriel’s city is running for reelection in October. He is a member of the [Partido dos Trabalhadores/Partido da Social Democracia Brasileira]. In Gabriel’s city, it is well known that the mayor [never takes bribes/frequently takes bribes] when giving out government contracts. The mayor has completed [few/many/omit the entire sentence] public works projects during his term in office. In this city, the election for mayor is expected to be very close. In your opinion, what is the likelihood that Gabriel(a) will vote for this mayor in the next election: very likely, somewhat likely, unlikely, not at all likely?  This design allowed the authors to determine if and when corruption would be punished by voters. If respondents overlooked corruption when the mayor completed many public works, the interpretation is that corruption is acceptable if it gets the job done. If respondents overlooked corruption when the mayor was a copartisan, the interpretation is that voters ignore the corruption of their own. By varying several related aspects of the scenario, Winters and Weitz-Shapiro (2013) could isolate the conditions under which credible information about corruption would be punished by voters.\n\n\nPitfalls of vignette/factorial experiments\nThe main pitfall of vignettes – a lack of information equivalence – is dealt with by factorial experiments. Researchers can randomize several aspects of the scenario, standardizing factors that could influence how the main treatment is perceived by respondents. Some combinations of different factors may not be realistic, however. Researchers must be sure that the various possible combinations of their factorial experiments seem credible to respondents.\nStatistical power is weak when factorial experiments vary many confounding traits. The more traits being varied, the more experimental conditions, the fewer respondents in each experimental condition, and the greater likelihood of imbalance between treatment conditions.\nIn enumerated surveys, there is also the possibility that certain enumerators are more often assigned certain factorial conditions and that enumerator effects could be mistaken for treatment effects (Steiner, Atzmüller, and Su 2016). Imagine the Winters and Weitz-Shapiro (2013) study, which had six functional treatment groups and ~2,000 respondents. If the survey was enumerated by twenty survey enumerators, then, in expectation, each enumerator has only ~17 subjects in each treatment category. In reality, it is likely that certain enumerators will more often enumerate some conditions than others and differences due to enumerators could appear as treatment effects.\n\n\nVariants/modifications\nResearchers can block treatment by enumerator so that enumerator effects cannot confound treatment effects (Steiner, Atzmüller, and Su 2016). Blocking and other techniques the authors propose should also increase statistical power by accounting for systematic error.\nConjoint experiments maintain many benefits of factorial experiments, but increase power by presenting multiple choice tasks instead of one. We discuss conjoint experiments in the next section.\n\n\n\n8 Survey Experiments to Determine a Causal Relationship: Conjoint Experiments\nConjoint experiments (Hainmueller, Hopkins, and Yamamoto 2014; Green and Rao 1971) have gained popularity in response to the limits of vignette and factorial designs. Vignette and factorial designs suffer from a lack of information equivalence if they do not provide sufficient details about potentially confounding aspects of the scenario or a lack of statistical power if they do vary several traits. A typical conjoint experiment attempts to solve these problems by repeatedly asking respondents to choose between two distinct options and randomly varying the characteristics of those two options. Respondents may also be asked to rate each option on a scale. In both cases, respondents express their preferences towards a large number of pairings with randomized attributes, drastically increasing statistical power to detect effects of any one attribute relative to a one-shot factorial design.\nHainmueller, Hopkins, and Yamamoto (2014) demonstrate the use of conjoint experiments in a study about support for immigration. The authors showed respondents two immigrant profiles and asked (a) which immigrant the respondent would prefer be admitted to the Unites States and (b) how the respondent rated each immigrant on a scale from 1-7. The authors randomly varied nine attributes of the immigrants (gender, education, employment plans, job experience, profession, language skills, country of origin, reasons for applying, and prior trips to the United States), yielding thousands of unique immigrant profiles. This process was repeated five times so that each respondents saw and rated five pairs of immigrants. Through this procedure, the authors can assess how these randomly varied components influence support for the immigrant.\nRespondents saw:\n\nThrough a conjoint experiment, researchers can learn about the average marginal effect of several aspects of a scenario, far more than would be feasible with a typical vignette or factorial design. Though researchers could include and vary an almost infinite number of characteristics, the best practice is to only vary traits that could confound the relationship between a primary explanatory variable and an outcome of interest, rather than varying any trait that might affect the outcome of interest (Diaz, Grady, and Kuklinski 2020).\n\nPitfalls of conjoint experiments\nThe costs and benefits of conjoint experiments are still being actively researched. Thus far, two classes of critiques are common.\nResults from conjoint experiments are difficult to interpret. Results of conjoint experiments’ target estimand, the Average Marginal Component Effect (AMCE), can “indicate the opposite of the true preference of the majority” (Abramson, Koçak, and Magazinnik 2019, 1). Other researchers have noted that AMCE’s depend on the reference category and are not comparable across survey subgroups (Leeper, Hobolt, and Tilley 2020). Bansak et al. (2020) provides guidance on how to interpret conjoint results and argues that AMCE’s do represent quantities of interest to empirical scholars.\nConjoint experiments also create unrealistic combinations and those unrealistic combinations lead to effect estimates that are not representative of the real world (Incerti 2020). Similarly, the large amount of information provided by conjoint experiments could misrepresent how individuals generally process information they encounter in the world (Hainmueller, Hopkins, and Yamamoto 2014). The large amount of information and demand on respondents has also led to concerns about satisficing, though Bansak et al. (2018) and Bansak et al. (2019) suggest satisficing is not a major concern for conjoint experiments.\nOther potential pitfalls can occur if the researcher varies too many characteristics. More randomly varied characteristics means a large number of potential hypothesis tests. The necessity of applying multiple hypothesis corrections to the vast number of potential hypothesis tests could decrease statistical power to detect specific effects, especially if researchers are interested in interactions between traits being varied.\n\n\n\n9 Limitations of Survey Experiments for Causal Identification\nSurvey experiments to determine causal relationships have the same benefits and drawbacks as other experiments, as well as benefits and drawbacks that derive from the survey context. The biggest three drawbacks generally applicable to survey experiments are confounding, information equivalence, and pre-treatment contamination (Diaz, Grady, and Kuklinski 2020). Researchers should think about these factors when designing and interpreting results from survey experiments.\nConfounding: Any experimental intervention A that is meant to trigger mental construct M could also trigger mental construct C. If C is not varied in the experimental design, researchers cannot determine whether M, C, or a combination of M and C affect outcomes of interest.\nInformation Equivalence: Any experimental intervention A can be interpreted differently by different respondents, effectively giving each respondent a different treatment (Dafoe, Zhang, and Caughey 2018). When these interpretations vary systematically by treatment condition, those conditions are not information equivalent and researchers cannot know that their treatment caused the observed effect.\nPre-treatment contamination: Respondents may encounter the treatment outside of the experiment, causing similar outcomes in the control group and treatment group even if the treatment affects outcomes (Gaines, Kuklinski, and Quirk 2007).\n\n\n10 Considerations when using Survey Experiments\nSurvey experiments can be an effective tool for researchers to measure sensitive attitudes and learn about causal relationships. They are cost-effective, can be done quickly and iteratively, can be included on mass online surveys because they do not require in-person contact to implement. This means that a researcher can plan a sequence of online survey experiments, changing the intervention and measured outcomes from one experiment to the next to learn about the mechanisms behind the treatment effect very quickly (Sniderman 2018).\nFor survey experiments as a measurement technique, the researcher first has to assess if the attitude of interest is explicit (consciously known to the respondent) or implicit (not consciously known to the respondent). If the researcher believes the respondent knows her own attitude but does not want to be identified with it, the researcher should make it possible for the respondent to express that attitude without the researcher knowing that attitude. List experiments, randomized response techniques, and endorsement experiments can help accomplish this task. If the researcher believes the respondent does not know her own attitude, the researcher should make that attitude salient through priming and then ask a question that should be implicitly affected by the prime.\nThere may be cases where survey experiments are not the best tool for measuring sensitive attitudes. As an alternative to survey experiments to measure explicit attitudes, researchers can use techniques like the Bogus Pipeline (Jones and Sigall 1971) or phrase questions about a sensitive topic so that they are not considered socially undesirable (Kinder and Sears 1981). As an alternative to survey experiments to measure implicit attitudes, researchers can use measures like the Implicit Association Test (IAT) (Greenwald, McGhee, and Schwartz 1998) and physiological measures like skin conductance (Rankin and Campbell 1955; Figner, Murphy, et al. 2011). These measures are beyond conscious control of the respondent. Many of these alternative measures are not currently flexible enough to be included on a mass survey, but technology, like heart-rate monitoring watches and other phone sensors, may soon make biometric outcomes measurable in mass surveys.\n\nFor all types of survey experiments, researchers should worry about the same issues that hamper other experiments: confounding, information equivalence, and pre-treatment contamination. To deal with confounding and information equivalence, researchers can design the experiment to manipulate characteristics that might confound the treatment. To account for pre-treatment, researchers can think about the everyday context of research subjects and assess whether all or a subset of respondents may already be treated before beginning the experiment. If only a subset will be affected, the researcher can block the experiment on that subset.\nSurvey experiments for measurement and survey experiments for estimating causal relationships are not binary categories, and the two types of survey experiments can overlap. Priming experiments, for example, can measure implicit attitudes and assess the effect of the prime on other outcomes of interest. Vignette or conjoint experiments can effectively measure a sensitive attitude by priming the sensitive attitude and providing lots of other information to distract respondents from the prime.\nFor more discussion of survey experiments, see:\n\nMutz (2011) “Population-Based Survey Experiments.”\nSniderman (2018) “Some Advances in the Design of Survey Experiments” in the Annual Review of Political Science.\nLavrakas et al. (2019) “Experimental Methods in Survey Research: Techniques that Combine Random Sampling with Random Assignment.”\nDiaz, Grady, and Kuklinski (2020) “Survey Experiments and the Quest for Valid Interpretation” in the Sage Handbook of Research Methods in Political Science and International Relations.\n\n\n\n\n\n\n\n\n\nReferences\n\nAbramson, Scott F, Korhan Koçak, and Asya Magazinnik. 2019. “What Do We Learn about Voter Preferences from Conjoint Experiments?” Unpublished Manuscript. Https://Pdfs. Semanticscholar. Org/023a/24a7dfaddfce626d011596b187f26361ee86. Pdf.\n\n\nAuspurg, Katrin, and Thomas Hinz. 2014. Factorial Survey Experiments. Vol. 175. Sage Publications.\n\n\nAzfar, Omar, and Peter Murrell. 2009. “Identifying Reticent Respondents: Assessing the Quality of Survey Data on Corruption and Values.” Economic Development and Cultural Change 57 (2): 387–411.\n\n\nBansak, Kirk, Jens Hainmueller, Daniel J Hopkins, and Teppei Yamamoto. 2018. “The Number of Choice Tasks and Survey Satisficing in Conjoint Experiments.” Political Analysis 26 (1): 112–19.\n\n\n———. 2019. “Beyond the Breaking Point? Survey Satisficing in Conjoint Experiments.” Political Science Research and Methods, 1–19.\n\n\n———. 2020. “Using Conjoint Experiments to Analyze Elections: The Essential Role of the Average Marginal Component Effect (AMCE).” Available at SSRN.\n\n\nBlair, Graeme, Kosuke Imai, and Yang-Yang Zhou. 2015. “Design and Analysis of the Randomized Response Technique.” Journal of the American Statistical Association 110 (511): 1304–19.\n\n\nBoruch, Robert F. 1971. “Assuring Confidentiality of Responses in Social Research: A Note on Strategies.” The American Sociologist, 308–11.\n\n\nBrader, Ted, Nicholas A Valentino, and Elizabeth Suhay. 2008. “What Triggers Public Opposition to Immigration? Anxiety, Group Cues, and Immigration Threat.” American Journal of Political Science 52 (4): 959–78.\n\n\nBullock, Will, Kosuke Imai, and Jacob N Shapiro. 2011. “Statistical Analysis of Endorsement Experiments: Measuring Support for Militant Groups in Pakistan.” Political Analysis 19 (4): 363–84.\n\n\nCohen, Geoffrey L. 2003. “Party over Policy: The Dominating Impact of Group Influence on Political Beliefs.” Journal of Personality and Social Psychology 85 (5): 808.\n\n\nDafoe, Allan, Baobao Zhang, and Devin Caughey. 2018. “Information Equivalence in Survey Experiments.” Political Analysis 26 (4): 399–416.\n\n\nDiaz, Gustavo, Christopher Grady, and James H Kuklinski. 2020. “Survey Experiments and the Quest for Valid Interpretation.” In SAGE Handbook of Research Methods in Political Science and International Relations. SAGE Publishing.\n\n\nDroitcour, Judith, Rachel A Caspar, Michael L Hubbard, Teresa L Parsley, Wendy Visscher, and Trena M Ezzati. 2004. “The Item Count Technique as a Method of Indirect Questioning: A Review of Its Development and a Case Study Application.” Measurement Errors in Surveys, 185–210.\n\n\nDruckman, James N, and Thomas J Leeper. 2012. “Learning More from Political Communication Experiments: Pretreatment and Its Effects.” American Journal of Political Science 56 (4): 875–96.\n\n\nEdgell, Stephen E, Samuel Himmelfarb, and Karen L Duchan. 1982. “Validity of Forced Responses in a Randomized Response Model.” Sociological Methods & Research 11 (1): 89–100.\n\n\nFigner, Bernd, Ryan O Murphy, et al. 2011. “Using Skin Conductance in Judgment and Decision Making Research.” A Handbook of Process Tracing Methods for Decision Research, 163–84.\n\n\nGaines, Brian J, James H Kuklinski, and Paul J Quirk. 2007. “The Logic of the Survey Experiment Reexamined.” Political Analysis 15 (1): 1–20.\n\n\nGarcia-Sanchez, Miguel, and Rosario Quierolo. n.d. “A Tale of Two Countries: Measuring Drug Consumption in Opposite Contexts - Evidence from Survey Experiments in Colombia and Uruguay.”\n\n\nGingerich, Daniel. 2015. “Randomized Response: Foundations and New Developments.” In Newsletter of the Comparative Politics Organized Section of the American Political Science Association.\n\n\nGingerich, Daniel W. 2010. “Understanding Off-the-Books Politics: Conducting Inference on the Determinants of Sensitive Behavior with Randomized Response Surveys.” Political Analysis 18 (3): 349–80.\n\n\nGlynn, Adam N. 2013. “What Can We Learn with Statistical Truth Serum? Design and Analysis of the List Experiment.” Public Opinion Quarterly 77 (S1): 159–72.\n\n\nGreen, Paul E, and Vithala R Rao. 1971. “Conjoint Measurement-for Quantifying Judgmental Data.” Journal of Marketing Research 8 (3): 355–63.\n\n\nGreenwald, Anthony G, and Mahzarin R Banaji. 1995. “Implicit Social Cognition: Attitudes, Self-Esteem, and Stereotypes.” Psychological Review 102 (1): 4.\n\n\nGreenwald, Anthony G, Debbie E McGhee, and Jordan LK Schwartz. 1998. “Measuring Individual Differences in Implicit Cognition: The Implicit Association Test.” Journal of Personality and Social Psychology 74 (6): 1464.\n\n\nHainmueller, Jens, Daniel J Hopkins, and Teppei Yamamoto. 2014. “Causal Inference in Conjoint Analysis: Understanding Multidimensional Choices via Stated Preference Experiments.” Political Analysis 22 (1): 1–30.\n\n\nIncerti, Trevor. 2020. “Corruption Information and Vote Share: A Meta-Analysis and Lessons for Experimental Design.” American Political Science Review, Forthcoming.\n\n\nJann, Ben, Julia Jerke, and Ivar Krumpal. 2011. “Asking Sensitive Questions Using the Crosswise Model: An Experimental Survey Measuring Plagiarism.” Public Opinion Quarterly 76 (1): 32–49.\n\n\nJones, Edward E, and Harold Sigall. 1971. “The Bogus Pipeline: A New Paradigm for Measuring Affect and Attitude.” Psychological Bulletin 76 (5): 349.\n\n\nKam, Cindy D. 2005. “Who Toes the Party Line? Cues, Values, and Individual Differences.” Political Behavior 27 (2): 163–82.\n\n\nKinder, Donald R, and David O Sears. 1981. “Prejudice and Politics: Symbolic Racism Versus Racial Threats to the Good Life.” Journal of Personality and Social Psychology 40 (3): 414.\n\n\nKramon, Eric, and Keith Weghorst. 2012. “List Experiments in the Field.” In Newsletter of the APSA Experimental Section.\n\n\n———. 2019. “(Mis) Measuring Sensitive Attitudes with the List Experiment: Solutions to List Experiment Breakdown in Kenya.” Public Opinion Quarterly 83 (S1): 236–63.\n\n\nKrosnick, Jon A. 1991. “Response Strategies for Coping with the Cognitive Demands of Attitude Measures in Surveys.” Applied Cognitive Psychology 5 (3): 213–36.\n\n\nKuklinski, James H, Paul M Sniderman, Kathleen Knight, Thomas Piazza, Philip E Tetlock, Gordon R Lawrence, and Barbara Mellers. 1997. “Racial Prejudice and Attitudes Toward Affirmative Action.” American Journal of Political Science, 402–19.\n\n\nLavrakas, Paul J, Michael W Traugott, Courtney Kennedy, Allyson L Holbrook, Edith D de Leeuw, and Brady T West. 2019. Experimental Methods in Survey Research: Techniques That Combine Random Sampling with Random Assignment. John Wiley & Sons.\n\n\nLeary, Mark R, and Robin M Kowalski. 1990. “Impression Management: A Literature Review and Two-Component Model.” Psychological Bulletin 107 (1): 34.\n\n\nLeeper, Thomas J, Sara B Hobolt, and James Tilley. 2020. “Measuring Subgroup Preferences in Conjoint Experiments.” Political Analysis 28 (2): 207–21.\n\n\nLensvelt-Mulders, Gerty JLM, Joop J Hox, Peter GM Van der Heijden, and Cora JM Maas. 2005. “Meta-Analysis of Randomized Response Research: Thirty-Five Years of Validation.” Sociological Methods & Research 33 (3): 319–48.\n\n\nLyall, Jason, Graeme Blair, and Kosuke Imai. 2013. “Explaining Support for Combatants During Wartime: A Survey Experiment in Afghanistan.” American Political Science Review 107 (4): 679–705.\n\n\nMacrae, C Neil, Galen V Bodenhausen, Alan B Milne, and Jolanda Jetten. 1994. “Out of Mind but Back in Sight: Stereotypes on the Rebound.” Journal of Personality and Social Psychology 67 (5): 808.\n\n\nMutz, Diana C. 2011. Population-Based Survey Experiments. Princeton University Press.\n\n\nNicholson, Stephen P. 2011. “Dominating Cues and the Limits of Elite Influence.” The Journal of Politics 73 (4): 1165–77.\n\n\n———. 2012. “Polarizing Cues.” American Journal of Political Science 56 (1): 52–66.\n\n\nNisbett, Richard E, and Timothy D Wilson. 1977. “Telling More Than We Can Know: Verbal Reports on Mental Processes.” Psychological Review 84 (3): 231.\n\n\nNorenzayan, Ara, and Norbert Schwarz. 1999. “Telling What They Want to Know: Participants Tailor Causal Attributions to Researchers’ Interests.” European Journal of Social Psychology 29 (8): 1011–20.\n\n\nNosek, Brian A. 2007. “Implicit–Explicit Relations.” Current Directions in Psychological Science 16 (2): 65–69.\n\n\nRankin, Robert E, and Donald T Campbell. 1955. “Galvanic Skin Response to Negro and White Experimenters.” The Journal of Abnormal and Social Psychology 51 (1): 30.\n\n\nRiambau, Guillem, and Kai Ostwald. 2019. “Placebo Statements in List Experiments: Evidence from a Face-to-Face Survey in Singapore.” Political Science Research and Methods, 1–8.\n\n\nRosenfeld, Bryn, Kosuke Imai, and Jacob N Shapiro. 2016. “An Empirical Validation Study of Popular Survey Methodologies for Sensitive Questions.” American Journal of Political Science 60 (3): 783–802.\n\n\nSchwarz, Norbert. 1999. “Self-Reports: How the Questions Shape the Answers.” American Psychologist 54 (2): 93.\n\n\nSchwarz, Norbert, and Gerald L Clore. 1983. “Mood, Misattribution, and Judgments of Well-Being: Informative and Directive Functions of Affective States.” Journal of Personality and Social Psychology 45 (3): 513.\n\n\nSimon, Herbert, and James March. 2006. “Administrative Behavior and Organizations.” Organizational Behavior 2: Essential Theories of Process and Structure 2 (41).\n\n\nSniderman, Paul M. 2018. “Some Advances in the Design of Survey Experiments.” Annual Review of Political Science 21: 259–75.\n\n\nSniderman, Paul M, Thomas Piazza, Philip E Tetlock, and Ann Kendrick. 1991. “The New Racism.” American Journal of Political Science, 423–47.\n\n\nSteiner, Peter M, Christiane Atzmüller, and Dan Su. 2016. “Designing Valid and Reliable Vignette Experiments for Survey Research: A Case Study on the Fair Gender Income Gap.” Journal of Methods and Measurement in the Social Sciences 7 (2): 52–94.\n\n\nWarner, Stanley L. 1965. “Randomized Response: A Survey Technique for Eliminating Evasive Answer Bias.” Journal of the American Statistical Association 60 (309): 63–69.\n\n\nWinters, Matthew S, and Rebecca Weitz-Shapiro. 2013. “Lacking Information or Condoning Corruption: When Do Voters Support Corrupt Politicians?” Comparative Politics 45 (4): 418–36.\n\n\nYu, Jun-Wu, Guo-Liang Tian, and Man-Lai Tang. 2008. “Two New Models for Survey Sampling with Sensitive Characteristic: Design and Analysis.” Metrika 67 (3): 251.\n\n\nZigerell, Lawrence J. 2011. “You Wouldn’t Like Me When i’m Angry: List Experiment Misreporting.” Social Science Quarterly 92 (2): 552–62."
  },
  {
    "objectID": "guides/getting-started/late_en.html",
    "href": "guides/getting-started/late_en.html",
    "title": "10 Things You Need to Know About the Local Average Treatment Effect",
    "section": "",
    "text": "Abstract\nSometimes a treatment or a program is delivered but for some reason or another only some individuals or groups actually take the treatment. In this case it can be hard to estimate treatment effects for the whole population. For example maybe people for whom the treatment would have had a big effect decided not to take up the treatment. In these cases it is still possible to estimate what’s called the “Local Average Treatment Effect,” or LATE. This guide1 discusses the LATE: what it is, how to estimate it, and how to interpret it.2\n\n\n1 What the LATE is\nWhen subjects do not receive the treatment to which they were assigned, the experimenter faces a “noncompliance” problem. Some subjects may need the treatment so badly that they will always take up treatment, irrespective of whether they are assigned to the treatment or to the control group. These are called “Always-Takers”. Other subjects may not take the treatment even if they are assigned to the treatment group: the “Never-Takers”. Some subjects are “Compliers”. These are the subjects that do what they are supposed to do: they are treated when assigned to the treatment group, and they are not treated when they are assigned to the control group. Finally, some subjects do the exact opposite of what they are supposed to do. They are called “Defiers”. Table 1 shows these four different types of subjects in the population.\n\nNoncompliance can make it impossible to estimate the average treatment effect (ATE) for the population. For example, say that in a population of 200, 100 people are randomly assigned to treatment and we find that only 80 people are actually treated. What is the impact of the treatment? One method to answer this question is simply to ignore the noncompliance and compare the outcome in the treatment (100 people) and control (100 people) groups. This method estimates the average intention-to-treat effect (ITT). (See our guide on different kinds of treatment effects for more on the ITT.) While informative, this method does not give a measure of the effect of the treatment itself. Another approach would be to compare the 120 really-untreated and 80 really-treated subjects. Doing so, however, might give you biased estimates. The reason is that the 20 subjects that did not comply with their assignment are likely to be a nonrandom subset of those that were assigned to treatment.\nSo what now? In some cases it is possible to estimate the “Local Average Treatment Effect” (LATE), also known as the “Complier Average Causal Effect” (CACE). The LATE is the average treatment effect for the Compliers. Under assumptions discussed below, the LATE equals the ITT effect divided by the share of compliers in the population.\n\n\n2 With one-sided noncompliance you need to satisfy an exclusion restriction to estimate the LATE\nThe example introduced above is termed one-sided noncompliance: 80% of the population respond to the treatment assignment (the “Compliers”) and 20% do not (the “Never-Takers”). Say that after the treatment, the experimenter measures the average outcome to be 50 in the treatment group and 10 in the control group. This situation is illustrated in Table 2. Note that only those indicated with blue in Table 2 were in fact treated.\n\nBefore we can calculate the LATE under one-sided noncompliance we need to make an assumption. The exclusion restriction (also called “excludability”) stipulates that outcomes respond to treatments, not treatment assignments. In normal words this simply means that the outcome for a Never-Taker is the same regardless of whether they are assigned to the treatment or control group: in both cases the subject is not treated, and that is what matters.\nBecause the treatment was randomly assigned, we know that if there are 20% Never-Takers in the treatment group (left column), there are probably about 20% Never-Takers in the control group. Because of the exclusion restriction, the Never-Takers have the same outcome under both assignment conditions, and thus the difference in average outcomes (40) cannot be attributed to the Never-Takers. We can thus attribute the entire ITT effect to the Compliers. The LATE can therefore be estimated by dividing the ITT estimate by the share of Compliers: 40/0.8 = 50.\n\n\n3 With two-sided noncompliance the LATE can be estimated assuming both the exclusion restriction and a “no defiers” assumption\nThe experimenter may also face two-sided noncompliance. In this case, some subjects in the treatment group go untreated and some in the control group receive the treatment. In this world, the population consists of the Compliers, the Never-Takers, the Always-Takers, and the Defiers. To estimate LATE under two-sided noncompliance we need a second assumption: that the population contains no Defiers (the assumption is also called the “monotonicity” assumption). To see the use of this assumption look at Table 3, which illustrates our example under two-sided noncompliance. Again, after the treatment the experimenter measures the average outcome to be 50 in the treatment group and 10 in the control group. Note that those subjects in blue were in fact treated.\n\nWith the exclusion restriction and the no Defiers assumption we can estimate the LATE. Because of the exclusion restriction, we can attribute the entire ITT effect to Compliers and not to Never-Takers or Always-Takers. Given that we have an estimate of the ITT effect (40), what remains is to estimate the share of Compliers in the population. (Table 3 shows the subjects’ types as seen by an omniscient deity. The experimenter cannot observe these types, but can estimate their shares, as explained below.)\nWe cannot observe whether any given subject is a Complier, but we do observe whether they took the treatment. In the treatment group, we observe that 90 people took the treatment (and thus must be either Compliers or Always-Takers) and 10 did not (and thus must be Never-Takers, since there are no Defiers). Thus, 10% of the treatment group are Never-Takers, and since treatment was assigned randomly, we estimate that about 10% of the control group are Never-Takers. Similarly, in the control group, we observe that 10 people took the treatment (and thus must be Always-Takers, since there are no Defiers) and 90 did not (and thus must be either Compliers or Never-Takers). Thus, 10% of the control group are Always-Takers, and we estimate that about 10% of the treatment group are Always-Takers. From all this, we can estimate the share of Compliers as 100% - 10% (Never-Takers) - 10% (Always-Takers) = 80%. Finally, we can estimate the LATE as 40/0.8=50.\n\n\n4 You can estimate the LATE using an instrumental variables approach\nThe LATE estimate is equivalent to an instrumental variables estimate. This is most easily illustrated following a set of regressions. Say that 50 individuals from a population of 100 are randomly assigned to treatment. Regressing treatment status (D) on the treatment assignment (Z) gives the estimated share of compliers: 80%. The ITT effect is estimated by regressing outcome Y on the assignment to treatment (Z). Again, LATE is estimated by dividing the ITT estimate by the estimated share of compliers. A researcher will get exactly the same results when running a two-stage least squares (2SLS) regression in which the outcome (Y) is regressed on the treatment (D), using the assignment to treatment as an instrumental variable (Z). This is shown in the code below.\n\nZ <- rep(0:1,50) # Assign 50 to treatment group (Z = 1), 50 to control group (Z = 0)\nD <- Z           # Compliers have D (treatment received) = Z (treatment assignment)\nD[1:10]  <- 0    # 10 Never Takers\nD[11:20] <- 1    # 10 Always Takers\nY        <- 50*D # Compliers have Y = 50 if treated, 0 if not treated\nY[1:10]  <- 100  # Never takers have high Y\nY[11:20] <- 0    # Always takers have low Y\n# Estimated share of compliers \nITTD <- coef(lm(D~Z))[2] \n# Estimated intention-to-treat effect\nITT  <- coef(lm(Y~Z))[2] \n# LATE estimate\nLATE <- ITT / ITTD\ncbind(Y_1 = mean(Y[Z==1]), Y_0=mean(Y[Z==0]), ITTD, ITT, LATE)\n\n  Y_1 Y_0 ITTD ITT LATE\nZ  50  10  0.8  40   50\n\n# library(AER) \nsummary(ivreg(Y~ D | Z)) \n\n\nCall:\nivreg(formula = Y ~ D | Z)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n   -55     -5     -5     -5     95 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    5.000      5.660   0.883    0.379    \nD             50.000      8.839   5.657 1.53e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 35.36 on 98 degrees of freedom\nMultiple R-Squared: -0.1136,    Adjusted R-squared: -0.125 \nWald test:    32 on 1 and 98 DF,  p-value: 1.528e-07 \n\n\n\n\n5 How to conduct statistical inference about the LATE\nBecause the LATE equals the ratio of the ITT and the share of Compliers, the LATE will be equal to zero whenever the ITT equals zero. To test the null hypothesis that the LATE is zero, you can thus rely on tests of the null hypothesis that the ITT is zero. This is a straightforward way to go if you would like to rely on randomization inference for hypothesis tests. (See our guides on hypothesis testing and randomization inference for more.) Alternatively, you can make use of the conventional standard errors and confidence intervals generated by instrumental variables regression, which rely on parametric assumptions about the sampling distribution. As a rule of thumb, the standard error of the CACE will be roughly equal to the standard error of the ITT divided by the share of Compliers.\n\n\n6 The LATE only reflects treatment effects among compliers\nWhile a LATE estimate is better than nothing, it provides a consistent estimate of the average treatment effect only for a subgroup of the population: the Compliers. It does not capture effects of the treatment among everyone in the experimental sample (ATE). For instance, Angrist and Evans (1998) study the effect of childbearing on a mother’s labor supply. In their paper, compliers account for only 6% of the total population, while we would like to know the effect for everyone, or at least for a larger subgroup of the population of interest.3 Whether or not only obtaining the effect for Compliers is problematic depends on an experimenter’s objectives. Sometimes the LATE is exactly what the researcher is interested in: the average effect on those that actually comply with the assignment to treatment. Moreover, LATE might not be very different from the ATE if the share of compliers is large and the treatment effects for the different types in the population are similar enough. To explore the latter, researchers can compare the background attributes of the Compliers and Never-takers in the treatment group. Another approach is to compare the average outcomes of Always-Takers and Compliers among those that are treated (those in blue in Table 3), and the average outcomes of Never-Takers and Compliers among those that are not treated.4 Finally, it is important to keep in mind that whether a subject is a Defier, Complier, Never-Taker or Always-Taker also depends on the experimental design and the context in which the experiment is conducted. For example, using phone calls instead of a monetary incentive to encourage treatment take-up can alter the share of compliers in the population. As a result, different instruments will estimate different LATEs.\n\n\n7 The LATE estimate is always larger than the ITT estimate\nThe LATE estimate is calculated as the intention-to-treat estimate (ITT) divided by the estimated share of Compliers in the population. With noncompliance, the share of Compliers in the population is smaller than one. As a result, the LATE estimate will always be larger than the ITT estimate. Another way to look at this is that following the exclusion restriction (reminder: the exclusion restriction states that the outcome for a Never-Taker or Always-Taker is the same regardless of whether they are assigned to the treatment or control group), the ITT effect for the Never-Takers and the Always-Takers is zero. Thus, given any positive number of Never and/or Always-Takers, the average ITT effect is smaller than the LATE.\n\n\n8 The LATE is an important estimand in “encouragement” designs and in downstream experiments\nEncouragement Designs: In an encouragement design, subjects are randomly invited to participate in the treatment. The reason to do so is that in some cases it might be unethical to make subjects adhere to the treatment assignment. In other cases, it might require unnecessarily large incentives to obtain adherence. As a result, in encouragement designs subjects self-select into treatment. For example, Hirano et al (2000) study the impact of a letter encouraging inoculation of patients at risk for flu, sent to a randomly selected set of physicians. The outcome of interest is an indicator for flu-related hospital visits. Needless to say, the incentives for the treatment (the letter) have only a limited effect on the actual treatment received (a flu shot by the physician), and thus the study population will consist of Compliers, Always-Takers, and Never-Takers. (We assume there are no Defiers, patients who would get the flu shot if and only if their physician did not get the letter.) It is thus easy to see how such an encouragement design corresponds to the two-sided noncompliance case.5\nDownstream experiments: Downstream experiments are studies in which an initial randomization (e.g. distribution of school vouchers) causes a change in an outcome (e.g. education level), and this outcome is then considered a treatment affecting a subsequent outcome (e.g. income).6 Also, these experiments correspond to our two-sided noncompliance setup. Noncompliance occurs because the random intervention is just one of many “encouragements” that cause people to take the treatment. Downstream experiments place particular pressure on the exclusion restriction, which requires that (following the example) school vouchers influences income only through higher education. This assumption would be violated if school vouchers affected income for reasons other than education.\n\n\n9 You can use a placebo-controlled design to identify the LATE\nAnother strategy to estimate the LATE involves designing your experiment in such a way that you can find out who the Compliers are even among those who did not receive the treatment. One way to do so is to create a placebo group that receives a version of the treatment without the treatment’s “active ingredient.” Gerber, Green, Kaplan, and Kern (2010), for example, study the effect of an automated phone call that delivers an encouragement to vote on whether study participants turn out to vote.7 The experiment included a placebo group in which study participants received automated phone calls that encouraged them to recycle. Because respondents in the placebo group also received calls, it is possible to observe which respondents in the placebo group answered the phone. Under the assumption that the content of the phone call does not affect who answers the phone, one can estimate the LATE by comparing turnout rates among those who answered the phone in the treatment group to turnout rates among those who answered the phone in the placebo group. The key assumption which makes this strategy work is that respondents who comply with the treatment (pick up the voting phone call) also comply with the placebo (pick up the recycling phone call) and vice versa. Gerber, Green, Kaplan, and Kern (2010) discuss the advantages of having both a pure control and a placebo group for estimating the LATE.\n\n\n10 Addressing partial compliance can be complicated\n“Partial compliance” occurs when a subject is assigned to a treatment but receives less than “all” of the treatment. This is possible in designs with compound treatments, multi-arm designs like factorial designs, and in dose-response trials where the treatment variable is continuous. For example, subjects assigned to a three-session job training program may only attend two of the three sessions. Patients in a clinical trial assigned to receive 100 mg dosages of an experimental drug once every week for five weeks may only receive four of the five assigned doses. Addressing partial compliance can be especially complicated because the effective number of treatment conditions exceeds the number intended in the original design. This expansion of the number of treatment conditions affects the definition of the LATE and how to estimate it. First, the number and definition of compliance statuses changes. The categories used in designs with a binary treatment (Always-Takers, Never-Takers, Compliers, and Defiers) no longer suffice. Instead, the set of possible compliance statuses is determined by all possible combinations of treatment assignment and treatment receipt. In the binary case, we ruled out Defiers. In the partial compliance case, we can make similar (design-specific) monotonicity assumptions that rule out some theoretically possible compliance statuses. Finally, we are no longer interested in a single LATE. Partial compliance means that the number of quantities we are trying to estimate increases. Unfortunately, the IV/2SLS estimator used under one- and two-way noncompliance in two-group designs is a biased estimator of LATEs under partial compliance. Instead, Bayesian approaches have emerged as an alternative method for inference.8\n\n\n\n\n\nFootnotes\n\n\nOriginating author: Peter van der Windt, 20 October 2014. Revisions: Winston Lin, 22 August 2016. The guide is a live document and subject to updating by EGAP members at any time; contributors listed are not responsible for subsequent edits. Thanks to Albert Fang for point 10.↩︎\nFor more extensive overviews, see: J. D. Angrist, G. W. Imbens, and D. B. Rubin (1996), “Identification of Causal Effects Using Instrumental Variables” (with discussion), Journal of the American Statistical Association 91: 444-472; J. D. Angrist (2006), “Instrumental Variables Methods in Experimental Criminological Research: What, Why and How,” Journal of Experimental Criminology 2: 23-44; J. D. Angrist and J.-S. Pischke (2009), Mostly Harmless Econometrics, sections 4.4-4.6 and 6.2; T. Dunning (2012), Natural Experiments in the Social Sciences; M. Baiocchi, J. Cheng, and D. S. Small (2014), “Instrumental Variable Methods for Causal Inference,” Statistics in Medicine 33: 2297-2340 (with correction in Statistics in Medicine 33: 4859-4860).↩︎\nAngrist, J. D. Evans, W. N. 1998. Children and their Parents’ Labor Supply: Evidence from Exogenous variation in family size. American Economic Review, 88(3), 450-477.↩︎\nSee: Imbens, G. W. (2010). Better LATE Than Nothing: Some Comments on Deaton (2009) and Heckman and Urzua (2009). Journal of Economic Literature, 48, 399-423. And Imbens, G. W., & Angrist, J. D. (1994). Identification and Estimation of Local Average Treatment Effects. Econometrica, 62(2), 467-475, and 2). And Imbens, G. W. & Wooldridge, J. 2007. Lecture notes and slides at: http://www.nber.org/minicourse3.html.↩︎\nHirano, K., Imbens, G.W., Rubin, D.B. & Zhou, X. (2000). Assessing the Effect of an Influenza Vaccine in an Encouragement Design. Biostatistics, 1(1), 69-88.]↩︎\nSee: Green, D. P. & Gerber, A. S. 2002. The Downstream Benefits of Experimentation. Political Analysis, Vol. 10(4), 394-402.↩︎\nSee Gerber, Alan S., Donald P. Green, Edward H. Kaplan, and Holger L. Kern. 2010. “Baseline, Placebo, and Treatment: Efficient Estimation for Three-Group Experiments.” Political Analysis, Vol.18(3), 297-315.↩︎\nSee Qi Long, Roderick J. A. Little, and Xihong Lin. 2010. Estimating Causal Effects in Trials Involving Multi-Treatment Arms Subject to Non-compliance: A Bayesian framework. Journal of the Royal Statistical Society: Series C (Applied Statistics), 59(3): 513-531.; Jin, H. & Rubin, D. B. (2009). Public schools versus private schools: Causal inference with Partial Compliance. Journal of Educational and Behavioral Statistics, 34, 24-45; Jin, H. & Rubin, D. B. (2008). Principal Stratification for Causal Inference with Extended Partial Compliance. Journal of the American Statistical Association, 103, 101-111.↩︎"
  },
  {
    "objectID": "guides/getting-started/pilots_en.html",
    "href": "guides/getting-started/pilots_en.html",
    "title": "10 Things to Know About Pilot Studies",
    "section": "",
    "text": "1. What is a pilot, and what is it good for?\nDuring the process of planning an experiment, researchers often face questions regarding their study’s theoretical and conceptual underpinnings, its measurement approach, and associated logistics. Pilot studies can help you to consider and improve these elements of your research whether you are running a survey, lab, or field experiment. In particular, a pilot study is a smaller scale preliminary test or trial run, used to assist with the preparation of a more comprehensive investigation. Pilots are typically administered before a research design is finalized in order to evaluate and improve the feasibility, reliability, and validity of the proposed study.\nWhile it may be tempting to think about a pilot as simply a miniature version of one’s final study, helpful for doing an initial test of one’s hypotheses, pilot studies are neither especially appropriate for hypothesis testing, nor are they limited to it. Given smaller sample sizes, pilots are typically underpowered for evaluating hypotheses. Besides, deciding whether to continue a study based on initial results contributes to the “file drawer” problem where important studies and results—including null results—are never published, leading to misrepresentative bodies of published research (Franco et al. 2014).\nFortunately, as depicted in the table below, pilots can be useful for a wide range of research purposes including theory development, research design, improving measurement, sampling considerations, evaluating logistics, pre-planning analysis, weighing ethical considerations, and communicating one’s research (van Teijlingen et al. 2001; Thabane et al. 2010). Each of these activities can help improve the quality of one’s main study and render it more compelling and ultimately successful.\nIn the sections below, we review these many benefits. First, we consider how pilots can assist with a study’s theory and measurement approach, evaluate its logistical feasibility, and provide information about the sample size needed to test hypotheses. In addition, we discuss how piloting may help you secure research funding and institutional support, gather feedback, and incorporate best practices in research ethics. Finally, we offer recommendations on how to use pilots to inform your main study design, reveal important unknowns, and contribute to your broader research agenda.\n\n\n\n\n\n\n\nTheory Development\nResearch Design\n\n\n\n\nRefining your research questions and hypotheses\nEvaluating suitability of study context for answering research questions\n\n\nDetermining initial plausibility of theorized mechanisms\nDetermining construction and strength of treatments\n\n\nIdentifying additional research questions of interest\nDetermining number, levels, and timing of treatments\n\n\nExploring unknowns related to your research questions and hypotheses\nConsidering alternative and/or multiple measures of participant response to treatment\n\n\n\n\n\n\n\n\n\n\nMeasurement\nSample\n\n\n\n\nTesting alternative treatments and outcomes to improve clarity, validity, and reliability\nDetermining power and sample size needed for main study\n\n\nUncovering issues related to asymmetry in delivery and measurement of treatments and/or outcomes\nEvaluating recruitment procedures and eligibility criteria\n\n\nConsidering covariate measurement approach\nTesting participant understanding of research tools and procedures\n\n\nEnsuring successful data collection and data entry\nAssessing participant compliance and treatment uptake\n\n\nIdentifying additional information that may need to be collected\nAssessing participant response rate, quality, and attrition\n\n\nConsidering the use of combined measures such as scales and indices\n\n\n\n\n\n\n\n\n\n\n\nLogistics\nAnalysis\n\n\n\n\nDetermining time and resources needed to conduct main study\nConsidering feasibility of software, statistical techniques, or other analytic tools given preliminary data\n\n\nEvaluating randomization procedure\nPreparing analysis procedures for main study\n\n\nAssessing research team understanding of protocols and procedures\nConsidering feasibility of incorporating data from partners or other sources\n\n\nDetermining partner capacity and willingness\nConsidering responses to missingness in data\n\n\nTraining research team members and partners\n\n\n\nAssessing inter-researcher reliability in data collection, procedures, and analysis\n\n\n\n\n\n\n\n\n\n\n\nEthics\nCommunicating Research\n\n\n\n\nEnsuring participant safety and well-being\nCollecting preliminary data to share externally\n\n\nEvaluating consent procedures\nSoliciting feedback on research design\n\n\nAssessing participant time and burdens\nLeveraging pilot results transparently to explain design choices for main study\n\n\nEnsuring normative acceptability of study elements in given context\nDemonstrating research team capacity and competence\n\n\nAssessing privacy and confidentiality procedures\nPersuading funders, ethics committees, and other stakeholders of study feasibility and value\n\n\nAssessing security of data and other study materials\nInforming interested scholars and other stakeholders\n\n\n\nEducating students about research practices\n\n\n\n\n\n2. Pilots are useful for improving your study’s measurement approach in relation to your theory\nFewer things are more frustrating for a researcher than investing significant resources and time into a study only to find that one’s outcome measures lack reliability to accurately assess the concepts of interest, or that participants did not receive treatments in the way that was anticipated. Unexpected results are common, even when one approaches measurement carefully, as operationalizing concepts into an effective measurement strategy in the social sciences is a complex endeavor.\nAs a remedy, pilots are great for testing different versions of treatments and outcomes to see which of them “work” or whether any changes may be needed to increase validity, reliability, and clarity. For example, you can include a larger set of outcome measures in a pilot than is feasible in the main study, and perform simple factor analysis to identify a preferred subset of measures. Pilot results can also inform whether it makes sense to create indices or scales in order to improve reliability and decrease variance. Subtle variation in the strength, nature, timing, or number of treatments can also significantly alter study findings. Pilots offer researchers the opportunity to evaluate multiple possibilities for one’s treatment design, and to assess how these options influence participant compliance, uptake of treatment, attrition, and more.\nWhen refining one’s approach to measuring treatments, outcomes, and covariates, it is especially important to keep in mind how these elements of one’s research design speak to the broader concepts and theory under study. Will the data you receive provide the necessary information regarding the theoretical elements and causal mechanisms under study? Are there other causal channels that may be in play, or heterogeneous effects within subgroups that you hadn’t thought about previously? These kinds of considerations can inform changes to your research design, such as alterations to your randomization strategy, the introduction of new dimensions in your treatments, and decisions about which aspects of your study are core and which can be saved for a later date.\nWhen you begin a pilot study, you may have an initial conception of your research questions, hypotheses, and measurement approach; but with a careful pilot, you have the opportunity to refine all of these aspects in a way that can increase your (and others’) confidence in the overall quality of the study.\n\n\n3. Pilots can also help you to prepare for the logistics of running your experiment\nPerhaps the most often emphasized purpose of pilots is to work out any logistical kinks that might impede the main study (Thabane et al. 2010). Logistical considerations include those implicating a project’s overall resources, the study team, the participants, and the administration of study instruments.\nIn terms of participant logistics, is important to establish whether your study participants understand the research tools and procedures, are able to receive treatment, and feel comfortable answering questions or performing tasks. Based on how successful the pilot is, you may find ways to improve your recruitment strategy, adjust your eligibility criteria, and improve the clarity of your research instruments. It is also important to ensure that basic elements like randomization and data collection are working as anticipated. Data simulations and pilots with very small samples can also be used to test certain study elements.\nSimilar considerations apply to your study team and partners. Do they understand the protocols or might they require additional training, for example, to promote reliability in procedures such as data collection? Are there any asymmetries in delivery or measurement of treatment or outcomes of which you were not aware? A pilot can also be very helpful for determining the resources needed to conduct the main study. For example, how much time does it take—for both members of the study team and participants—and what might this imply for the size of the sample and complexity of the research design that is ultimately feasible for the main study?\nWhile the specific logistical considerations will vary depending on whether your research design is centered around a field experiment, survey experiment, lab experiment, or something else, pilots will help you ensure that your experiment goes as planned. Thus, when constructing a pilot study, consider making a list of the logistical elements you want to evaluate and designing the pilot to facilitate answering associated questions. Asking your participants whether they were able to “hear the video” or “understood the instructions” can make your research easier down the road.\n\n\n4. You can use pilots for power analysis or for calculating minimum detectable effects\nAnother common purpose of pilot studies, in light of limited resources, is assessing statistical power, which helps to avoid the risk of false negatives (or false positives) from underpowered studies. As Coppock (2013) describes in the EGAP methods guide about power analysis, a researcher’s goal is to answer the following: “supposing there truly is a treatment effect and you were to run your experiment a huge number of times, how often will you get a statistically significant result?” To improve the likelihood that your main study will achieve a typical target power value of 80%, you can use a pilot study combined with careful simulations. These results are helpful for determining appropriate sample sizes, the extent to which you can subset your sample for various analyses, and whether adjustments may be necessary for increasing power.\nTraditionally, a pilot study is used to obtain an estimate of the effect size, which becomes the presumed estimate for simulations used to determine power and sample size in one’s main study. However, DeclareDesign (2019) cautions that effect size estimates are very noisy in small pilots, especially when true effect sizes are small (for example, under 0.2 sd). As an alternative, they recommend using pilot studies to estimate the standard deviation of the outcome variable. Using this estimate, one can easily obtain an unbiased estimate of a main study’s minimum detectable effect (MDE) for a given outcome and with 80% power as 2.8 times the estimated standard error of the associated outcome variable (Gelman and Hill 2006). The goal is to ensure that the MDE is small enough that the study would capture any substantively meaningful effect.\nUsing the recommendations from DeclareDesign (2019), we provide sample code based on a hypothetical pilot study. In the code below, we assume we have already conducted a pilot study and have calculated the standard deviation of a key outcome measure for both the control and treatment groups. Next, we use these estimates to calculate MDEs for different possible sample sizes, in order to inform our target sample size for a future main study.\n\nrequire(tidyverse)\n#Standard deviations of outcome measure for treatment & control groups calculated from pilot study results\n#The ones provided are for purposes of demonstration\n#We assume one treatment and control group--you will need to adjust to study specifics\nsd_control <- 1.5\nsd_treat <- 1.2\n#MDE calculation for various possible sample sizes\nN <- seq(from=500, to=3000, by=100)\nMDE <- vector()\nfor (i in 1:length(N)){\n  #We assume an equal number of participants in treatment and control--you will need to adjust to study specifics\n  MDE[i] <- 2.8 * sqrt((sd_control^2/(N[i]/2)) + (sd_treat^2/(N[i]/2))) #The true effect size must be at least 2.8 standard errors from zero to detect it with 80% probability using 95% confidence intervals (Gelman and Hill 2006). To estimate the standard error of the ATE, we use equation 3.6 in Gerber and Green (2012). Thus, simply multiply 2.8 by the standard error of the ATE to calculate the MDE. \n}\n#Visualizing results to identify target sample size for main study\nmde_data <- as.data.frame(cbind(N, MDE))\nmde_plot <- ggplot(mde_data, aes(x=N, y=MDE)) + geom_point() +\n  xlab(\"Sample Size\") + ylab(\"MDE\") + ggtitle(\"Minimum Detectable Effect (MDE) for Main Study by Sample Size\") + \n  theme_bw() +\n  geom_hline(yintercept = .2, lty = \"dashed\")\nmde_plot\n\n\n\n\nBased on the hypothetical standard deviations in the control and treatment groups from the pilot study, the main study would need a sample size of approximately 1,500 to ensure that effect sizes as small as 0.2 sd are detectable. While calculating the MDE based on pilot results is straightforward, determining how small an MDE should be is more subjective, and should be informed by theory and prior work.\nMDE calculations are based on the design of the pilot and the specific outcome measures. Thus, you may need to perform MDE calculations for each estimand of interest to determine what sample sizes are needed and which hypotheses can be addressed with sufficient power given your experimental design. Keep in mind that as you use pilot results to refine your study design, including treatments and outcomes, the relevant standard deviations of the outcome measures and the resultant MDE calculations may change. Another quick pilot is an option, though it is important to take resource constraints into account.\n\n\n5. Piloting may help you secure funding and support for your research\nPilots are not only helpful for improving the quality of your main study, they may also help to ensure you have the support and funding to enable your study to go forward. Given a general trend of tightening of per-researcher funding, particularly for smaller projects (Bloch and Sørensen 2015), and a movement towards evidence-based decision-making, you may wish to draw on pilots to provide initial evidence that your study is worthwhile. For example, the National Science Foundation (NSF) recognizes the need for more early-stage exploratory studies that can provide a basis for future larger-scale studies, and Time-sharing Experiments for the Social Sciences (TESS) notes that “Proposals that report trial runs of novel and focal ideas will be viewed as more credible.”\nThis does not entail that one needs to show that effect sizes are large enough nor that hypotheses are likely to be confirmed. Instead, a pilot can demonstrate that your research project is feasible in terms of time and resources, that your study design is adequate for answering the research questions proposed, and that your research team has the expertise and capacity to administer the study, perform analyses, and even present results in a compelling fashion (van Teijlingen et al. 2001). In a similar fashion, piloting can help you to recruit study team members and organizational partners, or solicit institutional support.\n\n\n6. You can use a pilot to get feedback on your study design\nSharing initial findings, successes, and challenges is a great way to help you prepare for your main study. In light of the growth of the open science movement (Christensen et al. 2020), conferences and workshops are increasingly open to accepting submissions based on pre-analysis plans and pilot results. Whether through these more formal venues, or by reaching out to colleagues or experts, you can use pilot results to receive feedback about your study design, such as strategies to address possible challenges and unexplored theoretical or empirical directions that you can incorporate in your main study.\nFurther, it can take a long time to complete an experimental study and publish results. Sharing intermediate findings allows you to coordinate with other researchers in the field, helping you to align your work and incorporate recent theoretical and empirical innovations relevant to your study.\nUltimately, the design and piloting stage is the best time to receive feedback, as you still have time to make improvements. In contrast, most key research design decisions will already have been finalized by the time your study undergoes formal peer review for publication.\n\n\n7. Keep an eye out for ethical considerations when piloting\nWhen you design your study initially, all of the relevant ethical considerations and risks may not be immediately apparent. The piloting stage is thus a good opportunity to review whether any risks or harms you anticipated may come into play and whether still other ethical considerations should be incorporated into your main study.\nYou can use your pilot to evaluate whether your procedures around informed consent are adequate and to assess the extent of burdens such as time required of participants. You may find, for example, that certain topics—such as mental health and personal identity—are more sensitive than anticipated, or that survey questions (even those based on validated and popular measures) use outdated and offensive language. The best way to find out is to ask. Consider including open-ended survey questions or talking to participants directly to determine what participants think of the study in terms of its normative and cultural acceptability in a given context.\nIn addition, the piloting process is a good time to practice your procedures around privacy, confidentiality, and security of data and other materials. You may find that other procedures for obtaining consent, ensuring participant safety and well-being, and promoting privacy and anonymity are necessary to improve the ethical dimensions of your main study. This can include decreasing risks as well as increasing benefits to participants, such as by providing helpful resources and information that may help to mitigate possible harms.\nNote that this doesn’t imply any less ethical consideration should be given to your pilot itself. All appropriate safeguards, including IRB review, apply to human subjects research for a pilot study as well.\n\n\n8. Be transparent about how your pilot informs the design of your main study\nAs noted, it’s helpful to pre-identify a set of questions about logistics, measurement, or other features of your study that you believe a pilot can help to answer. Putting these questions into writing, designing your pilot to facilitate answering them, and reporting on how the answers shape any modifications to your main study is a way in which you can promote transparency in research. This includes transparency within one’s study team about the purposes of a pilot, as well as for external audiences such as funders or peer reviewers. Transparency helps to alleviate concerns such as the file drawer problem, for example by demonstrating that a pilot study does not function as a method for cherry-picking statistically significant results. It also facilitates understanding and receipt of feedback.\nThe table below is a simple illustrative example of how one could transparently present lessons learned from a hypothetical pilot, for example, in a pre-analysis plan or research proposal. The first column indicates the question the pilot is intended to help answer, in this case questions related to logistical adequacy, manipulation checks, delivery of treatments, and measurement of outcomes. The second column presents the associated findings from the pilot, and the third column discusses how the lessons learned will inform design choices for the main study.\n\n\nQuitting from lines 148-157 (pilots_en.rmarkdown) Error in if (sum(t.width + extra.spaces.width) + 1 > split.tables & length(t.width) > : the condition has length > 1 Calls: .main … pander.data.frame -> pandoc.table -> cat -> pandoc.table.return\n\n\n\n\n9. Explore unknowns\nWhile you may begin your study with a set of preestablished questions—or “known unknowns”— that your pilot can help to address, keep in mind that there is a whole universe of “unknown unknowns” left to be explored. Of course, not all of these will be relevant to your study, but some are likely to be. For example, you may find that the participants have vastly different interpretations of an informational treatment, understand survey scales differently, or are reluctant to share responses given a perceived political bias.\nThere are a few ways to explore these unknowns in your pilot study. You might wish to include additional exploratory outcome questions, collect extra covariate data, or use a variety of treatments drawing on innovative or untested ideas. Another great way to identify unknowns is through open-ended questions of study participants. Regardless of your study design, you might consider surveying or interviewing participants to ask “what they think about topic X,” or “what comes to mind when they hear the term Y.” The world is often more complex than researchers model in their study designs, and study participants often have more diverse perspectives and relationships with the issues at hand than researchers expect.\nThrough this process, you may identify novel research questions, potential theoretical dimensions or causal mechanisms, and hypotheses that you had not originally formulated when you began studying the topic. Exploring unknowns can therefore lead to refining the ideas you had originally conceived of, as you are unlikely to have determined the best version of your study from the beginning. It can also lead to entirely novel considerations and ideas altogether, some of which you may be able to incorporate into the current study or future studies.\nOverall, pilots afford a wonderful opportunity for open-ended exploration and idea generation.\n\n\n10. A pilot is part of a broader sequence of research activities\nWhile we’ve talked about research in terms of “pilots” and “main studies,” this is an oversimplification of the research process. Indeed one can (and often should) employ multiple pilot studies, perhaps with different sample sizes, to evaluate different questions and to continue refining one’s research design as new questions and possibilities emerge. Pilot studies certainly do set the stage for main studies as well as subsequent “follow-up” studies, but there may not be a sharp demarcation between these kinds of studies. As depicted in the graphic, pilot studies do not merely lead to main studies. They can also help one develop new ideas and they may serve as a venue through which one can contribute to or coordinate with the broader research community around topics of shared interest.\n\n\n\n\nThe Role of Pilots in Experimental Research\n\n\n\n\nAs pilots are part of this broader sequence of research activities, an important consideration is what portion of one’s funding and resources should be allocated to a pilot study versus one’s main study. Researchers may be discouraged from devoting research resources to pilots due to a perception that pilots only provide preliminary data that cannot be used for final research products. While there is no simple “rule of thumb” about the share of the budget that researchers should apply to pilots, we believe that pilots can pay off when they empower researchers to improve their study designs and make more grounded decisions about their research.\nAdministering a second pilot study is especially prudent when pilot results or procedures deviate significantly from your expectations, or if you make substantial alterations to your design. Piloting one or more times is particularly beneficial for field experiments, as researchers often have just one opportunity to successfully implement their full study. However, when resources are constrained, you may opt instead to use simulations to evaluate alternative research designs based on results from your single pilot study. The DeclareDesign package in R is a helpful resource.\nIn short, pilots are neither pre-tests of hypotheses nor merely checks on basic study logistics that deplete one’s research funds. Instead, pilots are living and breathing elements of the broader research process that provide value in their own right.\n\n\nBibliography\nBloch, Carter, and Mads P. Sørensen. 2015. “The Size of Research Funding: Trends and Implications.” Science and Public Policy 42 (1): 30–43. https://doi.org/10.1093/scipol/scu019\nChristensen, Garret, Zenan Wang, Elizabeth Levy Paluck, Nicholas Swanson, David Birke, Edward Miguel, and Rebecca Littman. 2020. “Open Science Practices Are on the Rise: The State of Social Science (3S) Survey.” 106. Working Paper Series. Center for Effective Global Action, University of California. https://escholarship.org/uc/item/0hx0207r\nDeclareDesign. 2019. “Should a Pilot Study Change Your Study Design Decisions?” DeclareDesign. https://declaredesign.org/blog/2019-01-23-pilot-studies.html\nFranco, Annie, Neil Malhotra, and Gabor Simonovits. 2014. “Publication Bias in the Social Sciences: Unlocking the File Drawer.” Science 345 (6203): 1502–5. https://doi.org/10.1126/science.1255484.\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge, United Kingdom: Cambridge University Press.\nGerber, Alan S., and Donald P. Green. 2012. Field Experiments: Design, Analysis, and Interpretation. 1st ed. New York, New York: W. W. Norton.\nThabane, Lehana, Jinhui Ma, Rong Chu, Ji Cheng, Afisi Ismaila, Lorena P. Rios, Reid Robson, Marroon Thabane, Lora Giangregorio, and Charles H. Goldsmith. 2010. “A Tutorial on Pilot Studies: The What, Why and How.” BMC Medical Research Methodology 10 (1): 1-10. https://doi.org/10.1186/1471-2288-10-1\nvan Teijlingen, Edwin R., Anne-Marie Rennie, Vanora Hundley, and Wendy Graham. 2001. “The Importance of Conducting and Reporting Pilot Studies: The Example of the Scottish Births Survey.” Journal of Advanced Nursing 34 (3): 289–95. https://doi.org/10.1046/j.1365-2648.2001.01757.x."
  },
  {
    "objectID": "guides/getting-started/randomization-inference_en.html",
    "href": "guides/getting-started/randomization-inference_en.html",
    "title": "10 Things to Know About Randomization Inference",
    "section": "",
    "text": "1. Randomization inference is a method for calculating p-values for hypothesis tests 1 2\nOne of the advantages of conducting a randomized trial is that the researcher knows the precise procedure by which the units were allocated to treatment and control. Randomization inference considers what would have happened under all possible random assignments, not just the one that happened to be selected for the experiment at hand. Against the backdrop of all possible random assignments, is the actual experimental result unusual? How unusual is it?\n\n\n2. Randomization inference starts with a null hypothesis\nAfter we have conducted an experiment, we observe outcomes for the control group in their untreated state and outcomes for the treatment group in their treated state.3 In order to simulate all possible random assignments, we need to stipulate the counterfactual outcomes – what we would have observed among control units had they been treated or among treated units had they not been treated. The sharp null hypothesis of no treatment effect for any unit is a skeptical worldview that allows us to stipulate all of the counterfactual outcomes. If there were no treatment effect for any unit, then all the control units’ outcomes would have been unchanged had they been placed in treatment. Similarly, the treatment units’ outcomes would have been unchanged had they been placed in the control group. Under the sharp null hypothesis, we therefore have a complete mapping from our data to the outcomes of all possible experiments. All we need to do is construct all possible random assignments and, for each one, calculate the test statistic (e.g., the difference in means between the assigned treatment group and the assigned control group). The collection of these test statistics over all possible random assignments creates a reference distribution under the null hypothesis. If we want to know how unusual our actual experimental test statistic is, we compare it to the reference distribution. For example, our experiment might obtain an estimate of 6.5, but 24% of all random assignments produce an estimate of 6.5 or more even in the absence of any treatment effect. In that case, our one-tailed p-value would be 0.24.4\n\n# Worked example of randomization inference\nrm(list=ls())       # clear objects in memory\nlibrary(ri)         # load the RI package\nset.seed(1234567)   # random number seed, so that results are reproducible\n# Data are from Table 2-1, Gerber and Green (2012)\nY0 <- c(10, 15, 20, 20, 10, 15, 15)\nY1 <- c(15, 15, 30, 15, 20, 15, 30)\nZ <-  c(1,0,0,0,0,0,1)       # one possible treatment assignment\nY <-  Y1*Z + Y0*(1-Z)  # observed outcomes given assignment\nprobs <- genprobexact(Z,blockvar=NULL)   # no blocking is assumed when generating probability of treatment and probs are 2/7 for all units\nate <- estate(Y,Z,prob=probs)      # estimate the ATE\nperms <- genperms(Z,maxiter=10000,blockvar=NULL)   # set the number of simulated random assignments\n# show all 21 possible random assignments in which 2 units are treated\nperms\n\n  [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n1    1    1    1    1    1    1    0    0    0     0     0     0     0     0\n2    1    0    0    0    0    0    1    1    1     1     1     0     0     0\n3    0    1    0    0    0    0    1    0    0     0     0     1     1     1\n4    0    0    1    0    0    0    0    1    0     0     0     1     0     0\n5    0    0    0    1    0    0    0    0    1     0     0     0     1     0\n6    0    0    0    0    1    0    0    0    0     1     0     0     0     1\n7    0    0    0    0    0    1    0    0    0     0     1     0     0     0\n  [,15] [,16] [,17] [,18] [,19] [,20] [,21]\n1     0     0     0     0     0     0     0\n2     0     0     0     0     0     0     0\n3     1     0     0     0     0     0     0\n4     0     1     1     1     0     0     0\n5     0     1     0     0     1     1     0\n6     0     0     1     0     1     0     1\n7     1     0     0     1     0     1     1\n\n# --------------------------------------------------------------------\n# estimate sampling dist under the sharp null that tau=0 for all units\n# --------------------------------------------------------------------\nYs <- genouts(Y,Z,ate=0)    # create potential outcomes under the sharp null of no effect for any unit\n# show the apparent potential outcomes under the sharp null\nYs\n\n$Y0\n[1] 15 15 20 20 10 15 30\n\n$Y1\n[1] 15 15 20 20 10 15 30\n\ndistout <- gendist(Ys,perms,prob=probs)  # generate the sampling distribution  based on the implied schedule of potential outcomes implied by the null hypothesis\nate                             # estimated ATE\n\n[1] 6.5\n\nsort(distout)                   # list the distribution of possible estimates under the sharp null of no effect\n\n [1] -7.5 -7.5 -7.5 -4.0 -4.0 -4.0 -4.0 -4.0 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  3.0\n[16]  3.0  6.5  6.5  6.5 10.0 10.0\n\nsum(    distout  >=     ate )/nrow(as.matrix(distout))   # one-tailed comparison used to calculate p-value\n\n[1] 0.2380952\n\nsum(abs(distout) >= abs(ate))/nrow(as.matrix(distout))   # two-tailed comparison used to calculate p-value\n\n[1] 0.3809524\n\ndispdist(distout,ate)        # display p-values, 95% confidence interval, standard error under the null, and graph the sampling distribution under the null\n\n\n\n\n$two.tailed.p.value\n[1] 0.4761905\n\n$two.tailed.p.value.abs\n[1] 0.3809524\n\n$greater.p.value\n[1] 0.2380952\n\n$lesser.p.value\n[1] 0.9047619\n\n$quantile\n 2.5% 97.5% \n -7.5  10.0 \n\n$sd\n[1] 5.322906\n\n$exp.val\n[1] 0\n\n# Compare reuslts to traditional t-test with unequal variance\nt.test(Y~Z,\n       alternative = \"less\",\n       mu = 0, paired = FALSE, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  Y by Z\nt = -0.8409, df = 1.1272, p-value = 0.2708\nalternative hypothesis: true difference in means between group 0 and group 1 is less than 0\n95 percent confidence interval:\n     -Inf 33.94232\nsample estimates:\nmean in group 0 mean in group 1 \n           16.0            22.5 \n\nt.test(Y~Z,\n       alternative = \"two.sided\",\n       mu = 0, paired = FALSE, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  Y by Z\nt = -0.8409, df = 1.1272, p-value = 0.5416\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -82.03199  69.03199\nsample estimates:\nmean in group 0 mean in group 1 \n           16.0            22.5 \n\n\n\n\n3. Randomization inference gives exact p-values when all possible random assignments can be simulated\nWhen the reference distribution is known based on a complete census of possible random assignments, p-value calculations are exact – there are no theoretical approximations based on assumptions about the shape of the sampling distribution. Sometimes the set of possible random assignments is so large that a full census is infeasible. In that case, the reference distribution can be approximated to an arbitrary level of precision by randomly sampling from the set of possible random assignments a large number of times. Thousands or tens of thousands of simulated random assignments are recommended.\n\n\n4. Randomization inference requires the analyst to specify a test statistic and some are more informative than others\nIn principle, any test statistic can be used as input for randomization inference, which in turn outputs a p-value. Some test statistics provide more informative results than others, however. For example, although the simple difference-in-means often performs well, there are good arguments for other test statistics, such as the t-ratio using a robust standard error.5 In this case, the researcher would calculate the test statistic for the actual experiment and compare it to a reference distribution of robust t-statistics under the sharp null hypothesis of no effect.\n\n\n5. Randomization inference may give different p-values from conventional tests when the number of observations is small and when the distribution of outcomes is non-normal\nConventional p-values typically rely on approximations that presuppose either that the outcomes are normally distributed or that the subject pool is large enough that the test statistics follow a posited sampling distribution. When outcomes are highly skewed, as in the case of donations (a few people donate large sums, but the overwhelming majority donate nothing), conventional methods may produce inaccurate p-values. Gerber and Green (2012, p.65) give the following example in which randomization inference and conventional test statistics produce different results:\n\n\n\n6. Randomization inference is useful for clustered designs: clustering, fuzzy clustering\nCluster random assignment is notorious for throwing off inference, especially when the number of clusters is small. (See our guide 10 Things to Know About Cluster Randomization for more details.) Robust cluster standard errors tend to be downwardly biased when there are fewer than a dozen clusters, and bias is evident in simulations even when the number of clusters rises above thirty. Randomization inference sidesteps the problem of faulty t-ratios based on robust cluster standard errors. Instead, the reference distribution is calculated based on the set of possible clustered assignments, which takes account of the sampling variability associated with clustered assignment.\nRandomization inference is even more valuable when the clustered standard errors are difficult or impossible to compute. This situation arises in the context of “fuzzy clustering” – instances in which sets of observations have correlated assignments, but the correlation is less than perfect. Fuzzy clustering, for example, occurs in place-based random assignments in which treatment effects spill over to nearby units (e.g., anti-crime interventions that may displace crime to nearby areas). By simulating outcomes under alternative random assignments, randomization inference allows the researcher to readily gauge the p-values of test statistics under the null hypothesis of no treatment and no spillover effects (i.e., if the police intervention were ineffective and neither deterred crime nor pushed it around the corner).\n\n\n7. Randomization inference is valuable when randomization procedures are complicated\nSometimes the implementation of random assignment hinges on contingencies that are difficult to model statistically. A common case occurs when researchers test for covariate balance after conducting a random assignment and then redo the randomization if a balance test reveals significant covariate imbalance. Characterizing the sampling distribution of this procedure is challenging, especially if the treatment and control groups contain different numbers of subjects. Fortunately, randomization inference easily generates a reference distribution by simulating admissible random assignments (i.e., those that would have passed the balance test). The same goes for complicated blocking or adaptive designs.\n\n\n8. Randomization inference can be used to address issues of multiple comparisons6\nRandomized trials that involve multiple outcomes, multiple treatments, or multiple subgroup comparisons raise concerns that splashy results may pop up by chance even if there were no treatment effects. (See our guide 10 Things to Know About Multiple Comparisons for more details on this problem.) Randomization inference helps facilitate the evaluation of hypotheses of the form: under the null hypothesis of no treatment effect, what is the probability that at least three outcomes show nominal p-values of less than 0.05? For each random assignment under the sharp null of no treatment effect, calculate the number of apparent p < 0.05 coefficients in order to form the reference distribution. A similar approach may be applied to other targets of inference, such as the p-value of observing two significant treatment-by-covariate interactions or observing four significant treatment arms.\n\n\n9. Randomization inference cannot be applied without additional assumptions to inferences about quantities such as complier average causal effects\nRandomization inference is a flexible method that can accommodate null hypotheses other than the sharp null of no effect for any unit. The methods described above could also be applied to the null hypothesis that all units have a treatment effect of seven.\nOn the other hand, randomization inference cannot be applied with additional assumptions in cases where hypotheses focus on unobserved subgroups, such as compliers in cases of non-compliance with the treatment. We cannot simulate the sampling distribution of the estimated complier average causal effect when we lack knowledge of who the compliers are. (In the case of one-sided noncompliance, we know who the compliers are in the treatment group but not the control group.) In this case, the researcher may have to be content using randomization inference to assess the sharp null hypothesis that the intention-to-treat effect is zero.7\n\n\n10. In some applications, randomization inference may not be worth the trouble\nOld-fashioned approximate methods work well when the assumptions on which the approximations rest are sound. For example, when an experiment involves random assignment of individual subjects, outcomes are distributed more or less symmetrically around the mean, and the number of subjects is greater than 100, the difference between conventional p-values and RI p-values may be negligible. Randomization inference may still be useful as the final word, but it rarely changes inferences substantively under these circumstances. The method is valuable primarily for nonstandard applications in which outcomes are skewed, subject pools are small, or the method of assignment is complex.\nNote on available software for implementing randomization. For the latest R package for randomization inference, see here. For randomization inference code specifically tailored to the special features of binary outcomes, see here. Stata users may find an all-purpose package here.\n\n\nReferences\nChung, EunYi and Joseph P. Romano. 2013. “Exact and asymptotically robust permutation tests.” The Annals of Statistics, 41(2), 484-507.\nGerber, Alan S., and Donald P. Green. 2012. Field Experiments: Design, Analysis, and Interpretation. New York: W.W. Norton.\nSmall, Dylan S., Thomas R. Ten Have, and Paul R. Rosenbaum. 2008. “Randomization Inference in a Group–Randomized Trial of Treatments for Depression: Covariate Adjustment, Noncompliance, and Quantile Effects.” Journal of the American Statistical Association, 103(481), 271-279.\n\n\n\n\n\nFootnotes\n\n\nAcknowledgements: I am grateful to Winston Lin and Gareth Nellis, who commented on an earlier draft.↩︎\nWe focus here on randomization inference as applied to hypothesis testing. Randomization inference may also be used for construction of confidence intervals, but this application requires stronger assumptions. See Green and Gerber (2012), chapter 3.↩︎\nAs explained in other guides, the fundamental problem of causal inference is that we cannot observe what would have happened to those in control group had they be treated, nor can we observe what would have happened to those in the treatment group had they not been treated.↩︎\nOne-tailed tests consider the null hypothesis of no effect against an alternative hypothesis that the average treatment effect is positive (negative). Two-tailed tests evaluate a null hypothesis against the alternative that the ATE is nonzero, whether positive or negative. In that case, the p-value may be assessed by calculating the proportion of simulated random assignments that are at least as large as the observed test statistic in absolute value.↩︎\nSee Chung and Romano (2013). This “studentized” approach makes sense when there is reason to believe that the treatment changes the variance in outcomes in an experiment with different numbers of subjects in treatment and control.↩︎\nSee 10 Things to Know about Heterogeneous Treatment Effects, especially section 7 on multiple comparisons.↩︎\nThe alternative is to make stronger assumptions. See Small, Ten Have, and Rosenbaum (2008).↩︎"
  },
  {
    "objectID": "guides/getting-started/evaluation-conversations_en.html",
    "href": "guides/getting-started/evaluation-conversations_en.html",
    "title": "10 Conversations that Implementers and Evaluators Need to Have",
    "section": "",
    "text": "Abstract\nIf you are a practitioner or official from a governmental or non-governmental organization (an \"implementer\"), this guide is intended to help you to take advantage of opportunities to collaborate with external researchers (\"evaluators\") to evaluate your organization's policies or programs. Researchers working on evaluations can also benefit from this guide by understanding how to take implementers' needs into account.\nAs an implementer, what should you expect or request when collaborating with an external evaluator? How can that evaluation be designed to help your organization and the broader field to learn and innovate? What conversations should you have with an evaluator upfront so that the evaluation runs smoothly? How can you work with the evaluator to decide on and communicate actionable and convincing analyses about the impact of a program? How will you and the evaluator share what you learned to improve the practice of your organization and others? How can you ensure that what you learn helps the whole community of practice so that overall social welfare improves? This short guide offers questions to guide conversations with prospective evaluators, points to some practices that have worked well elsewhere, and explains why they might be important or useful in future evaluations.1\n\n\n1) What is the goal of the evaluation?\nThis may seem blatantly straightforward–the evaluation is to see if the program worked. Yet most policy evaluation efforts involve multiple goals. The program may be in a pilot stage, and you want to gather preliminary evidence before expanding the program. The program may have worked in one context (country, community), and you would like to know how generalizable it is. You may want to know who the program works for. Does it work differently for men or women? Marginalized communities or wealthy and powerful groups? You may want to know how part of your intervention affects other aspects of your existing interventions (e.g., cash vs. cash + training) or what combination of activities are most cost effective. You may want to know if you should try to scale the program. Or you may want to know not so much whether or not the program works, but why it works.\nAdditionally, many programs have multiple components. The program may not only be delivering a service (e.g., distributing information on how to access health services), but it may also be trying to build the capacity of a local agency; (e.g., help a public health agency to identify households for additional forms of support ). As a result, there are potentially both individual-level (i.e., micro) questions–the effect of school lunches on children–and more macro questions–how are local governments better able to identify vulnerable children?–that need answers. While both may be asked in one evaluation, the evaluators need to understand how the implementer prioritizes the two questions. This prioritization will affect the evaluation design (see the next Conversation).\nAs implementers, it's important to be clear on your goal of what you want to learn among yourselves and be clear about those goals with the evaluation team. The evaluation team or person, particularly if they are an academic or publish their work more widely, will also have goals about what they want to learn. Without being clear about what you and your organization need to learn, the evaluator may not design the most appropriate evaluation method. This takes us to the next question.\n\n\n2) What is the best method for the evaluation?\nSince evaluation efforts may serve multiple purposes for different audiences, no single research design is always best. We suggest that evaluation and implementation teams not take for granted any given approach: some evaluation teams specialize in human-centered design research which is very similar to ethnographic research in the social sciences; other teams specialize in randomized controlled trials (RCTs). The fact that any given team has expertise in one particular mode of research should not outweigh the need to connect purposes to evaluation design. We have seen such conversations yield realizations that, in fact, an \"evaluation\" ought to be a multi-step process with learning about the context and the intervention occurring along the way –- more akin to adding evidence to hone a learning agenda than to report on the estimated causal effect of a given intervention in a given place and time.\nOpenness about the evaluation approach can also prepare teams to adapt as new information arises. For example, at the end of an RCT one of the authors conducted in Nigeria, we learned that the program diffused from direct participants to some non- participants. However, we did not know why and could immediately think of competing explanations after the fact: Was it a result of non-participants witnessing cooperation among direct participants? Did the people who directly participated talk about their experience with non-participants? Did norms in these communities change? Pairing this work with interviews and observations earlier in the process would have helped us understand why we found the effect, and then we could have better tested these new ideas with follow-up studies.\nRCTs can be used to test insights that are derived from more unstructured observation. For example, the Los Angeles-based campaign specialist, David Fleischer, noticed from his own experience and those of others that certain interactions during door-to-door canvassing appeared to be particularly compelling for changing people's minds about sensitive topics. Communications between him and a pair of academics led to a series of RCTs to verify and unpack the mechanisms behind this phenomenon (called \"deep canvassing\").\nThis conversation about approach, method, and maybe iteration, between the evaluator and the implementer, can also help identify whether the intervention can be conducted in a way to better answer the questions of interest. For example, is there a way to phase in or layer parts of the program so you could test mechanisms? How might you recruit individuals into the program so that it is more representative of the populations you care about, or so that the results will be generalizable? Our general theme here (and elsewhere in this guide) is that the most useful evaluations often occur when both the evaluation and the program design are created collaboratively (see Conversation 6 below).\n\n\n3) Who are the stakeholders?\nOften there are multiple stakeholders both within the organization whose work is subject to the evaluation, but also externally (not to mention the evaluator themself, and the audiences they may want to influence by publicizing the results). Within an organization, some people work across contexts and want to understand how a program may vary in its results across those contexts. Some people implement in a specific context and want to know how to do it better in that place. Then there may be executives who are focused on external influence and raising the profile of the organization versus the program managers who are trying to use the information to adapt their programming. For example, when one of the authors was working on an evaluation related to host and refugee relationships, the HQ team cared most about the pooled results across Lebanon and Jordan. Such results allowed them to speak to the larger issue of host-refugee relations. In contrast, the field teams cared more about country-specific data so they could adapt their programming. The evaluation team worked with both the field and HQ teams to prioritize which results to produce first. In this case, the different needs within an organization did not change the evaluation design. But one could easily imagine a situation where an evaluation designed to be sensitive to the effects of a program in one place would be inappropriate in another context. If a multi-context analysis is of primary importance, then one will be glad to have had this conversation early in the design process rather than discovering that one cannot easily combine datasets.\nBoth implementers and evaluators who are committed to benefiting the larger social good will also want to influence external stakeholders, particularly policymakers. Discussing who these people are and what questions and information they will find most persuasive needs to happen not just after the data is analyzed, but at the design phase. Otherwise, the implementing organization may be disappointed that the results are not able to speak to certain debates and as a result may be less likely to invest in evidence generation in the future. We discuss more about publicizing results below.\n\n\n4) How can our evaluation enable learning within our organization?\nAn organization decides to evaluate the performance of a new (or old) policy because its members desire to learn how to improve. We state this point first and foremost because resistance to evaluation often arises from different stakeholders who think of the word \"evaluate\" and connect it with \"grade\" or \"rank\" or other attempts to measure that often distribute rewards or punishments. An organization that primarily evaluates as a form of ranking or grading will quickly run into resistance and all of the problems associated with replacing internal motivations with external carrots and sticks — carrots and sticks are blunt instruments that rarely lead to the best performance from anyone.\nIf, however, an organization uses evaluations to learn, then the structure of an evaluation should take the desire to learn into account. Some questions that might well orient any such evaluation include:\n\nWhat decisions will the organization make in the short term which would change depending on this evaluation?\nWhat hypothetical results from this evaluation could contribute to which pending and consequential decisions by the organization?\n\nEven when the purpose is to learn, not grade, some within an organization may still be fearful of the results, as they believe their careers are tied to achieving strong results. If they are not bought into the learning agenda, they may resist cooperation with the evaluators or refuse to use the results of the evaluation, limiting the evaluation's utility. Ensuring that the questions the team closest to the ground wants to learn about are incorporated into the evaluation helps ensure the success both in evaluating the program, and that people will use the results of the evaluation to learn and improve future programs.\n\n\n5) How might this evaluation connect to a broader and shared learning agenda?\nOne way to think about learning for an organization is to think about its high level goals: What is the mission of your organization and what part of that mission do you want your programs to contribute to? For example, say a county government has an overall goal of reducing racial inequality. Next imagine that you are implementing a program which provides reduced bus fares with the assumption that reduced bus fares increase employment opportunities with higher wages. You want to test this assumption, and evaluate whether the subsidies contributed to the larger goal of reducing racial inequality. In that case, one might imagine a figure or diagram illustrating this theory of change that by reducing bus fares, People of Color are better able to commute to jobs with higher wages, thus reducing racial inequality. The implementer can then work with the evaluator to design the evaluation to learn about (a) the effects of reducing bus fares on employment options and (b) about how reducing bus fares affects our overall goal of reducing racial inequality in our county.\"\nBy having this type of conversation–about how a specific program or policy may contribute to larger change–there is a recognition that one evaluation or one program is part of a larger learning agenda. This conversation then can shape a deeper collaboration between the implementer and evaluator (see the next Conversation). Evaluators are often based in academia and bring a passion to engage pressing questions of theory to their work with the hope that they uncover some underlying and generalizable truths. They also have read extensively in the field, and may be able to bring additional evidence to bear on your policy or program. If the high level learning goals of a community of evaluators and implementers can connect, then one hopes the results of individual evaluations more easily connect and cumulate into even more effective policy creation and implementation.\n\n\n6) On what parts of the project should we collaborate?\nIn our experience, when a new policy design arises from a creative collaboration between evaluators (who are often academics) and implementers, everyone wins. Often, for example, a better evaluation design can be incorporated into the implementation if discussed early, in a way that creates fewer burdens on the implementer and/or those benefiting from the new policy (e.g., aligning data collection or phasing in of intervention sites). Another benefit arising from such creative co-creation is that different people will bring different perspectives and evidence-bases to the table for the design of both the program and the evaluation. For example, the U.S. government's Office of Evaluation Sciences (OES) has institutionalized this process. They write:\nOur collaborators, who are civil servants with years of experience working to deliver programs across the government, are experts on how their programs work and often have the best ideas for how to improve them. OES team members support their efforts by bringing diverse academic and applied expertise to more deeply understand program bottlenecks and offer recommendations drawn from peer-reviewed evidence in the social and behavioral sciences.\nThe Immigration Policy Lab at Stanford uses this co-creation process as a basis for their collaboration with implementers. They work with organizations not only to research policies and programs but work with the implementer from the start. Through this process, evaluators bring the latest evidence related to a specific policy issue (e.g., refugee resettlement) and co-design the intervention with the implementer utilizing the best evidence with on-the-ground experience. Together, they then evaluate the intervention, adding to the knowledge on immigration policies.\nOther evaluations involve an evaluator brought on after the fact — perhaps the funders and/or the implementer desire a fresh perspective on the data and design, or the original implementation was rushed. Either way, \"how and on which parts should we collaborate\" is still a crucial conversation to have.\n\n\n7) How will we communicate within and across teams?\nCollaboration requires communication. This means that the parties to an evaluation agreement must agree to speedy feedback in mutually agreed upon forms and processes for how to do so. For example, some complex evaluations might create a quick web dashboard so that sample sizes and implementation can be easily seen by the whole team. Other, simpler projects may agree to use existing tools for project management and communication. Not everyone checks email all the time. And many people silence their phones as they try to focus. Having this conversation about communication allows new partners to avoid misunderstanding email or Slack silence.\nCalendars and deadlines are also an important part of this conversation. During certain periods, evaluators may be less available (e.g, when grades or a grant application is due); the implementer may need at least preliminary results for an important donor meeting. Knowing these time windows can help understand availability, know when responses may be slow, and allow both sides to plan accordingly.\nThe communication discussion is also an opportunity to envision the final products: the one-pager, the anticipated three challenges and hoped for three successes for the external-facing report, the number of drafts of the different final projects the team expects to go through, and who is expected to take the lead on which ones. This might be a time where evaluators and implementers share draft reports or report templates that they have liked in the past.\n\n\n8) How and where shall we make the results of this study public? (i.e., communicate externally)\nA well-done public evaluation report is a gift to humanity.\nConsider who will benefit from a public report of positive, negative, or null results:\n\nFuture members of your organization will be able to use the information to craft better policies.\nMembers of other organizations like yours and funders can also learn. If they share their results publically then policy learning can occur more quickly than if each organization or funder has to pay for and field its own evaluation.\nScientists and others who focus on theoretical explanations will move more quickly and will generate more ideas if they can see whether certain ideas work or not (or how they work). Published results and analysis plans help spur learning by the originating organization, by other organizations, and by science.\n\nPublishing results of evaluations also helps your organization in two important and related ways. 1) It enhances your organization's influence with donors and other policymakers, as you can inform key debates with evidence. 2) It enhances your organization's reputation as trustworthy and willing to provide a public good to improve the field. When one of the authors of this guide was working inside an implementing organization, donors often complimented the organization's commitment to sharing results that contradicted conventional wisdom.\nMoreover, publishing all results helps quell other criticisms that can be aimed at organizations that are contributing to knowledge generation. For example, some criticize evidence-based policy by calling it \"policy-based evidence.\" The criticism suggests that evaluations serve only to add a veneer of respectability to the pre-conceived notions of organizational leaders. This idea — that careful research and analyses are merely rhetoric — can diminish trust in individual organizations and in government and science as institutions. Instead, if an organization says, \"We publish all of our evaluations and invite others to scrutinize our results and join us in learning how to better serve the public,\" then it is hard for detractors to claim that the organization is hiding the truth and hard for others to pressure the organization to hide the truth in turn.\nThe timelines that implementers and evaluators, especially ones who come from academia, have for publishing results are likely different. Your organization may want to use or share the results as soon as possible, so that you can improve your programming or influence policy. However, cleaning and analyzing data takes time. And a review of the results before publishing can improve the quality of the presentation and analysis. Some academic publications prefer that the results not be shared publicly before publication in other formats. Given these benefits and constraints, it's important to talk with the evaluator about how to share the results and when and how.\n\n\n9) What if things do not go as expected or hoped?\nIn conducting an evaluation, numerous things may not go according to plan. There are many operational issues, and since an evaluation can be a multi-month or multi-year process, complications always arise. There may be staffing changes, and their commitment to the evaluation may vary. Or new staff members are interested in different questions. The context may change and the teams have to adapt. The Covid pandemic is a good example of this, as programming and how data was collected had to change practically overnight. Good communication within the team (see above) is essential for navigating these unexpected issues that will arise in some form, and ensuring the integrity of the program and evaluation.\nIn addition to these operational issues, we may find unexpected results from the evaluation. The evaluation may find that the program or policy had little effect. While null results can be disappointing, in of themselves, they can provide important learning. If the results show null effects, then the organization can re-evaluate and generate more ideas for how to implement the program. Null results can encourage more learning than one might think when they are combined with prior beliefs. Also, null results can arise for reasons that have more to do with sample size or other artifacts of the evaluation (e.g., how the outcomes were operationalized) than with the impact of the intervention. A null result arising from an intervention that, in theory, really should have had a big effect, can be particularly fruitful for science — it sends scientists back to the drawing board and forces re-evaluation of well-established theories. Perhaps those theories had only been tested in the context of university laboratories rather than in the real world. Or perhaps the real world in which the theories had been assessed in the past has now changed. See for example the handout on How to Use Unexpected and Null Results by the OES and our 10 Things Your Null Result Might Mean methods guide.\nWhile nulls or other unintended impacts provide important learning for an organization, they can create uncomfortable conversations with the evaluator and internally within an organization. As a result, thinking through these possibilities ahead of time can be very useful so that people are not surprised. There are a couple of tools that help the implementer and evaluator think through these together.\nOne approach arising from project management in business involves the creation of pre-mortems in which the implementer and evaluator begin by imagining that the evaluation has returned a null or negative result. What might have led to this result? The teams list the possibilities that they can imagine and thereby become prepared for those and similar problems.\nAnother related approach from public health uses the name 'Dark Logic' to evoke the idea that one uses imagination and logic together to create negative scenarios, which in turn may (1) help an organization and evaluator avoid worst-case scenarios and (2) prepare an organization to react to such scenarios should they occur.\nA third approach that is becoming the norm for experimental research in political science, economics, and social psychology is to use a pre-analysis plan. People can sometimes disregard an evaluation if the results do not confirm their beliefs. A pre-analysis plan helps prevent criticism of the results of the evaluation based on methods or analysis choices. See 10 Things to Know about Pre-Analysis Plans for more on why and how. See also Preregistration as a Tool for Strengthening Federal Evaluation from OES.\nA pre-analysis plan also can help stakeholders within an organization, perhaps stakeholders with conflicting prior beliefs, think through what analyses they would find convincing before the study has been fielded/data have been analyzed. For example, an evaluator could generate hypothetical tables and figures and show them to stakeholder meetings asking: \"What would your reaction be to a figure like this?\" This process helps stakeholders begin to assess the evaluation questions in more detail, ensure that measures are operationalized appropriately, and consider the level of implementation that is needed for the desired results. It also helps stakeholders become aware of the possibility that we may learn something unexpected.\n\n\n10) How can our evaluation contribute to the social good?\nA premise of this guide is that the evaluation will contribute to the social good. Yet we think that a conversation about this more generally can help implementers and evaluators collaborate more effectively by providing a superordinate goal to unite the whole team. Further, we encourage this conversation to ensure that all of the evaluation effort adds to the overall efforts to improve the world.\nAlthough we exhort such conversations in general terms, we also think that an evaluation can add to the social good fairly directly via transparency of methods and, to the extent possible, by sharing data. That is, in addition to the evaluation itself, the data you collect as part of the evaluation, the plans you make, the code the evaluators write, is also a social good. In the social sciences, there is a movement towards more openness of data–sharing the data and how it is analyzed – as an effort to speed scientific learning. This, in addition to a pre-analysis plan or pre-registration, is becoming required of many journals. We see this kind of transparency as adding to the impact of an evaluation outside of a given organization and providing a net benefit for your organization and the field for several reasons.2\n\nEfficiency of time and money: Collecting data is time consuming and expensive. The data you collect may be analyzed by others in additional ways to answer different questions, and help explore potential innovations.\nEnhances data quality: In some places, participants in programs may suffer from being over-surveyed. By sharing data, it reduces the need to ask people the same questions repeatedly. This helps reduce survey fatigue and enhances data quality.\nIncreases confidence in results: Others can replicate your findings with the same or different analysis choices. Real world data analysis involves many choices, and data flows from the world of administrative data, surveys, and interviews into the hands of the analyst via a twisting and turning path. Some of the reasonable choices made by one analyst may not be the reasonable choices made by another. Since both choices may be reasonable, and even correct, it is worth having a second team, blind to the results of the first analysis, follow the analysis plan following their own interpretation. The OES have institutionalized this practice: See \"Step 5: Ensure our work meets evaluation best practice\" of the OES Project Process for their re-analysis report document and explanation.\nIncreases the trust in your organization: That your organization is willing to share data demonstrates that you have nothing to hide and that you are willing to contribute to the public good.\n\nFor more on Open Science, see 10 Things to Know About Project Workflow which is based on the more fun titled paper \"How to Improve Your Relationship With Your Future Self.\"\n\n\n\n\n\nFootnotes\n\n\nMuch of this document is inspired by the Project Process of the OES as well as discussions hosted by the Causal Inference for Social Impact project at CASBS and the Evidence in Governance and Politics network. See the MIT Gov Lab Guide to Difficult Conversations for more guidance about academic-practitioner collaborations as well as the Research4Impact findings about cross-sector collaborations. We anticipate that this document will be open source and revised over time based on your comments and suggestions. Thanks much to Carrie Cihak, Matt Lisiecki, Ruth Ann Moss, Betsy Rajala, Cyrus Samii, Rebecca Thornton,, and folks at the organizations listed above for helpful comments.↩︎\nWe do recognize that some donors may not allow this (though many are becoming proponents of it), and the competitive nature of fundraising may make sharing data seem risky, especially as a first mover.↩︎"
  },
  {
    "objectID": "guides/getting-started/effect-types_en.html",
    "href": "guides/getting-started/effect-types_en.html",
    "title": "10 Types of Treatment Effect You Should Know About",
    "section": "",
    "text": "Abstract\nThis guide1 describes ten distinct types of causal effects that researchers may want to estimate. As discussed in our guide 10 Things to Know About Causal Inference, simple randomization allows one to produce estimates of the average of the unit level causal effects in a sample. This average causal effect or average treatment effect (ATE) is a powerful concept because it is one solution to the problem of not observing all relevant counterfactuals. Yet, it is not the only productive engagement with this problem. In fact, there are many different types of quantities of causal interest. The goal of this guide is to help you choose estimands (a parameter of interest) and estimators (procedures for calculating estimates of those parameters) that are appropriate and meaningful for your data.\n\n\n1 Average Treatment Effects\nWe begin by reviewing how, with randomization, a simple difference-of-means provides an unbiased estimate of the ATE. We take extra time to introduce some common statistical concepts and notation used throughout this guide.\nFirst we define a treatment effect for an individual observation (a person, household, city, etc.) as the difference between that unit’s behavior under treatment \\((Y_{i}(1))\\) and control \\((Y_{i}(0))\\):\n\\[τ_{i}=Y_{i}(1)−Y_{i}(0)\\]\nSince we can only observe either \\(Y_{i}(1)\\) or \\(Y_{i}(0)\\) the individual treatment effect is unknowable. A quantity that we can learn about, however, is the average treatment effect (ATE) across all observations in our experiment:\n\\[ATE≡\\frac{1}{N}∑^{N}_{i=1}τ_{i}=\\frac{∑^{N}_{1}Y_{i}(1)}{N}−\\frac{∑^{N}_{1}Y_{i}(0)}{N}\\]\nLet \\(D_{i}\\) be an indicator for whether we observe an observation under treatment or control. If treatment is randomly assigned, \\(D_{i}\\) is independent, not only of potential outcomes but also of any covariates (observed and unobserved) that might predict those outcomes \\(((Y_{i}(1),Y_{i}(0),X_{i}⊥⊥D_{i}))\\).2\nSuppose our design involves \\(m\\) units under treatment and \\(N−m\\) under control. Suppose we were to repeatedly reassign treatment at random many times and each time calculate the difference of means between treated and control groups and then to record this value in a list. In other words, for every repetition, we produce an estimate of the ATE using the the observed difference in means:3\n\\[\\widehat{ATE} =\\frac{∑^m_1Z_{i}Y_{i}}{m}−\\frac{∑^{N}_{m+1}(1−Z_{i})Y_{i}}{N−m}\\]\nThe average of the estimates in our list will be the same as the difference of the means of the true potential outcomes had we observed the full schedule of potential outcomes for all observations. That is \\(E(Y_i(1)|D=1)=E(Y_i(1)|D=0)=E(Y_i(1))\\) and \\(E(Y_i(0)|D=1)=E(Y_i(0)|D=0)=E(Y_i(0))\\). Another way to state this characteristic of the average treatment effect and its estimator is to say that the difference of observed means is an unbiased estimator of the average treatment effect.\nStatistical inference about the estimated ATE requires that we know how it will vary across randomizations. It turns out that we can write the variance of the ATE across randomizations as follows: \\(V(ATE) = \\frac{N}{N−1} [\\frac{V(Y_{i}(1))}{m}+\\frac{V(Y_{i}(0))}{N−m}]−\\frac{1}{N−1}[V(Y_{i}(1))+V(Y_{i}(0))−2∗Cov(Y_{i}(1),Y_{i}(0))]\\), and estimate this quantity from the sample estimates of the variance in each group.4\nA linear model regressing the observed outcome \\(Y_{i}\\) on a treatment indicator \\(D_{i}\\) provides a convenient estimator of the ATE (and with some additional adjustments, the variance of the ATE):\n\\[Y_{i}=Y_{i}(0)∗(1−D_{i})+Y_{i}(1)∗D_{i}=β_{0}+β_{1}D_{i}+u\\]\nsince we can rearrange terms so that \\(β_{0}\\) estimates the average among control observations \\((Y_{i}(0)∣D_{i}=0)\\) and \\(β_{1}\\) estimates the differences of means \\((Y_{i}(1)∣D_{i}=1)–(Y_{i}(1)∣D_{i}=0)\\). In the code below, we create a sample of 1,000 observations and randomly assign a treatment \\(D_i\\) with a constant unit effect to half of the units. We estimate the ATE using ordinary least squares (OLS) regression to calculate the observed mean difference. Calculating the means in each group and taking their difference would also produce an unbiased estimate of the ATE. Note that the estimated ATE from OLS is unbiased, but the errors in this linear model are assumed to be independent and identically distributed. When our treatment effects both the average value of the outcome and the distribution of responses, this assumption no longer holds and we need to adjust the standard errors from OLS using a Huber-White sandwich estimator to obtain the correct estimates (based on the variance of the ATE) for statistical inference.5 Finally, we also demonstrate the unbiasedness of these estimators through simulation.\n\nset.seed(1234) # For replication\nN = 1000 # Population size\nY0 = runif(N) # Potential outcome under control condition\nY1 = Y0 + 1 # Potential outcome under treatment condition\nD = sample((1:N)%%2) # Treatment: 1 if treated, 0 otherwise\nY = D*Y1 + (1-D)*Y0 # Outcome in population\nsamp = data.frame(D,Y)\nATE = coef(lm(Y~D,data=samp))[2] #same as with(samp,mean(Y[Z==1])-mean(Y[Z==0]))\n# SATE with Neyman/Randomization Justified Standard Errors\n# which are the same as OLS standard errors when no covariates or blocking\nlibrary(lmtest)\nlibrary(sandwich)\nfit<-lm(Y~D,data=samp)\ncoef(summary(fit))[\"D\",1:2]\n\n  Estimate Std. Error \n1.01820525 0.01841784 \n\nATE.se<-coeftest(fit,vcovHC(fit,type=\"HC2\"))[\"D\",2]\n# same as with(samp,sqrt(var(Y[D==1])/sum(D)+var(Y[D==0])/(n-sum(D)))\n# Assess unbiasedness and simulate standard errors\ngetATE<-function() {\n  D = sample((1:N)%%2) # Treatment: 1 if treated, 0 otherwise\n  Y = D*Y1 + (1-D)*Y0\n  coef(lm(Y~D))[[\"D\"]]\n}\nmanyATEs<-replicate(10000,getATE())\n## Unbiasedness:\nc(ATE=mean(Y1)-mean(Y0), ExpEstATE=mean(manyATEs))\n\n      ATE ExpEstATE \n1.0000000 0.9999077 \n\n## Standard Error\n### True SE formula\nV<-var(cbind(Y0,Y1))\nvarc<-V[1,1]\nvart<-V[2,2]\ncovtc<-V[1,2]\nn<-sum(D)\nm<-N-n\nvarestATE<-((N-n)/(N-1))*(vart/n) + ((N-m)/(N-1))* (varc/m) + (2/(N-1)) * covtc\n### Compare SEs\nc(SimulatedSE= sd(manyATEs), TrueSE=sqrt(varestATE), ConservativeSE=ATE.se)\n\n   SimulatedSE         TrueSE ConservativeSE \n    0.01835497     0.01842684     0.01841784 \n\n\n\n\n2 Conditional Average Treatment Effects\nThe problem with looking at average treatment effects only is that it takes attention away from the fact that treatment effects might be very different for different sorts of people. While the “fundamental problem of causal inference” suggests that measuring causal effects for individual units is impossible, making inferences on groups of units is not.\nRandom assignment ensures that treatment is independent of potential outcomes and any (observed and unobserved) covariates. Sometimes, however, we have additional information about the experimental units as they existed before the experiment was fielded, say \\(X_{i}\\), and this information can can help us understand how treatment effects vary across subgroups. For example, we may suspect that men and women respond differently to treatment, and we can test for this hetorogeneity by estimating conditional ATE for each subgroup separately \\((CATE=E(Y_{i}(1)−Y_{i}(0)∣D_{i},X_{i}))\\). If our covariate is continous, we can test its moderating effects by interacting the continous variable with the treatment. Note, however, that the treatment effect is now conditional on both treatment status and the value of the conditioning variable at which the effect is evaluated and so we must adjust our interpretation and standard errors accordingly.6\nA word of warning: looking at treatment effects across dimensions that are themselves affected by treatment is a dangerous business and can lead to incorrect inferences. For example if you wanted to see how administering a drug led to health improvements you could run separate analyses for men and women, but you could not run separate analyses for those who in fact took the drug and those who did not (this is an example of inference for compliers which requires separate techniques described in point 4 below).\n\n\n3 Intent-to-Treat Effects\nOutside of a controlled laboratory setting, the subjects we assign to treatment often are not the same as the subjects who actually receive the treatment. When some subjects assigned to treatment fail to receive it, we call this an experiment with one-sided non-compliance. When additionally, some subjects assigned to control also receive the treatment, we say there is two-sided non-compliance. For example, in a get-out-the-vote experiment, some people assigned to receive a mailer may not receive it. Perhaps they’ve changed addresses or never check their mail. Similarly, some observations assigned to control may receive the treatment. Perhaps they just moved in, and the previous tenant’s mail is still arriving.\nWhen non-compliance occurs, the receipt of treatment is no longer independent of potential outcomes and confounders. The people who actually read their mail probably differ in a number of ways from the people who throw their mail away (or read their neighbors’ mail) and these differences likely also affect their probability of voting. The difference-of-means between subjects assigned to treatment and control no longer estimates the ATE, but instead estimates what is called an intent-to-treat effect (ITT). We often interpret the ITT as the effect of giving someone the opportunity to receive treatment. The ITT is particularly relevant then for assessing programs and interventions with voluntary participation.\nIn the code below, we create some simple data with one-sided non-compliance. Although the true treatment effect for people who actually received the treatment is 2, our estimated ITT is smaller (about 1) because only some of the people assigned to treatment actually receive it.\n\nset.seed(1234) # For replication\nn = 1000 # Population size\nY0 = runif(n) # Potential outcome under control condition\nC = sample((1:n)%%2) # Whether someone is a complier or not\nY1 = Y0 + 1 +C # Potential outcome under treatment\nZ = sample((1:n)%%2) # Treatment assignment\nD = Z*C # Treatment Uptake\nY = D*Y1 + (1-D)*Y0 # Outcome in population\nsamp = data.frame(Z,Y)\nITT<-coef(lm(Y~Z,data=samp))[2]\n\n\n\n4 Complier Average Treatment Effects\nWhat if you are interested in figuring out the effects of a treatment on those people who actually took up the treatment and not just those people that were administered the treatment? For example what is the effect of radio ads on voting behavior for those people that actually hear the ads?\nThis turns out to be a hard problem (for more on this see our guide 10 Things to Know About the Local Average Treatment Effect). The reasons for non-compliance with treatment can be thought of as an omitted variable. While the receipt of treatment is no longer independent of potential outcomes, the assignment of treatment status is. As long as random assignment had some positive effect on the probability of receiving treatment, we can use it as an instrument to identify the effects of treatment on the sub-population of subjects who comply with treatment assignment.\nFollowing the notation of Angrist and Pischke (2008) let \\(Z\\) be an indicator for whether an observation was assigned to treatment and \\(D_{i}\\) indicates whether that subject actually received the treatment. Experiments with non-compliance are composed of always-takers (\\(D_{i}=1\\), regardless of \\(Z_{i}\\)), never-takers (\\(D_{i}=0\\) regardless of \\(Z_{i}\\)), and compliers (\\(D_{i}=1\\) when \\(Z_{i}=1\\) and \\(0\\) when \\(Z_{i}=0\\)).7 We can estimate a complier average causal effect (CACE), sometimes also called a local average treatment effect (LATE), by weighting the ITT (the effect of \\(Z\\) on \\(Y\\)) by the effectiveness of random assignment on treatment uptake (the effect of \\(Z\\) on \\(D\\)).\n\\[CACE= \\frac{Effect of Z on Y}{Effect of Z on D}=\\frac{E(Y_i∣Z_i=1)-E(Y_i|Z_i=0)}{E(D_i|Z_i=1)-E(D_i|Z_i=0)}\\]\nThe estimator above highlights the fact that the ITT and CACE converge as we approach full compliance. Constructing standard errors for ratios is somewhat cumbersome and so we usually estimate a CACE using two-stage-least-squares regression with random assignment, \\(Z_i\\), serving as instrument for treatment receipt \\(D_i\\) in the first stage of the model. This approach simplifies the estimation of standard errors and allows for the inclusion of covariates as additional instruments. We demonstrate both strategies in the code below for data with two-sided non-compliance. Note, however, that when instruments are weak (e.g. random assignment had only a small effect on the receipt of treatment), instrumental variable estimators and their standard errors can be biased and inconsistent.8\n\nset.seed(1234) # For replication\nn = 1000 # Population size\nY0 = runif(n) # Potential outcome under control condition\nY1 = Y0 + 1 # Potential outcome under treatment\nZ = sample((1:n)%%2) # Treatment assignment\npD<-pnorm(-1+rnorm(n,mean=2*Z)) # Non-compliance\nD<-rbinom(n,1,pD) # Treatment receipt with non-compliance\nY = D*Y1 + (1-D)*Y0 # Outcome in population\nsamp = data.frame(Z,D,Y)\n# IV estimate library(AER) CACE = coef(ivreg(Y ~ D | Z, data = samp))[2]\n# Wald Estimator ITT<-coef(lm(Y~Z,data=samp))[2] ITT.D<-coef(lm(D~Z,data=samp))[2] CACE.wald<-ITT/ITT.D\n\n\n\n5 Population and Sample Average Treatment Effects\nOften we want to generalize from our sample to make statements about some broader population of interest.9 Let \\(S_i\\) be an indicator for whether an subject is in our sample. The sample average treatment effect (SATE) is defined simply as \\(E(Y_i(1)−Y_i(0)|S_i=1)\\) and the population \\(E(Y_i(1)−Y_i(0))\\). With a large random sample from a well-defined population with full compliance with treatment, our SATE and PATE are equal in expectation and so a good estimate for one (like a difference of sample means) will be a good estimate for the other.10\nIn practice, the experimental pool may consist of a group of units selected in an unknown manner from a vaguely defined population of such units and compliance with treatment assignment may be less than complete. In such cases our SATE may diverge from the PATE and recovering estimates of each becomes more complicated. Imai, King, and Stuart (2008) decompose the divergence between these estimates into error that arises from sample selection and treatment imbalance. Error from sample selection arises from different distributions of (observed and unobserved) covariates in our sample and population. For example people in a medical trial often differ from the population for whom the drug would be available. Error from treatment imbalance reflects differences in covariates between treatment and control groups in our sample, perhaps because of non-random assignment and/or non-compliance.\nWhile there are no simple solutions to the problems created by such error, there are steps you can take in both the design of your study and the analysis of your data to address these challenges to estimating the PATE or CACE/LATE. For example, including a placebo intervention provides additional information on the probability of receiving treatment, that can be used to re-weight the effect of actually receiving it (e.g Nickerson (2008)) in the presence of non-compliance. One could also use a model to re-weight observations to adjust for covariate imbalance and the unequal probability of receiving the treatment, both within the sample and between a sample and the population of interest.11\nIn the code below, we demonstrate several approaches to estimating these effects implemented in the CausalGAM package for R.12 Specifically, the package produces regression, inverse-propensity weighting (IPW), and augmented inverse-propensity weighting estimates of the ATE. Combining regression adjustment with IPW, the AIPW has the feature of being “doubly robust” in that the estimate is still consistent even if we have incorrectly specified either the regression model or the propensity score for the probability weighting.\n\n# Example adapted from ?estimate.ATE\nlibrary(CausalGAM)\nset.seed(1234) # For replication\nn = 1000 # Sample size\nX1 = rnorm(n) # Pre-treatment covariates\nX2 = rnorm(n)\np = pnorm(-0.5 + 0.75*X2) # Unequal probabilty of Treatment\nD = rbinom(n, 1, p) # Treatment\nY0 = rnorm(n) # Potential outcomes\nY1 = Y0 + 1 + X1 + X2\nY = D*Y1 + (1-D)*Y0 # Observed outcomes\nsamp = data.frame(X1,X2,D,Y)\n# Estimate ATE with AIPW, IPW, Regression weights\nATE.out <- estimate.ATE(pscore.formula = D ~ X1 +X2,\n                        pscore.family = binomial,\n                        outcome.formula.t = Y ~ X1\n                        +X2,\n                        outcome.formula.c = Y ~ X1\n                        +X2,\n                        outcome.family = gaussian,\n                        treatment.var = \"D\",\n                        data=samp,\n                        divby0.action=\"t\",\n                        divby0.tol=0.001,\n                        var.gam.plot=FALSE, nboot=50)\n\n\n\n6 Average Treatment Effects on the Treated and the Control\nTo evaluate the policy implications of a particular intervention, we often need to know the effects of the treatment not just on the whole population but specifically for those to whom the treatment is administered. We define the average effects of treatment among the treated (ATT) and the control (ATC) as simple counter-factual comparisons:\n\\[ATT=E(Y_i(1)-Y_i(0)|D_i=1)=E(Y_i(1)|D_i=1)-E(Y_i(0)|D_i=1)\\] \\[ATC=E(Y_i(1)-Y_i(0)|D_i=0)=E(Y_i(1)|D_i=0)-E(Y_i(0)|D_i=0)\\]\nInformally, the ATT is the effect for those that we treated; ATC is what the effect would be for those we did not treat.\nWhen treatment is randomly assigned and there is full compliance, \\(ATE=ATT=ATC\\), since \\(E(Y_i(0)∣D_i=1)=E(Y_i(0)∣D_i=0)\\) and \\(E(Y_i(1)∣D_i=0)=E(Y_i(1)∣D_i=1)\\) Often either because of the nature of the intervention or specific concerns about cost and ethnics, treatment compliance is incomplete and the ATE will not in general equal the ATT or ATC. In such instances, we saw in the previous section that we could re-weight observations by their probability of receiving the treatment to recover estimates of the ATE. The same logic can be extended to produce estimates of the ATT and ATC in both our sample and the population.13\nBelow, we create a case where the probability of receiving treatment varies but can be estimated using a propensity score model.14 The predicted probabilities from this model are then used as weights to recover the estimates of the ATE, ATT, and ATC. Inverse propensity score weighting attempts to balance the distribution of covariates between treatment and control groups when estimating the ATE. For the ATT, this weighting approach treats subjects in the treated group as a sample from the target population (people who received the treatment) and weights subjects in the control by their odds of receiving the treatment. In a similar fashion, the estimate of the ATC weights treated observations to look like controls. The quality (unbiasedness) of these estimates is inherently linked to the quality of our models for predicting the receipt of treatment. Inverse propensity score weighting and other procedures produce balance between treatment and control groups on observed covariates, but unless we have the “true model” (and we almost never know the true model) the potential for bias from unobserved covariates remains and should lead us to interpret our estimates of the ATT or ATC in light of the quality of the model that produced it.\n\nset.seed(1234) # For replication\nn = 1000 # Sample size\nX1 = rnorm(n) # Pre-treatment covariates\nX2 = rnorm(n)\np = pnorm(-0.5 + 0.75*X2) # Unequal probabilty of Treatment\nD = rbinom(n, 1, p) # Treatment\nY0 = rnorm(n) # Potential outcomes\nY1 = Y0 +1 +X1 +X2\nY = D*Y1 + (1-D)*Y0 # Observed outcomes\nsamp = data.frame(X1,X2,D,Y)\n# Propensity score model\nsamp$p.score<-\npredict(glm(D~X1+X2,samp,family=binomial),type=\"response\")\n# Inverse Propability Weights\nsamp$W.ipw<-with(samp, ifelse(D==1,1/p.score,1/(1-p.score)))\nsamp$W.att<-with(samp, ifelse(D==1,1,p.score/(1-p.score)))\nsamp$W.atc<-with(samp, ifelse(D==1,(1-p.score)/p.score,1))\n# IPW: ATE, ATT, ATC\nATE.ipw<-coef(lm(Y~D,data=samp,weights=W.ipw))[2]\nATT.ipw<-coef(lm(Y~D,data=samp,weights=W.att))[2]\nATC.ipw<-coef(lm(Y~D,data=samp,weights=W.atc))[2]\n\n\n\n7 Quantile Average Treatment Effects\nThe ATE focuses on the middle, in a way on the effect for a typical person, but we often also care about the distributional consequences of our treatment. We want to know not just whether our treatment raised average income, but also whether it made the distribution of income in the study more or less equal.\nClaims about distributions are difficult. Even though we can estimate the ATE from a difference of sample means, in general, we cannot make statements about the joint distribution of potential outcomes \\((F(Yi(1),Yi(0)))\\) without further assumptions. Typically, these assumptions either limit our analysis to a specific sub-population15 or require us to assume some form of rank invariance in the distribution of responses to treatment effects.16\nIf these assumptions are justified for our data, we can obtain consistent estimates of quantile treatment effects (QTE) using quantile regression.17 Just as linear regression estimates the ATE as a difference in means (or, when covariates are used in the model, from a conditional mean), quantile regression fits a linear model to a conditional quantile and this model can then be used to estimates the effects of treatment for that particular quantile of the outcome. The approach can be extended to include covariates and instruments for non-compliance. Note that the interpretation of the QTE is for a given quantile, not an individual at that quantile.\nBelow we show a case where the ATE is 0, but the treatment effect is negative for low quantiles of the response and positive for high quantiles. Estimating quantile treatment effects provides another tool for detecting heterogeneous effects and allows us to describe distributional consequences of our intervention. These added insights come at the cost of requiring more stringent statistical assumptions of our data and more nuanced interpretations of our results.\n\nset.seed(1234) # For replication\nn = 1000 # Population size\nY0 = runif(n) # Potential outcome under control condition\nY1= Y0\nY1[Y0 <.5] = Y0[Y0 <.5]-rnorm(length(Y0[Y0 <.5]))\nY1[Y0 >.5] = Y0[Y0 >.5]+rnorm(length(Y0[Y0 >.5]))\nD = sample((1:n)%%2) # Treatment: 1 if treated, 0 otherwise\nY = D*Y1 + (1-D)*Y0 # Outcome in population\nsamp = data.frame(D,Y)\nlibrary(quantreg)\nATE = coef(lm(Y~D,data=samp))[2]\nQTE = rq(Y~D,tau =\nseq(.05,.95,length.out=10),data=samp,method = \"fn\")\nplot(summary(QTE),parm=2,main=\"\",ylab=\"QTE\",xlab=\"Quantile\",mar = c(5.1, 4.1, 2.1, 2.1))\n\n\n\n\n\n\n8 Mediation Effects\nSometimes we want to describe not just the magnitude and significance of an observed causal effect, but also the mechanism (or mechanisms) that produced it. Did our intervention raise turnout in the treatment group, in part, by increasing these subjects’ sense of political efficacy? If so, how much of that total effect can be attributed to the mediated effects of our treatment on efficacy and efficacy on turnout?\nBaron and Kenny (1986) offer a general framework for thinking about mediation by decomposing the total effect of treatment into its indirect effect on a mediator that then effects the outcome, called an average causal mediation effect (ACME), and the remaining average direct effect (ADE) of the treatment. Unbiased estimation of these effects, however, requires a set of strong assumptions about the relationship between treatment, mediators, outcomes, and potential confounders, collectively called sequential ignorability (Imai, Keele, and Yamamoto (2010), Bullock, Green, and Ha (2010)).18\nMost causal effects likely operate through multiple channels, and so an assumption of sequential ignorability for your experiment can be hard to justify. For example, the top row in the figure below illustrates situations in which sequential ignorability holds, while the bottom row depicts two (of many possible) cases in which sequential ignorability is violated, and mediation analysis is biased. In essence, specifying the effects of a particular mediator requires strong assumptions about the role of all the other mediators in the causal chain. While some experimental designs can, in theory, provide additional leverage (such as running a second, parallel experiment in which the mediator is also manipulated), in practice these designs are hard to implement and still sensitive to unobserved bias. In some cases, the insights we hope to gain from mediation analysis may be more easily acquired from subgroup analysis and experiments designed to test for moderation.\nImai and colleagues propose an approach to mediation analysis that allows researchers to test the sensitivity of their estimates to violations of sequential ignorability.19 In the code we demonstrate some of the features of their approach, implemented in the mediation package in R (Tingley et al. (2014)). We model the relationships with OLS, but the package is capable of handling other outcome processes, such as generalized linear models or general additive models, that may be more appropriate for your data. Most importantly, the package allows us to produce bounds that reflect the sensitivity of our point estimates to some violations of sequential ignorability. In our simulated data, just over 20 percent of the total effect is mediated by our proposed mediator, M and the bias from an unobserved pre-treatment confounder would have to be quite large (ρ=.7) before we would reject the finding of a positive ACME. These bounds are only valid, however, if we believe there are no unobserved post-treatment confounders (as in panel 4). Sensitivity analysis is still possible, but more complicated in such settings (Imai and Yamamoto (2013)).\n\n\nset.seed(1234) # Replication\nn = 1000 # Sample size\nY0 = runif(n) # Potential outcome under control condition\nD = sample((1:n)%%2) # Treatment: 1 if treated, 0 otherwise\nX<-rnorm(n) # Covariate\nM<-rnorm(n=n,mean=D+rnorm(n)) # Mediator influenced by Treatment\nY1 = Y0 + 1 + M # Potential outcome under treatment\nY = D*Y1 + (1-D)*Y0 # Outcome in population\nsamp<-data.frame(D,M,Y)\nlibrary(mediation)\nmed.f<-lm(M~D+X,data=samp) # Model for mediator\nout.f<-lm(Y~M+D+X,data=samp) # Model for outcome\n#Estimate ACME and ADE\nlibrary(mediation)\nmed.out<-\nmediate(med.f,out.f,treat=\"D\",mediator=\"M\",robustSE=T,sims=1000)\n# Sensitivity of ACME to unobserved pre-treatment confounder\ns.out<-medsens(med.out)\nplot(s.out) # Plot sensistivity bounds\n\n\n\n\n\n# Structural equations estimates of ACME ADE\n# f1<-formula(Y~D+X)\n# f2<-formula(M~D+X)\n# f3<-formula(Y~D+M+X)\n#\n# med.sys<-systemfit(list(f1,f2,f3),data=samp)\n# ACME<-coef(med.sys)[\"eq1_D\"]-coef(med.sys)[\"eq3_D\"]\n# ADE<-coef(med.sys)[\"eq3_D\"]\n\n\n\n9 Log-Odds Treatment Effects\nAverage treatment effects seem a bit hard to interpret when outcomes are not continuous. For example, a very common binary outcome in the study of elections is coded as 1 when subjects voted, and 0 when they did not. The average effect might be 0.2, but what does it really mean to say that a treatment increased my voting by 0.2? Estimating causal effects for dichotomous outcomes requires some additional care, particularly when including covariates. A common quantity of causal interest for dichotomous outcomes is our treatment’s effect on the log-odds of success, defined for the experimental pool as:\n\\[\\Delta = log\\frac{E(Y_i(1))}{1-E(Y_i(1))} - log\\frac{E(Y_i(0))}{1-E(Y_i(0))}\\]\nFreedman (2008) shows that logistic regression adjusting for covariates in a randomized experiments produces biased estimates of this causal effect. The basic intuition for Freedman’s argument comes from the fact that taking the log of averages is not the same as taking the average of logs and so the treatment coefficient estimated from a logistic regression conditioning on covariates will not provide a consistent estimator of log-odds of success. Instead, Freedman recommends taking the predicted probabilities varying subjects’ treatment status but maintaining their observed covariate profiles to produce a consistent estimator of the log-odds.\nThe basic procedure is outlined in the code below. The coefficients from the logistics regression controlling for covariate X, tend to overestimate the effect of treatment on the log odds, while the adjusted estimates from the predicted probabilities produce consistent results.\n\nset.seed(1234) # For replication\nn = 1000 # Sample size\nU = runif(n)\nX = runif(n) # Observed Covariate\nY0 = ifelse(U>.5,1,0) # Potential Outcomes\nY1 = ifelse(U+X>.75,1,0)\nD = rbinom(n,1,.75) # Randomly assign 3/4 to treatment\nY = D*Y1+Y0*(1-D)\nsamp = data.frame(X,D,Y)\naT<-with(samp, mean(Y[D==1]))\naC<-with(samp, mean(Y[D==0]))\n# Unconditional log odds\nlog.odds<-log(aT/(1-aT))-log(aC/(1-aC))\n# Logistic regression conditioning on X overestimates log odds\nfit<-glm(Y~D+X,data=samp,binomial(\"logit\"))\nlog.odds.logit<-\n  coef(glm(Y~D+X,data=samp,binomial(\"logit\")))[2]\n# Dataframes using original covariates for predicted probabilities\nD1<-data.frame(D=1,samp[,c(\"X\")])\nD0<-data.frame(D=0,samp[,c(\"X\")])\n#Adjusted log-odds produces consisted estimator of log-odds\naT.adj<-predict(fit,newdata=D1,type=\"response\")\naC.adj<-predict(fit,newdata=D0,type=\"response\")\nlog.odds.adj<-log(mean(aT.adj)/(1-mean(aT.adj)))-\n  log(mean(aC.adj)/(1-mean(aC.adj)))\n\n\n\n10 Attributable Effects\nWe conclude with a brief discussion of an alternative quantity of causal interest that may be particularly useful with binary outcomes: the attributable effect (Rosenbaum (n.d.)). Consider a simple case with a dichotomous outcome and treatment. Let \\(A\\) be the number of outcomes attributable to treatment, that is, the number of cases in which \\(Y_i\\) equaled 1 among treated subjects which would not have occurred had these units been assigned to control. For a range of \\(A\\)’s, we adjust the observed contingency table of outcomes among the treated, and compare this resulting distribution to a known null distribution (the distribution of outcomes we would have observed had treatment had no effect). The resulting range of \\(A\\)’s for which our test continues to reject the null hypothesis of no effect provides a range of effects that are attributable to our treatment.\n\n\n\nTable 1\n\\(D=1\\)\n\\(D=0\\)\n\n\n\n\n\\(Y=1\\)\n\\(\\sum Y_iD_i-A\\)\n\\((1-Y_i)(D_i)\\)\n\n\n\\(Y=0\\)\n\\(\\sum Y_i(1-D_i)+A\\)\n\\(\\sum (1-Y_i)(1-D_i)\\)\n\n\n\n(rosenbaum_2002?) shows extensions of this concept to different types of outcomes (such as continuous variables). A similar logic can also be applied to detecting uncommon but dramatic responses to treatment Rosenbaum and Silber (2008).\nHansen and Bowers (2009) use this approach to identify the number of additional votes attributable to different interventions in get-out-the-vote experiment with clustered treatment assignment and one-sided non-compliance. They show that, in large samples, one can approximate the confidence interval for attributable effects without assessing each attribution. Here is an example of that approach where covariates are used to increase precision.\nFirst, we define an attributable effect as \\(A=∑_iZ_iτ_i\\), where \\(τ_i=Y_i(1)−Y_i(0)\\) and \\(y∈0,1\\) following (rosenbaum_2002?). That is, the attributable effect is the number of “yes” or “success” or other “1” responses among those treated that we would not have seen if they had been assigned control.\nSecond, notice that if we write the set \\(U\\) as the experimental pool, and the set of control units is a subset of the whole pool, \\(C⊆U\\), then we can write \\(∑_{i∈C}Y_i−Y_i(0)=0\\). This means that we can represent \\(A\\) using totals:\n\\[A = ∑_{i=1}^NZ_iτ_i=∑_{i=1}^NZ_i(Y_i(1)−Y_i(0))=∑_{i∉C}y_i(1)−∑_{i∉C}y_i(0)\\] \\[  = ∑_{i∉C}Y_i−∑_{i∉C}Y_i(0)=∑_{i=1}^NY_i−∑_{i=1}^NY_i(0)=t_U−t_C\\]\n\n= observed total overall (fixed and observed) - total outcome under control (unobserved, to estimate)\n\nThird, this representation allows us to produce a design-based confidence interval for A^ by drawing on the survey sampling literature about statistical inference for sample totals because the observed total outcomes, tU, is fixed across randomizations. We can use covariates to increase precision here because the survey regression estimator allows us to estimate the total that we would have seen in the control group: \\(\\hat{t}_c=\\sum_{i∈U}\\hat{Y}_i+\\sum_{i∈U}(Y_i-\\hat{Y}_i)\\) with \\(\\hat{Y}_i=f(X_i,\\beta)\\) (Lohr 1999). The survey sampling literature shows that as \\(N→∞\\), \\(CI(\\hat{t}_c) \\approx \\hat{t}_c \\pm z_{a/2}SE(\\hat{t}_c)\\). So, one can calculate \\(\\widehat{SE}(\\hat{t}_c)\\) from standard sampling theory and then the \\(CI(\\hat{A}) \\approx t_U-\\widehat{CI}(\\hat{t}_c)\\).\nIn the code below, we provide an illustration using simulated data for a binary response and treatment. In 85 percent of the treatment group, \\(Y=1\\) compared to 52 percent in the control. A difference of this size is consistent with our treatment having caused \\(Y=1\\) for between 92 and 138 of subjects, for whom \\(Y\\) would have otherwise equaled 0 had they not received the treatment. The regression estimator, which leverages precision gained from including covariates, produces tighter confidence intervals (98.8 to 135.1) for the attributable effects.\n\nset.seed(1234) # For replication \nn = 1000 # Sample size \nX1 = rnorm(n) # Covariates \nX2 = rnorm(n) \np = pnorm(-0.5 + 0.75*X2) # Unequal probability of treatment \nD = rbinom(n, 1, p) \np0 = pnorm(rnorm(n)) # Potential outcomes for binary response \np1 = pnorm(X1 + X2+1) \nY0 = rbinom(n, 1, p0) \nY1 = rbinom(n, 1, p1) \nY = D*Y1 + (1-D)*Y0 # Observed outcome \nsamp = data.frame(D,Y,X1,X2) # Data frame \nattribute<-function(treat,out,A,data){ \n  # Contingency Table of Treatment Status and Outcome \n  attr.tab<-with(data,table(treat,out)) # \n  # Matrix of p-values for Attributable effects, A \n  attr.ps<-\n    matrix(NA,nc=2,nr=A,dimnames=list(NULL,c(\"A\",\"p\"))) \n  for(i in 1:A){ \n    attr.ps[i,]<-\n      c(i,fisher.test(attr.tab+matrix(c(0,i,0,-i),2,2))$p) \n    }\n  # Find range of effects \n  get.bounds<-function(){ \n    diffs<-ifelse(.05-attr.ps[,\"p\"]>0,.05-\n                    attr.ps[,\"p\"],99) \n    index<-(diffs %in% \n              c(min(diffs),min(diffs[diffs>min(diffs)]))) \n    index \n    }\n  # Return range of effects \n  return (attr.ps[get.bounds(),])\n  } \nwith(samp,table(D,Y))\n\n   Y\nD     0   1\n  0 318 339\n  1  51 292\n\nwith(samp,apply(table(D,Y),1,prop.table)) \n\n   D\nY           0        1\n  0 0.4840183 0.148688\n  1 0.5159817 0.851312\n\nattribute(treat = D, out= Y, A=200,data=samp) \n\n       A          p\n[1,]  92 0.04519869\n[2,] 138 0.04587804\n\n# Regression estimator \nfit1<-lm(Y~X1+X2,data=samp,subset=D==0) \nhatYcU<-predict(fit1,newdata=samp) \nec<-Y[D==0]-hatYcU[D==0] ## same as residuals(fit1) \nhatTotYc<-sum(hatYcU)+sum(ec) \nN<-length(Y) \nnctrls<-sum(1-D) \nthefpc<- (1 - (nctrls/N)) \nvarhattC<-N*thefpc*var(Y[D==0]) \nalpha<-c(.05, 1/3) \nalpha<-sort(c(alpha/2, 1-alpha/2)) \nciTotYc<-hatTotYc+sqrt(varhattC)*qnorm(alpha) \nciAE<-sort(sum(Y) - ciTotYc ) \nnames(ciAE)<-c(\"lower 95%\",\"lower \n66%\",\"upper 66%\",\"upper 95%\") \nprint(ciAE) \n\n  lower 95% lower \\n66%   upper 66%   upper 95% \n   98.78637   107.97975   125.90114   135.09451 \n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nAbadie, Alberto, Joshua Angrist, and Guido W. Imbens. 2002. “Instrumental Variables Estimates of the Effect of Subsidized Training on the Quantiles of Trainee Earnings.” Econometrica 70 (1): 91–117.\n\n\nAbbring, Jaap H., and James J. Heckman. 2007. “Handbook of Econometrics.” In, 6:5145–5303. Elsevier.\n\n\nAngrist, Joshua, and Jörn-Steffen Pischke. 2008. Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton University Press.\n\n\nAronow, Peter M., and Joel A. Middleton. 2013. “A Class of Unbiased Estimators of the Average Treatment Effect in Randomized Experiments.” Journal of Causal Inference 1 (1): 135–54.\n\n\nBaron, Ruben M., and David A. Kenny. 1986. “The Moderator–Mediator Variable Distinction in Social Psychological Research: Conceptual, Strategic, and Statistical Considerations.” Journal of Personality and Social Psychology 51 (6): 1173–82.\n\n\nBound, John, David A. Jaeger, and Regina M. Baker. 1995. “Problems with Instrumental Variables Estimation When the Correlation Between the Instruments and the Endogenous Explanatory Variable Is Weak.” Journal of the American Statistical Association 90 (430): 443–50.\n\n\nBrambor, Thomas, William R. Clark, and Matt Golder. 2006. “Understanding Interaction Models: Improving Empirical Analyses.” Political Analysis 14 (1): 63–82.\n\n\nBullock, John G., Donald P. Green, and Shang E. Ha. 2010. “Yes, but What’s the Mechanism? (Don’t Expect an Easy Answer).” Journal of Personality and Social Psychology 98 (4).\n\n\nDunning, Thad. 2010. “Design-Based Inference: Beyond the Pitfalls of Regression Analysis?” In Rethinking Social Inquiry: Diverse Tools, Shared Standards, 2nd Ed. Lanham, MD: Rowman; Littlefield.\n\n\nFreedman, David A. 2008. “Randomization Does Not Justify Logistic Regression.” Statistical Science 23 (2): 237–49.\n\n\nFrölich, Markus, and Blaise Melly. 2010. “Estimation of Quantile Treatment Effects with Stata.” Stata Journal 10 (3).\n\n\nGerber, Alan S., and Donald P. Green. 2012. Field Experiments: Design, Analysis, and Interpretation. WW Norton.\n\n\nGlynn, Adam N., and Kevin M. Quinn. 2010. “An Introduction to the Augmented Inverse Propensity Weighted Estimator an Introduction to the Augmented Inverse Propensity Weighted Estimator.” Political Analysis 18 (1): 36–56.\n\n\nHansen, Ben, and Jake Bowers. 2009. “Attributing Effects to a Cluster-Randomized Get-Out-the-Vote Campaign.” Journal of the American Statistical Association 104 (487): 873–85.\n\n\nHartman, Erin, R. D. Grieve, R. Ramsahai, and Jasjeet S. Sekhon. 2015. “From SATE to PATT: Combining Experimental with Observational Studies.” Journal of the Royal Statistical Society.\n\n\nHirano, Keisuke, Guido W. Imbens, and Geert Ridder. 2003. “Efficient Estimation of Average Treatment Effects Using the Estimated Propensity Score.” Econometrica 71 (4): 1161–89.\n\n\nHolland, Paul W. 1986. “Statistics and Causal Inference.” Journal of the American Statistical Association 81 (396): 945–60.\n\n\nImai, Kosuke, Luke Keele, Dustin Tingley, and Teppei Yamamoto. 2011. “Unpacking the Black Box of Causality: Learning about Causal Mechanisms from Experimental and Observational Studies.” American Political Science Review 105 (4): 765–89.\n\n\nImai, Kosuke, Luke Keele, and Teppei Yamamoto. 2010. “Identification, Inference and Sensitivity Analysis for Causal Mediation Effects.” Statistical Science, 51–71.\n\n\nImai, Kosuke, Gary King, and Elizabeth A. Stuart. 2008. “Misunderstandings Between Experimentalists and Observationalists about Causal Inference.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 171 (2): 481–502.\n\n\nImai, Kosuke, Dustin Tingley, and Teppei Yamamoto. 2013. “Experimental Designs for Identifying Causal Mechanisms.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 176 (1): 5–51.\n\n\nImai, Kosuke, and Teppei Yamamoto. 2013. “Identification and Sensitivity Analysis for Multiple Causal Mechanisms: Revisiting Evidence from Framing Experiments.” Political Analysis 21 (2): 141–71.\n\n\nImbens, Guido W., and Jeffrey M. Wooldridge. 2007. “What’s New in Econometrics?” National Bureau of Economic Research Summer Institute.\n\n\nKoenker, Roger, and Kevin Hallock. 2001. “Quantile Regression: An Introduction.” Journal of Economic Perspectives 15 (4): 43–56.\n\n\nLin, Winston. 2013. “Agnostic Notes on Regression Adjustments to Experimental Data: Reexamining Freedman’s Critique.” The Annals of Applied Statistics 7 (1): 295–318.\n\n\nNickerson, D. W. 2008. “Is Voting Contagious? Evidence from Two Field Experiments.” American Political Science Review 102 (1).\n\n\nRosenbaum, Paul R. n.d. Design of Observational Studies. New York, NY: Springer.\n\n\nRosenbaum, Paul R., and Donald B. Rubin. 1983. “The Central Role of the Propensity Score in Observational Studies for Causal Effects.” Biometrika 70 (1): 41–55.\n\n\nRosenbaum, Paul R., and Jeffrey H. Silber. 2008. “Aberrant Effects of Treatment.” Journal of the American Statistical Association 103 (481): 240–47.\n\n\nTingley, Dustin, Teppei Yamamoto, Kentaro Hirose, Luke Keele, and Kosuke Imai. 2014. “Mediation: R Package for Causal Mediation Analysis.” Journal of Statistical Software 59 (5): 1–38.\n\nFootnotes\n\n\nFor a more formal discussion of independence and the assumptions necessary to estimate causal effects, see Holland (1986) and Angrist and Pischke (2008).↩︎\nSee Holland (1986) and Angrist and Pischke (2008) again for more formal discussion of independence and the assumptions necessary to estimate causal effects.↩︎\nEstimates are often written with a hat ( \\(\\widehat{ATE}\\) ) to reflect the difference between the estimate from our particular sample and the estimand, target of our estimation that is unobserved. Unless otherwise stated, in this guide we focus on generating sample estimates and subsequently omit this explicit notation for brevity. See (gerger_green_2012?) for concise introduction to this distinction and Imbens and Wooldridge (2007) for a thorough treatment of these concepts.↩︎\nThe covariance of \\(Y_{i}(1),Y_{i}(0)\\) is impossible to observe but the “Neyman” estimator of the variance omitting the covariance term provides a conservative (too large) estimate of the true sample variance because we tend to assume that the covariance is positive. Since we are generally worried about minimizing type I error rate (incorrectly rejecting true null hypothesis), we prefer using conservative estimates of the variance. See also Dunning (2010) and Gerber and Green (2012) for justification of the conservative variance estimator.↩︎\nSee Lin (2013).↩︎\nSee Brambor, Clark, and Golder (2006).↩︎\nWe typically assume monotonicity, meaning there are no defiers or people who only take the treatment when assigned to control (\\(D_{i}=1\\) when \\(Z_i=0\\)) and refuse the treatment when assigned to treatment (\\(D_{i}=0\\) when \\(Z_{i}=1\\)).↩︎\nSee Angrist and Pischke (2008); Bound, Jaeger, and Baker (1995).↩︎\nSee Imai, King, and Stuart (2008) for a more detailed review of the issues discussed in this section.↩︎\nSee Imbens and Wooldridge (2007).↩︎\nAngrist and Pischke (2008) provide a brief introduction of topics covered in more detail by Hirano, Imbens, and Ridder (2003), Aronow and Middleton (2013), Glynn and Quinn (2010), and Hartman et al. (2015).↩︎\nSee Glynn and Quinn (2010).↩︎\nSee Hartman et al. (2015) for an example of efforts to combine experimental and observational data to move from a sample ATE to an estimate of a population ATT.↩︎\nSee Rosenbaum and Rubin (1983).↩︎\nSee Abadie, Angrist, and Imbens (2002).↩︎\n\nThat is, treatment can have heterogeneous effects but the ordering of potential outcomes is preserved. See Angrist and Pischke (2008). See Frölich and Melly (2010) for fairly concise discussions of these issues and Abbring and Heckman (2007) for a thorough overview.\n\n↩︎\nSee Koenker and Hallock (2001) for a concise overview of quantile regression.↩︎\nFormally, Imai, Keele, and Yamamoto (2010) define the necessary conditions of sequential ignorability as: \\({Y_i(d',m),M_i(d)}⊥D_i|X_i=x, Y_i(d',m)⊥M_i(d)|D_i=d,X_i=x\\). That is, first, given pre-treatment covariates, the potential outcomes of Y and M are independent of treatment D, and, second, that conditional on pre-treatment covariates and treatment status, potential outcomes are also independent of the mediator.↩︎\nSee for example Imai, Keele, and Yamamoto (2010), Imai et al. (2011), Imai, Tingley, and Yamamoto (2013), and Imai and Yamamoto (2013). Also see the discussion of Imai, Tingley, and Yamamoto (2013) for different perspectives on the desirability of addressing mediation-type claims with sensitivity or bounds-style analyses.↩︎"
  },
  {
    "objectID": "guides/getting-started/causal-inference_fr.html",
    "href": "guides/getting-started/causal-inference_fr.html",
    "title": "10 choses à savoir sur l’inférence causale",
    "section": "",
    "text": "Résumé\nLe philosophe David Lewis a décrit la causalité comme “quelque chose qui fait une différence, et cette différence doit être une différence par rapport à ce qui se serait passé sans elle”. Ceci est l’interprétation de la causalité pour la plupart des expérimentalistes. Même si la définition semble simple, elle a de nombreuses implications subtiles. Voici dix idées impliquées par cette notion de causalité qui importent pour la conception de recherche. 1\n\n\n1. Une assertion causale est une déclaration sur ce qui ne s’est pas produit.\nPour la plupart des expérimentalistes, la déclaration “\\(X\\) a causé \\(Y\\)” signifie que \\(Y\\) est présent et \\(Y\\) n’aurait pas été présent si \\(X\\) n’avait pas été présent. Cette définition requiert une notion de ce qui aurait pu arriver, mais ne s’est pas produit.2 De même, “l’effet” de \\(X\\) sur \\(Y\\) est la différence entre la valeur que \\(Y\\) aurait prise étant donné une valeur de \\(X\\) et la valeur que \\(Y\\) aurait prise étant donné une autre valeur de \\(X\\). En raison de l’accent mis sur les différences entre les résultats, cette approche est parfois appelée approche “des différences” ou “contrefactuelle” de la causalité.\nNote technique: Les statisticiens emploient le cadre des “résultats potentiels” pour décrire les relations contrefactuelles. Dans ce cadre, \\(Y_i(1)\\) désigne le résultat pour l’unité \\(i\\) qui serait observé sous une condition (par exemple, si l’unité \\(i\\) a reçu un traitement) et \\(Y_i(0)\\) désigne le résultat qui serait observé dans une autre condition (par exemple, si l’unité \\(i\\) n’a pas reçu le traitement). Un effet causal du traitement pour l’unité \\(i\\) pourrait être une simple différence des résultats potentiels \\(τ_i=Y_i(1)−Y_i(0)\\). Un traitement a un effet causal (positif ou négatif) sur \\(Y\\) pour l’unité \\(i\\) si \\(Y_i(1)≠Y_i(0)\\).\n\n\n2. Pas de causalité sans manipulation.\nLa définition “contrefactuelle” de la causalité exige que l’on soit capable de réfléchir aux résultats qui peuvent entraîner des conditions différentes. Quelle serait la situation si un parti plutôt qu’un autre était élu ? Les déclarations causales de tous les jours ne répondent souvent pas à cette exigence de l’une des deux manières suivantes.\n\nPremièrement, certaines déclarations ne précisent pas de conditions contrefactuelles claires. Par exemple, l’affirmation selon laquelle “la récession a été causée par Wall Street” n’indique pas de contrefactuel évident — devons-nous examiner s’il y aurait eu une récession si Wall Street n’avait pas existé ? Ou est-ce une déclaration sur des décisions particulières que Wall Street aurait pu prendre mais n’a pas prises ? Si oui, quelles décisions ? La validité de telles déclarations est difficile à évaluer et peut dépendre des conditions contrefactuelles impliquées par une déclaration.\nDeuxièmement, certaines déclarations impliquent des conditions contrefactuelles qui ne peuvent être imaginées. Par exemple, l’affirmation selon laquelle Peter a obtenu le poste parce qu’il est Peter implique une considération de ce qui se serait passé si Peter n’était pas Peter. Alternativement, l’affirmation selon laquelle Peter a obtenu le poste parce qu’il est un homme nécessite de considérer Peter comme autre qu’un homme. Le problème est que les contrefactuels dans ces cas impliquent un changement non seulement de la condition à laquelle fait face un individu, mais de l’individu lui-même.\n\nPour éviter de tels problèmes, certains statisticiens recommandent de restreindre les assertions causales aux traitements qui peuvent en théorie (pas nécessairement en pratique) être manipulés.3 Par exemple, alors que nous pourrions avoir des difficultés avec l’affirmation selon laquelle Peter a obtenu le poste parce qu’il était un homme, nous n’avons pas de telles difficultés avec l’affirmation selon laquelle Peter a obtenu le poste parce que l’agence de recrutement pensait qu’il était un homme.\n\n\n3. Les causes sont non rivales.\nMême si nous pouvons nous concentrer sur l’effet d’une seule cause \\(X\\) sur un résultat \\(Y\\), nous ne nous attendons généralement pas à ce qu’il n’y ait qu’une seule cause de \\(Y\\).4 De plus, si vous additionnez les effets causaux de différentes causes, il n’y a aucune raison de s’attendre à ce qu’ils totalisent 100 %. Par conséquent, il ne sert à rien d’essayer de “répartir” les résultats entre différents facteurs de causalité. En d’autres termes, les causes sont non rivales. La National Rifle Association soutient, par exemple, que les armes à feu ne tuent pas les gens, les gens tuent les gens. Cette déclaration n’a pas beaucoup de sens dans le cadre contrefactuel. Enlevez les armes à feu et vous n’aurez pas de morts par balles. Les armes à feu sont donc une cause. Enlevez les gens et vous n’aurez pas non plus de décès par balle, donc les gens sont aussi une cause. En d’autres termes, ces deux facteurs sont simultanément les causes des mêmes résultats.\n\n\n4. \\(X\\) peut causer \\(Y\\) même si \\(X\\) n’est pas une condition nécessaire ou une condition suffisante pour \\(Y\\).\nOn parle souvent des relations causales en termes déterministes. Même la citation de Lewis en haut de cette page semble suggérer une relation déterministe entre les causes et les effets. On pense parfois que les relations causales impliquent des conditions nécessaires (pour que \\(Y\\) se produise, \\(X\\) doit se produire); on pense parfois que de telles relations impliquent des conditions suffisantes (si \\(X\\) se produit, alors \\(Y\\) se produit). Mais une fois que nous parlons d’unités multiples, il y a au moins deux façons de penser que \\(X\\) cause \\(Y\\) même si \\(X\\) n’est ni une condition nécessaire ni une condition suffisante pour \\(Y\\). La première consiste à tout réinterpréter en termes probabilistes : par \\(X\\) cause \\(Y\\), on entend simplement que la probabilité de \\(Y\\) est plus élevée lorsque \\(X\\) est présent. Une autre consiste à tenir compte des contingences — par exemple, \\(X\\) peut causer \\(Y\\) si la condition \\(Z\\) est présente, mais pas dans le cas contraire.5\n\n\n5. Le problème fondamental de l’inférence causale.\nSi les effets causaux sont des déclarations sur la différence entre ce qui s’est produit et ce qui aurait pu se produire, alors les effets causaux ne peuvent pas être mesurés. Mauvaise nouvelle ! De manière prospective, vous pouvez organiser les choses de manière à observer ce qui se passe si une personne reçoit un traitement ou ce qui se passe si elle ne reçoit pas le traitement. Pourtant, pour la même personne, vous ne pourrez jamais observer ces deux résultats et leur différence. Cette incapacité à observer les effets causaux au niveau de l’unité est souvent appelée le “problème fondamental de l’inférence causale”.\n\n\n6. Vous pouvez estimer l’effet causal moyen même si vous ne pouvez observer aucun effet causal individuel.\nMême si vous ne pouvez pas observer si \\(X\\) cause \\(Y\\) pour une unité donnée, il est peut-être toujours possible de déterminer si \\(X\\) cause \\(Y\\) en moyenne. L’effet causal moyen est égal à la différence entre le résultat moyen pour toutes les unités si elles étaient toutes dans la condition de contrôle et le résultat moyen pour toutes les unités si elles étaient toutes dans la condition de traitement. De nombreuses stratégies d’identification causale (voir 10 stratégies pour déterminer si X a causé Y) se concentrent sur des façons d’en savoir plus sur ces résultats potentiels moyens.6\n10 choses à savoir sur les tests d’hypothèse décrit comment en savoir plus sur les effets causaux individuels plutôt que sur les effets causaux moyens étant donné le problème fondamental de l’inférence causale.\n\n\n7. L’estimation de l’effet causal moyen ne nécessite pas que les groupes de traitement et de contrôle soient identiques.\nUne stratégie que les gens utilisent pour en savoir plus sur l’effet causal moyen consiste à créer des groupes de traitement et de contrôle par randomisation (voir 10 Stratégies pour déterminer si X a causé Y). Ce faisant, les chercheurs s’inquiètent parfois s’ils constatent que les groupes de traitement et de contrôle qui en résultent ne sont pas comparables sur certaines dimensions importantes.\nLa bonne nouvelle est que l’argument expliquant pourquoi les différences dans les résultats moyens entre les groupes de traitement et de contrôle assignés de manière aléatoire capturent l’effet moyen du traitement (en espérance pour des randomisations répétées au sein du même groupe d’unités) ne repose pas sur le fait que les groupes de traitement et de contrôle ont des caractéristiques observées similaires. Il repose uniquement sur l’idée que, en moyenne, les résultats dans les groupes de traitement et de contrôle captureront les résultats moyens pour toutes les unités du groupe expérimental si elles étaient, respectivement, dans le traitement ou dans le contrôle. En pratique, les groupes de traitement et de contrôle réels ne seront pas identiques.7\n\n\n8. La corrélation n’est pas la causalité.\nUne corrélation entre \\(X\\) et \\(Y\\) est une déclaration sur les relations entre les résultats réels, et non sur la relation entre les résultats réels et les résultats contrefactuels. Ainsi, les déclarations sur les causes et les corrélations n’ont pas grand-chose à voir les unes avec les autres. Des corrélations positives peuvent être cohérentes avec des effets causaux positifs, aucun effet causal ou même des effets causaux négatifs. Par exemple, la prise de médicaments contre la toux est positivement corrélée à la toux mais a, espérons-le, un effet causal négatif sur la toux.8\n\n\n9. Si vous savez qu’en moyenne \\(A\\) cause \\(B\\) et \\(B\\) cause \\(C\\), cela ne veut pas dire qu’en moyenne \\(A\\) cause \\(C\\).\nVous pourriez vous attendre à ce que si \\(A\\) cause \\(B\\) et \\(B\\) cause \\(C\\), alors \\(A\\) cause \\(C\\).9 Mais il n’y a aucune raison que les relations causales moyennes soient transitives. Imaginez que \\(A\\) cause \\(B\\) pour les hommes mais pas les femmes et \\(B\\) cause \\(C\\) pour les femmes mais pas les hommes. Ensuite, en moyenne, \\(A\\) cause \\(B\\) et \\(B\\) cause \\(C\\), mais \\(A\\) ne cause pas \\(C\\) à travers \\(B\\).\n\n\n10. Il est plus facile d’en apprendre davantage sur les “effets des causes” que sur les “causes des effets”.\nBien que cela puisse sembler être deux façons de dire la même chose, il y a une différence entre comprendre quel est l’effet de \\(X\\) sur \\(Y\\) (les “effets d’une cause”) et si un résultat \\(Y\\) était dû à une cause \\(X\\) (la “cause d’un effet”).10 Considérez l’exemple suivant. Supposons que nous menions une expérience avec un échantillon qui contient un nombre égal d’hommes et de femmes. L’expérience assigne de manière aléatoire des hommes et des femmes à un traitement binaire \\(X\\) et mesure un résultat binaire \\(Y\\). De plus, supposons que \\(X\\) ait un effet positif de 1 pour tous les hommes, c’est-à-dire le résultat potentiel de contrôle des hommes est de zéro, \\(Y_i(0) = 0\\), et leur résultat potentiel traité est de un, \\(Y_i(1) = 1\\). Pour toutes les femmes, \\(X\\) a un effet négatif de \\(-1\\), c’est-à-dire que le résultat potentiel de contrôle des femmes est de un, \\(Y_i(0) = 1\\), et leur résultat potentiel traité est de zéro, \\(Y_i(1) = 0\\). Dans cet exemple, l’effet moyen de \\(X\\) sur \\(Y\\) est nul. Mais pour tous les participants du groupe de traitement avec \\(Y=1\\), il est vrai que \\(Y=1\\) car \\(X=1\\). De même, pour tous les participants du groupe de traitement avec \\(Y=0\\), il est vrai que \\(Y=0\\) car \\(X=1\\). L’expérimentation peut obtenir une réponse exacte à la question sur les “effets d’une cause”, mais il n’est généralement pas possible d’obtenir une réponse exacte à la question sur la “cause d’un effet”.11\n\n\n\n\n\nFootnotes\n\n\nAuteur d’origine : Macartan Humphreys. Révisions mineures : Winston Lin et Donald P. Green, 24 juin 2016. Révisions MH 6 janvier 2020. Révisions Anna Wilke mai 2021. Le guide est un document vivant et peut être mis à jour par les membres de EGAP à tout moment ; les contributeurs répertoriés ne sont pas responsables des modifications ultérieures.↩︎\nHolland, Paul W. “Statistics and causal inference.” Journal of the American Statistical Association 81.396 (1986): 945-960.↩︎\nHolland, Paul W. “Statistics and causal inference.” Journal of the American Statistical Association 81.396 (1986): 945-960.↩︎\nCertains appellent cela le “problème des causes de prodigalité”.↩︎\nMackie a présenté l’idée de conditions dites “INSS” (“INUS” en anglais) pour capturer la dépendance des causes sur d’autres causes. Une cause peut être une partie Insuffisante mais Nécessaire d’une condition qui est elle-même Superflue mais Suffisante. Par exemple, composer un numéro de téléphone est une cause de “contacter quelqu’un” car avoir une connexion et composer un numéro est suffisant (S) pour passer un appel téléphonique, alors que composer seul sans connexion ne suffirait pas (I), ni avoir un connexion (N). Il existe bien sûr d’autres moyens de contacter quelqu’un sans passer d’appels téléphoniques (S). Mackie, John L. “The cement of the universe.” London: Oxford Uni (1974).↩︎\nNote technique : La principale idée technique est que la différence des moyennes est la même que la moyenne des différences. C’est-à-dire, en utilisant “l’opérateur d’espérance”, \\(𝔼(τ_i)=𝔼(Y_i(1)−Y_i(0))=𝔼(Y_i(1))−𝔼(Y_i(0))\\). Les termes à l’intérieur de l’opérateur d’espérance dans la deuxième quantité ne peuvent pas être estimés, mais les termes à l’intérieur de l’opérateur d’espérance dans la troisième quantité peuvent l’être.6Voir l’illustration ici.↩︎\nPour cette raison, les tests-\\(t\\) pour vérifier si “la randomisation a fonctionné” n’ont pas beaucoup de sens, du moins si vous savez qu’une procédure randomisée a été suivie — simplement par hasard, 1 test sur 20 montrera des différences statistiquement détectables entre les groupes de traitement et de contrôle. En cas de doute sur la mise en œuvre correcte d’une procédure randomisée, ces tests peuvent être utilisés pour tester l’hypothèse selon laquelle les données ont bien été générées par une procédure randomisée. Ces tests peuvent alors être particulièrement importants pour des expériences de terrain où les chaînes de communication entre la personne randomisant et la personne mettant en œuvre l’assignation du traitement peuvent être longues et complexes.↩︎\nNote technique: soit \\(D_i\\) un indicateur pour savoir si l’unité \\(i\\) a reçu un traitement ou non. Alors, la différence des résultats moyens entre ceux qui reçoivent le traitement et ceux qui ne le reçoivent pas peut s’écrire \\(\\frac{∑_i D_i×Y_i(1)}{∑_iD_i}−\\frac{∑_i (1−D_i)× Y_i(0)}{∑_i (1−D_i)}\\). En l’absence d’informations sur la manière dont le traitement a été assigné, nous ne pouvons pas dire si cette différence est un bon estimateur de l’effet moyen du traitement, c’est-à-dire de la différence entre les résultats potentiels moyens pour les groupes de traitement et de contrôle pour toutes les unités. Ce qui importe est de savoir si \\(\\frac{∑_i D_i×Y_i(1)}{∑_iD_i}\\) est une bonne estimation de \\(\\frac{∑_i 1×Y_i(1)}{∑_i1}\\) et si \\(\\frac {∑_i (1−D_i)×Y_i(0)}{∑_i (1−D_i)}\\) est une bonne estimation de \\(\\frac{∑_i 1×Y_i(0)}{∑_i1}\\). Cela pourrait être le cas si ceux qui ont reçu un traitement sont un échantillon représentatif de toutes les unités, mais sinon il n’y a aucune raison de s’attendre à ce qu’il le soit.↩︎\nInterprétez “\\(A\\) cause \\(B\\), en moyenne” comme “l’effet moyen de \\(A\\) sur \\(B\\) est positif”.↩︎\nCertains réinterprètent la question des “causes des effets” comme suit : quelles sont les causes qui ont des effets sur les résultats. Voir Andrew Gelman and Guido Imbens, “Why ask why? Forward causal inference and reverse causal questions”, NBER Working Paper No. 19614 (Nov. 2013).↩︎\nVoir, par exemple, Tian, J., Pearl, J. 2000. “Probabilities of Causation: Bounds and Identification.” Annals of Mathematics and Artificial Intelligence 28:287–313.↩︎"
  },
  {
    "objectID": "guides/getting-started/x-cause-y_en.html",
    "href": "guides/getting-started/x-cause-y_en.html",
    "title": "10 Strategies for Figuring out if X Causes Y",
    "section": "",
    "text": "Abstract\nExperiments are a way of figuring out if something causes something else. The basic idea is: try it and find out. The tricky thing is figuring out how to try it out in a way that allows for confidence in beliefs about causal effects. A strategy that holds pride of place in the researcher’s toolkit is the randomized intervention. This is the strategy that is at the heart of most of the experimental research done by EGAP members. But there are other strategies that are sometimes more appropriate. Here we describe the ten most prominent strategies for figuring out causal effects.1\n\n\n1. Randomization\nThe strategy used in randomized control trials (or randomized interventions, randomized experiments) is to use some form of a lottery to determine who, among some group, will or won’t get access to a treatment or program (or perhaps who will get it first and who will get it later, or who will get one version and who will get another). The elegance of the approach is that it uses randomness to work out what the systematic effects of a program are. The randomness reduces the chance that an observed relationship between treatment and outcomes is due to “confounders”—other things that are different between groups (for example one might be worried that things look better in treatment areas precisely because programs choose to work in well-functioning areas, but knowing that the selection was random completely removes this concern). It is powerful because it guarantees that there is no systematic relationship between treatment and all other features that can affect outcomes, whether you are aware of them or not. For this reason it is often considered to be the gold standard. Randomization cannot be used always and everywhere however, both for ethical and practical reasons. But it can be used in many more situations than people think. See Humphreys and Weinstein for a discussion of strengths and limitations of the approach for research in the political economy of development.\n\n\n2. Experimental Control (induced unit homogeneity)\nA second strategy used more in lab settings and also in the physical sciences is to use experimental control to ensure that two units are identical to each other in all relevant respects except for treatment. For example if you wanted to see if a heavy ball falls faster than a lighter ball you might make sure that they have the same shape and size and drop them both at the same time, under the same weather conditions, and so on. You then attribute any differences in outcomes to the feature that you did not keep constant between the two units. This strategy is fundamentally different to that used in randomized trials. In randomized trials you normally give up on the idea of keeping everything fixed and seek instead to make sure that natural variation—on variables that you can or cannot observe—does not produce bias in your estimates; in addition you normally seek to assess average effects across a range of background conditions rather than for a fixed set of background conditions. The merits of the control approach depend on your confidence that you can indeed control all relevant factors; if you cannot, then a randomized approach may be superior.\n\n\n3. Natural experiments (as-if randomization)\nSometimes researchers are not able to randomize, but causal inference is still possible because nature has done the randomization for you. The key feature of the “natural experiment” approach is that you have reason to believe that variation in some natural treatment is “as-if random.” For example say that seats in a school are allocated by lottery. Then you might be able to analyze the effects of school attendance as if it were a randomized control trial. One clever study of the effects of conflict on children by Annan and Blattman used the fact that the Lord’s Resistance Army (LRA) in Uganda abducted children in a fairly random fashion. Another clever study on Disarmament, Demobilization, and Reintegration (DDR) programs by Gilligan, Mvukiyehe, and Samii used the fact that an NGO’s operations were interrupted because of a contract dispute, which resulted in a “natural” control group of ex-combatants that did not receive demobilization programs. See Dunning’s book for a guide to finding and analyzing natural experiments.\n\n\n4. Before/after comparisons\nOften the first thing that people look to in order to work out causal effects is the comparison of units before and after treatment. Here you use the past as a control for the present. The basic idea is very intuitive: you switch the lightswitch off and you see the light switch off; attributing the light change to the action seems easy even in the absence of any randomization or control. But for many social interventions the approach is not that reliable, especially in changing environments. The problem is that things get better or worse for many reasons unrelated to treatments or programs you are interested in. In fact it is possible that because of all the other things that are changing, things can get worse in a program area even if the programs had a positive effect (so they get worse but are still not as bad as they would have been without the program!). A more sophisticated approach than simple before/after comparison is called “difference in differences” – basically you compare the before/after difference in treatment areas with those in control areas. This is a good approach but you still need to be sure that you have good control groups and in particular that control and treatment groups are not likely to change differently for reasons other than the treatment.\n\n\n5. Ex Post Controlling I: Regression\nPerhaps the most common approach to causal identification in applied statistical work is the use of multiple regression to control for possible confounders. The idea is to try to use whatever information you have about why treatment and control areas are not readily comparable and adjust for these differences statistically. This approach works well to the extent that you can figure out and measure the confounders and how they are related to treatment, but is not good if you don’t know what the confounders are. In general we just don’t know what all the confounders are and that exposes this approach to all kinds of biases (indeed if you control for the wrong variables it is possible to introduce bias where none existed previously).\n\n\n6. Ex Post Controlling II: Matching and Weighting\nA variety of alternative approaches seek to account for confounding variables by carefully matching treatment units to one or many control units. Matching has some advantages over regression (for example, estimates can be less sensitive to choices of functional form), but the basic idea is nevertheless similar, and indeed matching methods can be implemented in a regression framework using appropriate weights. Like regression, at its core, this strategy depends on a conviction that there are no important confounding variables that the researcher is unaware of or is unable to measure. Specific methods include:\n\noptimal full- and pair-matching and see the optmatch package\noptimal pair-matching with fine-balance via mixed integer programming. See also the designmatch package and the paper comparing approaches\noptimal multi-level matching (for designs with schools and students)\nsparse optimal matching\ngeneralized full matching\ncoarsened exact matching\ngenetic matching\nentropy balancing\ninverse propensity weighting\nstable balancing weights, and the use of\nsynthetic controls.\n\n\n\n7. Instrumental variables (IV)\nAnother approach to identifying causal effects is to look for a feature that explains why a given group got a treatment but which is otherwise unrelated to the outcome of interest. Such a feature is called an instrument. For example say you are interested in the effect of a livelihoods program on employment, and say it turned out that most people who got access to the livelihoods program did so because they were a relative of a particular program officer. Now suppose that being a relative of the program officer does not affect job prospects in any way other than through its effect on getting access to the livelihoods program. If so, then you can work out the effect of the program by understanding the effect of being a relative of the program officer on job prospects. This has been a fairly popular approach but the enthusiasm for it has died a bit, basically because it is hard to find a good instrument. One smart application are studies on the effects of poverty on conflict which use rainfall in Africa as an instrument for income/growth. While there are worries that the correlation between conflict and poverty may be due to the fact that conflict causes poverty, it does not seem plausible that conflict causes rainfall! So using rainfall as an instrument here gave a lot more confidence that really there is a causal, and not just correlational, relationship between poverty and conflict.\n\n\n8. Regression discontinuity designs (RDD)\nThe regression discontinuity approach works as follows. Say that some program is going to be made available to a set of potential beneficiaries. These potential beneficiaries are all ranked on a set of relevant criteria, such as prior education levels, employment status, and so on. These criteria can be quantitative; but they can also include qualitative information such as assessments from interviews. These individual criteria are then aggregated into a single score and a threshold is identified. Candidates scoring above this threshold are admitted to the program, while those below are not. “Project” and “comparison” groups are then identified by selecting applicants that are close to this threshold on either side. Using this method we can be sure that treated and control units are similar, at least around the threshold. Moreover, we have a direct measure of the main feature on which they differ (their score on the selection criteria). This information provides the key to estimating a program effect from comparing outcomes between these two groups. The advantage of this approach is that all that is needed is that the implementing agency uses a clear set of criteria (which can be turned into a score) upon which they make treatment assignment decisions. The disadvantage is that really reliable estimates of impact can only be made for units right around the threshold. For overviews of RDD, see Skovron and Titiunik and Lee and Lemieux; for two interesting applications, see Manacorda et al. on Uruguay and Samii on Burundi.\n\n\n9. Process tracing\nIn much qualitative work researchers try to establish causality by looking not just at whether being in a program is associated with better outcomes but (a) looking for steps in the process along the way that would tell you whether a program had the effects you think it had and (b) looking for evidence of other outcomes that should be seen if (or perhaps: if and only if) the program was effective. For example not just whether people in a livelihoods program got a job but whether they got trained in something useful, got help from people in the program to find an employer in that area, and so on. If all these steps are there, that gives confidence that the relationship is causal and not spurious. If a program was implemented but no one actually took part in it, this might give grounds to suspect that any correlation between treatment and outcomes is spurious. The difficulty with this approach is that it can be hard to know whether any piece of within-case evidence has probative value. For example a program may have positive (or negative) effects through lots of processes that you don’t know anything about and processes that you think are important, might not be. See Humphreys and Jacobs for a description of the Bayesian logic underlying process tracing and illustrations of how to combine it with other statistical approaches.\n\n\n10. Front Door Strategies (Argument from mechanisms)\nA final approach, conceptually close to process tracing, is to make use of mechanisms. Say you know, as depicted in the picture below, that \\(A\\) can cause \\(C\\) only through \\(B\\). Say moreover that you know that no third variable causes both \\(B\\) and \\(C\\) (other than, perhaps, via \\(A\\)) and no third variable causes both \\(A\\) and \\(B\\). Then covariation between \\(A\\) and \\(B\\) and between \\(B\\) and \\(C\\) can be used to assess the effect of \\(A\\) on \\(C\\). The advantage is that causality can be established even in the presence of confounders — for example even if, as in the picture below, unobserved variables cause both \\(A\\) and \\(C\\). The difficulty however is that the strategy requires a lot of confidence in your beliefs about the structure of causal relations. For more see Pearl (2000).\n\n\n\n\n\n\nFootnotes\n\n\nOriginating author: Macartan Humphreys. Minor revisions: Winston Lin, 30 August 2016. The guide is a live document and subject to updating by EGAP members at any time; contributors listed are not responsible for subsequent edits.↩︎"
  },
  {
    "objectID": "guides/getting-started/survey-implementation_en.html",
    "href": "guides/getting-started/survey-implementation_en.html",
    "title": "10 Things to Know About Survey Implementation",
    "section": "",
    "text": "There are two discrete skill sets needed on the ground when implementing a survey. The first skill set is focused on administration/logistics, and the second skill set is focused on research design. The first set of skills is needed for administration: budgeting, creating route plans, recruitment, and management of staff. Administration requires a level of familiarity with local conditions; for example, the ability to quickly estimate costs and troubleshoot logistical issues are important here. The second skill set requires knowledge of research methods to ensure that survey implementation is consistent with the research design. Researchers must be able to recognize any deviations from the protocol and address them in a way that leads to as little bias as possible. Important here is a deep understanding of the survey protocols and possible alternatives, in the case that changes need to be made on the ground.\nIf you will not be present during survey administration, you will need to either hire a firm or individuals who can work together to cover both sets of needs. There are clear advantages to hiring a firm if you have the budget, the biggest being that firms coordinate internally and balance both sets of needs, ensuring that logistics accommodate the design and vice versa. A possible drawback is that firms frequently have their own protocols, and these default procedures are usually at a lower standard than the latest protocol being used in academia. Upgrading protocols is a costly process and firms may push back against the use of stricter, or simply different, practices.\nIf you will be present during enumeration but you have a small budget, or do not feel you could manage the entire implementation (administration and design) yourself, a good alternative is to hire a field coordinator from a survey or research firm on a consultant basis. This person can help with administration while you take on the design-related work. Additionally, hiring someone for administration locally can do a lot to help with cross-cultural management. The types of management procedures that might work to motivate or sanction employees in the US may not work in another context, so someone who knows what is acceptable and effective can add a lot of value.\n\n\nWhen setting up contracts with local firms it is important to get the incentives right—thorough and good work should also be the most profitable for the firm. You can do a lot to set expectations and incentives in the contract. For example, pay on delivery where possible (although it is customary to pay some costs upfront to cover fixed expenses like transport and early salaries). You can also choose to impose financial penalties for late or low-quality data, but be sure to make these requirements clear up front and provide specific rules for what constitutes low-quality work and how late penalties will be assigned.\nIn addition to direct costs, it is reasonable for a local firm to charge overhead. This can vary from context to context, and it is best to check against the budgets of other similar projects to make sure the rate is reasonable."
  },
  {
    "objectID": "guides/getting-started/survey-implementation_en.html#salaries",
    "href": "guides/getting-started/survey-implementation_en.html#salaries",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Salaries",
    "text": "Salaries\nEstimating total salary costs before drawing the sample (needed in order to determine the teams and route plans) requires a bit of guesswork. One approach is to estimate the work-hours needed to conduct the survey (survey length x sample size) and divide by some estimated number of enumerators to come up with the number of enumerator days you will need to pay. The per diem may need to cover food and lodging, and make this clear to enumerators so they can plan accordingly. For surveys that will require long fieldwork, it is good practice to pay salary on a rest day each week although some enumerators prefer to work continuously in order to finish sooner and return home. This choice is context-specific."
  },
  {
    "objectID": "guides/getting-started/survey-implementation_en.html#per-diems",
    "href": "guides/getting-started/survey-implementation_en.html#per-diems",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Per Diems",
    "text": "Per Diems\nPer diems cover enumerator’s expenses associated with doing fieldwork. This means lodging for overnight stays, all meals, and sometimes also transportation. Per diems should also be paid on rest days that fall in between work days. In the case that the variation in lodging and food costs is low, it is not important to change the per diem rate according to location. Teams will know when to save and when to spend.\n\nQuick calculator:\n((survey time to complete * sample size)/workable hours in a day)/# of enumerators = number of days\nnumber of days * (daily rate + per diem) + supervisors = approx. total salary cost"
  },
  {
    "objectID": "guides/getting-started/survey-implementation_en.html#transportation",
    "href": "guides/getting-started/survey-implementation_en.html#transportation",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Transportation",
    "text": "Transportation\nIt’s important to, ex ante, be as accurate as possible in estimating the full cost of transportation as this is frequently both least flexible and most variable cost. Typically, it is good practice to build in contingency on the cost of fuel, as the price can change over the several months it takes to go from the grant application stage to the implementation stage. If you are budgeting before drawing your sample, pay particular attention to hard-to-reach areas in your population (islands, places without road access) and pad your transport line for the possibility you randomly sample enumeration areas that carry these higher costs."
  },
  {
    "objectID": "guides/getting-started/survey-implementation_en.html#equipment",
    "href": "guides/getting-started/survey-implementation_en.html#equipment",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Equipment",
    "text": "Equipment\nLater on in this guide we present the benefits of using personal digital assistants (PDAs) or tablets for data collection (see section 3). PDAs/tablets can be either purchased using survey funds or leased from a research firm, university, or other researchers."
  },
  {
    "objectID": "guides/getting-started/survey-implementation_en.html#why-personal-digital-assistants-pdas-or-tablets-are-better-if-you-have-the-budget",
    "href": "guides/getting-started/survey-implementation_en.html#why-personal-digital-assistants-pdas-or-tablets-are-better-if-you-have-the-budget",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Why Personal Digital Assistants [PDAs] or Tablets are better (if you have the budget)",
    "text": "Why Personal Digital Assistants [PDAs] or Tablets are better (if you have the budget)\nUse of a PDA/tablet allows the collection of more accurate and detailed data (Goldstein, 2012) because of:\n\nAutomated skip patterns\nMore detail, e.g. the ability to program a multi-stage code list\nThe ability to program some randomization algorithms (permuted block randomization, for example)\nSensitive questions can be recorded by the respondent themselves on the tablet (instead of the enumerator). There are even ways, using sound and video playback, to do this with illiterate respondents\nThe ability to audio record responses for later transcription\n\nPDAs/tablets have lower error rates than paper-based surveys (Caeyers, 2010) and have superior quality control options including:\n\nReal-time data upload\nReal-time survey modification in the case of error or oversight in terms of questions included\nAudio recording of portions of surveys to verify enumerator delivery\nTimers that measure how long respondents spend on the entire survey and each individual question\nReal-time validation checks to make sure numerical questions don’t have answers that are nonsensical\nThe ability to generate several orthogonal treatments within a single survey (either for multiple experiments or for conjoint experiments)"
  },
  {
    "objectID": "guides/getting-started/survey-implementation_en.html#tips-for-pda-or-tablet-use",
    "href": "guides/getting-started/survey-implementation_en.html#tips-for-pda-or-tablet-use",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Tips for PDA or tablet use",
    "text": "Tips for PDA or tablet use\n\nAlways buy extra equipment – chargers, battery packs, power strips, and tablets can go missing, be stolen, or get broken. In many countries you can’t buy extra equipment, even in capital cities, and it’s often more expensive and lower quality than what you can get at your home base. Buying 10-20% more equipment than you need can be expensive, but it is usually far cheaper than the salaries that you will have to pay for enumerators with equipment problems who do not have backups.\nPay close attention to battery life when you buy your equipment. If you want full days of enumeration, some of the cheaper tablets will not work.\nBudget for extra battery packs for your enumerators to carry in the field, particularly if they will travel to rural areas where they may not always be able to charge the tablets every night.\nForecast how frequently teams will be able to upload recorded data. The PDAs/tablets need to be able to store data from completed interviews until uploading is possible. In rural environments this can mean quite a lot of memory is needed, particularly if the survey is long and/or complex.\nBudget extra costs for charging of the PDAs or tablets in the field (i.e. paying for extra generator time from hotels) and for potential delays because of lack of power.\nBudget extra time for exhaustive testing of the PDA/tablet once the survey is fully coded. Code failures can be disastrous the more complex the code/randomization becomes. Run through as many different responses to the survey as you can yourself, and do a “fake” pre-test during training in which you collect data and inspect it to make sure there are no errors.\nHave someone available to make on-the-spot changes to the code in case problems are discovered in the field that stall enumeration until the change is implemented. In addition, give your enumerators enough paper versions of the survey to last for one or more days to serve as a holdover until the code is remedied.\nIf possible, name your variables in the survey software to avoid a really laborious process of manually re-naming later. This also makes it easier to inspect the data in real-time.\nCode answer values (i.e. the values that will be outputted into the dataset) in advance so that you can standardize scales and easily clean the data (for example, use different negative numbers for “don’t know” and “refuse” options so a 10-character command in R can clean the whole dataset).\nTake advantage of having more space for text by giving enumerators directions for complicated items in the displayed question text itself."
  },
  {
    "objectID": "guides/getting-started/survey-implementation_en.html#surveying",
    "href": "guides/getting-started/survey-implementation_en.html#surveying",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Surveying",
    "text": "Surveying\nField teams are made up of enumerators and a team leader. Team leaders report to a field manager, or in a case of a large survey, a regional supervisor.\n\nThe enumerator’s role is sampling and selecting households and respondents within enumeration areas, gain consent, and conduct the interview. The enumerator’s tasks include:\n\nSelecting the household. For an enumerator this is the first stage in the random selection process and is done according to a clearly specified procedure, which should be easily referenced in both the manual and the survey instrument itself.\nSelecting respondents. Once the household has been selected, the enumerator should follow a similarly-specified random (or systematic) selection process in order to select the subject.\nConsent. Enumerators need to know the definition of informed consent and how to make sure the respondent understands his/her rights during the interview.\nConducting the interview. This will involve asking questions and closely following the instructions communicated in training and on the questionnaires. A hard copy of question-by-question instructions should be provided to enumerators for use as a reference.\nControlling the interview situation. The enumerator must work towards reducing or eliminating suspicion and prejudice within the interview environment. This may involve asking bystanders not to congregate or dealing with sensitive situations with respect to other family members within the home.\nAvoiding bias. The enumerator’s personal views must not be reflected in the data collected. This means, among other things, that the enumerator must remain neutral and respectful during the data collection by not expressing his or her opinion and ensuring that the respondent trusts that the enumerator will respect his or her privacy. Emphasize during training that there really is no right answer and that the goal of the survey is to find out what people really think – because without knowing that, we can’t find solutions to problems.\nPresentation. Interpersonal skills such as manners, dressing, body language and ability to persuade are all important for data quality and will help in obtaining the target respondents for each day.\n\nThe team leader manages a group of enumerators and can conduct interviews him/herself. The team leader is responsible for:\n\nLogistics. The team leader is responsible for organizing the transport of teams, gathering materials, transporting paper instruments, and managing the technology.\nPermissions. Team leaders make contact with local authorities to introduce and explain the survey and get permission to work.\nSupervising. Team Leaders ensure the team arrives on time to the enumeration area and proceeds to oversee within-enumeration area selection of households.\nCorrecting. Once interviewing has begun, team leaders should move between enumerators and check they are following protocols. Supervision should not, however, make subjects feel uncomfortable.\nData Quality. Team Leaders check all questionnaires in the field and at the end of the day. If PDAs/tablets are used either the team leader or an RA will check data. The team leader ensures that data errors are fixed by revisiting respondents."
  },
  {
    "objectID": "guides/getting-started/survey-implementation_en.html#oversight",
    "href": "guides/getting-started/survey-implementation_en.html#oversight",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Oversight",
    "text": "Oversight\nEnumeration teams and field management can quite easily deviate from important protocols—these deviations can range from replacing sampled households based on the ease of getting respondents to creating fake data. In many cases, cutting corners is not easy to detect and can save money and time for the enumerators, field managers, and even the survey research firm. PDAs/tablets can reduce the number of total possible types of fraud, but some level of field supervision is always necessary. A parallel reporting structure, with independent oversight, can help guard against these deviations.\nOn the oversight side, there are two types of checks that should be conducted– audits and backchecks:\n\nAuditors arrive at randomly selected villages on the days they are slated for enumeration, without advance warning to the team. When an auditor visits a team, they make sure the team is in the correct enumeration area, that they have sought consent from local leaders, complied with the household selection procedure, and that all team members are working. The auditor then tracks the performance of the team throughout the day— as they seek consent, build rapport, conduct within-enumeration area sampling, and survey respondents.\nA backcheck consists of a revisit to a respondent who completed a survey not more than a few days prior. Using the data collected, they locate respondents and verify responses on a few key questions, for which the response was not likely to have changed (for example, age or household size) in the period since they were initially contacted. Backchecking can both help to identify enumerators who are not performing and establish an error rate. If phone numbers are being collected in the survey, this can be done more cheaply by telephone.\n\nConducting both audits and backchecks means that for each individual survey there is some non-zero probability that the work will be checked in some way. In the case that there are only backchecks, teams will never be monitored in terms of their adherence to protocols as they sample and conduct interviews. In the case that there are only audits, if a team is not visited on a particular day of work there is no chance to check that they actually interviewed subjects and recorded their responses accurately.\nAuditors and backcheckers must report directly to survey management. Imagine an example: Say a village is difficult to find and the team of enumerators chooses a replacement (rather than resampling by the PIs), and the auditors visit the sampled village and uncover it was not surveyed. If this error is communicated to someone also managing the enumerators, their best response is to cover this up or try to fix it without the PI knowing. This prevents having to admit a management mistake, and having to add a day of work to revisit the original or resampled village. If the survey team is notified directly, there is an opportunity to fix the mistake and make personnel changes as needed. Unmonitored communication between auditing teams and enumeration teams can result in a lot of unauthorized fixes and unexplainable data patterns."
  },
  {
    "objectID": "guides/getting-started/survey-implementation_en.html#sources",
    "href": "guides/getting-started/survey-implementation_en.html#sources",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Sources",
    "text": "Sources\nIf you are working alone, recruit experienced enumerators through contacts at survey firms, NGOs, or universities. It is important that enumerators are experienced, literate, educated, and able to build rapport with subjects. Hiring enumerators who are connected, in some way, with the survey leaders or local coordinator, e.g. through a youth organization or other social tie, can help immensely with oversight as the enumerators have bigger reputational costs if they shirk their duties."
  },
  {
    "objectID": "guides/getting-started/survey-implementation_en.html#languages",
    "href": "guides/getting-started/survey-implementation_en.html#languages",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Languages",
    "text": "Languages\nThe foremost requirement is that the enumerators speak the required local languages. We know that coethnicity between enumerators and subjects can reduce bias, so recruitment of coethnic interviewers, and balancing across the sample if using treatment and control groups, is important."
  },
  {
    "objectID": "guides/getting-started/survey-implementation_en.html#gender-parity",
    "href": "guides/getting-started/survey-implementation_en.html#gender-parity",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Gender parity",
    "text": "Gender parity\nHaving a team of mostly male enumerators interview a sample with equal numbers of men and women there can introduce response bias. For sensitive questions, such as questions on sexual behavior or violence, it is strongly recommended that women interview other women. If it is difficult to recruit experienced women enumerators, it usually makes sense to hold a special training for women candidates with less experience in order to ensure teams are balanced in the end."
  },
  {
    "objectID": "guides/getting-started/survey-implementation_en.html#standards",
    "href": "guides/getting-started/survey-implementation_en.html#standards",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Standards",
    "text": "Standards\nTrainings establish consistent standards for data collection. If you are contracting a survey firm and are not on the ground yourself, training is the most important part of the process to personally attend. It’s a key moment to communicate quality standards, expectations, the intended meaning of each question, and teach important procedures that may be more technical than what the firm is used to, such as a list experiment. It is also a key moment to motivate the team, by communicating the project’s goals and importance. In order to ensure that the each member of the team is prepared to a certain standard, it is a good idea to test each team member at the end of the training period. There should be an expectation that some team members will be asked not to proceed any further with the project as a result of the test, which will emphasize the importance of taking training to heart and taking the test seriously."
  },
  {
    "objectID": "guides/getting-started/survey-implementation_en.html#assessment",
    "href": "guides/getting-started/survey-implementation_en.html#assessment",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Assessment",
    "text": "Assessment\nTrainings are a key moment for assessment as well. If you train teams together it is easy to spot management issues and leadership capabilities. A good practice is to train teams together, and select team leaders at the end of training—this gives you a few days to gauge skills and also incentivizes trainees to perform during the training."
  },
  {
    "objectID": "guides/getting-started/survey-implementation_en.html#participation",
    "href": "guides/getting-started/survey-implementation_en.html#participation",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Participation",
    "text": "Participation\nTrainings set the tone for the rest of fieldwork. Beyond communicating standards and expectations, this is also a key moment to create a culture of participation. Encouraging trainees to speak out about issues with the survey can show that you are open to feedback and increase the chances that they will report adverse events or challenges during the actual data collection.\nTraining sections:\n\nProtocol review, e.g. how to get permission to work in village/household, how to select respondent, etc.\nDiscussion of unforeseen contingencies, e.g. what happens when you get a refusal, when enumerators are targeted or threatened, when enumerators observe perverse reactions to or consequences of the survey\nQuestion by question review\nInterviewing techniques (rapport building, discussions)\nReview translated instrument\nUsing a PDA/tablet and conducting the interview on a PDA/tablet\nUsing GPS\nGroup practice (enumerators interview each other and get feedback)\nField practice– at least one day of training (or more for complicated or long surveys) should be spent interviewing real people who are similar to the survey subjects\nCertification exam\n\nTraining usually takes several more days than you expect. See below for a rough guide to realistic training schedules.\n\nList of documents needed for training:\n\nInstrument + translated instrument\nQuestion-by-question guide\nManual with expectations, instructions for filling responses, tips on interviewing etc.\nProtocol for sampling, consent, reporting, and unforeseen contingencies\n\nBefore being deployed to the field, each enumerator must:\n\nBe able to correctly list, sample and interview individuals in the enumeration area\nUnderstand their role\nUnderstand and correctly follow interviewing protocols\nBe informed about oversight procedures\nComplete an IRB-approved module on human subjects protection\n\nData from mock surveys must be individually assessed and feedback given to each enumerator. You can check whether certain enumerators are entering data differently than their peers, for example by entering lots of “Don’t know” or “Refuse” answers, finding low prevalence of sensitive behaviors, or entering data that is logically inconsistent. However, there is a lot that you can’t tell from the data alone. Spending a lot of time observing enumerators while they run surveys can greatly increase the quality of the data by improving their training, allowing you to select the best enumerators more accurately, and allowing you to understand how the questions are being implemented in the field."
  },
  {
    "objectID": "guides/getting-started/survey-implementation_en.html#expectations-and-quality-control",
    "href": "guides/getting-started/survey-implementation_en.html#expectations-and-quality-control",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Expectations and Quality Control",
    "text": "Expectations and Quality Control\nExpectations should be laid out clearly during training, in a manual, and reiterated in clearly worded contracts (signed after training).\nBasic expectations of enumerators:\n\nBeing on time\nAdhering to within-enumeration area sampling and replacement scheme\nGetting informed consent\nBuilding rapport with subjects\nAccurately recording responses\nCommunication with supervisors"
  },
  {
    "objectID": "guides/getting-started/survey-implementation_en.html#contracts-and-payment",
    "href": "guides/getting-started/survey-implementation_en.html#contracts-and-payment",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Contracts and Payment",
    "text": "Contracts and Payment\nAs much as possible, make payment dependent on delivery. Enumerators have less and less incentive to stick with the project towards the end of fieldwork. The marginal returns are lower and they may be concerned about finding new work. In order to offset this, it is good practice to withhold a portion of their total salary (+/- 30%) until the end of fieldwork, and sometimes until data has been thoroughly reviewed if using paper instruments that need to be entered manually. At the same time, enumerators are often living paycheck to paycheck and may have expenses to cover during their long absence in the field. It is important to pay an advance up front to allow enumerators to take care of personal expenses that may otherwise make them anxious and unhappy during fieldwork. Having a strong local manager who understands the enumerators financial situations can help you create incentives while still making sure that they perceive the compensation structure as fair and adequate."
  },
  {
    "objectID": "guides/getting-started/survey-implementation_en.html#soft-incentives",
    "href": "guides/getting-started/survey-implementation_en.html#soft-incentives",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Soft Incentives",
    "text": "Soft Incentives\nSoft incentives help to keep teams happy and motivated throughout work. Some examples are:\n\nPerformance-based bonuses: Allowing managers to give performance-based bonuses for exceptional performance on a daily or weekly basis.\nCertificates: Survey trainings often involve learning portable skills, like the use of tablets or PDAs. Certificates can help enumerators prove to new employers that that they have these skills.\nLetters of Recommendation\nRecommendations to other survey firms, NGOs, etc.\nWrap party"
  },
  {
    "objectID": "guides/getting-started/survey-implementation_en.html#checking-data-entered-from-paper-instruments",
    "href": "guides/getting-started/survey-implementation_en.html#checking-data-entered-from-paper-instruments",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Checking data entered from paper instruments",
    "text": "Checking data entered from paper instruments\nAfter interviewing the team leader needs to review all instruments for completeness and accuracy. If there are missing data or other inconsistencies, the team leader should send the enumerator back to revisit the respondent to correct all problems before leaving the area.\nOnce instruments are collected, data entry should commence as soon as possible. All data should be entered twice, and any discrepancies should be checked by a supervisor against the paper instrument."
  },
  {
    "objectID": "guides/getting-started/survey-implementation_en.html#checking-data-gathered-using-pdas-or-tablets",
    "href": "guides/getting-started/survey-implementation_en.html#checking-data-gathered-using-pdas-or-tablets",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Checking data gathered using PDAs or tablets",
    "text": "Checking data gathered using PDAs or tablets\nWhen using tablets or PDAs, checking the data is the responsibility of the RA and PIs. In addition to using a script that checks for patterns and outliers, it is also best practice to record selected portions of the interview and listen to a subsample of responses, both for errors and quality."
  },
  {
    "objectID": "guides/getting-started/causal-inference_esp.html",
    "href": "guides/getting-started/causal-inference_esp.html",
    "title": "10 cosas que debe saber sobre la inferencia causal",
    "section": "",
    "text": "Resumen\nEl filósofo David Lewis describió la causalidad como “algo que marca la diferencia, y esa diferencia que hace, debe ser la diferencia entre lo que fue y lo que hubiera sido sin ese algo”. Esta es la interpretación que dan a la causalidad la mayoría de los experimentalistas. Aunque la definición parece simple, tiene muchas implicaciones sutiles. Aquí les presentamos diez ideas implícitas en esta noción de causalidad que son importantes para el diseño de investigación.1\n\n\n1. Una afirmación causal es un enunciado sobre lo que no sucedió\nPara la mayoría de investigadores que realizan experimentos, el enunciado “\\(X\\) causó \\(Y\\)” significa que \\(Y\\) ocurrió y que no habría ocurrido si \\(X\\) no hubiera estado presente. Esta definición requiere que tengamos una noción de lo que podría haber sucedido, pero no sucedió.2 De manera similar, el “efecto” de \\(X\\) en \\(Y\\) se considera la diferencia entre el valor que \\(Y\\) habría tomado dado un valor de \\(X\\) y el valor que \\(Y\\) habría tomado dado otro valor de \\(X\\). Debido al enfoque en la diferencia de los resultados, este enfoque a veces se conoce como el enfoque de causalidad basado en “hacer diferencias” o en lo “contrafactual”.\nNota técnica: Los estadísticos emplean el marco de “resultados potenciales” para describir las relaciones contrafactuales. En este marco, dejamos que \\(Y_i(1)\\) denote el valor que la unidad \\(i\\) tomaría bajo la condición uno (por ejemplo, si la unidad \\(i\\) recibió un tratamiento) y \\(Y_i(0)\\) el valor que habría sido observado en otra condición (por ejemplo, si la unidad \\(i\\) no recibió el tratamiento). Un efecto causal del tratamiento para la unidad \\(i\\) puede ser una simple diferencia de los resultados potenciales \\(\\tau_i = Y_i(1)-Y_(0)\\). Un tratamiento tiene un efecto causal (positivo o negativo) en \\(Y\\) para la unidad \\(i\\) si \\(Y_i (1) \\neq Y_i (0)\\).\n\n\n2. No hay relación causal sin manipulación.\nLa definición “contrafactual” de causalidad requiere que uno sea capaz de pensar qué valores podemos observar en diferentes condiciones. ¿Cómo serían las cosas si se eligiera un partido en lugar de otro? Las declaraciones causales cotidianas a menudo no cumplen con este requisito en alguna de estas dos formas:\n\nPrimero, algunas declaraciones no especifican condiciones contrafactuales claras. Por ejemplo, la afirmación de que “la recesión fue causada por Wall Street” no apunta a un contrafactual obvio: ¿debemos considerar si habría habido una recesión si Wall Street no existiera? ¿O es la afirmación realmente una afirmación sobre acciones particulares que Wall Street podría haber tomado pero no lo hizo? Si es así, ¿qué acciones? Es difícil evaluar la validez de tales declaraciones. Además, puede depender de qué condiciones contrafactuales estén implícitas en una afirmación.\nEn segundo lugar, algunos enunciados implican condiciones contrafactuales que no son posibles de imaginar. Por ejemplo, la afirmación de que Peter consiguió el trabajo porque es Peter implica una consideración de lo que habría sucedido si Peter no fuera Peter. Alternativamente, la afirmación de que Peter consiguió el trabajo porque es un hombre requiere considerar a Peter como algo diferente de un hombre. El problema es que los contrafactuales en estos casos implican un cambio no solo en la condición que enfrenta un individuo sino en el propio individuo.\n\nPara evitar estos problemas, algunos estadísticos instan a restringir las afirmaciones causales a los tratamientos que pueden manipularse, al menos en la imaginación, y no necesariamente en la práctica.3 Por ejemplo, si bien podríamos tener dificultades con la afirmación de que Peter consiguió el trabajo porque es hombre, no tendríamos las mismas dificultades con la afirmación de que Peter consiguió el trabajo porque la agencia de contratación pensó que era hombre\n\n\n3. Las causas no tienen por qué ser rivales.\nAunque nos podemos centrar en el efecto de una sola causa \\(X\\) en un variable de resultado \\(Y\\), generalmente no esperamos que \\(Y\\) tenga solo una causa.4 Además, si sumamos los efectos causales de diferentes causas, no hay razón para esperar que sumen el 100%. Por lo tanto, no tiene mucho sentido tratar de “distribuir” los resultados entre diferentes factores causales. En otras palabras, las causas no tienen por qué ser rivales. La Asociación Nacional del Rifle de Estados Unidos sostiene, por ejemplo, que las armas no matan a la gente, la gente mata a la gente. Esa afirmación no tiene mucho sentido en el marco hipotético. Quita las armas y no tendrás muertes por heridas de bala. Entonces las armas son una causa. Quita a la gente y tampoco tendrás muertes por heridas de bala, por lo que las personas también son una causa. Dicho de otra manera, estos dos factores son simultáneamente causas de los mismos resultados.\n\n\n4. \\(X\\) puede causar \\(Y\\) incluso si \\(X\\) no es una condición necesaria o una condición suficiente para que \\(Y\\) ocurra.\nA menudo hablamos de relaciones causales en términos deterministas. Incluso la cita de Lewis en la parte superior de esta página parece sugerir una relación determinista entre causas y efectos. A veces se piensa que las relaciones causales implican condiciones necesarias (para \\(Y\\) que ocurra, \\(X\\) tiene que suceder ); a veces se piensa que tales relaciones implican condiciones suficientes (si ocurre \\(X\\), entonces ocurre \\(Y\\)). Pero una vez que hablamos de múltiples unidades, hay al menos dos formas en las que podemos pensar en que \\(X\\) causa \\(Y\\) incluso si \\(X\\) no es una condición necesaria ni suficiente para \\(Y.\\) La primera es reinterpretar todo en términos probabilísticos: que \\(X\\) cause \\(Y\\), simplemente quiere decir que la probabilidad de \\(Y\\) es mayor cuando \\(X\\) está presente. Otra forma es permitir contingencias. Por ejemplo, \\(X\\) puede causar \\(Y\\) si la condición \\(Z\\) está presente, pero no de otra manera.5\n\n\n5. Existe un problema fundamental de la inferencia causal\nSi los efectos causales son enunciados sobre la diferencia entre lo que sucedió y lo que podría haber sucedido, entonces no los podemos medir. Malas noticias. De manera prospectiva, puede organizar las cosas para que pueda observar lo que sucede si alguien recibe un tratamiento o lo que sucede si no recibe el tratamiento. Sin embargo, para la misma persona nunca podrá observar ambos resultados y, por lo tanto, tampoco la diferencia entre ellos. Esta incapacidad para observar efectos causales a nivel de la unidad de estudio a menudo se denomina “problema fundamental de la inferencia causal”.\n\n\n6. Usted puede estimar el efecto causal promedio aun cuando no pueda observar ningún efecto causal individual.\nAunque no pueda observar si \\(X\\) causa \\(Y\\) para una unidad determinada, sí es posible determinar si \\(X\\) causa \\(Y\\) en promedio. La idea clave aquí es que el efecto causal promedio es igual a la diferencia entre la variable de resultado promedio para todas las unidades, si todas están en la condición de control y la variable de resultado promedio para todas las unidades si todas están en la condición de tratamiento. Muchas estrategias para la identificación causal (ver 10 estrategias para determinar si X causó Y) se enfocan en formas de aprender acerca de estas resultados potenciales promedio.6\n10 cosas que debe saber sobre las pruebas de hipótesis nos muestra cómo podemos aprender acerca de efectos causales individuales en vez de efectos promediodato el problema fundamental de la inferencia causal.\n\n\n7. La estimación del efecto causal promedio no requiere que los grupos de tratamiento y control sean idénticos.\nUna estrategia que la gente usa para aprender acerca del efecto causal promedio es crear grupos de tratamiento y control a través de la aleatorización (ver 10 estrategias para determinar si X causó Y). Es común que algunos investigadores se preocupen por que los grupos de tratamiento y control resultantes no sean similares en dimensiones relevantes.\nLa buena noticia es que la razón por la cual las diferencias en los resultados promedio entre los grupos de control y tratamiento asignados aleatoriamente capturan los efectos promedio del tratamiento (en valor esperado a través de aleatorizaciones repetidas dentro del mismo grupo de unidades) no se basa en que los grupos de tratamiento y control sean similares en las características observadas. Sino que se basa únicamente en la idea de que, en promedio, los resultados en los grupos tratados y de control capturarán los resultados promedio para todas las unidades en el grupo experimental si estuvieran, respectivamente, en tratamiento o en control. En la práctica, los grupos de tratamiento y de control no serán idénticos.7\n\n\n8. Correlación no es igual a causalidad.\nUna correlación entre \\(X\\) y \\(Y\\) es un enunciado sobre las relaciones entre los valores reales de estas variables y no sobre la relación entre los valores reales y los valores contrafactuales. Entonces las afirmaciones sobre causas y correlaciones no tienen mucho que ver entre sí. Las correlaciones positivas pueden ser consistentes con efectos causales positivos, efectos causales nulos o incluso con efectos causales negativos. Por ejemplo, tomar medicamentos para la tos se correlaciona positivamente con la tos, pero es de esperar que tenga un efecto causal negativo sobre la tos.8\n\nSi usted sabe que, en promedio, \\(A\\) causa $ B $ y \\(B\\) causa \\(C\\), esto no significa que, en promedio, \\(A\\) cause \\(C\\).\n\n== Se podría esperar que si \\(A\\) causa \\(B\\) y \\(B\\) causa \\(C\\), entonces \\(A\\) causa \\(C\\).9 Pero no hay razón para creer que las relaciones causales promedio sean transitivas. Para entender por qué, imagine que \\(A\\) causó \\(B\\) en los hombres pero no en las mujeres y \\(B\\) causó \\(C\\) en las mujeres pero no en los hombres. Entonces, en promedio, \\(A\\) causa \\(B\\) y \\(B\\) causa \\(C\\), pero es posible que no haya nadie para quien \\(A\\) tenga un efecto en \\(C\\) mediado por \\(B\\).\n\n\n10. Es más fácil aprender sobre los “efectos de las causas” que aprender sobre las “causas de los efectos”.\nAunque puedan parecer dos formas de decir exactamente lo mismo, existe una diferencia entre comprender cuál es el efecto de \\(X\\) en \\(Y\\) (los “efectos de una causa”) y si el valor que tomó \\(Y\\) se debió a \\(X\\) (la “causa de un efecto”).10 Considere el siguiente ejemplo. Supongamos que realizamos un experimento con una muestra que contiene el mismo número de hombres y mujeres. El experimento asigna aleatoriamente a hombres y mujeres a un tratamiento binario \\(X\\) y mide una variable de resultado binaria \\(Y\\). Además suponga que \\(X\\) tiene un efecto positivo de 1 para todos los hombres, es decir, el resultado potencial del control de los hombres es cero (\\(Y_i(0) = 0\\)) y la salida potencial cuando son tratados es uno (\\(Y_i(1) = 1\\)). Para todas las mujeres, \\(X\\) tiene un efecto negativo de \\(-1\\), es decir, la salida potencial de las mujeres bajo el control es uno (\\(Y_i (0) = 1\\)) y su salida potencial cuando son tratadas es cero (\\(Y_i (1) = 0\\)) En este ejemplo, el efecto promedio de \\(X\\) en $ Y $ es cero. Pero la razón para que los participantes en el grupo de tratamiento tengan \\(Y= 1\\), es porque \\(X = 1\\). De manera similar, todos los participantes en el grupo de tratamiento con \\(Y = 0\\), tienen \\(Y = 0\\) porque \\(X = 1\\). Los experimentos nos permiten obtener una respuesta exacta a la pregunta sobre los “efectos de una causa”, pero en general no es posible obtener una respuesta exacta a la pregunta sobre la “causa de un efecto”.11\n\n\n\n\n\nFootnotes\n\n\nAutor: Macartan Humphreys. Revisiones menores: Winston Lin y Donald P. Green, 24 de junio de 2016. Revisiones MH 6 de enero de 2020. Revisiones Anna Wilke de mayo de 2021. Esta guía es un documento dinámico y está sujeta a actualización por parte de los miembros de EGAP; los colaboradores enumerados no son responsables de las ediciones posteriores.↩︎\nHolland, Paul W. “Statistics and causal inference.” Journal of the American Statistical Association 81.396 (1986): 945-960.↩︎\nHolland, Paul W. “Statistics and causal inference.” Journal of the American Statistical Association 81.396 (1986): 945-960.↩︎\nEsto se conoce a veces como el “Problema de las causas excesivas”.↩︎\nDe acuerdo a Mackie, a veces se invoca la idea de condiciones “INUS” para capturar la dependencia de las causas de otras causas. Según esta explicación, una causa puede ser parte Insuficiente pero Necesaria de una condición que en sí misma es Innecesaria pero Suficiente. Por ejemplo, marcar un número de teléfono es una causa de contacto con alguien, ya que tener una conexión y marcar un número es suficiente (S) para hacer una llamada telefónica, mientras que marcar solo sin una conexión no sería suficiente (I), ni tener una conexión (N). Por supuesto, hay otras formas de contactar a alguien sin hacer llamadas telefónicas (U). Mackie, John L. “El cemento del universo”. Londres: Oxford Uni (1974).↩︎\nNota técnica: La idea técnica clave es que la diferencia de promedios es la misma que el promedio de diferencias. Es decir, usando el “operador de expectativas”, \\(\\text{E}(\\tau_i) = \\text{E}(Y_i (1) -Y_i (0)) = \\text{E}(Y_i (1)) - \\text{E}(Y_i (0))\\). Los términos dentro del operador de esperanzas en la segunda cantidad no se pueden estimar, pero los términos dentro de los operadores de expectativas en la tercera cantidad si se pueden ser estimados6 Vea la ilustración [aquí] (https://raw.githubusercontent.com/egap/ guías-métodos / maestro / inferencia-causal / PO.jpg).↩︎\nPor esta razón usar las pruebas \\(t\\) para verificar si “la asignación aleatoria funcionó bien” no tiene mucho sentido, al menos si se sabe que se siguió una procedimiento aleatorio: por simple chance, 1 de cada 20 de esas pruebas mostrará diferencias estadísticamente detectables entre los grupos tratados y de control. Si existen dudas sobre si la asignación aleatoria se realizó correctamente, estas pruebas se pueden utilizar para probar la hipótesis de que los datos se generaron efectivamente mediante un procedimiento aleatorio. Esta última razón para las pruebas de aleatorización puede ser especialmente importante en experimentos de campo donde las cadenas de comunicación entre la persona que crea los números aleatorios y la persona que implementa la asignación del tratamiento son largas y complejas.↩︎\nNota técnica: Sea \\(D_i\\) un indicador de si la unidad \\(i\\) ha recibido un tratamiento o no. Entonces la diferencia en los resultados promedio entre los que reciben el tratamiento y los que no lo reciben se puede escribir como \\(\\frac{\\sum_i D_i × Y_i (1)} {\\sum_iD_i} - \\frac {\\sum_i(1 - D_i) \\times Y_i (0)}{\\sum_i (1 - D_i)}\\). Sin información sobre cómo se asignó el tratamiento, no hay mucho por decir sobre si esta diferencia es un buen estimador del efecto promedio del tratamiento. Es decir, de la diferencia en los resultados potenciales promedio de las unidades en el grupo de tratamiento y control para todas las unidades. Lo que importa es si \\(\\frac{\\sum_i D_i × Y_i (1)} {\\sum_iD_i}\\) es una buena estimación de \\(\\frac{\\sum_i 1 × Y_i (1)} {\\sum_i1}\\) y si \\(\\frac{\\sum_i (1 - D_i) × Y_i (0)}{\\sum_i(1 - D_i)}\\) es una buena estimación de \\(\\frac{\\sum_i 1 × Y_i (0)} {\\sum_i1}\\). Este puede ser el caso si los que recibieron tratamiento son una muestra representativa de todas las unidades, pero de lo contrario no hay razón para esperar que así sea.↩︎\nEntiéndase la expresión “\\(A\\) causa \\(B\\), en promedio” como “el efecto promedio de \\(A\\) sobre \\(B\\) es positivo”.↩︎\nA veces se reinterpreta la pregunta “causas de los efectos” en el sentido de: ¿cuáles son las causas que tienen efectos sobre las variable de resultado? Véase Andrew Gelman and Guido Imbens, “Why ask why? Forward causal inference and reverse causal questions”, NBER Working Paper No. 19614 (Nov. 2013).↩︎\nVer, por ejemplo, Tian, J., Pearl, J. 2000. “Probabilities of Causation: Bounds and Identification.” Annals of Mathematics and Artificial Intelligence 28:287–313.↩︎"
  },
  {
    "objectID": "guides/getting-started/power_en.html",
    "href": "guides/getting-started/power_en.html",
    "title": "10 Things You Need to Know About Statistical Power",
    "section": "",
    "text": "Abstract\nThis guide1 will help you assess and improve the power of your experiments. We focus on the big ideas and provide examples and tools that you can use in R and Google Spreadsheets.\n\n\n1 What Power Is\nPower is the ability to distinguish signal from noise.\nThe signal that we are interested in is the impact of a treatment on some outcome. Does education increase incomes? Do public health campaigns decrease the incidence of disease? Can international monitoring decrease government corruption?\nThe noise that we are concerned about comes from the complexity of the world. Outcomes vary across people and places for myriad reasons. In statistical terms, you can think of this variation as the standard deviation of the outcome variable. For example, suppose an experiment uses rates of a rare disease as an outcome. The total number of affected people isn’t likely to fluctuate wildly day to day, meaning that the background noise in this environment will be low. When noise is low, experiments can detect even small changes in average outcomes. A treatment that decreased the incidence of the disease by 1% percentage points would be easily detected, because the baseline rates are so constant.\nNow suppose an experiment instead used subjects’ income as an outcome variable. Incomes can vary pretty widely – in some places, it is not uncommon for people to have neighbors that earn two, ten, or one hundred times their daily wages. When noise is high, experiments have more trouble. A treatment that increased workers’ incomes by 1% would be difficult to detect, because incomes differ by so much in the first place.\nA major concern before embarking on an experiment is the danger of a false negative. Suppose the treatment really does have a causal impact on outcomes. It would be a shame to go to all the trouble and expense of randomizing the treatment, collecting data on both treatment and control groups, and analyzing the results, just to have the effect be overwhelmed by background noise.\nIf our experiments are highly-powered, we can be confident that if there truly is a treatment effect, we’ll be able to see it.\n\n\n2 Why You Need It\nExperimenters often guard against false positives with statistical significance tests. After an experiment has been run, we are concerned about falsely concluding that there is an effect when there really isn’t.\nPower analysis asks the opposite question: supposing there truly is a treatment effect and you were to run your experiment a huge number of times, how often will you get a statistically significant result?\nAnswering this question requires informed guesswork. You’ll have to supply guesses as to how big your treatment effect can reasonably be, how many subjects will answer your survey, how many subjects your organization can realistically afford to treat.\nWhere do these guesses come from? Before an experiment is run, there is often a wealth of baseline data that are available. How old/rich/educated are subjects like yours going to be? How big was the biggest treatment effect ever established for your dependent variable? With power analysis, you can see how sensitive the probability of getting significant results is to changes in your assumptions.\nMany disciplines have settled on a target power value of 0.80. Researchers will tweak their designs and assumptions until they can be confident that their experiments will return statistically significant results 80% of the time. While this convention is a useful benchmark, be sure that you are comfortable with the risks associated with an 80% expected success rate.\nA note of caution: power matters a lot. Negative results from underpowered studies can be hard to interpret: Is there really no effect? Or is the study just not able to figure it out? Positive results from an underpowered study can also be misleading: conditional upon being statistically significant, an estimate from an underpowered study probably overestimates treatment effects. Under powered studies are sometimes based on overly optimistic assumptions; a convincing power analysis makes these assumptions explicit and should protect you from implementing designs that realistically have no chance of answering the questions you want to answer.\n\n\n3 The Three Ingredients of Statistical Power\nThere are three big categories of things that determine how highly powered your experiment will be. The first two (the strength of the treatment and background noise) are things that you can’t really control – these are the realities of your experimental environment. The last, the experimental design, is the only thing that you have power over – use it!\n\nStrength of the treatment. As the strength of your treatment increases, the power of your experiment increases. This makes sense: if your treatment were giving every subject $1,000,000, there is little doubt that we could discern differences in behavior between the treatment and control groups. Many times, however, we are not in control of the strength of our treatments. For example, researchers involved in program evaluation don’t get to decide what the treatment should be, they are supposed to evaluate the program as it is.\nBackground noise. As the background noise of your outcome variables increases, the power of your experiment decreases. To the extent that it is possible, try to select outcome variables that have low variability. In practical terms, this means comparing the standard deviation of the outcome variable to the expected treatment effect size — there is no magic ratio that you should be shooting for, but the closer the two are, the better off your experiment will be. By and large, researchers are not in control of background noise, and picking lower-noise outcome variables is easier said than done. Furthermore, many outcomes we would like to study are inherently quite variable. From this perspective, background noise is something you just have to deal with as best you can.\nExperimental Design. Traditional power analysis focuses on one (albeit very important) element of experimental design: the number of subjects in each experimental group. Put simply, a larger number of subjects increases power. However, there are other elements of the experimental design that can increase power: how is the randomization conducted? Will other factors be statistically controlled for? How many treatment groups will there be, and can they be combined in some analyses?\n\n\n\n4 Key Formulas for Calculating Power\nStatisticians have derived formulas for calculating the power of many experimental designs. They can be useful as a back of the envelope calculation of how large a sample you’ll need. Be careful, though, because the assumptions behind the formulas can sometimes be obscure, and worse, they can be wrong.\nHere is a common formula used to calculate power2\n\\[\\beta = \\Phi \\left(\\frac{|\\mu_t-\\mu_c|\\sqrt{N}}{2\\sigma}-\\Phi^{-1} \\left(1-\\frac{\\alpha}{2}\\right) \\right)\\]\n\n\\(\\beta\\) is our measure of power. Because it’s the probability of getting a statistically significant result, β will be a number between 0 and 1.\n\\(\\Phi\\) is the CDF of the normal distribution, and \\(\\Phi^{-1}\\) is its inverse. Everything else in this formula, we have to plug in:\n\\(\\mu_t\\) is the average outcome in the treatment group. Suppose it’s 65.\n\\(\\mu_c\\) is the average outcome in the control group. Suppose it’s 60.\nTogether, assumptions about μt and μc define our assumption about the size of the treatment effect: 65-60= 5.\n\\(\\sigma\\) is the standard deviation of outcomes. This is how we make assumptions about how noisy our experiment will be — one of the assumptions we’re making is that sigma is the same for both the treatment and control groups. Suppose \\(\\sigma=20\\)\n\\(\\alpha\\) is our significance level – the convention in many disciplines is that α should be equal to 0.05. \\(N\\) is the total number of subjects. This is the only variable that is under the direct control of the researcher. This formula assumes that every subject had a 50/50 chance of being in control. Suppose that \\(N=500\\).\n\nWorking through the formula, we find that under this set of assumptions, \\(β = 0.80\\), meaning that we have an 80% chance of recovering a statistically significant result with this design. Click here for a google spreadsheet that includes this formula. You can copy these formulas directly into Excel. If you’re comfortable in R, here is code that will accomplish the same calculation.\n\npower_calculator <- function(mu_t, mu_c, sigma, alpha=0.05, N){ \n  lowertail <- (abs(mu_t - mu_c)*sqrt(N))/(2*sigma) \n  uppertail <- -1*lowertail \n  beta <- pnorm(lowertail- qnorm(1-alpha/2), lower.tail=TRUE) + 1- pnorm(uppertail- qnorm(1-alpha/2), lower.tail=FALSE) \n  return(beta) \n  } \n\n\n\n5 When to Believe Your Power Analysis\nFrom some perspectives the whole idea of power analysis makes no sense. You want to figure out the size of some treatment effect but first you need to do a power analysis which requires that you already know your treatment effect and a lot more besides.\nSo in most power analyses you are in fact seeing what happens with numbers that are to some extent made up. The good news is that it is easy to find out how much your conclusions depend on your assumptions: simply vary your assumptions and see how the conclusions on power vary.\nThis is most easily seen by thinking about how power varies with the number of subjects. A power analysis that looks at power for different study sizes simply plugs in a range of values in for N and seeing how β changes.\nUsing the formula in section 4, you can see how sensitive power is to all of the assumptions: Power will be higher if you assume the treatment effect will be larger, or if you’re willing to accept a higher alpha level, or if you have more or less confidence in the noisiness of your measures.3\n\n\n\n\n6 How to Use Simulation to Estimate Power\nPower is a measure of how often, given assumptions, we would obtain statistically significant results, if we were to conduct our experiment thousands of times. The power calculation formula takes assumptions and return an analytic solution. However, due to advances in modern computing, we don’t have to rely on analytic solutions for power analysis. We can tell our computers to literally run the experiment thousands of times and simply count how frequently our experiment comes up significant.\nThe code block below shows how to conduct this simulation in R.\n\npossible.ns <- seq(from=100, to=2000, by=40) # The sample sizes we'll be considering\nstopifnot(all( (possible.ns %% 2)==0 )) ## require even number of experimental pool\npowers <- rep(NA, length(possible.ns)) # Empty object to collect simulation estimates \nalpha <- 0.05 # Standard significance level \nsims <- 500 # Number of simulations to conduct for each N \n#### Outer loop to vary the number of subjects #### \nfor (j in 1:length(possible.ns)){ N <- possible.ns[j] # Pick the jth value for N \n  Y0 <- rnorm(n=N, mean=60, sd=20) # control potential outcome \n  tau <- 5 # Hypothesize treatment effect \n  Y1 <- Y0 + tau # treatment potential outcome                                   \n  significant.experiments <- rep(NA, sims) # Empty object to count significant experiments \n                                  \n  #### Inner loop to conduct experiments \"sims\" times over for each N #### \n        Y0 <- rnorm(n=N, mean=60, sd=20) # control potential outcome \n        tau <- 5 # Hypothesize treatment effect \n        Y1 <- Y0 + tau # treatment potential outcome \n  for (i in 1:sims){ \n        ## Z.sim <- rbinom(n=N, size=1, prob=.5) # Do a random assignment  by coin flip\n        Z.sim <- sample(rep(c(0,1),N/2)) ## Do a random assignment ensuring equal sized groups\n        Y.sim <- Y1*Z.sim + Y0*(1-Z.sim) # Reveal outcomes according to assignment \n        fit.sim <- lm(Y.sim ~ Z.sim) # Do analysis (Simple regression) \n        p.value <- summary(fit.sim)$coefficients[2,4] # Extract p-values \n        significant.experiments[i] <- (p.value <= alpha) # Determine significance according to p <= 0.05\n        }\n  powers[j] <- mean(significant.experiments) # store average success rate (power) for each N \n  } \npowers \n\n [1] 0.188 0.282 0.362 0.498 0.508 0.604 0.614 0.674 0.670 0.766 0.794 0.828\n[13] 0.850 0.840 0.896 0.892 0.884 0.910 0.968 0.970 0.980 0.970 0.972 0.982\n[25] 0.974 0.986 0.992 0.994 0.980 0.990 0.998 0.994 0.994 1.000 0.998 1.000\n[37] 0.998 1.000 0.996 0.998 1.000 1.000 1.000 0.998 1.000 1.000 1.000 1.000\n\n\nThe code for this simulation and others is available here. Simulation is a far more flexible, and far more intuitive way to think about power analysis. Even the smallest tweaks to an experimental design are difficult to capture in a formula (adding a second treatment group, for example), but are relatively straightforward to include in a simulation.\nIn addition to counting up how often your experiments come up statistically significant, you can directly observe the distribution of p-values you’re likely to get. The graph below shows that under these assumptions, you can get expect to get quite a few p-values in the 0.01 range, but that 80% will be below 0.05.\n\n\n\n7 How to Change your Design to Improve Your Power\nWhen it comes to statistical power, the only thing that that’s under your control is the design of the experiment. As we’ve seen above, an obvious design choice is the number of subjects to include in the experiment. The more subjects, the higher the power.\nHowever, the number of subjects is not the only design choice that has consequences for power. There are two broad classes of design choices that are especially important in this regard.\n\nChoice of estimator. Are you using difference-in-means? Will you be doing some transformation, such as a logit or a probit? Will you be controlling for covariates? Will you be using some kind of robust standard error estimator? All of these choices will make a difference for the statistical significance of your results, and therefore for the power of your experiment. One easy way to think about this is to imagine what command you’ll be running in R or Stata after the experiment has come back; that’s your estimator!\nRandomization Protocol. What kind of randomization will you be employing? Simple randomization gives all subjects an equal probability of being in the treatment group, and then performs a (possibly weighted) coin flip for each. Complete randomization is similar, but it ensures that exactly a certain number will be assigned to treatment. Block randomization is even more powerful — it ensures that a certain number within a subgroup will be assigned to treatment. A restricted random assignment rejects some random assignments based on some set of criteria — lack of balance perhaps. These various types of random assignment can dramatically increase the power of an experiment at no extra cost. Read up on randomization protocols here.\n\nThere are too many choices to cover in this short article, but check out the Simulation for Power Analysis code page for some ways to get started. But to give a flavor of the simulation approach, consider how you would conduct a power analysis if you wanted to include covariates in your analysis.\nIf the covariates you include as control variables are strongly related to the outcome, then you’ve dramatically increased the power of your experiment.Unfortunately, the extra power that comes with including control variables is very hard to capture in a compact formula. Almost none of the power formulas found in textbooks or floating around on the internet can provide guidance on what the inclusion of covariates will do for your power.\nThe answer is simulation.\n\nSuppose we’re studying the effect of an educational intervention on income\nSuppose we have good data on the relationship between two covariates and income: age and gender. In this economy, men earn more than women, and older people earn more than younger people.\nRun a regression of income on age and gender and record the coefficients, using pre-existing survey data (better yet: use baseline data from future participants in your experiment!) *Generate fake covariate data — N total subjects, but broken up by age and gender in a way that reflects your experimental subject pool.\nGenerate fake control data — where the outcome is a function of age and gender according to your regression estimates\nHypothesize a treatment effect to generate fake treatment data\nRun the experiment 10,000 times, and record how often, using a regression with controls, your experiment turns up significant.\n\nHere’s a graph that compares the power of an experiment that does control for background attributes to one that does not. The R-square of the regression relating income to age and gender is pretty high — around .66 — meaning that the covariates that we have gathered (generated) are highly predictive. For a rough comparison, sigma, the level of background noise that the unadjusted model is dealing with, is around 33. This graph shows that at any N, the covariate-adjusted model has more power — so much so that the unadjusted model would need 1500 subjects to achieve what the covariate-adjusted model can do with 500.\n\nThis approach doesn’t rely on a formula to come up with the probability of getting a statistically significant result: it relies on brute force! And because simulation lets you specify every step of the experimental design, you do a far more nuanced power analysis than simply considering the number of subjects.\n\n\n8 Power Analysis for Multiple Treatments\nMany experiments employ multiple treatments which are compared both to each other and to a control group. This added complication changes what we mean when we say the “power” of an experiment. In the single treatment group case, power is just the probability of getting a statistically significant result. In the multiple treatment case, it can mean a variety of things: A) the probability of at least one of the treatments turning up significant, B) the probability of all the treatments turning up significant (versus control) or C) the probability that the treatments will be ranked in the hypothesized order, and that those ranks will be statistically significant.\nThis question of multiple treatment arms is related to the problem of multiple comparisons. (See our guide on this topic for more details.) Standard significance testing is based on the premise that you’re conducting a single test for statistical significance, and the p-values derived from these tests reflect the probability under the null of seeing such a larger (or larger) treatment effect. If, however, you are conducting multiple tests, this probability is no longer correct. Within a suite of tests, the probability that at least one of the tests will turn up significant even when the true effect is zero is higher, essentially because you have more attempts. A commonly cited (if not commonly used) solution is to use the Bonferroni correction: specify the number of comparisons you will be making in advance, then divide your significance level (alpha) by that number.\nIf you are going to be using a Bonferroni correction, then standard power calculators will be more complicated to use: you’ll have to specify your Bonferroni-corrected alpha levels and calculate the power of each separate comparison. To calculate the probability that all the tests are significant, multiply all the separate powers together. To calculate the probability that at least one of the tests is significant, calculate the probability that none are, then subtract from one.\nOr you can use simulation. An example of a power calculation done in R is available on the simulations page.\n\n\n9 How to Think About Power for Clustered Designs\nWhen an experiment has to assign whole groups of people to treatment rather than individually, we say that the experiment is clustered. This is common in educational experiments, where whole classrooms of children are assigned to treatment or control, or in development economics, where whole villages of individuals are assigned to treatment or control. (See our guide on cluster randomization for more details.)\nAs a general rule, clustering decreases your power. If you can avoid clustering your treatments, that is preferable for power. Unless you face concerns related to spillover, logistics, or ethics, take the variation down to the lowest level that you can.\nThe best case scenario for a cluster-level design is when which cluster a subject is in provides very little information about their outcomes. Suppose subjects were randomly assigned to clusters — the cluster wouldn’t help to predict outcomes at all. If the cluster is not predictive of the outcome, then we haven’t lost too much power to clustering.\nWhere clustering really causes trouble is when there is a strong relationship between the cluster and the outcome. To take the villages example, suppose that some villages are, as a whole, much richer than others. Then the clusters might be quite predictive of educational attainment. Clustering can reduce your effective sample size from the total number of individuals to the total number of clusters.\nThere are formulas that can help you understand the consequences of clustering — see Gelman/Hill page 447-449 for an extended discussion. While these formulas can be useful, they can also be quite cumbersome to work with. The core insight however is a simple one: you generally get more power from increasing the number of clusters than you do from increasing the number of subjects within clusters. Better to have 100 clusters with 10 subjects in each than 10 clusters with 100 subjects in each.\nAgain, a more flexible approach to power analysis when dealing with clusters is simulation. See the (Declare Design library for block and cluster randomized experiments)[https://declaredesign.org/r/designlibrary/reference/block_cluster_two_arm_designer.html] for some starter code. The (DeclareDesign)[https://declaredesign.org] software aims to make simulations for power analysis (among many other tasks) easier. See also Gelman/Hill page 450-453 for another simulation approach.\n\n\n10 Good Power Analysis Makes Preregistration Easy\nWhen you deal with power you focus on what you cannot control (noise) and what you can control (design). If you use the simulation approach to power analysis then you will be forced to imagine how your data will look and how you will handle it when it comes in. You will get a chance to specify all of your hunches and best guesses in advance, so that you can launch your experiments with clear expectations of what they can and cannot show. That’s some work but the good news is that if you really do it you are most of the way to putting together a comprehensive and registerable pre-analysis plan.\n\n\n\n\n\nFootnotes\n\n\nOriginating author: Alex Coppock, 20 Nov 2013. The guide is a live document and subject to updating by EGAP members at any time. Coppock is not responsible for subsequent edits to this guide↩︎\nReproduced from Gerber and Green 2012, page 93↩︎\nFor an additional online power visualization tool, see Kristoffer Magnusson’s R Psychologist blog.↩︎"
  },
  {
    "objectID": "guides/getting-started/spillovers_en.html",
    "href": "guides/getting-started/spillovers_en.html",
    "title": "10 Things to Know About Spillovers",
    "section": "",
    "text": "Abstract\nThis guide1 helps you think through how to design and analyze experiments when there is a risk of “interference” between units. This has been an important area of research in recent years and there have been real gains in our understanding of how to detect spillover effects. Spillovers arise whenever one unit is affected by the treatment status of another unit. Spillovers make it difficult to work out causal effects (we say why below). Experimentalists worry a lot about them, but the complications that spillovers create are not unique to randomized experiments.\n\n\n1. What they are\nSpillovers refer to a broad class of instances in which a given subject is influenced by whether other subjects are treated.\nHere are some examples of how spillovers (or “interference”) might occur:\n\nPublic Health: Providing an infectious disease vaccine to some individuals may decrease the probability that nearby individuals become ill.\nCriminology: Increased enforcement may displace crime to nearby areas.\nEducation: Students may share newly acquired knowledge with friends.\nMarketing: Advertisements displayed to one person may increase product recognition among her work colleagues.\nPolitics: Election monitoring at some polling stations may displace fraud to neighboring polling stations.\nEconomics: Lowering the cost of production for one firm may change the market price faced by other firms.\nWithin-subjects experiments across many domains: the possibility that treatment effects persist or that treatments are anticipated can be modeled as a kind of spillover.\n\nThese examples share some features:\n\nAn intervention: the vaccine, increased enforcement, election monitoring;\nAn outcome: incidence of disease, crime rates, electoral fraud; and\nA “network” that links units together: face-to-face social interaction, geographic proximity within a city, road distance between polling stations.\n\nThe network is a crucial feature of any spillover analysis. For each unit, it describes the set of other units whose treatment assignments “matter.” To take the education example: it may matter to me if you treat another student in my classroom, but it probably doesn’t matter if you treat a student in a different city. I’m connected to the other students in my classroom but not to students in other cities.\n\n\n2. If ignored, spillovers may “bias” treatment effect estimates\nIf unaddressed, spillovers “bias” standard estimates of treatment effects (e.g., differences-in-means). “Bias” is in scare quotes because those estimators will return unbiased estimates of causal effects, just not the causal effects that most researchers are interested in.\nImagine an experiment in which there are 50 villages. A treatment (such as a vaccination program) is randomly assigned to some villages but not others. Let’s assume that a village receives spillovers if another village within a 5km radius is treated. Imagine the outcome is some measure of health (such as the prevalence of an infectious disease). If we naively compare treated villages to untreated villages, we may not recover an unbiased estimate of the direct effect of treating a village. The reason is that each village’s outcome is affected not only by whether that village is treated, but also by whether neighboring villages are treated.\nIn order to see how spillovers can distort estimated treatment effects, consider the graph below:\n\nThe graph considers a situation in which the true direct effect of treating a village is 1, and shows how estimated treatment effects can be higher or lower than 1 depending on the direction and size of spillovers as well as the number of villages treated.\nIn this case, positive spillovers cause a negative bias and vice-versa. This is because when spillovers are positive, the control group mean is inflated, so the difference-in-means is smaller than it otherwise would have been.2 The extent of the bias, however, depends on the number of villages treated as well as the magnitude of the spillover effect. In this example, the more villages are treated, the smaller the bias resulting from spillovers. This is because when more villages are treated, both the treatment and control group means are similarly inflated by positive spillovers and deflated by negative spillovers.\nOften, evaluators are trying to estimate what would happen if a program were rolled out to everyone. Evidence from an RCT that ignores spillover could greatly over or underestimate the total effects of the intervention.\n\n\n3. Most experimental analyses implicitly or explicitly assume that there are no spillovers.\nThe assumption that there are no spillovers is known as the non-interference assumption; it is part of a somewhat more elaborate assumption sometimes referred to as the Stable Unit Treatment Value Assumption (or SUTVA) that is usually invoked in causal inference.\nWhat does the non-interference assumption mean? Subjects can only reveal one of two “potential outcomes”: either their treated outcome or their untreated outcome. Which of these they reveal depends on their own treatment status only. The treatment status of all the other subjects in the experiment doesn’t matter at all.\nWe can state the non-interference assumption more formally using potential outcomes notation: \\(y_i(z_i,Z)=y_i(z′_i,Z′)\\), if \\(z_i=z′_i\\), where \\(Z\\) and \\(Z′\\) represent any two possible random assignment vectors. In words, this expression states that subject \\(i\\) is unaffected by other subjects’ treatment assignments.\nHow reasonable is the non-interference assumption? The answer depends on the domain. Every study that finds a statistically significant impact of spillovers is providing evidence that the assumption is incorrect in that particular application. Most papers discussing spillovers tend to focus on examples in which the non-interference assumption is false. But other studies suggest that spillovers are sometimes surprisingly weak. Sinclair, McConnell, and Green (2012) for example find no evidence of within-zip code spillovers of experimental encouragements to vote, bolstering the non-interference claims made by the dozens of previous turnout experiments.\n\n\n4. You need some kind of non-interference assumption whenever you try to estimate spillover effects\nThe usual non-interference assumption is very strong: it says that there are no spillover effects. When you try to estimate spillovers, you are replacing this strong assumption with a (slightly) weaker one. Perhaps you think that spillovers take place in geographic space — the treatment status of one location may influence the outcomes of nearby units. Allowing spillovers to take place in geographic space requires the assumption that they do not also occur in, for example, social space. This assumption would be violated if the treatment status of, say, Facebook friends in faraway places affects which potential outcome is revealed. To restate this point more generally: When you relax the non-interference assumption, you replace it with a new assumption: no unmodeled spillovers. The modeling of spillovers itself requires strong, often untestable assumptions about how spillovers can and cannot occur.\nSuppose we were to model spillovers in the following way. Every unit has four potential outcomes, which we’ll write as \\(Y(Z_i,Z_j)\\), where \\(Z_i\\) refers to a unit’s own treatment assignment, and \\(Z_j\\) refers to the treatment assignment of neighboring units (i.e., other units within a specified radius). \\(Z_j=1\\) when any neighboring units are treated and \\(Z_j=0\\) otherwise.\n\n\\(Y_{00} \\equiv Y(Z_i=0,Z_j=0)\\): Pure Control\n\\(Y_{10} \\equiv Y(Z_i=1,Z_j=0)\\): Directly treated, no spillover\n\\(Y_{01} \\equiv Y(Z_i=0,Z_j=1)\\): Untreated, with spillover\n\\(Y_{11} \\equiv Y(Z_i=1,Z_j=1)\\): Directly treated, with spillover\n\nWhat assumptions are we invoking here? First, we are stipulating that the treatment assignments of non-neighboring units do not alter a unit’s potential outcomes. Second, we are modeling spillovers as a binary event: either some neighboring unit is treated, or not — we are ignoring the number of neighboring units that are treated, and indeed, their relative proximity.\nThis potential outcome space is already twice as complex as the one allowed by the conventional non-interference assumption. However, it is important to bear in mind that this potential outcome space can be incorrect in the sense that it does not accurately reflect the underlying social process at work in the experiment.\n\n\n5. Spillovers are only indirectly “randomly assigned”\nThe beauty of randomized experiments is that treatment assignments are directly under the control of the researcher. Interestingly in an experiment, spillovers are also randomly determined by the treatment assignment – after all, you’re assigning some unit’s neighbor to treatment or control on a random basis. The trouble is that the probability that a unit is in a spillover condition is no longer directly under the control of the experimenter. Units that are close to many other units, for example, might be more likely to be in the spillover condition than units that are off on their own.\nTake a look at the graph below of 50 units arrayed in geographic space. The 10 red units (both filled and unfilled) were randomly selected for direct treatment and yellow units for control. A filled point represents a unit in a spillover condition, whereas an unfilled point represent a unit that has no treated neighbors within the 5km radius. Notice that the units closer to the center of the graph have a much higher chance of being in a spillover condition than do units towards the edges.\n\n\n\n6. To estimate spillovers you need to account for differential probabilities of assignment to the spillover\nWhen we estimate causal effects, we have to take account of the probability with which units are assigned to a given treatment condition. Sometimes this is done through matching; sometimes it is done using inverse probability weighting (IPW).\nSometimes, the only practical way to calculate assignment probabilities is through computer simulation (though analytic probabilities can be calculated for some designs). For example you could conduct 10,000 simulated random assignments and count up how often each unit is in each of the four conditions described in the previous section. In R:\n\n# Define two helper functions\ncomplete_ra <- function(N,m){\n  assign <- ifelse(1:N %in% sample(1:N,m),1,0)\n  return(assign)\n}\n \nget_condition <- function(assign, adjmat){\n  exposure <-  adjmat %*% assign\n  condition <- rep(\"00\", length(assign))\n  condition[assign==1 & exposure==0] <- \"10\"\n  condition[assign==0 & exposure>0] <- \"01\"\n  condition[assign==1 & exposure>0] <- \"11\"\n  return(condition)\n}\n \nN <- 50  # total units\nm <- 20  # Number to be treated\n \n# Generate adjacency matrix\nset.seed(343)\ncoords <- matrix(rnorm(N*2)*10, ncol = 2)\ndistmat <- as.matrix(dist(coords))\ntrue_adjmat <- 1 * (distmat<=5) # true radius = 5\ndiag(true_adjmat) <-0\n \n# Run simulation 10000 times\nZ_mat <- replicate(10000, complete_ra(N = N, m = m))\ncond_mat <- apply(Z_mat, 2, get_condition, adjmat=true_adjmat)\n \n# Calculate assignment probabilities\nprob00 <- rowMeans(cond_mat==\"00\")\nprob01 <- rowMeans(cond_mat==\"01\")\nprob10 <- rowMeans(cond_mat==\"10\")\nprob11 <- rowMeans(cond_mat==\"11\")\n\nWe can display the resulting probabilities plotted below against the number of units within the 5km radius. The further from the center a unit is, the higher the probability of not being in the spillover condition.\n\nWe must account for these differential probabilities of assignment using IPW. Below is a block of R code that shows how to include IPWs in a regression context.\n\n# Define helper functions\nget_prob <- function(cond,prob00,prob01,prob10, prob11){\n  prob <- prob00\n  prob[cond==\"10\"] <- prob10[cond==\"10\"]\n  prob[cond==\"01\"] <- prob01[cond==\"01\"]\n  prob[cond==\"11\"] <- prob11[cond==\"11\"]\n  return(prob)\n}\n \nget_Y <- function(cond, Y00, Y01, Y10, Y11){\n  Y <- Y00\n  Y[cond==\"10\"] <- Y10[cond==\"10\"]\n  Y[cond==\"01\"] <- Y01[cond==\"01\"]\n  Y[cond==\"11\"] <- Y11[cond==\"11\"]\n  return(Y)\n}\n \n# Generate potential outcomes as a function of position\nY00 <- rnorm(N)\n \n# Treatment Effects\nt10 <- 10   # direct effect\nt01 <- -3   # indirect effect\nt11 <- 5    # direct + indirect\n \nY01 <- Y00 + t01\nY10 <- Y00 + t10\nY11 <- Y00 + t11\n \n# Randomly generate treatment assignment\nassign <- complete_ra(N, m)\n \n# Reveal true conditions\ncond <- get_condition(assign = assign, adjmat = true_adjmat)\n \n# Reveal potential outcomes\nY <- get_Y(cond = cond, Y00 = Y00, Y01=Y01, Y10=Y10, Y11=Y11)\n \n# calculate weights\nweights <- 1/get_prob(cond=cond, prob00=prob00,prob01=prob01,prob10=prob10,prob11=prob11)\n \n# combine data into a dataframe\ndf <- data.frame(Y, cond, weights, prob00, prob01, prob10, prob11)\n \n# conduct estimation comparing the spillover condition to the pure control\nfit <- lm(Y ~ cond==\"01\", weights=weights,\n          data = subset(df, prob00 >0 & prob00 <1 & prob01 >0 & prob01 < 1 & cond %in% c(\"00\", \"01\")))\n\nThere are two very important things to remember when using IPW:\n\nOnly include units that have a non-zero and non-one probability of being in all conditions being compared. The code above only compares the pure control condition to the untreated spillover condition (see the subsetting in the lm call).\nRemember the IPW mantra: units are weighted by the inverse of the probability of being in the condition that they are in.\n\n\n\n7. Choosing the wrong interference assumption will yield incorrect estimates\nYou might be tempted to simply construct a model for a particular type of spillover and estimate it. But unfortunately, just as spillovers can produce biased estimates of treatment effects, incorrectly modeled spillovers can create biased estimates of spillover effects (as well as treatment effects).\nTo get some intuition for the problem, the simulator below lets you pick an interference assumption: the radius beyond which spillovers cannot occur. As in section 4, we assume there are only 4 potential outcomes. The three causal effects that interest us are the average differences between \\(Y_{00}\\) and the other three potential outcomes. The tension in the simulator is between the true (in principle, unknown) spillover network that generates outcomes and the assumed spillover network used for estimation.\nThe causal effect estimates are only correct when the spillover assumption is correct. The potential outcomes were generated under a true radius of 5km. When any radius other than 5km is selected, some if not all of the estimates are biased. This simulator underlines a discouraging point about spillover analysis: it is generally not possible to know if you’ve got the “correct” model of spillovers. Short of doing so, the answers yielded by the model will be incorrect.\n\n\nDownload R code for the above Shiny App from github\nApplied researchers often favor two responses to the “unknowability” of the spillover process. First, they specify “theoretically-driven” models of spillover. Usually, this involves the careful application of qualitative information from the experimental context. Second, researchers conduct robustness checks: they present estimates under a series of spillover assumptions, for example the estimates under increasing radii.\n\n\n8. Sometimes you can avoid spillovers with “buffer rows”\nOne approach to addressing the problem of spillovers is to ensure that other units’ treatment assignments cannot interfere with potential outcomes, by including “buffer rows” between experimental units. The buffer row analogy comes from agricultural studies in which experimental crop rows were physically separated by non-experimental rows that presumably prevented interference due to local changes in soil nitrogen content, insect behavior, or water usage.\nThe analogous design choice in our villages experiment would be to sample a set of 50 experimental villages from a larger set of villages, such that all 50 experimental villages were a healthy distance away from each other – say, separated by a minimum of 75km. Of course, we still must make a non-interference assumption along the lines of: “No spillovers between villages that are 75km or more apart.” This assumption also rules out spillovers that might take place over non-geographic networks, such as an information network via radio, telephone, or internet.\nThe main advantage of buffer-row-inspired design is the massive reduction in complexity. You can get a clean estimate of a direct treatment effect using standard analytic techniques, without needing to posit complicated assumptions about the possible avenues for spillover.\nThe main disadvantage of this design, however, is that by design you cannot estimate natural spillover patterns — which could be critical in understanding normal social processes. (Note: if you do do a buffer design, don’t ignore the buffers themselves — data on these can give you a better handle on spillover effects even though they are never going to receive treatment directly.)\n\n\n9. There are other design-based approaches for detecting spillover effects.\nSome researchers employ a “multilevel” design for exploring spillover effects. The “levels” of the experiment correspond to the spillover network. For example, Sinclair, McConnell, and Green (2012) employ a multilevel design to investigate the possible spillover effects of an encouragement to vote. The levels in their experiment are the neighborhood (nine-digit ZIP code), the household, and the individual. The authors’ non-interference assumption is that the treatment assignments of units in other neighborhoods do not matter. What determines which potential outcome is revealed is a combination of three things:\n\nAn individual’s own treatment assignment\nThe treatment assignment of his or her housemate\nThe treatment assignment of others in the neighborhood\n\nFollowing a relatively complex randomization scheme, the authors assigned treatments so as to create variation in all three levels.\nWhat are the advantages of this design? First, it requires the researcher to stipulate a non-interference assumption ex ante, so there can be no question of fiddling around with interference assumptions until a statistically significant result pops up. Second, it assigns individuals to treatment (including spillover) conditions with known probabilities, so IPW can proceed without having to resort to the simulation method discussed above.\nWhat are the disadvantages? As ever, the difficulty is that the non-interference assumption used in the design stage could be wrong. Perhaps there are significant spillovers across neighborhoods – after all, neighborhood boundaries as described by nine-digit ZIP codes are arbitrary; it could be that the best of friends happen to straddle these boundaries. Or it could be that the spillover network is only indirectly governed by geography. Workplace social ties may be the true means by which the treatment assignment of one unit influences the outcome expressed by others. Of course, nothing about a multilevel randomization scheme prevents the exploration of such alternative spillover structures.\n\n\n10. Even if a treatment is binary, spillovers might not be. The right model might require dealing with “dosage”\nWe’ve explored a non-parametric approach to estimating spillover effects. (See Aronow and Samii 2015 for a fuller treatment of this method as well as a conservative variance estimator in the presence of spillovers.) Units were randomly assigned to one of four conditions with a complex (but knowable) probability. Our estimates of causal effects were calculated as differences in weighted average outcomes between the treatment conditions. This approach has the advantage of making IPW estimation easy – simply weight each observation by the inverse of the probability of it being in the condition that it’s in.\nBut what about “dosage”? Perhaps in fact spillovers work as a decreasing function of the distance to every other treated unit or in some other more complex way. The spillover is then a continuous variable that describes the “dosage” of exposure to spillovers. The non-parametric IPW approach would require us to chop up the continuous variable in to bins and then calculate average outcomes according to the bin. The IPW estimator quickly becomes quite noisy, as fewer and fewer units occupy each bin.\nBowers, Fredrickson, and Panagopoulos (2013) propose a framework that can accommodate any causal model that maps treatment assignments into potential outcomes. The potential outcomes can be in discrete categories (as we’ve been assuming for most of this guide) or a continuous function of the dosage of spillovers.\nA schematic sketch of their method is as follows. Suppose the causal model has two parameters: \\(\\beta_1\\), the direct treatment effect and \\(\\beta_2\\), the indirect effect of a single unit of spillover dosage. A joint test of the hypothesis that \\(\\beta_1 = \\beta_2 = 0\\) is equivalent to a test of the sharp null hypothesis of no effect. Such a test yields a p-value — the probability that the observed data were generated according to the causal model in which \\(\\beta_1 = \\beta_2 = 0\\).\nBut we aren’t restricted to only obtaining p-values for the hypothesis that \\(\\beta_1 = \\beta_2 = 0\\). Those parameters could take on any values, and we could associate a p-value with any hypothesized pair of values. The essence of their proposed estimation method is to pick the pair that generates the highest p-value by searching through the set of plausible pairs.\n\n\nFor further reading\nAronow, Peter M., and Cyrus Samii (2015). “Estimating Average Causal Effects Under Interference Between Units.” arXiv\nAthey, Susan, and Guido W. Imbens (2017a). “The Econometrics of Randomized Experiments.” In Handbook of Economic Field Experiments, vol. 1 (E. Duflo and A. Banerjee, eds.). arXiv DOI\nAthey, Susan, and Guido W. Imbens (2017b). “The State of Applied Econometrics: Causality and Policy Evaluation.” Journal of Economic Perspectives 31(2): 3–32.\nBowers, Jake, Mark M. Fredrickson, and Costas Panagopoulos (2013). “Reasoning about Interference Between Units: A General Framework.” Political Analysis 21: 97–124.\nGerber, Alan S., and Donald P. Green (2012). Field Experiments: Design, Analysis, and Interpretation, chapter 8.\nGlennerster, Rachel, and Kudzai Takavarasha (2013). Running Randomized Evaluations: A Practical Guide, modules 4.2, 7.3, and 8.2.\nPaluck, Elizabeth Levy, Hana Shepherd, and Peter M. Aronow (2016). “Changing Climates of Conflict: A Social Network Experiment in 56 Schools.” Proceedings of the National Academy of Sciences 113: 566–571.\nSinclair, Betsy, Margaret McConnell, and Donald P. Green (2012). “Detecting Spillover Effects: Design and Analysis of Multilevel Experiments.” American Journal of Political Science 56: 1055–1069.\n\n\n\n\n\nFootnotes\n\n\nOriginating author: Alex Coppock, 31 Jul 2014. Minor revisions: Don Green and Winston Lin, 20 July 2016. The guide is a live document and subject to updating by EGAP members at any time; contributors listed are not responsible for subsequent edits.↩︎\nNote though, it is generally very difficult to guess the direction of the bias that would be induced by spillover. Claims like, “spillover would only make our treatment effects appear stronger” usually depend on assumptions of treatment (and spillover) effect homogeneity.↩︎"
  },
  {
    "objectID": "guides/getting-started/missing-data_en.html",
    "href": "guides/getting-started/missing-data_en.html",
    "title": "10 Things to Know About Missing Data",
    "section": "",
    "text": "1. What is missing data?\nWhen variables are missing some data values, we say that there is “missing data.” Depending on your software and the coding of the dataset, missing values may be coded as NA, ., an empty cell (\"\"), or a common numeric code (often -99 or 99).\nThe consequences of missing data for estimation and interpretation depend on the type of variable missing the data. For our purposes, we will consider three types of variables: pretreatment covariates, treatment indicator(s), and outcome (or dependent) variables. Pretreatment covariates, often known simply as “covariates,” are variables that we observe and measure before treatment is assigned. Outcome (or dependent) variables refer to outcomes that are measured after the assignment of treatment.\nMissing data emerges for different reasons. In survey data, a respondent could decline to answer a question or quit the survey without completing all questions. In a panel survey, some subjects may skip the second or later waves. With administrative data, records may be lost at some point in the process of collecting or recording data. To the extent that we can know the process by which data becomes missing, we can better understand the consequences of missing data for our analysis and inferences.\n\n\n2. Missing treatment or outcome data can bias our ability to describe empirical patterns and estimate causal effects.\nMissing data can induce bias in our estimates of descriptive patterns and causal effects. Consider a researcher trying to describe the income distribution in a country with survey data. Some individuals’ incomes are missing but the researcher describes the non-missing data at hand. Suppose low-income individuals are less likely to report their income than high-income individuals, thus missingness concentrates in the lower portion of the distribution. Then, the researcher’s characterization of the income distribution is apt to be biased. For example, the researcher’s estimate of median income is bound to be higher than the true (unknown) median income because more data is missing from the lower portion of the distribution. Since missingness is correlated with the variable that we are trying to describe, our characterization of the median of the distribution is biased.\nWe can illustrate two types of missingness by considering a variable \\(x\\) – for example, income, as above. The left panel shows the full distribution of the variable \\(x\\) without missing data. The middle panel depicts a scenario in which simulated missingness in \\(x\\) is not independent of \\(x\\) – low values of \\(x\\) are much more likely to be missing. The right panel depicts a scenario in which simulated missingness in \\(x\\) is independent of \\(x\\) (often called “missing at random”). The red line represents the median of the distribution without missingness. The blue lines represent the median of the observed data in each simulation. Where missingness is not independent of \\(x\\), we observe that the median of the nonmissing data is, in this instance, higher than the true median. This illustrates that missing data can bias our descriptions of single variables.\n\nlibrary(rmutil)\nlibrary(dplyr)\nlibrary(ggplot2)\ndata.frame(x = rep(rchisq(n = 1000, df = 5), 3),\n           panel = rep(c(\"No missing data\", \"Missingness is not\\nindependent of x\", \"Missingness is\\nindependent of x\"), each = 1000)) %>%\n  mutate(panel = factor(panel, levels  = unique(panel)),\n         missing_cor = rbinom(n = 3000, size = 1,  prob = (1 - pchisq(x, df = 5))),\n         missing_ind = rbinom(n = 3000, size = 1, prob = .5),\n         obs = ifelse(missing_cor == 1 & panel == \"Missingness is not\\nindependent of x\", NA, \n                      ifelse(missing_ind == 1 & panel == \"Missingness is\\nindependent of x\", NA, x))) %>%\n  group_by(panel) %>%\n  mutate(med_x = median(x),\n         med_obs = median(obs, na.rm = T),\n         med_obs = ifelse(panel == \"No missing data\", NA, med_obs)) %>%\n  ggplot(aes(x = obs)) +\n  geom_histogram(bins = 50) +\n  facet_grid(~panel) +\n  geom_vline(aes(xintercept = med_x), col = \"red\", lwd = 1) +\n  geom_vline(aes(xintercept = med_obs), col = \"blue\", lty = 2, lwd = 1) +\n  scale_x_continuous(\"x\") +\n  theme_minimal()\n\n\n\n\nSimilarly, when we seek to estimate causal effects, some patterns of missing data can lead to biased estimates of causal effects. In particular, missingness of the treatment indicator or the outcome variable of interest can induce bias in estimates of the ATE. First, consider missingness of an outcome variable \\(Y_i(Z)\\). Adopting some notation from Gerber and Green (2013), define “reporting” as a potential outcome of a treatment, \\(Z\\), as \\(R_i(Z) \\in \\{0, 1\\}\\). In this notation, \\(R_i(Z) = 0\\) indicates that \\(Y_i(Z)\\) is missing and \\(R_i(Z)=1\\) indicates that the outcome is non-missing. Using this notation, we can express the ATE as:\n\\[\\begin{align}\n\\underbrace{E[Y_i(1)]-E[Y_i(0)]}_{ATE} =& \\underbrace{E[R_i(1)]E[Y_i(1)|R_i(1) = 1]}_{Z = 1\\text{ and }Y_i \\text{ is not missing}} + \\underbrace{(1-E[R_i(1)])(E[Y_i(1)|R_i(1) = 0])}_{Z = 1 \\text{ and } Y_i \\text{ is missing}} - \\\\\n&\\underbrace{E[R_i(0)]E[Y_i(0)|R_i(0) = 1]}_{Z = 0 \\text { and } Y_i \\text{ is not missing }} - \\underbrace{(1-E[R_i(0)])E[Y_i(0)|R_i(0) = 0]}_{Z = 0 \\text { and } Y_i \\text{ is missing }}\n\\end{align}\\]\nIf we condition our analysis on on non-missing outcomes, our estimator of the ATE is:\n\\[E[Y_i(1)|R_i(1)=1] - E[Y_i(0) |R_i(0)=1]\\]\nExamining the preceding equations, the estimator using only only the non-missing outcomes is only (necessarily) equivalent to the ATE if:\n\nThere is no missingness, \\(E[R_i(1)] = 1\\) and \\(E[R_i(0)] = 1\\).\nMissingness is independent of potential outcomes: \\[\\underbrace{E[Y_i(1)|R_i(1) = 1]}_{E[Y_i(1)] \\text{ if } Y_i(1) \\text{ is not missing}} =  \\underbrace{E[Y_i(1)|R_i(1) = 0]}_{E[Y_i(1)] \\text{ if } Y_i(1) \\text{ is missing}} \\text{ and } \\underbrace{E[Y_i(0)|R_i(0) = 1]}_{E[Y_i(0)] \\text{ if } Y_i(0) \\text{ is not missing}} = \\underbrace{E[Y_i(0)|R_i(0) = 0]}_{E[Y_i(0)] \\text{ if } Y_i(0) \\text{ is missing}}\\]\n\nOtherwise, the analysis conditional upon observing \\(Y_i(Z)\\) can induce an unknowable (but boundable) amount of bias in our estimate of the ATE.\nWe often do not think of missingness of the treatment indicator in experiments. Indeed, competent administration of an experiment generally ensures against missing treatment values. Nevertheless, it is important to note that missingness of the treatment indicator can also produce bias if missingness is not independent of potential outcomes.\nThe following simulation shows the consequences of two types of missingness for estimation of the ATE. We set the true ATE to 0.5 (the red vertical lines) in all cases. We simulate missingness through two types of data generating processes. In both cases, all missingness occurs among subjects in treatment (\\(Z = 1\\)). In the top panel, missingness is most likely among subjects in treatment with higher values of the outcome \\(Y_i(1)\\). In the bottom pannel, missingness is independent of the value of \\(Y_i(1)\\). If missingness is correlated with potential outcomes, the estimator of the ATE is biased (top row). This occurs whether we are missing values of the outcome (left column) or the treatment indicator (right column). In contrast, when missing data is independent of potential outcomes, the estimator is unbiased (bottom row).\n\nlibrary(randomizr)\nlibrary(estimatr)\n  \nextract_ests <- function(estimatr_model){summary(estimatr_model)$coef[2,1]}\nsimulation <- function(){\n  Y0 <- rnorm(2000)\n  Z <- complete_ra(2000)\n  Yobs <- Y0 + Z * .5 # .5 standard deviation treatment effect\n  Z_miss <- rbinom(n = 2000, prob = Z * pnorm(Yobs), size = 1)\n  Z_miss_ind <- rbinom(n = 2000, prob = sum(Z_miss)/2000, size = 1)\n  Y_miss <- rbinom(n = 2000, prob = Z * pnorm(Yobs), size = 1)\n  Y_miss_ind <- rbinom(n = 2000, prob = sum(Y_miss)/2000, size = 1)\n  \n  m1 <- lm_robust(Yobs ~ Z, subset = Z_miss == 0)\n  m2 <- lm_robust(Yobs ~ Z, subset = Z_miss_ind == 0)\n  m3 <- lm_robust(Yobs ~ Z, subset = Y_miss == 0)\n  m4 <- lm_robust(Yobs ~ Z, subset = Y_miss_ind == 0)\n  return(sapply(list(m1, m2, m3, m4), FUN = extract_ests))\n}\nreps <- replicate(n = 500, expr = simulation())\ndata.frame(ests = as.vector(t(reps)),\n           missing = rep(c(\"Missing Treatment Indicator\", \"Missing Outcome\"), each = 1000),\n           pos = rep(c(\"Missing is\\nNot Independent of POs\", \"Missingness is\\nIndependent of POs\"), each = 500)) %>%\n  ggplot(aes(x = ests)) + \n  facet_grid(pos ~ missing) +\n  geom_histogram(bins = 100) +\n  geom_vline(xintercept = .5, col = \"red\") +\n  theme_minimal() +\n  xlab(\"ATE Estimates\")\n\n\n\n\n\n\n3. The potential for bias increases in the proportion of treatment or outcome data that is missing.\nReturning to the equations in #2, it is useful to identify the terms that we can estimate (know) as analysts of a dataset. Rewriting the expression for the ATE in the presence of missing data, we can estimate:\n\nThe proportion of missingness in treatment and control: \\(\\color{red}{E[R_i(1)]}\\) and \\(\\color{red}{E[R_i(0)]}\\).\nThe expectation of the outcome variable among reporters (non-missing data) in treatment and control: \\(\\color{red}{E[Y_i(1)|R_i(1) = 1]}\\) and \\(\\color{red}{E[Y_i(0)|R_i(0)=1]}\\)\n\nThese expressions are colored in the following equation.\n\\[\\begin{align}\n\\underbrace{E[Y_i(1)]-E[Y_i(0)]}_{ATE} =& \\underbrace{\\color{red}{E[R_i(1)]E[Y_i(1)|R_i(1) = 1]}}_{Z = 1\\text{ and }Y_i \\text{ is not missing}} + \\underbrace{(1-E[R_i(1)])(E[Y_i(1)|R_i(1) = 0])}_{Z = 1 \\text{ and } Y_i \\text{ is missing}} - \\\\\n&\\underbrace{\\color{red}{E[R_i(0)]E[Y_i(0)|R_i(0) = 1]}}_{Z = 0 \\text { and } Y_i \\text{ is not missing }} - \\underbrace{(1-E[R_i(0)])E[Y_i(0)|R_i(0) = 0]}_{Z = 0 \\text { and } Y_i \\text{ is missing }}\n\\end{align}\\]\nWe are ultimately interested in estimating the ATE, \\(E[Y_i(1)]-E[Y_i(0)]\\). One straightforward implication of this expression is that the magnitude of bias possible for an estimator of the ATE on non-missing observations increases in the amount of missingness. As \\(E[R_i(1)] \\rightarrow 1\\) and \\(E[R_i(0)]\\rightarrow 1\\), bias stemming from missing outcome data converges to 0. In contrast, when we are missing a large proportion of the data, the magnitude of possible bias increases. Thus, our concerns about missing data should increase as the amount (proportion) of missingness increases.\n\n\n4. The consequences of missing data for bias in estimates of causal effects depend on the type of variable that is missing.\nMissingness of pretreatment covariates need not induce bias in our estimates of the ATE. However, researchers can actually induce bias through improper treatment of missing pretreatment covariate data. If treatment is randomly assigned, treatment assignment should be orthogonal to pre-treatment missingness. In other words, pre-treatment missingness should be balanced across treatment assignment conditions.\nHowever, we should avoid “dropping” (excluding) observations based on pretreatment missingness for two reasons. First, it is possible to induce bias in our estimate of an ATE by dropping observations with pre-treatment missingness. After dropping these observations, we can estimate an unbiased estimate the local average treatment effect (LATE) among observations with no missing pretreatment data. However, if treatment effects vary with missingness of pretreatment variables, this LATE may be quite different than the ATE. Second, as we drop observations the number of observations decreases, reducing our power to detect a given ATE. In sum, we should refrain from dropping observations based on pre-treatment covariates to avoid inducing bias or efficiency loss in our estimates of the ATE.\nIn contrast, missingness of the treatment indicator or outcome variable(s) can induce bias in our estimates of causal effects, as demonstrated in #2. This categorization informs the strategies that we adopt to address the consequences of missing data.\n\n\n5. What assumptions do we invoke when we “ignore” treatment or outcome missingness in estimation?\nSuppose that we analyze a dataset with missing treatment or outcome data while ignoring the missingness. If we “ignore” the missingness, we drop observations for which we lack a value for one of these variables. If we proceed to estimate the ATE on the subsample of data for which we have full data for both treatment and outcomes, we estimate:\n\\[E[Y_i(1)|R_i(1)]- E[Y_i(0)|R_i(1)]\\]\nThis is necessarily equivalent to the ATE estimand \\(E[Y_i(1)]-E[Y_i(0)]\\) if missingness is independent of potential outcomes (MIPO), i.e. \\(Y_i(Z) \\perp R_i(Z)\\) (Gerber and Green, 2013). Thus, to interpret \\(E[Y_i(1)|R_i(1)]- E[Y_i(0)|R_i(1)]\\) as an unbiased estimator of the ATE, we must impose an assumption that MIPO.\nThe assumption that MIPO is most plausible when missingness occurs by some random process. Under what conditions might this assumption be plausible? Perhaps we were only able to gather a random subset of administrative outcome data. In this case, missingness would be independent of both potential outcomes and pretreatment covariates. Alternatively, idiosyncratic behavior in data collection (enumerator absence or error) that is plausibly unrelated to potential outcomes may also be consistent with MIPO. On the other hand, survey non-response or other forms of outcome measurement dependent on subject reporting are often much harder to justify under MIPO. Because we cannot validate MIPO, we depend on researchers’ consideration of whether the assumption is plausible under a given data generating process. Where MIPO is not plausible, we should not simply “ignore” missing data in estimation. In these cases, researchers should consider the methods described in #6-#10.\n\n\n6. Why should we assess whether missingness of outcomes is related to treatment assignment?\nWhen researchers encounter missingness in an experiment, they often examine the relationship between missingness of outcomes and treatment assignment. We have denoted reporting (or non-missingness) as a potential outcome of treament assignment, \\(R_i(Z)\\). For a binary treatment, we can denote four “types” of subjects in the experiment, as denoted in the following table.\n\n\n\nType\nProportion\n\\(R_i(1)\\)\n\\(R_i(0)\\)\n\n\n\n\nAlways Reporter\n\\(\\pi_A\\)\n1\n1\n\n\nIf Treated Reporter\n\\(\\pi_T\\)\n1\n0\n\n\nIf Untreated Reporter\n\\(\\pi_U\\)\n0\n1\n\n\nNever Reporter\n\\(\\pi_N\\)\n0\n0\n\n\n\nIn the case in which missingness is related to potential outcomes but not to treatment assignment, we are generally not able to identify an unbiased estimate of the ATE. However, we may be interested in identifying the LATE among always reporters – those subjects for which we observe the outcome regardless of treatment assignment. This effect can be a main estimand of interest and even the most policy-relevant estimand in certain settings. However, to estimate this LATE, we want to be sure that the outcomes that we observe are those of always reporters, not if-treated or if-untreated reporters.\nTo assess the plausibility of ths conjecture, we often examine the relationship between treatment assignment and reporting (non-missingness). Using the notation from the table, we can estimate:\n\\[\\begin{align}\nE[R_i(1)-R_i(0)] &= \\pi_A + \\pi_T - (\\pi_A + \\pi_U)\\\\\n&=\\pi_T - \\pi_U\n\\end{align}\\]\nIf we find no difference in reporting across treatment groups, this test provides no evidence that \\(\\pi_T \\neq \\pi_U\\). However, to further justify that the complete observations in the data are those of “always reporters,” we further must know that \\(\\pi_T = \\pi_U = 0\\). We cannot test this by examining the relationship between reporting and treatment assignment. As such, we generally complement this test with a justification of why reporting should not be endogenous to treatment assignment: for example by assessing covariate balance between treated and control units who are not missing, or by assessing covariate balance between units with missing vs non-missing outcomes. If reporting is not endogenous, then in principle the assumption that \\(\\pi_T = \\pi_U = 0\\) holds. As such, if we find no evidence of differential levels of reporting and see no way that reporting should be endogenous to treatment, we can justify that the \\(E[Y_i(1)|R_i(1) = 1] - E[Y_i(0)|R_i(0) = 1]\\) estimates the LATE among always reporters.\n\n\n7. What is imputation?\nThe most common approaches used to deal with missing data involve the imputation or “filling in” of missing values. In the following dataset with missingness, imputation procedures “fill in” the missing data – the NAs.\n\n\n\n\n\nXobs\nZ\nYobs\n\n\n\n\n-1.2070\n0\n3\n\n\nNA\n0\n2\n\n\n1.0840\n1\n3\n\n\n-2.3460\n1\n4\n\n\n0.4291\n1\n2\n\n\n0.5061\n0\nNA\n\n\nNA\n1\nNA\n\n\nNA\n1\n3\n\n\n-0.5645\n0\n2\n\n\nNA\n0\n1\n\n\n-0.4772\n0\n2\n\n\n-0.9984\n1\n4\n\n\n\n\n\nJust as the consequences of missingness vary by the type of variable that is missing, the imputation methods advocated to address missingness also vary. Importantly, these methods vary in the strength of the assumptions about missingness that are invoked to identify causal effects in the presence of missingness. We review three common approaches to imputation in #8-#10.\n##8. How do we address missingness of pre-treatment covariates and why does this matter?\nAs mentioned in #4, we should never “drop” observations on account of missing pre-treatment data. In order to estimate a model with covariate adjustment, thus, we need to “fill in” missing values to avoid dropping observations. We outline two forms of imputation advocated for missing pre-treatment covariates. The most common approach to address missingness of pre-treatment covariates is to create indicators for missingness and include these as covariates. To do this form of imputation:\n\nSubstitute a numerical value for the NA (as necessary). In the dataset below, we impute a 0 for all values of Xobs that are NAs. The new variable is named Ximputed.\nCreate an indicator for the missingness in each pretreatment variable. This indicator, Xmissing takes the value 1 whenever Xobs is NA, and a 1 otherwise.\n\nOur imputed dataset is thus:\n\n\n\n\n\nXobs\nXimputed\nXmissing\nZ\nYobs\n\n\n\n\n-1.2070\n-1.2070\n0\n0\n3\n\n\nNA\n0.0000\n1\n0\n2\n\n\n1.0840\n1.0840\n0\n1\n3\n\n\n-2.3460\n-2.3460\n0\n1\n4\n\n\n0.4291\n0.4291\n0\n1\n2\n\n\n0.5061\n0.5061\n0\n0\nNA\n\n\nNA\n0.0000\n1\n1\nNA\n\n\nNA\n0.0000\n1\n1\n3\n\n\n-0.5645\n-0.5645\n0\n0\n2\n\n\nNA\n0.0000\n1\n0\n1\n\n\n-0.4772\n-0.4772\n0\n0\n2\n\n\n-0.9984\n-0.9984\n0\n1\n4\n\n\n\n\n\nConsider how this imputation changes the specification of a regression model. Instead of estimating:\n\nmodel1 <- lm(Yobs ~ Z + Xobs)\n\nwe estimate:\n\nmodel2 <- lm(Yobs ~ Z + Ximputed + Xmissing)\n\nThe estimator model2 does not drop observations on the basis of the missing pre-treatment variable Xobs like model1 does.\nAlternatively, if pre-treatment missingness is minimal (less than 10% of observations), or when working with very small datasets with few observations relative to the number of covariates and missingness indicators, Lin, Green, and Coppock (2016) advocate imputing the (unconditional) mean of the observed pre-treatment covariate. Call this variable Ximputed_mean.\n\n\n\n\n\nXobs\nXimputed_mean\nZ\nYobs\n\n\n\n\n-1.2070\n-1.2070000\n0\n3\n\n\nNA\n-0.4467375\n0\n2\n\n\n1.0840\n1.0840000\n1\n3\n\n\n-2.3460\n-2.3460000\n1\n4\n\n\n0.4291\n0.4291000\n1\n2\n\n\n0.5061\n0.5061000\n0\nNA\n\n\nNA\n-0.4467375\n1\nNA\n\n\nNA\n-0.4467375\n1\n3\n\n\n-0.5645\n-0.5645000\n0\n2\n\n\nNA\n-0.4467375\n0\n1\n\n\n-0.4772\n-0.4772000\n0\n2\n\n\n-0.9984\n-0.9984000\n1\n4\n\n\n\n\n\nIf we are simply imputing the pretreatment mean, then instead of estimating model1 (as above) our regression model should be:\n\nmodel3 <- lm(Yobs ~ Z + Ximputed_mean)\n\nSince we have imputed missing values in Xobs when constructing Ximputed_mean, we will not lose observations on the basis of the missing pretreatment covariate when estimating model3.\n\n\n9. We can bound ATEs to account for missing outcome data without making assumptions about the distribution of missing outcomes.\nIf we know the maximum and minimum values of the outcome variable, we can construct bounds on the ATE even in the presence of missing values of the dependent variable. For example, the range of the variable Yobs is 1 to 4. While we do not know what values missing values of Yobs would have been, we can take advantage of the fact that we know the maximum and minimum possible value to construct bounds on the ATE. This allows us to construct an interval estimate of the ATE instead of the point estimate we would construct if we had no missing data. These bounds are known as “extreme value bounds” or “Manski bounds” and do not invoke any additional assumptions about the value of Yobs.\nManski bounds consist of a maximum and minimum bound (estimate) of the possible ATE. To create these bounds, we impute the maximum or minimum value of the outcome variable. as a function of treatment assignment. Thus we construct:\n\nA maximum bound by imputing the maximum value of the outcome for missing values in treatment (\\(Z=1\\)) and imputing the minimum value of the outcome for missing values in control (\\(Z=0\\)).\nA minimum bound by imputing the minimum value of the outcome for missing values in treatment (\\(Z=1\\)) and imputing the maximum value of the outcome for missing values in control (\\(Z=0\\)).\n\nUsing the above dataset, we can construct two variables Y_maxbound and Y_minbound as follows:\n\n\n\n\n\nXobs\nZ\nYobs\nY_maxbound\nY_minbound\n\n\n\n\n-1.2070\n0\n3\n3\n3\n\n\nNA\n0\n2\n2\n2\n\n\n1.0840\n1\n3\n3\n3\n\n\n-2.3460\n1\n4\n4\n4\n\n\n0.4291\n1\n2\n2\n2\n\n\n0.5061\n0\nNA\n1\n4\n\n\nNA\n1\nNA\n4\n1\n\n\nNA\n1\n3\n3\n3\n\n\n-0.5645\n0\n2\n2\n2\n\n\nNA\n0\n1\n1\n1\n\n\n-0.4772\n0\n2\n2\n2\n\n\n-0.9984\n1\n4\n4\n4\n\n\n\n\n\nWithout covariate adjustment, we can obtain our interval estimate of the ATE by estimating:\n\nupper <- lm(Y_maxbound ~ Z)\nlower <- lm(Y_minbound ~ Z)\ncoef(upper)[2]\n\n  Z \n1.5 \n\ncoef(lower)[2]\n\n  Z \n0.5 \n\n\nOur interval estimate of the ATE using Manski bounds is thus [0.5, 1.5].\n\n\n10. Multiple imputation for missing outcomes allows for point estimation of ATEs, but relies on stronger assumptions than bounding.\nThe methods in #8 and #9 describe methods of single imputation, where a single value is substituted for missing values. In multiple imputation, we impute missing values of the dataset multiple times according to an assumed stochastic data generating process. Different methods for multiple imputation impose different structures and assumptions about the probability distributions governing the data generating processes used to impute missing values. In general, multiple imputation proceeds via three stages:\n\nImputation: Missing values are imputed via a random draw of plausible values under the specified data generating process. This creates a full dataset without missing values. Typically, researchers will impute at least five complete datasets. The only differences across these imputed datasets are the values that were missing in the original data.\nEstimation: Estimate the ATE (or other estimand of interest) using each imputed dataset. This generates as many estimates and standard errors as there are imputed datasets.\nPooling estimates: Finally, researchers combine estimates from the different imputed datasets to generate a point estimate of the ATE and its stanard error. Typically, this point estimate can be calculated using Rubin’s rules (2004).\n\nMultiple imputation is implemented in many software packages and relatively straightforward to implement. However, the specification of a data generating process from which datasets are imputed relies on additional assumptions about the correctness of the model of missingness (the data generating process). These assumptions are generally untestable and stronger than the standard experimental assumptions invoked to identify an interval estimate of the ATE using Manski bounds.\n\n\nReferences:\nGerber, Alan S. and Donald P. Green. (2013). Field Experiments: Design, Analysis, and Interpretation. New York: W.W. Norton.\nLin, Winston, Donald P. Green, and Alexander Coppock. (2016). “Standard Operating Procedures for Don Green’s Lab at Columbia.” Available at https://alexandercoppock.com/Green-Lab-SOP/Green_Lab_SOP.html.\nRubin Donald B. (2004). Multiple Imputation for Nonresponse in Surveys. New York: John Wiley and Sons."
  },
  {
    "objectID": "guides/getting-started/mechanisms_en.html",
    "href": "guides/getting-started/mechanisms_en.html",
    "title": "10 Things to Know About Mechanisms",
    "section": "",
    "text": "As social scientists, we are fascinated by causal questions. As soon as we learn that X causes Y, we want to better understand why X causes Y. This guide explores the role of “mechanisms” in causal analysis and will help you to understand what kinds of conclusions you may draw about them.\n\nMechanisms are pathways through which X causes outcome Y.\nMechanisms have long been at the heart of medicine. Every time a doctor prescribes a treatment, she does so out of an understanding of which chemical or physical factors cause a disease, and she prescribes a treatment that is effective because it interrupts these factors. For example, many clinical psychologists recommend exercise to patients dealing with depression. Exercise raises endorphins in the body’s chemistry, which trigger positive feelings and also act as analgesics, which reduce the perception of pain. Endorphins, therefore, are a mechanism by which exercise helps reduce depression. Exercise may have positive effects on a number of other dependent variables (e.g. heart disease) through other mechanisms (e.g. elevating heart rates), but the mechanism that causes it to affect depression in particular is endorphins. We could also conclude that another treatment, such as a drug that raised endorphins, may have similar effects on depression.\nMechanisms are just as important for social sciences. Take, for example, recent research that has connected climate change to an increase in civil conflict. One study1 claims to identify the causal effect of climate shocks on violent conflict by studying the rate of civil conflict in El Nino-affected countries during El Nino versus non-El Nino years. Suppose this study is correct. Why would experiencing a climate shock cause a country to have elevated levels of conflict? One mechanism could be poverty: climate shocks hurt the economy, and with lower opportunity costs, individuals are more inclined to join armed groups. An alternative mechanism is physiological: people are physically wired to be more aggressive in hotter temperatures. Perhaps the mechanism is migration: climate shocks displace people in coastal regions, and this produces social conflict between migrants and natives. In reality, several or all of these mechanisms (as well as others not listed here) could be operating simultaneously, even in the same case! In many of the most interesting social science questions, there are several channels (“M”s) that could transmit the total effect of X on Y.\n\n\nWhile we don’t need to know the mechanism to conclude that X causes Y, there are several reasons why we want to.\nIn the climate/conflict example above, we can have full confidence in the researchers’ ability to causally identify that climate shocks cause conflict, and yet have no evidence of which mechanism(s) is/are at work. But social scientists are interested in learning about mechanisms because they tightly relate to social science theories. For example, the “poverty” mechanism above closely relates to Gurr’s2 theory that individuals rebel when their opportunity costs of conflict are low, whereas the “migration” mechanism could support a theory of conflict based on grievances between social groups. It is no wonder that upon learning that X causes Y, social scientists immediately ask what the mechanism is – they want to relate this finding to theory!\nUnderstanding mechanisms has not only theoretical but practical benefits. First, knowing M allows us to guess for which populations X will lead to Y. If the mechanism for climate/conflict is physiological response to heat, then climate shocks may produce conflict only when temperature is quite warm. Second, knowing M helps us to consider other outcomes that may be affected by X. If the mechanism for climate/conflict is migration, then we might also expect climate shocks to result in overuse of public goods in urban areas. Third, knowing M helps us to consider other ways to cause or avoid causing changes in Y. If the mechanism for climate/conflict is poverty, then development programs could decrease conflict by reducing the sensitivity of incomes to climate shocks, even though they can’t change climate shocks.\n\n\nBut it is extremely challenging to identify causal mechanisms because the mechanisms themselves are not randomly assigned…\nConsider an experimental example. Chong et al. (2015)3 used a field experiment to study the effect of corruption information on voter turnout. They randomly assigned some polling precincts in Mexico to receive information about the corrupt use of funds within that municipality. Surprisingly, they found that treated precincts turned out to vote at lower rates than control precincts. They suggest the following mechanism at work: corruption information convinces voters that the municipality is so severely corrupt that electing a good politician will not change it, so individuals find their vote to carry less value.\nIn short, their argument is:4\nReceiving corruption information (X) \\(\\xrightarrow{+}\\) Believes corruption too severe (M) \\(\\xrightarrow{+}\\) Stay home (Y)\nChong et al. face a common obstacle in interpreting their results: their proposed mechanism was not randomly assigned. Some people are more inclined to believe that “all politicians are rascals” while others have a tendency to push for “change we can believe in.” Unfortunately, we can observe only the random treatment an individual received and their non-random belief about corruption; we can’t tell what belief about corruption they would have had if they had received the other treatment condition. This makes it impossible for us to determine, for each individual, the extent to which her decision to turn out to vote was caused by the proposed mechanism versus other mechanisms.\nSome researchers try to get around this problem by estimating the average effect of the treatment on the mechanism and then estimating the average effect of the mechanism on the outcome. One reason that this is problematic is that we can imagine several factors other than the treatment that could be causing both M and Y. Suppose that the level of apathy – let’s call it Q – varies among the citizens in our study, and Q has a very strong effect on both M and Y. Highly apathetic individuals might be more likely to believe that problems are beyond solving, and they might also be more likely to stay at home on election day. We are therefore likely to observe a strong correlation between M and Y that is driven by the confounder of Q, not by our treatment X. Mechanically, our results will be biased in favor of finding evidence of X’s effect on Y via M simply because Q has produced a relationship between M and Y.\n\n\n…and because treatment effects are rarely homogeneous.\nThe other problem with trying to decompose the average effects of X on M and then M on Y is that this approach assumes that every subject responds to the treatment identically. Recalling our example in which X is the information treatment, M is the belief that corruption is too severe, and Y is staying home, we can imagine two types of respondents. Type A thought that corruption was too severe to ever solve until she received a postcard containing information about corruption in her district. She was surprised to see that the problem was not as bad as she had expected. Formally, for Type A, \\(M(X=0)=1\\) and \\(M(X=1)=0\\), so X has a negative effect on M. Type B thought corruption was a manageable problem until she received a postcard containing information about corruption in her district. She was surprised by how extensive the problem was and gave up hope of solving the problem. Formally, for Type B, \\(M(X=0)=0\\) and \\(M(X=1)=1\\), so X has a positive effect on M. If we were to average the effects for these two types, we would see no relationship between X and M.\n\n\n\n\n\n\n\n\n\n\n\n\nType\nX (Info treatment)\nM conditional on X=0 (unobserved)\nM conditional on X=1 (observed)\nEffect of X on M\nEffect of M on Y\nY (Stay home)\n\n\n\n\nA\n1\n1\n0\nnegative\nnegative\n1\n\n\nB\n1\n0\n1\npositive\npositive\n1\n\n\n\nEstimating the role of M can be further complicated when the relationship between M and Y is also heterogeneous. Imagine that Type A only votes when she’s angry (in other words, M has a negative effect on Y). Type A was planning on voting to express her anger over the pervasiveness of corruption in her district, even though she knew it would not have changed anything, until she learned that corruption was not as bad as she had expected it to be. Her fiery passion gone, she chooses to stay home on election day. However, Type B only votes when she thinks her vote can make a difference (in other words, M has a positive effect on Y). Type B was going to vote for the non-corrupt politicians in her district until she learned that they were all corrupt. Without any hope of changing the situation, she also decided to stay home on election day. For both Type A and Type B, there is an “indirect effect” of M (in other words, X affects Y through M). But we will miss this relationship in the aggregate because we will be unable to obtain unbiased estimates of the average effect of X on M.5\nWe can imagine many more “types” than just A and B – the point here is to demonstrate intuitively that because M is not randomly assigned, and because it is unlikely that the effects of X on M and M on Y are identical for everyone, it will be very difficult to accurately characterize how much of our effect is mediated through M.\n\n\nMany studies try to decompose a total treatment effect into its “direct” and “indirect” effects.\nBecause learning about mechanisms holds such rich theoretical promise, researchers would love to quantify how much of an effect of X on Y operates via M. Sometimes researchers will try to do this through a technique called “decomposition of effects.”\nA decomposition of effects analysis tries to decompose a total effect of X on Y into the effect X has on Y directly and the effect of X on Y that occurs indirectly through M. The “total effect” refers to the Average Treatment Effect (ATE), which is simply the average effect that X has on Y. Any experiment that randomly assigns a treatment in order to observe its effects on some outcome is estimating the ATE. Next, the researcher tries to quantify the size of the effect that X has on Y through the mechanism M. This is often known as the “indirect effect” – because X is affecting Y indirectly through M – or the Average Causally Mediated Effect (ACME). Finally, the researcher will try to estimate the effect of X on Y that doesn’t go through M. This is known as the “direct effect” of X on Y or the Average Controlled Direct Effect (ACDE), because it is the effect of X on Y when we control for the work that M is doing.\n\n\nBut be cautious about using regression analysis to decompose effects.\nAlthough commonly used, using mediation regression analysis presupposes some strong and often unrealistic assumptions. We will use some code to illustrate what this method entails and demonstrate the conditions under which it can produce biased estimates.\nThe basic idea is that if we have data on the treatment an individual received (X), whether they exhibit the proposed mechanism (M), and what the outcome is (Y), then we can distinguish these effects using the following three regressions.\n\n\\[ M_i = \\alpha_1+aX_i+e_{1i} \\]\n\\[ Y_i = \\alpha_2+cX_i+e_{2i} \\]\n\\[ Y_i = \\alpha_3+dX_i+bM_i+e_{3i} \\]\n\nHow would we do this? Using equation 1, we can regress M on X to obtain the direct effect of X on M, which is the coefficient \\(a\\). Next, we turn to equation 3, in which we regress Y on M and X. In this regression, the coefficient \\(b\\) represents the direct effect of M on Y when we control for X. A decomposition of effects analysis would multiply \\(a*b\\) to reveal the indirect effect of X on Y via M. To find the direct effect of X on Y, we need look no further than \\(d\\), which is the coefficient on X in equation 3 when we control for M. In other words, \\(d\\) is the effect of X on Y that does not go through M. If we add the indirect effect and direct effect, we will come up with the “total effect” of X on Y, which is equal to \\(c\\). To summarize, the decomposition of effects analysis ostensibly disaggregates the total effect into the effect that is mediated via M and the effect that is not mediated via M, enabling the researcher to conclude how important M is for explaining the relationship between X and Y.6\nThe problem is that this arithmetic only works under some very strong assumptions. One of these assumptions is that the error terms in regressions 1 and 3 are unrelated to each other—in other words, M can’t be predicted by unobservable factors that also predict Y. We described this problem intuitively in point 3 when we introduced Q, a confounding variable that contributes both to M and to Y and therefore engenders a very strong relationship between them, even if X’s effect on Y is not operating through M at all. Now let’s describe this problem using a simulation.\nIn the following code, we start by creating this Q variable for each individual and defining the “true” effects of X on M, M on Y, and X on Y. Next, we create hypothetical potential outcomes for M—that is, for each individual, we define what value of M they would reveal if they were treated, and what value of M they would reveal if they were untreated. These values are related not only to the “true” effect of X on M, but also to Q. Then we can also define hypothetical potential outcomes for Y. We do this for four scenarios, all of which assume constant effects, an assumption we will relax later. Two of these are simple potential outcomes of Y: the Y exhibited by the individual who is untreated and reveals her untreated M potential outcome, and the Y exhibited by the individual who is treated and reveals her treated M potential outcome. However, we also define two complex potential outcomes of Y: the Y exhibited by the individual who is untreated but reveals her treated M potential outcome, and the Y exhibited by the individual who is treated but reveals her untreated M potential outcome. While these potential outcomes bend the mind a bit, they are important to define in the hypothetical so that we can calculate the “true” (but inherently unobservable) direct and indirect effects to compare our decomposition analysis to.\nIn the second half of the code, we conduct a random assignment of treatment and proceed with the decomposition of effects analysis described above, using the data we “observe.” Under the (strong) assumptions that the error terms are uncorrelated and effects are constant across subjects, \\(a*b\\) = ACME, \\(d\\) = ACDE, and \\(c\\) = ATE. However, the simulation reveals that \\(a*b\\) > ACME and \\(d\\) < ACDE; that is, we overestimated the average indirect or mediated effect (ACME) and underestimated the average direct effect (ACDE). Our decomposition of effects analysis was biased because the first assumption – uncorrelated error terms – did not hold: unobserved variable Q predicted both M and Y, and this led us to overestimate the role of the mechanism M.\n\nrm(list = ls())\nset.seed(20160301)\nN <- 1000000\n# Simulate Data, Create Potential Outcomes, Estimate \"True\" Effects -----------------------\n# build in an ideosyncratic unobserved characteristic\nQ_i <- rnorm(N)\n# create the \"true model\" by defining our treatment effects (tau)\ntau_X_on_M <- 0.2 # X's effect on M\ntau_M_on_Y <- 0.1 # M's effect on Y\ntau_X_on_Y <- 0.5 # total effect of X on Y (ATE), both through M and not through M\n# build the potential outcomes (POs) for the mediator\n# individual reveals M_1 if treated; M_0 if untreated\n# M is a function of both treatment and the unobserved characteristic\nM_0 <- 0 * tau_X_on_M + Q_i\nM_1 <- 1 * tau_X_on_M + Q_i\n# we can estimate the unbiased Average Treatment Effect (ATE) of X on M\nATE_M <- mean(M_1 - M_0)\nATE_M\n\n[1] 0.2\n\n# build POs for the outcome variable\nY_M0_X0 <- tau_M_on_Y * (M_0) + tau_X_on_Y * 0 + Q_i\nY_M1_X1 <- tau_M_on_Y * (M_1) + tau_X_on_Y * 1 + Q_i\nY_M0_X1 <- tau_M_on_Y * (M_0) + tau_X_on_Y * 1 + Q_i # this is a \"complex\" PO\nY_M1_X0 <- tau_M_on_Y * (M_1) + tau_X_on_Y * 0 + Q_i # this is a \"complex\" PO\n# some of these POs are \"complex\" because we are imagining what Y we would\n# observe if we assigned treatment but observed the untreated M PO or\n# if we assigned control but observed the treated M PO\n# building these complex POs is necessary for estimating the \"true\" direct and indirect effects\n# we can estimate the unbiased Average Causally Mediated Effect (ACME)\n# we estimate the effects of M holding X constant\n# they are the same\n# this is the \"indirect effect\"\nACME_X0 <- mean(Y_M1_X0 - Y_M0_X0)\nACME_X1 <- mean(Y_M1_X1 - Y_M0_X1)\nACME <- mean(((Y_M1_X1 - Y_M0_X1) + (Y_M1_X0 - Y_M0_X0)) / 2)\n# we can estimate the unbiased Average Controlled Direct Effect (ACDE)\n# we estimate the effects of X holding M constant\n# they are the same\n# this is the \"direct effect\"\nACDE_M0 <- mean(Y_M0_X1 - Y_M0_X0)\nACDE_M1 <- mean(Y_M1_X1 - Y_M1_X0)\nACDE <- mean(((Y_M0_X1 - Y_M0_X0) + (Y_M1_X1 - Y_M1_X0)) / 2)\n# now we build the simple POs for Y\nY_1 <- tau_M_on_Y * (M_1) + tau_X_on_Y * 1 + Q_i\nY_0 <- tau_M_on_Y * (M_0) + tau_X_on_Y * 0 + Q_i\n# we estimate the true ATE of X on Y\n# this is the \"total effect\"\nATE <- mean(Y_1 - Y_0)\nATE\n\n[1] 0.52\n\nACDE + ACME # note that the direct and indirect effects sum to the total\n\n[1] 0.52\n\nACDE\n\n[1] 0.5\n\nACME\n\n[1] 0.02\n\nATE_M\n\n[1] 0.2\n\n# Random Assignment, Revelation of POs, Attempt to Decompose Effects ---------------------------------\n# we assign half of our sample to treatment and half to control\nX <- sample(c(rep(1, (N / 2)), rep(0, (N / 2))))\n# we reveal POs for M and Y based on treatment assignment\nM <- X * M_1 + (1 - X) * M_0\nY <- X * Y_1 + (1 - X) * Y_0\nmodel1 <- lm(M ~ X)\na <- coef(model1)[2] # extract the coefficient to get the effect of X on M\na\n\n        X \n0.2001291 \n\nmodel2 <- lm(Y ~ X)\nc <- coef(model2)[2] # extract the coefficient to get the total effect of X on Y\nc\n\n       X \n0.520142 \n\nmodel3 <- lm(Y ~ X + M)\nd <- coef(model3)[2] # extract this coefficient to get the effect of X on Y controlling for M\nb <- coef(model3)[3] # extract this coefficient to get the effect of M on Y controlling for X\n# some would now multiply the average effect of X on M and the average effect of M on Y to get the average indirect/mediated effect of X on Y via M (ACME)\na * b\n\n       X \n0.220142 \n\n# but when we compare this to the true ACME, we see that this is biased\nACME\n\n[1] 0.02\n\n# some would also interpret the average effect of X on Y controlling for M as the average controlled direct effect (ACDE)\nd\n\n  X \n0.3 \n\n# but when we compare this to the true ACDE, we see that this is biased\nACDE\n\n[1] 0.5\n\n# note that we have OVERestimated the average indirect effect and UNDERestimated the average direct effect\n# the estimates that are unbiased are the average effects of X on Y and X on M because X is randomly assigned\na\n\n        X \n0.2001291 \n\nATE_M\n\n[1] 0.2\n\nc\n\n       X \n0.520142 \n\nATE\n\n[1] 0.52\n\n\nLet’s tie this exercise back to the issue raised in point 3. This simulation illustrated that quantifying the mediated effect proves difficult when background predictive variables confound the relationship between M and Y. Because M is not randomly assigned, it is important for us to think about how likely it is that our M and our Y are both affected by unobserved variables. In principle, if there are no confounding variables in this relationship, then a decomposition of effects analysis may be unbiased, but this assumption is strong and usually hard to prove.\nWhile we did not demonstrate it in this simulation, it is also possible to show that decomposition of effects also breaks down when treatment effects are heterogeneous (we introduced the intuition for this in point 4). The technical reason for this comes from our law of expectations, which is: \\(E[a*b]\\) = \\(E[a]E[b]+cov(a,b)\\). If we have constant treatment effects, then \\(a\\) and \\(b\\) do not covary, the covariance term drops out, and we can simply multiply \\(a*b\\) to get the ACME. However, if the covariance term is non-zero, then we are not able to estimate this indirect effect from these two coefficients obtained from separate regressions. We constructed constant treatment effects in order to be able to demonstrate the process of decomposing effects, but if we were to re-do the simulation with heterogeneous treatment effects that covary, then we would not even be able to calculate the ACME or ACDE using the potential outcomes approach at the beginning of our code.\nWhat you can do… Before you embark on a decomposition of effects analysis, ask yourself:\n\nCan I imagine any unobserved variables that predict both M and Y?\nIs it possible that my subjects respond to treatment effects in different ways?\n\nIf the answer to any of these questions is yes, we strongly recommend that you proceed with caution. In particular, think carefully about how unobserved variables and heterogeneous treatment effects would affect your estimation strategy.\n\n\nSometimes subgroup analysis can provide suggestive evidence for or against a mechanism.\nIn points 3-6, we’ve cautioned researchers against trying to confidently quantify the proportion of an effect that is mediated by a particular mechanism, but there may be other ways to learn more about mechanisms at work in a particular study. In point 1, we underscored the tight relationship between mechanisms and theory. Just because it’s challenging to quantify evidence of a mechanism directly does not mean we cannot explore the testable predictions of the theory in which our mechanism is featured!\nOne strategy is to use subgroup analysis, or treatment-by-covariate interactions, to see whether different populations respond to the treatment differently in accordance with our theories. For example, suppose we wanted to learn more about the role of income in mediating the climate/conflict relationship. One of the testable implications of a theory in which income plays a mediating role is that we would expect climate shocks to be associated with conflict in areas where income is sensitive to climate shocks but not where income is independent of climate shocks. Sarsons (2015)7 does exactly this. Exploiting the fact that districts downstream of irrigation dams do not depend on rainfall for income while districts upstream of irrigation dams do, she explores the income mechanism by testing whether rain shocks predict riot incidence in downstream districts but not in upstream districts. Formally, she tests these hypotheses:\n\nX\\(\\rightarrow\\)Y in places where X is known to affect M [Rainfall shocks will increase riots in areas where rainfall shocks will negatively affect income (upstream of dam).]\nX has no effect on Y in places where X has no effect on M [Rainfall shocks will have no effect on riots in areas where income is not sensitive to rainfall (downstream of dam).]\n\nHowever, she found that the relationship between rainfall shocks and riot incidence held just as tightly in the downstream districts where income was not sensitive to rainfall. She interprets this finding as “suggestive” evidence against the income mechanism. To be clear, Sarsons did not conduct any mediation analysis: she did not measure the income of each village and quantify the direct effect of rainfall shocks on riots and the indirect effect of rainfall shocks on riots through income. Instead, she looked for the heterogeneous treatment effects the theory would have implied and, finding no evidence of them, concluded that the income channel may be less important than previously thought.\nWhat you can do… In future projects, ask yourself: If the mechanism is M, which groups or units would I expect to exhibit a treatment effect, and which groups or units would I expect not to respond to treatment? Next, test whether these predictions are supported by your data and interpret this as suggestive evidence for or against your proposed mechanism M. Keep in mind that such evidence is not decisive because the groups could differ in other ways that could affect their responsiveness to the treatment.\n\n\nWe can also look for suggestive evidence by looking at the effects of our treatment on various outcomes.\nAgain, while it’s difficult to quantify evidence of a mechanism at work, we can always explore the testable implications of the theory in which our mechanism features. In point 7, we did this by exploring whether the treatment affected the particular subgroups for which a treatment effect is implied by our theory. Another approach is to explore whether the treatment affects only the outcomes implied by our theory.\nFor example, many social scientists are interested in how mass education influences democracy. Several theories of democratization expect different mechanisms would connect education and democracy. First, according to modernization theory, education could facilitate the smooth functioning of democracy by undermining group attachments (such as ethnicity or religion) in favor of merit.8 Second, according to social theorists of oppression, education could undermine democracy by reinforcing obedience to authority, which is inherent in a classroom structure.9 Third, according to many political scientists and psychologists, education can encourage democratic participation by empowering individuals with the ability to acquire and act on knowledge.10 Friedman et al. (2011)11 decide to tease apart these mechanisms by investigating the results of a field experiment in which Kenyan girls were randomly assigned to receive an education subsidy. They followed up with the students five years after the program and asked them several questions designed to test which of these three mechanisms were at work: Did the girls accept a husband’s right to beat his wife? Was a parent involved in selecting their spouse? How strongly did the girl identify with her religious or ethnic group? Did the girl regularly read news?\nThe following table outlines the direction of the effects that each theory would suggest. Note that the various mechanisms being tested here result from theories with diverging predictions on some of these outcomes. The predictions from each of the three mechanisms are outlined on the rows, followed by the actual results. We can see that two of the outcomes collected provided support for modernization theory. However, modernization theory would have predicted a decrease in religious or ethnic group association (in reality, there was no effect) and had no predictions for newspaper readership (in reality, readership increased). None of the predictions of the obedience to authority mechanism were supported by the data. However, the data supported all four of the predictions of individual empowerment theory. The authors conclude that it is more likely that \\(X→M3→Y\\) than \\(X→M1,M2→Y\\).12\n\n\n\n\n\n\n\n\n\n\nMechanism\nAcceptance of husband’s right to beat wives (Y1)\nParent involved in selecting spouse (Y2)\nAssociation with religion, ethnic identity (Y3)\nReads news (Y4)\n\n\n\n\n(M1) Modernization\n\\(\\downarrow\\)\n\\(\\downarrow\\)\n\\(\\downarrow\\)\nNo effect\n\n\n(M2) Obedience to authority\n\\(\\uparrow\\)\n\\(\\uparrow\\)\n\\(\\uparrow\\)\nNo effect\n\n\n(M3) Individual empowerment\n\\(\\downarrow\\)\n\\(\\downarrow\\)\nNo effect\n\\(\\uparrow\\)\n\n\nActual effect\n\\(\\downarrow\\)\n\\(\\downarrow\\)\nNo effect\n\\(\\uparrow\\)\n\n\n\nThis study, like Sarsons’s study, is not trying to quantify how much of the effect of X on Y is conveyed via M. However, through thoughtful investigation of various outcomes, the authors are able to provide suggestive evidence of which mechanisms seem most plausible.\nWhat you can do… In future projects, ask yourself: If the mechanism is M, which outcomes would I expect to be affected by my treatment, and which outcomes would I expect not to be affected by my treatment? Next, test whether these predictions are supported by your data and interpret this as suggestive evidence for or against your proposed mechanism M.\n\n\nDesigning complex treatments can help narrow our understanding of what part of the treatment is “doing the work.”\nSometimes experimental researchers will try to better understand mechanisms by adding or subtracting elements of the treatment that are thought to trigger different mechanisms. This approach is sometimes called “implicit mediation analysis” because different components of X are thought to implicitly manipulate certain mechanisms. This, of course, is an assumption: because we are not measuring M directly, we are relying on a theoretical claim that component A will trigger M, whereas component B will not.\nFor example, many governments including Mexico, Brazil, Tanzania, and Uganda have created conditional cash transfer programs to address poverty. These programs provide cash to poor individuals, but they frequently come with conditions such as attending school or a job training program. Until recently, we knew only that these programs (X) successfully reduced poverty (Y) and that X caused Y either via cash or via the required attendance at school or job programs. To distinguish between these mechanisms, Baird et al. (2011)13 conducted an experiment in Malawi, where they assigned one group of families to receive a conditional cash transfer for the regular school attendance of their girls, another group of families to receive the cash unconditionally, and a control group to receive no transfer. This design “implicitly” manipulated M: while girls in the unconditional transfer group could also seek out education, school attendance (the condition under study) would likely be higher in the group that was required to seek it out. Unsurprisingly, school attendance and test performance was better for the group receiving conditional cash transfers. However, their measures of Y–the rate at which the girls became pregnant or married–were actually better (lower) in the group receiving the unconditional cash transfers. The authors concluded that attendance requirements associated with conditional cash transfers were probably not the mechanism responsible for the success of these programs in reducing the symptoms of poverty.\nStudies like these help not only social scientists to learn more about the channels through which X causes Y, but also policymakers to explore and discover new treatments. After several other studies joined Baird et al. in demonstrating the remarkable effects of unconditional cash transfers, many governments and organizations have begun to implement unconditional cash transfer programs.\nWhat you can do… In future projects, ask yourself: Can my treatment be “unpacked” into multiple treatment arms, some that implicitly manipulate M, and some that do not? Consider using a factorial design to identify the effects of different treatment arms. If you have ample power, comparing the various treatment arms will provide you with suggestive evidence for or against M.\n\n\nDespite the difficulties in empirically measuring mechanisms, it is worth paying serious attention to them but being cautious in our language.\nAttempting to identify causal mechanisms is a noble endeavor. Articulating causal mechanisms is what allows us to unpack “black box” treatments and understand why and how certain treatments work. Even though causal claims can be (and often are) made without evidence for a causal mechanism, exploring causal mechanisms is what enables us to extend the research frontier and re-evaluate how our evidence maps on to our theories. For these reasons, audiences (be they the general public or academic reviewers) are often understandably eager for you to expound upon causal mechanisms after demonstrating evidence for a provocative causal claim. In anticipation of this, it is worth considering whether it is possible to design a way to test causal mechanisms in advance of implementing an experiment. If not, consider whether certain outcome measures or treatment by covariate interactions would provide some support for a particular causal mechanism, and be explicit about the limitations of this kind of analysis in your write-up. Mechanisms are an exciting domain of inquiry and should be considered both in the design and analysis of an experiment, but we should be sure to discuss mechanisms with caution appropriate to our ability to identify a particular mechanism and avoid overselling the argument.\n\n\n\n\n\nFootnotes\n\n\nSolomon M. Hsiang, Kyle C. Meng, and Mark A. Cane, “Civil Conflicts Are Associated with the Global Climate,” Nature 476.7361 (2011): 438-441.↩︎\nTed Gurr, Why Men Rebel, Princeton University Press, 1970.↩︎\nAlberto Chong, Ana L. de la O, Dean Karlan, and Leonard Wantchekon, “Does Corruption Information Inspire the Fight or Quash the Hope? A Field Experiment in Mexico on Voter Turnout, Choice, and Party Identification,” The Journal of Politics 77.1 (2015): 55-71.↩︎\nNote that Chong et al. test their argument using data at the precinct level, not the individual level, but we’ve adapted their argument to the individual level for ease of exposition.↩︎\nFor a more rigorous discussion of these fallacies, see Adam N. Glynn, “The Product and Difference Fallacies for Indirect Effects,” American Journal of Political Science 56.1 (2012): 257-269.↩︎\nExplanation adapted from Alan Gerber and Donald Green, Field Experiments, W.W. Norton and Company, 2012, chapter 10.↩︎\nHeather Sarsons, “Rainfall and Conflict: A Cautionary Tale.” Journal of Development Economics 115 (2015): 62-72.↩︎\nMarion Joseph Levy, Modernization and the Structure of Societies, Princeton University Press, 1966.↩︎\nFrantz Fanon, The Wretched of the Earth, Grove Press, 1964. John Lott, Jr., “Public Schooling, Indoctrination and Totalitarianism,” Journal of Political Economy 107(6), 1999.↩︎\nGabriel Almond and Sidney Verba, The Civic Culture: Political Attitudes and Democracy in Five Nations, Sage Publications, 1963. Robert Mattes and Michael Bratton, “Learning about Democracy in Africa: Awareness, Performance, and Experience,” American Journal of Political Science, 51(1), 2007.↩︎\nWilla Friedman, Michael Kremer, Edward Miguel, and Rebecca Thornton, “Education as Liberation?” NBER Working Paper 16939, 2011.↩︎\nIn the actual study, the authors were surprised to uncover evidence that education also increased individuals’ acceptance of political violence. While they still argue that individual empowerment is responsible for the relationship between education and democracy, they caution that education does not always lead to democratization (that is, M3→Y but it is also possible that M3→NOT Y). Nonetheless, their approach is a useful demonstration of how multiple outcomes may shed light on mechanisms.↩︎\nSarah Baird, Craig McIntosh, Berk Ozler, “Cash or Condition? Evidence from a Cash Transfer Experiment,” Quarterly Journal of Economics 126, 2011.↩︎"
  },
  {
    "objectID": "guides/getting-started/cluster-randomization_en.html",
    "href": "guides/getting-started/cluster-randomization_en.html",
    "title": "10 Things to Know About Cluster Randomization",
    "section": "",
    "text": "1 What clustering is\nCluster randomized1 experiments allocate treatments to groups, but measure outcomes at the level of the individuals that compose the groups. It is this divergence between the level at which the intervention is assigned and the level at which outcomes are defined that classifies an experiment as cluster randomized.\nConsider a study that randomly assigns villages to receive different development programs, where the well-being of households in the village is the outcome of interest. This is a clustered design because, while the treatment is assigned to the village as a whole, we are interested in outcomes defined at the household level. Or consider a study that randomly assigns certain households to receive different get-out-the-vote messages, where we are interested in the voting behavior of individuals. Because the unit of assignment for the message is the household, but the outcome is defined as individual behavior, this study is cluster randomized.\nNow consider a study in which villages are selected at random, and 10 people from each village are assigned to treatment or control, and we measure the well-being of those individuals. In this case, the study is not cluster randomized, because the level at which treatment is assigned and the level at which outcomes are defined is the same. Suppose that a study randomly assigned villages to different development programs and then measured social cohesion in the village. Though it contains the same randomization procedure as our first example, it is not clustered because we are interested here in village-level outcomes: the level of treatment assignment and of outcome measurement is the same.\nClustering matters for two main reasons. On the one hand, clustering reduces the amount of information in an experiment because it restricts the number of ways that the treatment and control groups can be composed, relative to randomization at the individual level. If this fact is not taken into account, we often underestimate the variance in our estimator, leading to over-confidence in our the results of the study. On the other hand, clustering raises the question of how to combine information from different parts of the same experiment into one quantity. Especially when clusters are not of equal sizes and the potential outcomes of units within them are very different, conventional estimators will systematically produce the wrong answer due to bias. However, several approaches at the design and analysis phases can overcome the challenges posed by cluster randomized designs.\n\n\n2 Why clustering can matter I: information reduction\nWe commonly think of the information contained in studies in terms of the sample size and the characteristics of the units within the sample. Yet two studies with exactly the same sample size and the same participants could in theory contain very different amounts of information depending on whether units are clustered. This will greatly affect the precision of the inferences we make based on the studies.\nImagine an impact evaluation with 10 people, where 5 are assigned to the treatment group and 5 to control. In one version of the experiment, the treatment is assigned to individuals - it is not cluster randomized. In another version of the experiment, the 5 individuals with black hair and the 5 individuals with some other color of hair are assigned to treatment as a group. This design has two clusters: ‘black hair’ and ‘other color’.\nA simple application of the n choose k rule shows why this matters. In the first version, the randomization procedure allows for 252 different combinations of people as the treatment and control groups. However, in the second version, the randomization procedure assigns all the black-haired subjects either to treatment or to control, and thus only ever produces two ways of combining people to estimate an effect.\nThroughout this guide, we will illustrate points using examples written in R code. You can copy and paste this into your own R program to see how it works.\n\n[Click to show code]\n\n\n\nCode\nset.seed(12345)\n# Define the sample average treatment effect (SATE)\ntreatment_effect     <- 1\n# Define the individual ids (i)\nperson               <- 1:10\n# Define the cluster indicator (j)\nhair_color           <- c(rep(\"black\",5),rep(\"brown\",5))\n# Define the control outcome (Y0)\noutcome_if_untreated <- rnorm(n = 10)\n# Define the treatment outcome (Y1)\noutcome_if_treated   <- outcome_if_untreated + treatment_effect\n# Version 1 - Not cluster randomized\n# Generate all possible non-clustered assignments of treatment (Z)\nnon_clustered_assignments <- combn(x = unique(person),m = 5)\n# Estimate the treatment effect\ntreatment_effects_V1 <-\n     apply(\n          X = non_clustered_assignments,\n          MARGIN = 2,\n          FUN = function(assignment) {\n               treated_outcomes   <- outcome_if_treated[person %in% assignment]\n               untreated_outcomes <- outcome_if_untreated[!person %in% assignment]\n               mean(treated_outcomes) - mean(untreated_outcomes)\n          }\n     )\n# Estimate the true standard error\nstandard_error_V1 <- sd(treatment_effects_V1)\n# Plot the histogram of all possible estimates of the treatment effect\nhist(treatment_effects_V1,xlim = c(-1,2.5),breaks = 20)\n\n\n\n\n\n\n[Click to show code]\n\n\n\nCode\n# Version 2 - Cluster randomized\n# Generate all possible assignments of treatment when clustering by hair color (Z)\nclustered_assignments     <- combn(x = unique(hair_color),m = 1)\n# Estimate the treatment effect\ntreatment_effects_V2 <-\n     sapply(\n          X = clustered_assignments,\n          FUN = function(assignment) {\n               treated_outcomes   <- outcome_if_treated[person %in% person[hair_color==assignment]]\n               untreated_outcomes <- outcome_if_untreated[person %in% person[!hair_color==assignment]]\n               mean(treated_outcomes) - mean(untreated_outcomes)\n          }\n     )\n# Estimate the true standard error\nstandard_error_V2 <- sd(treatment_effects_V2)\n# Plot the histogram of all possible estimates of the treatment effect\nhist(treatment_effects_V2,xlim = c(-1,2.5),breaks = 20)\n\n\n\n\n\nAs the histograms show, clustering provides a much ‘coarser’ view of the treatment effect. Irrespective of the number of times one randomizes the treatment and of the number of subjects one has, in a clustered randomization procedure the number of possible estimates of the treatment effect will be strictly determined by the number of clusters assigned to the different treatment conditions. This has important implications for the standard error of the estimator.\n\n[Click to show code]\n\n\n\nCode\n# Compare the standard errors\nkable(round(data.frame(standard_error_V1,standard_error_V2),2))\n\n\n\n\n\nstandard_error_V1\nstandard_error_V2\n\n\n\n\n0.52\n1.13\n\n\n\n\n\nWhile the sampling distribution for the non-clustered estimate of the treatment effect has a standard error of about .52, that of the clustered estimate is more than twice as large, at 1.13. Recall that the data going into both studies is identical, the only difference between the studies resides in the way that the treatment assignment mechanism reveals the information.\nRelated to this issue of information is the question of how much units in our study vary within and between clusters. Two cluster randomized studies with \\(J=10\\) villages and \\(n_j=100\\) people per village may have different information about the treatment effect on individuals if, in one study, differences between villages are much greater than the differences in outcomes within them. If, say, all of the individuals in any village acted exactly the same but different villages showed different outcomes, then we would have on the order of 10 pieces of information: all of the information about causal effects in that study would be at the village level. Alternatively, if the individuals within a village acted more or less independently of each other, then we would have on the order of 10 \\(\\times\\) 100=1000 pieces of information.\nWe can formalize the idea that the highly dependent clusters provide less information than the highly independent clusters with the intracluster correlation coefficient. For a given variable, \\(y\\), units \\(i\\) and clusters \\(j\\), we can write the intracluster correlation coefficient as follows:\n\\[ \\text{ICC} \\equiv \\frac{\\text{variance between clusters in } y}{\\text{total variance in } y} \\equiv \\frac{\\sigma_j^2}{\\sigma_j^2 + \\sigma_i^2} \\]\nwhere \\(\\sigma_i^2\\) is the variance between units in the population and \\(\\sigma_j^2\\) is the variance between outcomes defined at the cluster level. Kish (1965) uses this description of dependence to define his idea of the “effective N” of a study (in the sample survey context, where samples may be clustered):\n\\[\\text{effective N}=\\frac{N}{1+(n_j -1)\\text{ICC}}=\\frac{Jn}{1+(n-1)\\text{ICC}},\\]\nwhere the second term follows if all of the clusters are the same size (\\(n_1 \\ldots n_J \\equiv n\\)).\nIf 200 observations arose from 10 clusters with 20 individuals within each cluster, where ICC = .5, such that 50% of the variation could be attributed to cluster-to-cluster differences (and not to differences within a cluster), Kish’s formula would suggest that we have an effective sample size of about 19 observations, instead of 200.\nAs the foregoing discussion suggests, clustering reduces information most when it a) greatly restricts the number of ways in which subjects can be assigned to treatment and control groups, and b) produces units whose outcomes are strongly related to the cluster they are a member of (i.e. when it increases the ICC).\n\n\n3 What to do about information reduction\nOne way to limit the extent to which clustering reduces the information contained in our design would be to form clusters in such a way that the intracluster correlation is low. For example, if we were able to form clusters randomly, there would be no systematic relationship between units within a cluster. Hence, using cluster random assignment to assign these randomly formed clusters to experimental conditions would not result in any information reduction. Yet, opportunities to create clusters are typically rare. In most cases, we have to rely on naturally formed clusters (e.g., villages, classrooms, cities). In most scenarios that involve clustered assignment, we hence will have to make sure that our estimates of uncertainty about treatment effects correctly reflect the resulting information loss from clustering.\nIn order to characterize our uncertainty about treatment effects, we typically want to calculate a standard error: an estimate of how much our treatment effect estimates would vary, were we able to repeat the experiment a very large number of times and, for each repetition, observe units in their resulting treated or untreated state.\nHowever, we are never able to observe the true standard error of an estimator, and therefore must use statistical procedures to infer this unknown quantity. Conventional methods for calculating standard errors do not take into account clustering. Thus, in order to avoid over-confidence in experimental findings, we need to modify the way in which we calculate uncertainty estimates.\nIn this section we limit our attention to so-called ‘design-based’ approaches to calculating the standard error. In the design-based approach we simulate repetitions of the experiment to derive and check ways of characterizing the variance of the estimate of the treatment effect, accounting for clustered randomization. We contrast these with ‘model-based’ approaches further on in the guide. In the model-based approach we state that the outcomes were generated according to a probability model and that the cluster-level relationships also follow a probability model.\nTo begin, we will create a function which simulates a cluster randomized experiment with fixed intracluster correlation, and use it to simulate some data from a simple cluster-randomized design.\n\n[Click to show code]\n\n\n\nCode\nmake_clustered_data <- function(J = 10, n = 100, treatment_effect = .25, ICC = .1){\n     ## Inspired by Mathieu et al, 2012, Journal of Applied Psychology\n     if (J %% 2 != 0 | n %% 2 !=0) {\n          stop(paste(\"Number of clusters (J) and size of clusters (n) must be even.\"))\n     }\n     Y0_j         <- rnorm(J,0,sd = (1 + treatment_effect) ^ 2 * sqrt(ICC))\n     fake_data    <- expand.grid(i = 1:n,j = 1:J)\n     fake_data$Y0 <- rnorm(n * J,0,sd = (1 + treatment_effect) ^ 2 * sqrt(1 - ICC)) + Y0_j[fake_data$j]\n     fake_data$Y1 <- with(fake_data,mean(Y0) + treatment_effect + (Y0 - mean(Y0)) * (2 / 3))\n     fake_data$Z  <- ifelse(fake_data$j %in% sample(1:J,J / 2) == TRUE, 1, 0)\n     fake_data$Y  <- with(fake_data, Z * Y1 + (1 - Z) * Y0)\n     return(fake_data)\n}\nset.seed(12345)\npretend_data <- make_clustered_data(J = 10,n = 100,treatment_effect = .25,ICC = .1)\n\n\nBecause we have created the data ourselves, we can calculate the true standard error of our estimator. We firstly generate the true sampling distribution by simulating every possible permutation of the treatment and calculating the estimate each time. The standard deviation of this distribution is the standard error of the estimator.\n\n[Click to show code]\n\n\n\nCode\n# Define the number of clusters\nJ <- length(unique(pretend_data$j))\n# Generate all possible ways of combining clusters into a treatment group\nall_treatment_groups <- with(pretend_data,combn(x = 1:J,m = J/2))\n# Create a function for estimating the effect\nclustered_ATE <- function(j,Y1,Y0,treated_clusters) {\n     Z_sim    <- (j %in% treated_clusters)*1\n     Y        <- Z_sim * Y1 + (1 - Z_sim) * Y0\n     estimate <- mean(Y[Z_sim == 1]) - mean(Y[Z_sim == 0])\n     return(estimate)\n}\nset.seed(12345)\n# Apply the function through all possible treatment assignments\ncluster_results <- apply(\n  X = all_treatment_groups, MARGIN = 2,\n                         FUN = clustered_ATE,\n                         j  = pretend_data$j,Y1 = pretend_data$Y1,\n  Y0 = pretend_data$Y0\n)\ntrue_SE <- sd(cluster_results)\ntrue_SE\n\n\n[1] 0.2567029\n\n\nThis gives a standard error of 0.26. We can compare the true standard error to two other kinds of standard error commonly employed. The first ignores clustering and assumes that treatment effect estimates are identically and independently distributed according to a normal distribution. We will refer to this as the I.I.D. standard error. To take clustering into account, we can use the following formula for the standard error:\n\\[\\text{Var}_\\text{clustered}(\\hat{\\tau})=\\frac{\\sigma^2}{\\sum_{j=1}^J \\sum_{i=1}^{n_j} (Z_{ij}-\\bar{Z})^2} (1-(n-1)\\rho)\\]\nwhere \\(\\sigma^2=\\sum_{j=1}^J \\sum_{i=1}^{n_j} (Y_{ij} - \\bar{Y}_{ij})^2\\) (following Arceneaux and Nickerson (2009) ). This adjustment to the IID standard error is commonly known as the “Robust Clustered Standard Error” or RCSE.\n\n[Click to show code]\n\n\n\nCode\nATE_estimate <- lm(Y ~ Z,data = pretend_data)\nIID_SE <- function(model) {\n     return(sqrt(diag(vcov(model)))[[\"Z\"]])\n}\nRCSE <- function(model, cluster,return_cov = FALSE){\n  require(sandwich)\n  require(lmtest)\n  M <- length(unique(cluster))\n  N <- length(cluster)\n  K <- model$rank\n  dfc <- (M/(M - 1)) * ((N - 1)/(N - K))\n  uj <- apply(estfun(model), 2, function(x) tapply(x, cluster, sum))\n  rcse.cov <- dfc * sandwich(model, meat = crossprod(uj)/N)\n  rcse.se <- as.matrix(coeftest(model, rcse.cov))\n  if(return_cov){\n    return(rcse.cov)\n  }else{\n    return(rcse.se)\n  }\n}\nIID_SE_estimate <- IID_SE(model = ATE_estimate)\nRCSE_estimate   <- RCSE(model = ATE_estimate,cluster = pretend_data$j)\nknitr::kable(round(\n  data.frame(\n     true_SE         = true_SE,\n     IID_SE_estimate = IID_SE_estimate,\n    RCSE_estimate   = RCSE_estimate[\"Z\", \"Std. Error\"]\n  ),\n  2\n))\n\n\n\n\n\ntrue_SE\nIID_SE_estimate\nRCSE_estimate\n\n\n\n\n0.26\n0.08\n0.26\n\n\n\n\n\nWhen we ignore the clustered-assignment, the standard error is too small: we are over-confident about the amount of information provided to us by the experiment. The RCSE is slightly more conservative than the true standard error in this instance, but is very close. The discrepancy is likely because the RCSE is not a good approximation of the true standard error when the number of clusters is as small as it is here. To illustrate the point further, we can compare a simulation of the true standard error generated through random permutations of the treatment to the IID and RC standard errors.\n\n[Click to show code]\n\n\n\nCode\ncompare_SEs <- function(data) {\n     simulated_SE <- sd(replicate(\n          5000,\n          clustered_ATE(\n               j = data$j,\n               Y1 = data$Y1,\n               Y0 = data$Y0,\n               treated_clusters = sample(unique(data$j),length(unique(data$j))/2)\n    )\n  ))\n     ATE_estimate <- lm(Y ~ Z,data)\n     IID_SE_estimate <- IID_SE(model = ATE_estimate)\n     RCSE_estimate <- RCSE(model = ATE_estimate,cluster = data$j)[\"Z\", \"Std. Error\"]\n     return(round(c(\n          simulated_SE = simulated_SE,\n          IID_SE = IID_SE_estimate,\n          RCSE = RCSE_estimate\n     ),3))\n}\nJ_4_clusters    <- make_clustered_data(J = 4)\nJ_10_clusters   <- make_clustered_data(J = 10)\nJ_30_clusters   <- make_clustered_data(J = 30)\nJ_100_clusters  <- make_clustered_data(J = 100)\nJ_1000_clusters <- make_clustered_data(J = 1000)\nset.seed(12345)\nknitr::kable(rbind(\n  c(J = 4,compare_SEs(J_4_clusters)),\n  c(J = 30,compare_SEs(J_30_clusters)),\n  c(J = 100,compare_SEs(J_100_clusters)),\n  c(J = 1000,compare_SEs(J_1000_clusters))\n  ))\n\n\n\n\n\nJ\nsimulated_SE\nIID_SE\nRCSE\n\n\n\n\n4\n0.270\n0.127\n0.260\n\n\n30\n0.161\n0.047\n0.146\n\n\n100\n0.085\n0.027\n0.088\n\n\n1000\n0.027\n0.008\n0.027\n\n\n\n\n\nAs these simple examples illustrate, the clustered estimate of the standard error (RCSE) gets closer to the truth (the simulated standard error) as the number of clusters increases. Meanwhile, the standard error ignoring clustering (assuming IID) tends to be smaller than either of the other standard errors. The smaller the estimate of the standard error is, the more precise the estimates seem to us, and the more likely we will find results that appear ‘statistically significant’. This is problematic: in this case, the IID standard error is leads us to be over confident in our results because it ignores intra-cluster correlation, the extent to which differences between units can be attributed to the cluster they are a member of. If we estimate standard errors using techniques that understate our uncertainty, we are more likely to falsely reject null hypotheses when we should not.\nAnother way to approach the problems that clustering introduces into the calculation of standard errors is to analyze the data at the level of the cluster. In this approach, we take averages or sums of the outcomes within the clusters, and then treat the study as though it only took place at the level of the cluster. Hansen and Bowers (2008) show that we can characterize the distribution of the difference of means using what we know about the distribution of the sum of the outcome in the treatment group, which varies from one assignment of treatment to another.\n\n[Click to show code]\n\n\n\nCode\n# Aggregate the unit-level data to the cluster level\n# Sum outcome to cluster level\nYj <- tapply(pretend_data$Y,pretend_data$j,sum)\n# Aggregate assignment indicator to cluster level\nZj <- tapply(pretend_data$Z,pretend_data$j,unique)\n# Calculate unique cluster size\nn_j <- unique(as.vector(table(pretend_data$j)))\n# Calculate total sample size (our units are now clusters)\nN <- length(Zj)\n# Generate cluster id\nj <- 1:N\n# Calculate number of clusters treated\nJ_treated <- sum(Zj)\n# Make a function for the cluster-level difference in means estimator (See Hansen & Bowers 2008)\ncluster_difference <- function(Yj,Zj,n_j,J_treated,N){\n     ones <- rep(1, length(Zj))\n     ATE_estimate <- crossprod(Zj,Yj)*(N/(n_j*J_treated*(N-J_treated))) -\n          crossprod(ones,Yj)/(n_j*(N-J_treated))\n     return(ATE_estimate)\n}\n# Given equal sized clusters and no blocking, this is identical to the\n# unit-level difference in means\nATEs <- colMeans(data.frame(\n  cluster_level_ATE =\n               cluster_difference(Yj,Zj,n_j,J_treated,N),\n          unit_level_ATE =\n    with(pretend_data, mean(Y[Z == 1]) - mean(Y[Z == 0]))\n))\nknitr::kable(data.frame(ATEs),align = \"c\")\n\n\n\n\n\n\nATEs\n\n\n\n\ncluster_level_ATE\n0.3417229\n\n\nunit_level_ATE\n0.3417229\n\n\n\n\n\nIn order to characterize uncertainty about the cluster-level ATE, we can exploit the fact that the only random element of the estimator is now the cross-product between the cluster-level assignment vector and the cluster-level outcome, \\(\\mathbf{Z}^\\top\\mathbf{Y}\\), scaled by some constant. We can estimate the variance of this random component through permutation of the assignment vector or through an approximation of the variance, assuming that the sampling distribution follows a normal distribution.\n\n[Click to show code]\n\n\n\nCode\n# Approximating variance using normality assumptions\nnormal_sampling_variance <-\n     (N/(n_j*J_treated*(N-J_treated)))*(var(Yj)/n_j)\n# Approximating variance using permutations\nset.seed(12345)\nsampling_distribution <- replicate(10000,cluster_difference(Yj,sample(Zj),n_j,J_treated,N))\nses <- data.frame(\n  sampling_variance = c(sqrt(normal_sampling_variance), sd(sampling_distribution)),\n  p_values = c(\n    2 * (1 - pnorm(abs(ATEs[1]) / sqrt(normal_sampling_variance), mean = 0)),\n                               2*min(mean(sampling_distribution>=ATEs[1]),mean(sampling_distribution<=ATEs[1]))\n  )\n)\nrownames(ses) <- c(\"Assuming Normality\",\"Permutations\")\nknitr::kable(ses)\n\n\n\n\n\n\nsampling_variance\np_values\n\n\n\n\nAssuming Normality\n0.2848150\n0.2302145\n\n\nPermutations\n0.2825801\n0.2792000\n\n\n\n\n\nThis cluster-level approach has the advantage of correctly characterizing uncertainty about effects when randomization is clustered, without having to use the RCSE standard errors for the unit-level estimates, which are overly permissive for small N. Indeed, the false positive rate of tests based on RCSE standard errors tend to be incorrect when the number of clusters is small, leading to over-confidence. As we shall see below, however, when the number of clusters is very small (\\(J=4\\)) the cluster-level approach is overly conservative, rejecting the null with a probability of 1. Another drawback of the cluster-level approach is that it does not allow for the estimation of unit-level quantities of interest, such as heterogeneous treatment effects.\n\n\n4 Why clustering can matter II: different cluster sizes and bias\nWhen clusters are of different sizes, this can pose a unique class of problems related to the estimation of the treatment effect. Especially when the size of the cluster is in some way related to the potential outcomes of the units within it many conventional estimators of the sample average treatment effect (SATE) can be biased.\nTo fix ideas, imagine an intervention targeted at firms of different sizes, which seeks to increase worker productivity. Due to economies of scale, the productivity of employees in big firms is increased much more proportional to that of employees in smaller firms. Imagine that the experiment includes 20 firms ranging in size from one-person entrepreneurs to large outfits with over 500 employees. Half of the firms are assigned to the treatment, and the other half are assigned to control. Outcomes are defined at the employee level.\n\n[Click to show code]\n\n\n\nCode\nset.seed(1000)\n# Number of firms\nJ <- 20\n# Employees per firm\nn_j <- rep(2^(0:(J/2-1)),rep(2,J/2))\n# Total number of employees\nN <- sum(n_j)\n# 2046\n# Unique employee (unit) ID\ni <- 1:N\n# Unique firm (cluster) ID\nj <- rep(1:length(n_j),n_j)\n# Firm specific treatment effects\ncluster_ATE <- n_j^2/10000\n# Untreated productivity\nY0 <- rnorm(N)\n# Treated productivity\nY1 <- Y0 + cluster_ATE[j]\n# True sample average treatment effect\n(true_SATE <- mean(Y1-Y0))\n\n\n[1] 14.9943\n\n\n\n[Click to show code]\n\n\n\nCode\n# Correlation between firm size and effect size\ncor(n_j,cluster_ATE)\n\n\n[1] 0.961843\n\n\nAs we see, there is high correlation in the treatment effect and cluster size. Now let us simulate 1000 analyses of this experiment, permuting the treatment assignment vector each time, and taking the unweighted difference in means as an estimate of the sample average treatment effect.\n\n[Click to show code]\n\n\n\nCode\nset.seed(1234)\n# Unweighted SATE\nSATE_estimate_no_weights <- NA\nfor(i in 1:1000){\n     # Clustered random assignment of half of the firms\n     Z <- (j %in% sample(1:J,J/2))*1\n     # Reveal outcomes\n     Y <- Z*Y1 + (1-Z)*Y0\n     # Estimate SATE\n     SATE_estimate_no_weights[i] <- mean(Y[Z==1])-mean(Y[Z==0])\n     }\n# Generate histogram of estimated effects\nhist(SATE_estimate_no_weights,xlim = c(true_SATE-2,true_SATE+2),breaks = 100)\n# Add the expected estimate of the SATE using this estimator\nabline(v=mean(SATE_estimate_no_weights),col=\"blue\")\n# And add the true SATE\nabline(v=true_SATE,col=\"red\")\n\n\n\n\n\nThe histogram shows the sampling distribution of the estimator, with the true SATE in red and the unweighted estimate thereof in blue. The estimator is biased: in expectation, we do not recover the true SATE, instead underestimating it. Intuitively, one might correctly expect that the problem is related to the relative weight of the clusters in the calculation of the treatment effect. However, in this situation, taking the difference in the weighted average of the outcome among treated and control clusters is not enough to provide an unbiased estimator (see the code below).\n\n[Click to show code]\n\n\n\nCode\nset.seed(1234)\n# Weighted cluster-averages\nSATE_estimate_weighted <- NA\nfor(i in 1:1000){\n     # Define the clusters put into treatment\n     treated_clusters <- sample(1:J,J/2,replace = F)\n     # Generate unit-level assignment vector\n     Z <- (j %in% treated_clusters)*1\n     # Reveal outcomes\n     Y <- Z*Y1 + (1-Z)*Y0\n     # Calculate the cluster weights\n     treated_weights <- n_j[1:J%in%treated_clusters]/sum(n_j[1:J%in%treated_clusters])\n     control_weights <- n_j[!1:J%in%treated_clusters]/sum(n_j[!1:J%in%treated_clusters])\n     # Calculate the means of each cluster\n     treated_means <- tapply(Y,j,mean)[1:J%in%treated_clusters]\n     control_means <- tapply(Y,j,mean)[!1:J%in%treated_clusters]\n     # Calculate the cluster-weighted estimate of the SATE\n     SATE_estimate_weighted[i] <-\n          weighted.mean(treated_means,treated_weights) -\n          weighted.mean(control_means,control_weights)\n}\n# Generate histogram of estimated effects\nhist(SATE_estimate_weighted,xlim = c(true_SATE-2,true_SATE+2),breaks = 100)\n# Add the expected estimate of the unweighted SATE\nabline(v=mean(SATE_estimate_no_weights),col=\"blue\")\n# Add the expected estimate of the weighted SATE\nabline(v=mean(SATE_estimate_weighted),col=\"green\")\n# And add the true SATE\nabline(v=true_SATE,col=\"red\")\n\n\n\n\n\nThe histogram shows the sampling distribution of the weighted estimator, with the true SATE in red and the unweighted estimate in blue, and the weighted estimate in green. In expectation, the weighted version of the estimator in fact gives the same estimate of the SATE as the non-weighted version. What is the nature of the bias?\nInstead of assigning treatment to half of the clusters and comparing outcomes at the level of the ‘treatment’ and ‘control’ groups, imagine that we paired each cluster with one other cluster, and assigned one to treatment within each pair. Our estimate of the treatment effect is then the aggregate of the pair-level estimates. This is analogous to the complete random assignment procedure employed above, in which \\(J/2\\) firms were assigned to treatment. Now, we will instead refer to the \\(k\\)’th of the \\(m\\) pairs, where \\(2m = J\\).\nGiven this setup, Imai, King, and Nall (2009) give the following formal definition of the bias in the cluster-weighted difference-in-means estimator\n\\[\\frac{1}{n}\n\\sum^m_{k = 1}\n\\sum^2_{l = 1}\n\\left[\n\\left(\n\\frac{n_{1k} + n_{2k}}{{2} - n_{lk}}\n\\right)\n\\times\n\\sum^{n_{lk}}_{i = 1}\n\\frac{Y_{ilk}(1) - Y_{ilk}(0)}{n_{lk}}\n\\right],\\]\nwhere \\(l = 1,2\\) indexes the clusters within each pair. Thus, \\(n_{1k}\\) refers to the number of units in the first of the \\(k\\)’th pair of clusters.\nThis expression indicates that bias from unequal cluster sizes arises if and only if two conditions are met. Firstly, the sizes of at least one pair of clusters must be unequal: when \\(n_{1k}=n_{2k}\\) for all \\(k\\), the bias term is reduced to 0. Secondly, the weighted effect sizes of at least one pair of clusters must be unequal: when \\(\\sum_{i = 1}^{n_{1k}}(Y_{i1k}(1)-Y_{i1k}(0))/n_{1k} = \\sum_{i = 1}^{n_{2k}}(Y_{i2k}(1)-Y_{i2k}(0))/n_{2k}\\) for all \\(k\\), the bias is also reduced to 0.\n\n\n5 What to do about different cluster sizes\nTwo other approaches work to alleviate this problem of bias from the correlation of cluster size and treatment effect: (1) condition on cluster size directly and (2) use the Horvitz-Thompson estimator. This DeclareDesign Blog Post discusses both options. Here we demonstrate a pairing approach to conditioning on cluster-size given the example data that we generated above.\nAs the above expression suggests, in order to reduce the bias from unequal cluster sizes to almost 0, it is sufficient to put clusters into pairs that either are of equal size or have almost identical potential outcomes.\nWe demonstrate this approach below using the same data as we examined in the example of a hypothetical firm-randomized employee productivity experiment.\n\n\n\n[Click to show code]\n\n\n\nCode\nset.seed(1234)\n# Make a function that matches pairs based on size\npair_sizes <- function(j,n_j){\n     # Find all of the unique sizes\n     unique_sizes <- unique(n_j)\n     # Find the number of unique sizes\n     N_unique_sizes <- length(unique_sizes)\n     # Generate a list of candidates for pairing at each cluster size\n  possible_pairs <- lapply(unique_sizes, function(size) {\n    which(n_j == size)\n  })\n     # Find the number of all possible pairs (m)\n     m_pairs <- length(unlist(possible_pairs))/2\n     # Generate a vector with unique pair-level identifiers\n     pair_indicator <- rep(1:m_pairs,rep(2,m_pairs))\n     # Randomly assign units of the same cluster size into pairs\n     pair_assignment <-\n          unlist(lapply(\n               possible_pairs,\n               function(pair_list){\n        sample(pair_indicator[unlist(possible_pairs) %in% pair_list])\n      }\n    ))\n     # Generate a vector indicating the k'th pair for each i unit\n     pair_assignment <- pair_assignment[match(x = j,table = unlist(possible_pairs))]\n     return(pair_assignment)\n}\npair_indicator <- pair_sizes(j = j , n_j = n_j)\nSATE_estimate_paired <- NA\nfor(i in 1:1000){\n     # Now loop through the vector of paired assignments\n     pair_ATEs <- sapply(unique(pair_indicator),function(pair){\n          # For each pair, randomly assign one to treatment\n          Z <- j[pair_indicator==pair] %in% sample(j[pair_indicator==pair],1)*1\n          # Reveal the potential outcomes of the pair\n          Y <- Z*Y1[pair_indicator==pair] + (1-Z)*Y0[pair_indicator==pair]\n          clust_weight <- length(j[pair_indicator==pair])/N\n          clust_ATE <- mean(Y[Z==1])-mean(Y[Z==0])\n          return(c(weight = clust_weight, ATE = clust_ATE))\n     })\n     SATE_estimate_paired[i] <- weighted.mean(x = pair_ATEs[\"ATE\",],w = pair_ATEs[\"weight\",])\n}\n# Generate histogram of estimated effects\nhist(SATE_estimate_paired,xlim = c(true_SATE-2,true_SATE+2),breaks = 100)\n# Add the expected estimate of the paired SATE\nabline(v=mean(SATE_estimate_paired),col=\"purple\")\n# And add the true SATE\nabline(v=true_SATE,col=\"red\")\n\n\n\n\n\n\n\n\n[Click to show code]\n\n\n\nCode\n# The paired estimator is much closer to the true SATE\nkable(round(data.frame(\n  true_SATE = true_SATE,\n  paired_SATE = mean(SATE_estimate_paired),\n  weighted_SATE = mean(SATE_estimate_weighted),\n  unweighted_SATE = mean(SATE_estimate_no_weights)\n),2))\n\n\n\n\n\ntrue_SATE\npaired_SATE\nweighted_SATE\nunweighted_SATE\n\n\n\n\n14.99\n14.99\n13.45\n13.45\n\n\n\n\n\nIn spite of unequal cluster sizes, the bias is completely eliminated by this technique: in expectation, the paired estimator recovers the true Sample Average Treatment Effect, whereas the cluster-weighted and non-weighted difference-in-means estimators are biased.\nNote also that the variance in the sampling distribution is much lower for the pair-matched estimator, giving rise to much more precise estimates. Thus, pair-matching not only promises to reduce bias, but can also greatly mitigate the problem of information reduction that clustering induces.\nSuch pre-randomization pair matching, however, does impose some constraints on the study, some of which may be difficult to meet in practice. For example, it may be difficult or even impossible to find perfectly-matched pairs for every cluster size, especially when there are multiple treatments (such that, instead of pairs, treatment is randomized over triplets or quadruplets). In such cases, researchers may adopt several other solutions, such as creating pairs by matching on observed covariates prior to the randomization, whereby, for example, the within-pair similarity of observed covariates is maximized. Imai, King, and Nall (2009) recommend a mixture model for post-randomization pair-matched estimation, and spell out some of the assumptions that must be made for such estimates to be valid. And this DeclareDesign Blog Post demonstrates conditioning on cluster-size without strict pairing.\n\n\n6 Why clustering can matter III: within-cluster spillovers\nIn many, or most experiments, we would like to estimate the average causal effect of the treatment within a population or a sample. Denoting \\(Y_{z_i}\\) the outcome \\(Y\\) of unit \\(i\\) when assigned to the treatment status \\(z_i \\in \\{1,0\\}\\), we can define this quantity – the ATE (Average Treatment Effect) – as the expected value of the difference between the sample when assigned to treatment, \\(Y_1\\) and the sample when assigned to control \\(Y_0\\): \\(E[Y_1 - Y_0]\\).\nHowever, it may be the case that a unit’s outcome depends on the treatment status \\(z_j\\) of another unit, \\(j\\), inside the same cluster. In that case, we denote potential outcomes \\(Y_{z_j,z_i} \\in \\{ Y_{00}, Y_{10}, Y_{01}, Y_{11} \\}\\), where an untreated unit with an untreated cluster neighbor is defined as \\(Y_{00}\\), an untreated unit with a treated cluster neighbor as \\(Y_{10}\\), a treated unit with an untreated cluster neighbor as \\(Y_{01}\\), and a treated unit with a treated cluster neighbor as \\(Y_{11}\\). When we conduct a cluster-randomized experiment, we typically assume that a unit’s outcome is not a function of the treatment status of the units with whom it shares a cluster, or formally \\(Y_{01}=Y_{11}=Y_1\\) and \\(Y_{10}=Y_{00}=Y_0\\). Yet, for all sorts of reasons this may not be the case: depending on whom someone finds themselves in the same cluster with, and whether or not that cluster is assigned to treatment, their outcomes may be very different.\nConsider an experiment in which five pairs of students living in dorms are randomly assigned to either receive or not receive a food subsidy, and their stated well-being is the outcome of interest. Let us assume that four students are vegetarian (V) and six are meat-eaters (M). When a VV, MM, or VM pair is assigned to control, they do not receive the subsidy and their well-being is unaffected. However, when assigned to treatment, VM pairs quarrel and this reduces their well-being, whereas VV and MM pairs do not fight and are affected only by the treatment. Let us denote \\(x_k \\in \\{0,1\\}\\) an indicator for whether the pair is mismatched, where the unit’s outcome is denoted \\(Y_{z_j,z_j,x_k}\\). This implies that \\(Y_{110} = Y_1\\) and \\(Y_{000} = Y_{001} = Y_0\\), whereas \\(Y_{111} \\neq Y_1\\). To understand how this matters, let us simulate such an experiment.\n\n\n\n[Click to show code]\n\n\n\nCode\n# Create experimental data\nN <- 10\ntypes <- c(rep(\"V\",.4*N),rep(\"M\",.6*N))\nID <- 1:length(types)\nbaseline <- rnorm(length(ID))\n# The true treatment effect is 5\ntrue_ATE <- 5\n# If a pair is mismatched (VM, MV), they get a spillover of -10\nspillover_or_not <- function(type_i,type_j){\n  ifelse(type_i==type_j,yes = 0,no = -10)\n}\n# A function for forming pairs\nform_pairs <- function(ID,types){\n  N <- length(ID)\n  k <- rep(1:(N/2),2)\n  pair_place <- rep(c(1,2),c(N/2,N/2))\n  ID_draw <- sample(ID)\n  type_i <- types[ID_draw]\n  pair_1 <- type_i[pair_place==1]\n  pair_2 <- type_i[pair_place==2]\n  ID_j_1 <- ID_draw[pair_place==1]\n  ID_j_2 <- ID_draw[pair_place==2]\n  type_j <- c(pair_2,pair_1)\n  j <- c(ID_j_2,ID_j_1)\n  return(data.frame(i = ID_draw,j = j,k = k, type_i = type_i, type_j = type_j))\n}\n# A function for assigning treatment and revealing the outcome\nassign_reveal_est <- function(k,i,effect,spillover){\n  Z <- (k %in% sample(k,N/2))*1\n  Y <- baseline[i] + Z*effect + Z*spillover\n  mean(Y[Z==1])-mean(Y[Z==0])\n}\n# A function for simulating the experiment\nsimulate_exp <- function(){\n  data <- form_pairs(ID,types)\n  spillover <- spillover_or_not(data$type_i,data$type_j)\n  estimate <- assign_reveal_est(k = data$k,effect = true_ATE,spillover = spillover,i = data$i)\n  return(estimate)\n}\n# Estimate the effects one thousand times\nest_effects <- replicate(n = 1000,expr = simulate_exp())\n# Plot the estimates as bars, the expected ATE in blue, and the true ATE in red\nhist(est_effects,breaks = 100,xlim = c(-7,7))\nabline(v = true_ATE,col = \"red\")\nabline(v = mean(est_effects,na.rm = T),col = \"blue\")\n\n\n\n\n\nAs the plot above shows, this is a biased estimator of the true individual level treatment effect, \\(Y_{01} - Y_{00}\\). In expectation, we estimate an effect close to 0, obtaining very negative effects in almost half of the simulations of this experiment. The key point here is that the estimand is changed: rather than the ATE, we obtain a combination of the true treatment effect among those who are matched (do not experience spillovers) \\(E[Y_{110}-Y_{00x_k}]\\), and the combined treatment and spillover effect for those who are unmatched \\(E[Y_{111}-Y_{00x_k}]\\). Crucially, however, we cannot identify the impact of the spillover, \\(E[Y_{101}-Y_{00x_k}]\\), independently of the direct effect. This is because the randomization is clustered: it is not possible to observe \\(Y_{101}\\) in a cluster-randomized scheme, because all units within a cluster are always treated. Generally speaking, this issue is true of any cluster-randomized study: in order to make the claim that we identify the individual-level effect of the treatment, we must assume that \\(Y_{11}=Y_{1}\\) and \\(Y_{00}=Y_{0}\\).\n\n\n7 What to do about within-cluster spillovers\nIf there are strong reasons to believe that intra-cluster spillovers occur, then researchers can take different approaches depending on the manner in which clusters are formed. In some studies researchers must themselves sort units into groups for the purposes of experimentation: for example, in a study involving a vocational programme, the researcher may be able to decide who is recruited into which class. In such cases, if the researcher can make plausible assumptions about spillovers, then the individual-level treatment effect may be recoverable.\nConsider a researcher conducting the previous study above who correctly assumed that spillovers would occur between mismatched pairs. In this case, the researcher can recover the true individual treatment effect by forming clusters that are not susceptible to spillover.\n\n\n\n[Click to show code]\n\n\n\nCode\nform_matched_pairs <- function(ID,types){\n  pair_list <- lapply(unique(types),function(type){\n    ID <- ID[types == type]\n    types <- types[types == type]\n    draw_ID <- 1:length(types)\n    matched_pairs <- form_pairs(ID = draw_ID,types = types)\n    matched_pairs$i <- ID[matched_pairs$i]\n    matched_pairs$j <- ID[matched_pairs$j]\n    matched_pairs$k <- paste0(type,\"_\",matched_pairs$k)\n    return(matched_pairs)\n  })\n  data <- rbind(pair_list[[1]],pair_list[[2]])\n  return(data)\n}\nsimulate_matched_exp <- function(){\n  data <- form_matched_pairs(ID,types)\n  spillover <- spillover_or_not(data$type_i,data$type_j)\n  estimate <- assign_reveal_est(k = data$k,effect = true_ATE,spillover = spillover,i = data$i)\n  return(estimate)\n}\n# Estimate the effects one thousand times\nest_matched_effects <- replicate(n = 1000,expr = simulate_matched_exp())\nhist(est_matched_effects,breaks = 100,xlim = c(-7,7))\nabline(v = true_ATE,col = \"red\")\nabline(v = mean(est_matched_effects,na.rm = T),col = \"blue\")\n\n\n\n\n\nIn the case that researchers are not able to control how clusters are formed, they can still investigate cluster-level heterogeneity in treatment effects as a way of understanding possible spillovers. However, in both cases, assumptions must be made about the nature of spillovers. Strictly speaking, these cannot be causally identified due to the unobservability of the outcomes \\(Y_{01}\\) and \\(Y_{10}\\). Ultimately, one would need to combine clustered and non-clustered randomization schemes in order to estimate the effects of intra-cluster spillover, \\(Y_{11} - Y_{01}\\) and \\(Y_{01} - Y_{00}\\). Therefore, in the interests of interpreting results correctly, researchers should be careful when defining their estimand to take account of the potential for intra-cluster spillover.\n\n\n8 Performance of design- vs. model-based analyses of clustered studies\nIn our discussion of information loss, we assessed approaches that require (1) that treatment was randomized as planned and (2) that the treatment assigned to one unit did not change the potential outcomes for any other unit. In cases where these assumptions may be violated, it is sometimes simpler to specify statistical models that attempt to describe the features of complex designs. Even if we do not believe the models as scientific descriptions of a known process, this can be a more informative and flexible way of analyzing an experiment than to derive complex new expressions for design-based estimators.\nIn model-based approaches, the sampling distribution of an estimator is approximated using probability distributions to characterize our uncertainty about unknown quantities, such as the true treatment effect or the true mean of the outcome at the cluster level. Such approaches are referred to as ‘model-based’, because they depict causal relationships as arising from interrelated probability distributions. Often, such approaches use ‘multilevel models’, in which unknown parameters — such as differences between clusters — are themselves understood as arising from probability distributions. Thus, for example, there might be a model for individual-level outcomes, whose intercept and/or coefficients vary from one cluster to another. In this manner, it is possible to model the ‘effect of being a unit in cluster A’, separately from the estimation of the treatment effect. The advantage of such approaches is that they allow for ‘partial pooling’ of the variance in the population and the variance among clusters. When a given cluster is poorly estimated, it contributes less weight to the estimation, and vice versa. Such models therefore often work well in situations where there is very little data in some clusters: through the specification of a Bayesian posterior distribution, they are able to leverage information from all parts of the study. The trade-off is that such assumption-heavy models are only correct to the extent that the assumptions underlying them are correct.\nHere we show that the estimated effect is the same whether we use a simple difference of means (via OLS) or a multilevel model in our very simplified cluster randomized trial setup.\n\n\n\n[Click to show code]\n\n\n\nCode\nlibrary(lme4)\nsimple_OLS <- lm(Y ~ Z,data = J_30_clusters)\nmultilevel <- lmer(Y ~ Z + (1 | j),\n                   data = J_30_clusters,\n  control = lmerControl(optimizer = \"bobyqa\"),\n  REML = TRUE\n)\nkable(round(data.frame(\n  OLS=coef(simple_OLS)[\"Z\"],Multilevel=fixef(multilevel)[\"Z\"]\n  ),3))\n\n\n\n\n\n\nOLS\nMultilevel\n\n\n\n\nZ\n-0.13\n-0.13\n\n\n\n\n\nThe confidence intervals differ even though the estimates are the same — and there is more than one way to calculate confidence intervals and hypothesis tests for multilevel models. The software in R (Bates et al. 2015) includes three methods by default and Gelman and Hill (2006) recommend MCMC sampling from the implied posterior. Here we focus on the Wald method only because it is the fastest to compute.\n\n\n\n[Click to show code]\n\n\n\nCode\n# This function calculates confidence intervals for linear models\n# with custom variance-covariance matrices\nconfint_HC<-function (coefficients, df, level = 0.95, vcov_mat, ...) {\n  a <- (1 - level)/2\n  a <- c(a, 1 - a)\n  fac <- qt(a, df)\n  ses <- sqrt(diag(vcov_mat))\n  coefficients + ses %o% fac\n}\nsimple_OLS_CI <-\n  confint_HC(\n    coefficients = coef(simple_OLS),\n    vcov_mat = RCSE(\n      model = simple_OLS,\n                             cluster = J_30_clusters$j,\n      return_cov = TRUE\n    ),\n    df = simple_OLS$df.residual\n  )[\"Z\", ]\nmulti_wald_CI <- lme4::confint.merMod(\n  multilevel,\n  parm = \"Z\", method = \"Wald\"\n)[\"Z\", ]\nmulti_profile_CI <- lme4::confint.merMod(\n  multilevel,\n  parm = 4, method = \"profile\"\n)[\"Z\", ]\nknitr::kable(round(rbind(\n  Design_Based_CI = simple_OLS_CI,\n  Model_Based_Wald_CI = multi_wald_CI,\n  Model_Based_Profile_CI = multi_profile_CI\n),3))\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\nDesign_Based_CI\n-0.416\n0.156\n\n\nModel_Based_Wald_CI\n-0.421\n0.161\n\n\nModel_Based_Profile_CI\n-0.420\n0.161\n\n\n\n\n\nWe can calculate an estimate of the ICC directly from the model quantities (the variance of the Normal prior that represents the cluster-to-cluster differences in the intercept over the total variance of the Normal posterior).\n\n\n\n[Click to show code]\n\n\n\nCode\nVC <- as.data.frame(lme4:::VarCorr.merMod(multilevel))\nround(c(ICC = VC$vcov[1] / sum(VC$vcov)),2)\n\n\n ICC \n0.09 \n\n\nIn order to assess the performance of this model-based approach, as opposed to the robust-clustered standard-error (RCSE) and cluster-aggregated approaches outlined above, we can check how often the different approaches falsely reject the sharp null hypothesis of no effects for any unit, when we know that this null is true.\nTo do so, we write a function that firstly breaks the relationship between the treatment assignment and the outcome by randomly shuffling the assignment, and then tests whether 0 is in the 95% confidence interval for each of the three approaches, as it should be. Recall that, valid tests would have error rates within 2 simulation standard errors of .95 - this would mean that a correct null hypothesis would be rejected no more than 5% of the time.\n\n\n\n[Click to show code]\n\n\n\nCode\n# Make a function for checking whether 0 is in the confidence interval of\n# the RCSE, cluster-aggregated, and multilevel estimation approaches\nsim_0_ate <- function(J,Y) {\n  #   Make the true relationship between treatment and outcomes equal zero by\n  #   shuffling Z but not revealing new potential outcomes\n  z.sim <- sample(1:max(J), max(J) / 2)\n  Z_new <- ifelse(J %in% z.sim == TRUE, 1, 0)\n  # Estimate using the linear model for RCSE\n  linear_fit <- lm(Y ~ Z_new)\n  linear_RCSE <- RCSE(\n    model = linear_fit,\n                  cluster = J,\n    return_cov = TRUE\n  )\n  linear_CI <- confint_HC(\n    coefficients = coef(linear_fit),\n                      vcov_mat = linear_RCSE,\n    df = linear_fit$df.residual\n  )[\"Z_new\", ]\n  # Check if the confidence interval bounds 0\n  zero_in_CI_RCSE <- (0 >= linear_CI[1]) & (0 <= linear_CI[2])\n  # Estimate using cluster-aggregated approach (Hansen and Bowers 2008)\n  Yj <- tapply(Y, J, sum)\n  Zj <- tapply(Z_new, J, mean)\n  m0 <- unique(table(J))\n  n  <- length(Zj)\n  nt <- sum(Zj)\n  # Do Hansen and Bowers 2008 based test for difference of means\n  # with cluster-level assignment (assuming same size clusters)\n  ones <- rep(1, length(Yj))\n  dp   <- crossprod(Zj,Yj) * (n / (m0 * nt * (n - nt))) -\n    crossprod(ones,Yj) / (m0 * (n - nt))\n  obs_ATE <- dp[1,1]\n  # Two tailed p-value for the test of the null of no effects\n  Vdp <- (n / (m0 * nt * (n - nt))) * (var(Yj) / m0)\n  HB_pval <- 2 * (1 - pnorm(abs(obs_ATE) / sqrt(Vdp)))\n  # Check if the p-value is greater than .05\n  zero_not_rej_HB <- HB_pval >= .05\n  # Estimate using a multilevel model\n  multilevel_fit <- lmer(Y ~ Z_new + (1 | J),\n    control = lmerControl(optimizer = \"bobyqa\"),\n    REML = FALSE\n  )\n  multilevel_CI <- lme4:::confint.merMod(\n    multilevel_fit,\n    parm = \"Z_new\", method = \"Wald\"\n  )\n  # Check if the confidence interval bounds 0\n  zero_in_CI_multilevel <- (0 >= multilevel_CI[1]) & (0 <= multilevel_CI[2])\n  return(\n    c(\n      ATE = fixef(multilevel_fit)[\"Z_new\"],\n      zero_in_CI_RCSE = zero_in_CI_RCSE,\n      zero_not_rej_HB = zero_not_rej_HB,\n      zero_in_CI_multilevel = zero_in_CI_multilevel\n    )\n  )\n}\n# Now simulate each of the estimates 1000 times\nJ_4_comparison <- replicate(1000, sim_0_ate(J = J_4_clusters$j, Y = J_4_clusters$Y))\nJ_4_error_rates <- apply(J_4_comparison,1,mean)\nJ_4_error_rates[-1] <- 1-J_4_error_rates[-1]\nJ_10_comparison <- replicate(1000, sim_0_ate(J = J_10_clusters$j, Y = J_10_clusters$Y))\nJ_10_error_rates <- apply(J_10_comparison,1,mean)\nJ_10_error_rates[-1] <- 1-J_10_error_rates[-1]\nJ_30_comparison <- replicate(1000, sim_0_ate(J = J_30_clusters$j, Y = J_30_clusters$Y))\nJ_30_error_rates <- apply(J_30_comparison,1,mean)\nJ_30_error_rates[-1] <- 1-J_30_error_rates[-1]\nJ_100_comparison <- replicate(1000, sim_0_ate(J = J_100_clusters$j, Y = J_100_clusters$Y))\nJ_100_error_rates <- apply(J_100_comparison,1,mean)\nJ_100_error_rates[-1] <- 1-J_100_error_rates[-1]\nerror_comparison <- data.frame(round(rbind(\n  J_4_error_rates,\n  J_10_error_rates,\n  J_30_error_rates,\n  J_100_error_rates\n),3))\ncolnames(error_comparison) <- c(\n  \"Estimated ATE\",\n                                \"OLS + RCSE\",\n                                \"Cluster-Level\",\n  \"Multi-Level\"\n)\nkable(error_comparison,align = \"c\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimated ATE\nOLS + RCSE\nCluster-Level\nMulti-Level\n\n\n\n\nJ_4_error_rates\n0.002\n0.000\n0.000\n0.334\n\n\nJ_10_error_rates\n0.005\n0.101\n0.042\n0.101\n\n\nJ_30_error_rates\n0.003\n0.061\n0.040\n0.063\n\n\nJ_100_error_rates\n-0.001\n0.058\n0.052\n0.059\n\n\n\n\n\nIn our simple setup, the individual-level approaches behave about the same way: neither the design-based nor the model-based approach produces valid statistical inferences until the number of clusters is at least 30. This makes sense: both approaches rely on central limit theorems so that a Normal law can describe the distribution of the test statistic under the null hypothesis. The cluster-level approach is always valid, but sometimes produces overly large confidence intervals (when the number of clusters is small). When the number of clusters is large (say, 100), then all approaches are equivalent in terms of their error rates. Designs with few clusters should consider either the cluster-level approach using the normal approximation shown here or even direct permutation based approaches to statistical inference.\n\n\n9 Power analysis for clustered designs\nWe want designs that are likely to reject hypotheses inconsistent with the data, and unlikely to reject hypotheses consistent with the data. We’ve seen that the assumptions required for the validity of common tests (typically, large numbers of observations, or large quantities of information in general) are challenged by clustered designs, and the tests which account for clustering can be invalid if the number of clusters is small (or information is low at the cluster level in general). We’ve also seen that we can produce valid statistical tests for hypotheses about the average treatment effect using either Robust Clustered Standard Errors (RCSE), multilevel models or using the cluster-level approach described by Hansen and Bowers (2008), and that pair-matching can drastically minimize bias in designs with unequal cluster sizes.\nThe most important rule regarding the statistical power of clustered designs is that more small clusters are better than fewer larger ones. This can be demonstrated through simulated experiments. Generally speaking, the most flexible way to evaluate the power of a design is through simulation, as it allows for complex clustering and blocking schemes, and can incorporate covariates. In the following we use the OLS estimator with Robust Clustered Standard Errors, in order to save on computation time, but the same analysis can be achieved using any estimator and test statistic.\n\n\n\n[Click to show code]\n\n\n\nCode\n# A function to test the null hypothesis and the true hypothesis\ntest_H0_and_Htrue <- function(J = J,n = n,treatment_effect = treatment_effect,ICC = ICC) {\n  # Make data:\n  data <- make_clustered_data(\n    J = J,\n                      n = n,\n                      treatment_effect = treatment_effect,\n    ICC = ICC\n  )\n  linear_fit <- lm(Y ~ Z,data = data)\n  RCSE_CI <- confint_HC(\n    coefficients = coef(linear_fit),\n    vcov_mat = RCSE(\n      model = linear_fit,\n                             cluster = data$j,\n      return_cov = TRUE\n    ),\n    df = linear_fit$df.residual\n  )[\"Z\", ]\n  # Zero should not be in this CI very often as the null of 0 is false here\n  correct_reject <- !((0 >= RCSE_CI[1]) & (0 <= RCSE_CI[2]))\n  # Test null of true taus (first attempt is use true, second is to make 0 true)\n  # Reassign village level treatment so that Y is indep of Z --- so true effect is 0\n  data$Z_new <- ifelse(data$j %in% sample(1:J, max(J)/2), 1, 0)\n  linear_fit_true <-lm(Y ~ Z_new,data = data)\n  RCSE_CI_true <- confint_HC(\n    coefficients = coef(linear_fit_true),\n    vcov_mat = RCSE(\n      model = linear_fit_true,\n                                             cluster = data$j,\n      return_cov = TRUE\n    ),\n    df = linear_fit$df.residual\n  )[\"Z_new\", ]\n  # Zero should be in this CI very often as the null of 0 is true here\n  false_positive <-  !((0 >= RCSE_CI_true[1]) & (0 <= RCSE_CI_true[2]))\n  return(c(\n    correct_reject = correct_reject,\n    false_positive = false_positive\n  ))\n}\n\n\nWe can now analyze how power is affected when the number of clusters and the size of clusters vary, holding the ICC constant at .01 and the treatment effect constant at .2. We look both at the power — how often we correctly reject the null of no effect when there really is one - as well as at the error — how often we incorrectly reject the null of no effect when there really isn’t an effect. Typically we want the power to be around .8 and the error rate to be around .5 (when using a 95% confidence level).\n\n\n\n[Click to show code]\n\n\n\nCode\n# The numbers of clusters we will consider\nJs <- c(8,20,40,80,160,320)\n# The cluster sizes we will consider\nn_js <- c(8,20,40,80,160,320)\n# Create an empty matrix to store the results\n# The first stores the power and the second stores the error\npower_J_n <- error_J_n <- matrix(\n  data = NA,\n  nrow = length(Js),\n  ncol = length(n_js),\n  dimnames = list(\n    paste(\"J =\",Js),\n    paste(\"n_j =\",n_js)\n  )\n)\n# Set the number of simulations\nsims <- 100\n# Loop through the different cluster numbers\nfor( j in 1:length(Js)){\n  # Loop through the different cluster sizes\n  for(n in 1:length(n_js)){\n    # Estimate the power and error rate for each combination\n    test_sims <- replicate(\n      n = sims,\n      expr = test_H0_and_Htrue(\n        J = Js[j],\n                                       n = n_js[n],\n                                       treatment_effect = .25,\n        ICC = .01\n      )\n    )\n    power_error <- rowMeans(test_sims)\n    # Store them in the matrices\n    power_J_n[j,n] <- power_error[1]\n    error_J_n[j,n] <- power_error[2]\n  }\n}\n# Plot the power under the different scenarios\nmatplot(power_J_n, type = c(\"b\"),pch=1,axes = F,ylim = c(0,1.5),ylab = \"power\")\naxis(side = 1,labels = rownames(power_J_n),at = 1:6)\naxis(side = 2,at = seq(0,1,.25))\nabline(h=.8)\nlegend(\"top\", legend = colnames(power_J_n),col = 1:6 ,pch=1,horiz = TRUE)\n\n\n\n\n\n\n\n\n[Click to show code]\n\n\n\nCode\n# Plot the error rate under the different scenarios\nmatplot(error_J_n, type = c(\"b\"),pch=1,axes = F,ylim = c(0,.5),ylab = \"error rate\")\naxis(side = 1,labels = rownames(error_J_n),at = 1:6)\naxis(side = 2,at = seq(0,1,.25))\nabline(h=.05)\nlegend(\"top\", legend = colnames(error_J_n),col = 1:6 ,pch=1,horiz = TRUE)\n\n\n\n\n\nWe see that power is always low when the number of clusters is low, regardless of how large the clusters are. Even with huge clusters (with 320 units each), the statistical power of the study is still relatively low when the number of clusters is 8. Similarly, it takes a large number of clusters to power a study with small clusters: although it is sufficient to have many clusters in order to power an experiment, irrespective of cluster size, power increases much more rapidly when the clusters are larger. Note also that while error rates appear systematically related to the number of clusters, the same is not true for cluster sizes.\nNext, we can evaluate how the intra-cluster correlation affects power. We will hold the structure of the sample size constant at \\(J=80,n_j=80\\) and \\(J=160,n_j=160\\), and compare across a range of ICC values, from low (.01) to high (.6).\n\n\n\n[Click to show code]\n\n\n\nCode\nJ_njs <- c(80,160)\nICCs <- seq(0,.6,.1)+c(.01,0,0,0,0,0,0)\npower_ICC <- error_ICC <- matrix(\n  data = NA,\n  nrow = length(ICCs),\n  ncol = length(J_njs),\n  dimnames = list(\n    paste(\"ICC =\",ICCs),\n    paste(\"J =\",J_njs,\"n_j = \",J_njs)\n  )\n)\n# Set the number of simulations\nsims <- 100\n# Loop through the different ICCs\nfor( i in 1:length(ICCs)){\n  # Loop through the different cluster sizes\n  for(j in 1:length(J_njs)){\n    # Estimate the power and error rate for each combination\n    test_sims <- replicate(\n      n = sims,\n      expr = test_H0_and_Htrue(\n        J = J_njs[j],\n                                       n = J_njs[j],\n                                       treatment_effect = .25,\n        ICC = ICCs[i]\n      )\n    )\n    power_error <- rowMeans(test_sims)\n    # Store them in the matrices\n    power_ICC[i,j] <- power_error[1]\n    error_ICC[i,j] <- power_error[2]\n  }\n}\n# Plot the power under the different scenarios\nmatplot(power_ICC, type = c(\"b\"),pch=1,axes = F,ylim = c(0,1.5),ylab = \"power (high ICC)\")\naxis(side = 1,labels = rownames(power_ICC),at = 1:7)\naxis(side = 2,at = seq(0,1,.25))\nabline(h=.8)\nlegend(\"top\", legend = colnames(power_ICC),col = 1:2 ,pch=1,horiz = TRUE)\n\n\n\n\n\n\n\n\n[Click to show code]\n\n\n\nCode\n# Plot the error rate under the different scenarios\nmatplot(error_ICC, type = c(\"b\"),pch=1,axes = F,ylim = c(0,.5),ylab = \"error rate\")\naxis(side = 1,labels = rownames(error_ICC),at = 1:7)\naxis(side = 2,at = seq(0,1,.25))\nabline(h=.05)\nlegend(\"top\", legend = colnames(error_ICC),col = 1:2 ,pch=1,horiz = TRUE)\n\n\n\n\n\nAs this example illustrates, high ICC can severely diminish the statistical power of the study, even with many large clusters.\n\n\n10 How to check balance in clustered designs\nRandomization checks in clustered designs follow the same form as the preceding discussion. A valid test for a treatment effect is a valid test for placebo or covariate balance. The only difference from our preceding discussion is that one uses a background covariate or baseline outcome — some variable putatively uninfluenced by the treatment — in place of the outcome itself. So, randomization tests with small numbers of clusters may be too quick to declare an experiment ill-randomized if the analyst is not aware of the methods of error-rate analysis that we described above.\nOne new problem does arise in the context of randomization tests. Often one has many covariates which could be used to detect unlucky imbalances or field problems with the randomization itself. And, if one uses hypothesis tests, then, of course, a valid test which encourages us to declare “imbalance” when \\(p<.05\\) would do so falsely for one in every twenty variables tested. For this reason, we recommend using one-by-one testing as an exploratory tool and using omnibus tests (like the Hotelling T-test or an F-test or the Hansen and Bowers (2008) \\(d^2\\) test), which can combine information across many dependent tests into one test statistic to make balance tests directly. However, these tests must account for the clustered nature of the design: a simple F-test without accounting for the clustered-design will likely mislead an analyst into declaring a design unbalanced and perhaps charging the field staff with a randomization failure.\nSince cluster randomized experiments tend to have cluster-level covariates (say, village size, etc..) balance checks at the cluster level make sense and do not require explicit changes to account for clustered-assignment. Hansen and Bowers (2008) develop such a test and provide software to implement it. So, for example, if we had 10 covariates measured at the village level, and we had a large number of villages we could assess an omnibus balance hypothesis using this design-based but large-sample tool.\nHere we show only the omnibus test results. The one-by-one assessments that make up the omnibus test are also available in the balance_test object. Here, the omnibus test tell us that we have little information against the null that these observations arose from a randomized study.\n\n\n\n[Click to show code]\n\n\n\nCode\nlibrary(RItools)\n\n\nLoading required package: SparseM\n\n\n\nAttaching package: 'SparseM'\n\n\nThe following object is masked from 'package:base':\n\n    backsolve\n\n\nLoading required package: dgof\n\n\n\nAttaching package: 'dgof'\n\n\nThe following object is masked from 'package:stats':\n\n    ks.test\n\n\nWarning: replacing previous import 'stats::ks.test' by 'dgof::ks.test' when\nloading 'RItools'\n\n\nCode\noptions(digits=3)\n# Make a village level dataset\nvillages <- aggregate(pretend_data,by = list(village = pretend_data$j), mean)\n# Generate 10 fake covariates\nset.seed(12345)\nvillages[paste(\"x\",1:10,sep=\"\")] <- matrix(rnorm(nrow(villages)*10), ncol=10)\nbalance_formula <- reformulate(paste(\"x\",1:10,sep=\"\"), response=\"Z\")\n# Do a design-based, large sample balance test\nbalance_test <-xBalance(balance_formula,\n                        strata=list(noblocks=NULL),\n                        data=villages,\n  report = c(\n    \"std.diffs\", \"z.scores\", \"adj.means\",\n    \"adj.mean.diffs\", \"chisquare.test\", \"p.values\"\n  )\n)\n# The results of the 1-by-1 balance tests\nkable(round(balance_test$results,2),align = \"c\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nControl.noblocks\nTreatment.noblocks\nadj.diff.noblocks\nstd.diff.noblocks\nz.noblocks\np.noblocks\n\n\n\n\nx1\n-0.24\n-0.02\n0.22\n0.26\n0.43\n0.67\n\n\nx2\n0.68\n-0.11\n-0.78\n-1.01\n-1.47\n0.14\n\n\nx3\n0.37\n-0.20\n-0.57\n-0.47\n-0.77\n0.44\n\n\nx4\n1.15\n0.30\n-0.86\n-0.71\n-1.11\n0.27\n\n\nx5\n-0.16\n0.04\n0.20\n0.14\n0.24\n0.81\n\n\nx6\n1.08\n-0.54\n-1.62\n-1.55\n-1.97\n0.05\n\n\nx7\n-0.04\n0.08\n0.12\n0.09\n0.15\n0.88\n\n\nx8\n0.76\n0.12\n-0.63\n-0.58\n-0.92\n0.36\n\n\nx9\n1.32\n0.37\n-0.95\n-1.30\n-1.76\n0.08\n\n\nx10\n-0.33\n0.28\n0.61\n0.51\n0.82\n0.41\n\n\n\n\n\n\n\n\n[Click to show code]\n\n\n\nCode\n# The overall omnibus p-value\nkable(round(balance_test$overall,2),align = \"c\")\n\n\n\n\n\n\nchisquare\ndf\np.value\n\n\n\n\nnoblocks\n9\n9\n0.44\n\n\n\n\n\nIn this case, we cannot reject the omnibus hypotheses of balance even though, as we expected, we have a few covariates with falsely low \\(p\\)-values. One way to interpret this omnibus result is to say that such imbalances on a few covariates would not appreciably change any statistical inferences we make about treatment effects as long as these covariates did not strongly predict outcomes in the control group. Alternatively, we could say that any large experiment can tolerant chance imbalance on a few covariates (no more than roughly 5% if we are using \\(\\alpha=.05\\) as our threshold to reject hypotheses).\n\n\n\n\n\n\n\n\n\nReferences\n\nArceneaux, Kevin, and David W Nickerson. 2009. “Modeling Certainty with Clustered Data: A Comparison of Methods.” Political Analysis 17 (2): 177–90.\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical Software 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge university press.\n\n\nHansen, Ben B, and Jake Bowers. 2008. “Covariate Balance in Simple, Stratified and Clustered Comparative Studies.” Statistical Science, 219–36.\n\n\nImai, Kosuke, Gary King, and Clayton Nall. 2009. “The Essential Role of Pair Matching in Cluster-Randomized Experiments, with Application to the Mexican Universal Health Insurance Evaluation.” Statistical Science 24 (1): 29–53.\n\n\nKish, Leslie. 1965. Survey Sampling. New York: John Wiley & Sons.\n\nFootnotes\n\n\nThis guide was originally written by Jake Bowers and Ashlea Rundlett (22 Nov 2014). Updates made by Jasper Cooper (25 Aug 2015) and Anna Wilke (Mar 2022).↩︎"
  },
  {
    "objectID": "guides/getting-started/multisite-experiments_en.html",
    "href": "guides/getting-started/multisite-experiments_en.html",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "",
    "text": "A multisite or block-randomized trial is a randomized experiment “in which sample members are randomly assigned to a program or a control group within each of a number of sites” (S. W. Raudenbush and Bloom (2015)).\nThis guide focuses on multisite educational trials for illustration, although multisite trials are not unique to education. Multisite trials are a subset of multilevel randomized controlled trials (RCTs), in which units are nested within hierarchical structures, such as students nested within schools nested within districts. This guide uses as an illustrative example the case where each site is a school, although they could also be districts or classrooms; thus the term “site” and “school” are used interchangeably.\nAn advantage of multisite trials is that they allow a researcher to study average impact across units or sites, while also getting a sense of heterogeneity across sites (S. W. Raudenbush and Bloom (2015)). However, the opportunities provided by multisite trials also come with their own challenges. Much of the rest of this guide will discuss the choices that researchers must make when analyzing multisite trials, and the consequences of these choices.\n\n\nBefore diving in, let’s introduce the definitions of estimand, estimator, and estimate. These concepts are sometimes conflated, but disentangling them increases clarity and understanding. The main distinction is that the estimand is the goal, while the estimator is the analysis we do in order to reach that goal.\nAn estimand is an unobserved quantity of interest about which the researcher wishes to learn. In this guide, the only type of estimand considered is the overall average treatment effect (ATE). Other options include focusing on treatment effect for only a subgroup, or calculating a different summary, such as an odds ratio. After choosing an estimand, the researcher chooses an estimator, which is a method used to calculate the final estimate which should tell the researcher something about the estimand. Finally, the researcher must also choose a standard error estimator if she wants to summarize how the estimates might vary if the research design or underlying data generating process were repeated.\nFirst, to provide context, let’s consider an example. The researcher decides their estimand will be the average treatment effect for the pool of subjects in the experiment. In this example, the researchers observe all of the subjects for whom they want to estimate an effect. As with any causal analysis, the researchers do not observe the control outcomes of the subjects assigned to the active treatment, or the treated outcomes of the subjects assigned to the control treatment. Thus, causal inference is sometimes referenced as a missing data problem, because it is impossible to observe both potential outcomes (the potential outcome given active treatment and the potential outcome given control treatment). See 10 Things to Know About Causal Inference and 10 Types of Treatment Effect You Should Know About for a discussion of other common estimands.\nGiven an estimand, the researchers choose their estimator to be the coefficient from an OLS regression of the observed outcome on site-specific fixed effects and the treatment indicator. To calculate standard errors, they use Huber-White robust standard errors. All these choices result in a point estimate (e.g. the program increased reading scores by \\(5\\) points) and a measure of uncertainty (e.g. a standard error of \\(2\\) points).\nWe’ll also need some notation. This guide follows the Neyman-Rubin potential outcomes notation (Splawa-Neyman, Dabrowska, and Speed (1923/1990), Imbens and Rubin (2015)). The observed outcomes are \\(Y_{ij}\\) for unit \\(i\\) in site \\(j\\). The potential outcomes are \\(Y_{ij}(1)\\), the outcome given active treatment, and \\(Y_{ij}(0)\\), the outcome given control treatment. The quantity \\(B_{ij}\\) is the unit-level intention-to-treat effect (ITT) \\(B_{ij} = Y_{ij}(1) - Y_{ij}(0)\\). If there is no noncompliance, the ITT is the ATE, as defined above. Then \\(B_j\\) is the average impact at site \\(j\\), \\(B_j = 1/N_j \\sum_{i = 1}^{N_j} B_{ij}\\) where \\(N_j\\) is the number of units at site \\(j\\). Finally, \\(N = \\sum_{j = 1}^{J} N_j\\).\nThis guide is structured around the choices an analyst must make concerning estimand and estimators, and the resulting consequences. The choice of estimand impacts the substantive conclusion that a researcher makes. The choice of estimator and standard error estimator results in different statistical properties, including a potential trade off between bias and variance. This guide summarizes material using the framework provided by Miratrix, Weiss, and Henderson (2021)."
  },
  {
    "objectID": "guides/getting-started/multisite-experiments_en.html#a-multisite-trial-is-fundamentally-a-blocked-or-stratified-rct.",
    "href": "guides/getting-started/multisite-experiments_en.html#a-multisite-trial-is-fundamentally-a-blocked-or-stratified-rct.",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "A multisite trial is fundamentally a blocked or stratified RCT.",
    "text": "A multisite trial is fundamentally a blocked or stratified RCT.\nA multisite trial is a blocked RCT with 2 levels: randomization occurs at the student level (level 1) within blocks defined by sites/schools (level 2). For example, in a study of a new online math tool for high school students, randomization occurs at the student level within blocks defined by sites/schools. Perhaps half of students at each school are assigned to the status quo / control treatment (no additional math practice), and half are assigned to the active treatment (an offer of additional math practice at home using an online tool).\nBecause of the direct correspondence between multisite trials and blocked experiments, statistical properties of blocked experiments also translate directly to multisite experiments. The main difference between a traditional blocked RCT and a multisite experiment is that in many blocked RCTs, the researcher is able to choose the blocks. For example, in a clinical trial, a researcher may decide to block based on gender or specific age categories. Blocking can help increase statistical power overall or ensure statistical power to assess effects within subgroups (such as those defined by time of entering the study, or defined by other important covariates that might predict the outcome) (Moore 2012; Moore and Moore 2013; Bowers 2011). Pashley and Miratrix (2021) makes the distinction between fixed blocks, where the number and covariate distribution of blocks is chosen by the researcher, and structural blocks, where natural groupings determine the number of blocks and their covariate distributions. Multisite experiments have structural blocks, such as districts, schools, or classrooms. The type of block can impact variance estimation, as shown in Pashley and Miratrix (2021) and Pashley and Miratrix (2022).\nThe EGAP Metaketa Projects are also multisite trials: the 5 to 7 countries that contain sites for each study are fixed and chosen in advance by the different research teams."
  },
  {
    "objectID": "guides/getting-started/multisite-experiments_en.html#a-multisite-trial-is-not-a-cluster-randomized-trial",
    "href": "guides/getting-started/multisite-experiments_en.html#a-multisite-trial-is-not-a-cluster-randomized-trial",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "A multisite trial is not a cluster-randomized trial",
    "text": "A multisite trial is not a cluster-randomized trial\nA different type of RCT is a cluster-randomized design, in which entire schools are assigned to either the active treatment or control treatment. This video explains the difference between cluster and block-randomized designs. In a multisite trial, treatment is assigned within a block to individual units. In a cluster-randomized trial, treatment is assigned to groups of units. Some designs combine cluster- and block-randomization.\nAnother design that is not a multisite or block-randomized trial is an experiment that takes place in only one school and assigns individual students to active treatment and control treatment. This type of study has only one site and thus differences between sites do not matter in this design."
  },
  {
    "objectID": "guides/getting-started/multisite-experiments_en.html#why-choose-a-multisite-or-block-randomized-trial-design",
    "href": "guides/getting-started/multisite-experiments_en.html#why-choose-a-multisite-or-block-randomized-trial-design",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "Why choose a multisite or block-randomized trial design?",
    "text": "Why choose a multisite or block-randomized trial design?\nIn most contexts, blocking reduces estimation error over an unblocked (completely randomized) experiment (Moore 2012; Gerber and Green 2012). Thus, blocked experiments generally offer higher statistical power than unblocked experiments. Blocking is most helpful in increasing precision and statistical power in the setting where there is variation in the outcome, and where the blocks are related to this variation.\nIn multisite trials as compared to block-randomized trials, the researcher typically cannot purposely construct blocks to reduce variation, because they are defined by pre-existing sites. However, the researcher can hope, and often expect, that sites naturally explain some between-site variation. For example, if some schools tend to have higher outcomes than others, then blocked randomization using the school as a block improves efficiency over complete randomization.\nRandomizing with purposefully created blocks or pre-existing sites also helps analysts learn about how treatment effects may vary across the sites or groups of people categorized into the blocks. If a new treatment should help the lowest performing students most, but in any given study most students are not the lowest performing, then researchers may prefer to create blocks of students within schools with the students divided by their previous performance. This blocking within site would allow comparisons of the treatment effects on the relatively rare lowest performing students with the treatment effects on the relatively rare highest performing students."
  },
  {
    "objectID": "guides/getting-started/multisite-experiments_en.html#why-not-block",
    "href": "guides/getting-started/multisite-experiments_en.html#why-not-block",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "Why not block?",
    "text": "Why not block?\nOften, in a multisite trial with treatment administered by site administrators (like principals of schools), an analyst has no choice but to randomize within site. In other studies, the construction and choice of blocking criteria is a choice. Pashley and Miratrix (2022) shows that blocking is generally beneficial, but also explores settings in which it may be harmful. Blocking does result in fewer degrees of freedom, but in practice this reduction is rarely an issue, unless an experiment is very small (Imai, King, and Stuart 2008). Any use of blocking requires that an analyst keep track of the blocks and also that an analyst reflect the blocks in subsequent analysis: in many circumstances estimating average treatment effects from a block-randomized experiment while ignoring the blocks will yield biased estimates of the underlying targeted estimands (see “The trouble with ‘controlling for blocks’” and “Estimating Average Treatment Effects in Block Randomized Experiments” for demonstrations of bias arising from different approaches to weighting by blocks)."
  },
  {
    "objectID": "guides/getting-started/multisite-experiments_en.html#common-linear-regression-models",
    "href": "guides/getting-started/multisite-experiments_en.html#common-linear-regression-models",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "Common linear regression models",
    "text": "Common linear regression models\nFixed effects with a constant treatment (FE)\nWith this model, the researcher assumes that there are site-specific fixed effects (intercepts), but a common overall ATE. The assumed model is \\(Y_{ij} = \\sum_{k = 1}^{J} \\alpha_k \\text{Site}_{k,ij} + \\beta T_{ij} + e_{ij}\\), where \\(\\text{Site}_{k,ij}\\) is an indicator for unit \\(ij\\) being in site \\(k\\) (out of \\(J\\) sites), \\(T_{ij}\\) is a treatment indicator, and \\(e_{ij}\\) is an \\(iid\\) error term. For more discussion, see S. W. Raudenbush and Bloom (2015).\nFixed effects with interactions (FE-inter)\nWith this model, the researcher assumes site-specific heterogeneous treatment effects, so in addition to fitting a separate fixed effect for the intercepts for each site, a separate treatment impact coefficient is found for each site. \\[Y_{ij} = \\sum_{k = 1}^{J} \\alpha_k \\text{Site}_{k,ij} +\n\\sum_{k = 1}^{J} \\beta_k \\text{Site}_{k,ij} T_{ij} + e_{ij}\\]\nGiven a series of site-specific treatment estimates \\(\\hat{\\beta}_j\\), these estimates are then averaged, with weights by either simple weighting (see Clark and Silverberg (2011)) or by site size."
  },
  {
    "objectID": "guides/getting-started/multisite-experiments_en.html#common-multilevel-models",
    "href": "guides/getting-started/multisite-experiments_en.html#common-multilevel-models",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "Common multilevel models",
    "text": "Common multilevel models\nOnce an analyst selects multilevel modeling, for site intercepts and site impacts they must decide: what is considered random, and what is considered fixed?\nFixed intercept, random treatment coefficient (FIRC)\nThis model is similar to the fixed effects models above, but assumes that the site impact \\(\\beta_j\\) is drawn from a shared distribution. The FIRC model was more recently designed to handle bias issues that arise when the proportion of units treated varies across sites.\n\\[\\begin{align*}\n\\text{Level 1}\\qquad & Y_{ij} = \\sum_{k = 1}^{J} \\alpha_k\n\\text{Site}_{k,ij} + \\beta_j T_{ij} + e_{ij}\\\\\n\\text{Level 2}\\qquad & \\beta_j = \\beta + b_j\n\\end{align*}\\] See S. W. Raudenbush and Bloom (2015) and Bloom and Porter (2017).\nRandom intercept, random treatment coefficient (RIRC)\nThis model is an older version of multilevel models, and assumes that both the site intercept and site impact are drawn from shared distributions. \\[\\begin{align*}\n\\text{Level 1}\\qquad & Y_{ij} = A_j + \\beta_j T_{ij} + e_{ij}\\\\\n\\text{Level 2}\\qquad & \\beta_j = \\beta + b_j\\\\\n& A_j = \\alpha + a_j\n\\end{align*}\\]\nRandom intercept, constant treatment coefficient (RICC)\nFinally, this model assumes that the site intercepts are drawn from a shared distribution, but the treatment impact is shared. \\[\\begin{align*}\n\\text{Level 1}\\qquad & Y_{ij} = A_j + \\beta T_{ij} + e_{ij}\\\\\n\\text{Level 2}\\qquad & A_j = \\alpha + a_j\\\\\n\\end{align*}\\] As noted previously, the multilevel framework generally naturally corresponds to the super population perspective. However, for RICC models, the site impacts are not assumed to be drawn from a super population; only the site intercepts are assumed to be random. Thus, when it comes to estimating treatment impacts, RICC models actually take a finite population perspective.\nThere are also weighted versions of both traditional regressions and multilevel models. For example, a fixed-effects model can weigh each person by their inverse chance of treatment to help increase precision. Weighted regression for traditional regression is discussed in Miratrix, Weiss, and Henderson (2021), and weighted regression for multilevel models is discussed in Raudenbush S. W. and Schwartz (2020)."
  },
  {
    "objectID": "guides/getting-started/multisite-experiments_en.html#design-based-estimators",
    "href": "guides/getting-started/multisite-experiments_en.html#design-based-estimators",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "Design-based estimators",
    "text": "Design-based estimators\nDesign-based estimators are the most straightforward, as they are composed of simple weighted combinations of means. First, the site-specific treatment impact estimates \\(\\hat{B_j}\\) are calculated by taking differences in means between the active treatment and control treatment groups for each site. Then, the overall estimate is a weighted combination of these estimates, weighted by either person or site weighting.\nThe design-based estimators are \\[\\begin{align*}\n\\hat{\\beta}_{DB-persons} &= \\sum_{j = 1}^{J} \\frac{N_j}{N} \\hat{B_j} \\\\\n\\hat{\\beta}_{DB-sites} &= \\sum_{j = 1}^{J} \\frac{1}{J} \\hat{B_j}.\n\\end{align*}\\] Design-based estimators are generally unbiased for their corresponding estimands (person-weighted or site-weighted). Unbiasedness does not hold for one super population model; see Pashley and Miratrix (2022) for more details."
  },
  {
    "objectID": "guides/getting-started/multisite-experiments_en.html#linear-regression-estimators",
    "href": "guides/getting-started/multisite-experiments_en.html#linear-regression-estimators",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "Linear regression estimators",
    "text": "Linear regression estimators\nConsider the FE model (fixed effects with a constant treatment). This regression model results in a precision-weighted estimate, in which each site impact is weighted by the estimated precision of estimating that site’s impact. The estimator is \\(\\hat{\\beta}_{FE} = \\sum_{j = 1}^{J} \\frac{N_j p_j (1 - p_j)}{Z} \\hat{B_j}\\), where \\(p_j\\) is the proportion treated at site \\(j\\). The quantity \\(Z\\) is a normalizing constant, so \\(Z\\) is defined as \\(\\sum_{j = 1}^{J} N_j p_j (1-p_j)\\) to ensure the weights sum to one. The weights are \\(N_j p_j (1 - p_j)\\), which is the inverse of \\(Var(\\hat{\\beta_j})\\), so the weights are related to the precision of the estimate for each site. This expression shows that sites with larger \\(N_j\\), or that have \\(p_j\\) closer to \\(0.5\\), have larger weights.\nThe FE estimator is not generally unbiased for either person-weighted or site-weighted estimands. If the impact size \\(B_j\\) is related to the weights (\\(N_j p_j (1 - p_j)\\)), then the estimator could be biased. For example, if sites that treat a higher proportion of treated units also experience a larger treatment impact, then \\(B_j\\) can be related to \\(p_j (1- p_j)\\). This setting is plausible for example if sites with more resources to intervene on more students also implement the intervention more effectively. If larger sites are more effective, then \\(B_j\\) can be related to \\(N_j p_j (1- p_j)\\).\nInstead, the FE estimator is unbiased for an estimand that weights the site impacts by \\(N_j p_j (1- p_j)\\). However, this estimand does not have a natural substantive interpretation. Although the FE estimator is generally biased for the estimands of interest, it may have increased precision and thus a lower mean squared error.\nIn contrast, the FE-inter model ends up with weights identical to the design-based estimators, depending on if the estimated site impacts are weighted equally or by size."
  },
  {
    "objectID": "guides/getting-started/multisite-experiments_en.html#multilevel-model-estimators",
    "href": "guides/getting-started/multisite-experiments_en.html#multilevel-model-estimators",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "Multilevel model estimators",
    "text": "Multilevel model estimators\nMultilevel models also result in precision weighting, but in these models the estimated precision also takes into account the assumed underlying variance in site impacts. For example, the FIRC model can be expressed roughly as: \\[\\hat{\\beta}_{ML-FIRC*} = \\sum_{j = 1}^{J} \\frac{1}{Z}\n\\left(\\frac{\\sigma^2}{N_j p_j ( 1 - p_j)} + \\tau^2\\right)^{-1} \\hat{B_j}\\], where \\(Z\\) is again a normalizing constant, \\(Z = \\sum_{j = 1}^{J} \\left(\\frac{\\sigma^2}{N_j p_j ( 1 - p_j)} + \\tau^2\\right)^{-1}\\). This equation assumes that the \\(b_j\\) have known variance \\(\\tau^2\\), and the \\(e_{ij}\\) have known variance \\(\\sigma^2\\). In general, we do not know these quantities, and instead must estimate them. However, we can see that the implied precision weights incorporate the additional uncertainty assumed in the value of \\(b_j\\).\nThe RIRC model imposes the same structure on the site impacts, and thus the weights are similar to the FIRC model. The RICC model assumes a constant treatment impact, and thus is essentially equivalent to the precision-weighted fixed effects with constant treatment model (FE) when it comes to estimating the site impacts.\nWe summarize the weights in the table below. The following table includes additional estimators that are not discussed in this guide; for more information about these additional estimators, see Miratrix, Weiss, and Henderson (2021).\n\n\n\n\n\n\n\n\nWeight name\nWeight\nEstimators\n\n\n\n\nUnbiased person-weighting\n\\(w_j \\propto N_j\\)\n\\(\\hat{\\beta}_{DB-FP-person}\\), \\(\\hat{\\beta}_{DB-SP-person}\\), \\(\\hat{\\beta}_{FE-weight-person}\\), \\(\\hat{\\beta}_{FE-inter-person}\\)\n\n\nFixed-effect precision-weighting\n\\(w_j \\propto N_j p_j (1 - p_j)\\)\n\\(\\hat{\\beta}_{FE}\\), \\(\\hat{\\beta}_{FE-HW}\\), \\(\\hat{\\beta}_{FE-CR}\\), \\(\\hat{\\beta}_{ML-RICC}\\) (approximately)\n\n\nRandom-effect precision-weighting\n\\(w_j \\propto \\left[\\hat{\\tau} + N_j p_j (1 - p_j)\\right]^{-1}\\)  (approximately)\n\\(\\hat{\\beta}_{ML-FIRC}\\), \\(\\hat{\\beta}_{ML-RIRC}\\)\n\n\nUnbiased site-weighting\n\\(w_j \\propto 1\\)\n\\(\\hat{\\beta}_{DB-FP-site}\\), \\(\\hat{\\beta}_{DB-SP-site}\\), \\(\\hat{\\beta}_{FE-weight-site}\\), \\(\\hat{\\beta}_{FE-inter-site}\\)"
  },
  {
    "objectID": "guides/getting-started/multisite-experiments_en.html#point-estimates",
    "href": "guides/getting-started/multisite-experiments_en.html#point-estimates",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "Point estimates",
    "text": "Point estimates\nFirst, they consider the impact of choices on point estimates. The authors ask, “to what extent can the choice of estimator of the overall average treatment effect result in a different impact estimate?” In general, the authors find that the choice of estimator can substantially impact the point estimates, although the degree of impact depends on the choice. The authors reach the following conclusions.\nPerson-weighted estimands can result in a different conclusion than site-weighted estimands.\nIn some trials, estimates resulting from person-weighted estimands differed substantially from estimates resulting from site-weighted estimands. These discrepancies could be due to a difference in the true underlying values of the estimands, but they could also be due to estimation error from the estimation procedure. Through empirical exploration, they found that the difference is likely due to the estimands themselves being different. They found that “the range of estimates across all estimators is rarely meaningfully larger than the range between the person- and site-weighted estimates alone.”\nFor person-weighted estimands, the choice of estimator generally does not matter.\nThe unbiased design-based estimator and the precision-weighted fixed effect estimate both target the person-weighted estimand. There was little difference in estimates between these estimators. Most likely, “this implies that the potential bias in the bias-precision trade off to the fixed effect estimators is negligible in practice.” Other authors have been able to create situations in which the bias-precision trade off is more severe.\nFor site-weighted estimands, the choice of estimator can matter.\nFIRC estimates did differ from the unbiased design-based site estimator. FIRC can be seen as an adaptive estimator: when there is little estimated variation in impacts between sites, it tends to be more similar to the person-weighted estimate instead of the site-weighted estimate.\nDifferent estimators have different bias-variance trade offs.\nFinally, the authors consider the empirical bias-variance trade off of different estimators, and find:\n\nFE estimators have little bias, but also do not improve precision much over design-based estimators.\nFIRC tends to have lower mean squared error than design-based estimators.\nLarger site impact heterogeneity results in more biased estimates for FIRC.\nEven with more site impact heterogeneity, the mean squared error for FIRC estimators is still generally lower.\nCoverage for design-based estimators is more reliable, especially when site size is variable and site size is correlated with impact."
  },
  {
    "objectID": "guides/getting-started/multisite-experiments_en.html#standard-errors",
    "href": "guides/getting-started/multisite-experiments_en.html#standard-errors",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "Standard errors",
    "text": "Standard errors\nThe second question concerns the choice of standard error estimators. The authors ask, “to what extent can the choice of estimator of the standard error of the overall average treatment effect result in a different estimated standard error?”\nThe choice of standard error estimator can substantially impact the estimated standard error. The authors reach the following conclusions.\nThe choice of estimand impacts the standard error.\nSuper population estimators generally have larger standard errors than finite population estimators. Site-weighted estimators generally have larger standard errors than person-weighted estimators.\nGiven a particular estimand, the choice of estimator matters in some contexts and not others.\nFor finite population estimands (including both person and site-weighted estimands) or super population person-weighted estimands, the choice of standard error estimator generally does not matter. In practice, Miratrix, Weiss, and Henderson (2021) found that estimators that attempt to improve precision by trading bias may not actually result in gains in precision in practice. The use of robust standard errors also does not differ much from non-robust standard errors in practice.\nFor super population site-weighted estimands, the choice of standard error estimator can matter a lot. In most cases, standard error estimates differed substantially between the design-based super population estimator and FIRC. The authors further conclude that for super population site-weighted estimands, the wide-ranging standard error estimates stem from instability in estimation. Through a simulation study, they find that super population standard errors can underestimate the true error. The design-based super population standard error estimator is particularly prone to underestimate the standard error compared to multilevel models, and can be unstable, in that it estimates a wide range of different values across simulations."
  },
  {
    "objectID": "guides/getting-started/randomization_en.html",
    "href": "guides/getting-started/randomization_en.html",
    "title": "10 Things You Need to Know About Randomization",
    "section": "",
    "text": "Abstract\nThis guide will help you design and execute different types of randomization in your experiments. We focus on the big ideas and provide examples and tools that you can use in R. For why to do randomization see this methods guide.\n\n\n1 Some ways are better than others\nThere are many ways to randomize. The simplest is to flip a coin each time you want to determine whether a given subject gets treatment or not. This ensures that each subject has a .5 probability of receiving the treatment and a .5 probability of not receiving it. Done this way, whether one subject receives the treatment in no way affects whether the next subject receives the treatment, every subject has an equal chance of getting the treatment, and the treatment will be independent from all confounding factors — at least in expectation.\nThis is not a bad approach but it has shortcomings. First, using this method, you cannot know in advance how many units will be in treatment and how many in control. If you want to know this, you need some way to do selections so that the different draws are not statistically independent from each other (like drawing names from a hat). Second, you may want to assert control over the exact share of units assigned to treatment and control. That’s hard to do with a coin. Third, you might want to be able to replicate your randomization to show that there was no funny business. That’s hard to do with coins and hats. Finally, as we show below, there are all sorts of ways to do randomization to improve power and ensure balance in various ways that are very hard to achieve using coins and hats.\nFortunately though, flexible replicable randomization is very easy to do with freely available software. The following simple R code can, for example, be used to generate a random assignment, specifying the number of units to be treated. Here, N (100) is the number of units you have and m (34) is the number you want to treat. The “seed” makes it possible to replicate the same draw each time you run the code (or you can change the seed for a different draw).1 2\n\nlibrary(randomizr)\nset.seed(343)\ncomplete_ra(100, 34)\n\n\n\n2 Block randomization: You can ensure that treatment and control groups are balanced\nIt is possible, when randomizing, to specify the balance of particular factors you care about between treatment and control groups, even though it is not possible to specify which particular units are selected for either group.\nFor example, you can specify that treatment and control groups contain equal ratios of men to women. In doing so, you avoid any randomization that might produce a distinctly male treatment group and a distinctly female control group, or vice-versa.\nWhy is this desirable? Not because our estimate of the average treatment effect would otherwise be biased, but because it could be really noisy. Suppose that a random assignment happened to generate a very male treatment group and a very female control group. We would observe a correlation between gender and treatment status. If we were to estimate a treatment effect, that treatment effect would still be unbiased because gender did not cause treatment status. However, it would be more difficult to reject the null hypothesis that it was not our treatment but gender that was producing the effect. In short, the imbalance produces a noisy estimate, which makes it more difficult for us to be confident in our estimates.\nBlock (sometimes called stratified) randomization helps us to rig our experiment so that our treatment and control groups are balanced along important dimensions but are still randomly assigned. Essentially, this type of randomization design constructs multiple mini-experiments: for example, it might take women and randomly assign half to treatment and half to control, and then it would assign half of men to treatment and half to control. This guarantees a gender balance when treatment and control groups are pooled.\nAnother advantage of block randomization is that it ensures that we will be able to estimate treatment effects for subgroups of interest. For example, imagine that we are interested in estimating the effect of the treatment among women. If we do not block on gender, we may, by chance, end up with a random assignment that puts only few women into the treatment group. Our estimate of the treatment effect among women would then be very noisy. However, if we assign treatment separately among women and among men, we can ensure that we will have enough women in, respectively, the treatment and control group to obtain a precise estimate among this subgroup.\nThe blockTools package is a useful package for conducting block randomization. Let’s start by generating a fake data set for 60 subjects, 36 of whom are male and 24 of whom are female.\nSuppose we would like to block on gender. Based on our data, blockTools will generate the smallest possible blocks, each a grouping of two units with the same gender, one of which will be assigned to treatment, and one to control.\n\nrm(list = ls())\nlibrary(blockTools)\nlibrary(dplyr)\nlibrary(randomizr)\ndat <-\n  tibble(\n    id = seq(1:60),\n    female = c(rep(0, 36), rep(1, 24)),\n    age = sample(18:65, size = 60, replace = TRUE)\n  )\n# one covariate block\ndat <-\n  dat %>%\n  mutate(Z_block_1 = block_ra(female))\nwith(dat, table(female, Z_block_1))\n# matched quartets\nout <- block(dat, n.tr = 4, id.vars = \"id\", \n             block.vars = c(\"female\", \"age\"))\ndat <-\n  dat %>%\n  mutate(\n    block_id = createBlockIDs(out, dat, id.var = \"id\"),\n    Z_block_2 = block_ra(block_id))\nwith(dat, table(Z_block_2, block_id))\n\nYou can check the mean of the variable on which you blocked for treatment and control to see that treatment and control groups are in fact perfectly balanced on gender.\n\n\n3 Factorial designs: You can randomize multiple treatments at the same time without using up power\nSuppose there are multiple components of a treatment that you want to test. For example, you may want to evaluate the impact of a microfinance program. Two specific treatments might be lending money to women and providing them with training. A factorial design looks at all possible combinations of these treatments: (1) Loans, (2) Training, (3) Loans + Training, and (4) Control. Subjects are then randomly assigned to one of these four conditions.\n\nFactorial designs are especially useful when evaluating interventions that include a package of treatments. As in the example above, many development interventions come with several arms, and it is sometimes difficult to tell which arms are producing the observed effect. A factorial design separates out these different treatments and also allows us to see the interaction between them.\nThe following code shows you how to randomize for a factorial design.\n\ndat <-\n  tibble(\n    Z_loan = complete_ra(80, 40),\n    Z_training = block_ra(blocks = Z_loan)\n  )\nwith(dat, table(Z_loan, Z_training))\n\n\n\n4 You can randomize whole clusters together (but the bigger your clusters, the weaker tends to be your power)\nSometimes it is impossible to randomize at the level of the individual. For example, a radio appeal to get individuals to a polling station must inherently be broadcast to a whole media market; it is impossible to broadcast just to some individuals but not others. Whether it is by necessity or by choice, sometimes you will randomize clusters instead of individuals.\nThe disadvantage of cluster randomization is that it reduces your power, since the number of randomly assigned units now reflects the number of clusters and not simply your total number of subjects. If your sample consisted of two clusters of 1,000 individuals each, the functional number of units might be closer to 2, not 2,000. For this reason, it is preferable to make clusters as small as possible.\nThe degree to which clustering reduces your power depends on the extent to which units in the same cluster resemble each other. It is desirable to have heterogeneity within your clusters so that they are as representative as possible of your broader population. If the individuals within clusters are very similar to each other, they may have similar potential outcomes, which means that groups of individuals with similar potential outcomes will all be assigned to treatment or control together. If a cluster has particularly high or low potential outcomes, this assignment procedure will increase the overall correlation between potential outcomes and treatment assignment. As a result, your estimates become more variable. In brief, if your clusters are more representative of the broader population, your estimates of the average treatment effect will be more precise. See our guide on cluster random assignment for more details.\nA frequently asked question is how cluster randomization differs from block randomization. Block randomization is conducted in order to achieve balance based on pre-treatment covariates. For example, an education intervention might block randomize on the previous year’s test scores in order to track the progress of both low- and high-performing students. Cluster randomization is when multiple units are treated as a group–they all receive treatment or control status together. For example, the same education intervention might randomize at the level of the classroom, so the classrooms constitute the clusters. It is possible to block and cluster randomize simultaneously. In our example, you might calculate the average test score for each classroom and block randomize based on the classroom’s average score.\nThe following graphic demonstrates what your data might look like in the cases of block, cluster, and block + cluster randomization, relative to a simple case of randomization with no blocking or clustering. The example imagines that we conduct an experiment in four schools, where each school comprises four classrooms with four students in each classroom. The top left panel represents simple random assignment. Note that the number of students assigned to treatment (tiles shaded in blue) varies across schools. For example, in school 1, 9 students are assigned to treatment but in school 4, the treatment group comprises only 6 students. The top right panel represents a random assignment procedure that uses schools as blocks. This procedure ensures that exactly eight students are assigned to treatment in each school. The bottom left panel shows cluster random assignment. The idea is that the four students in each quadrant of a school are in the same classroom, and we assign entire classrooms to treatment. Note again that, because there is no blocking involved, the number of classrooms assigned to treatment varies across schools. For example, school 1 assigns three classrooms to treatment while school 3 assigns all classrooms to control. Finally, the bottom right panel corresponds to assignment that is both blocked by school and clustered by classroom. Blocking ensures that exactly two classrooms per school are assigned to treatment.\n\n\n\n\n\nIllustration of patterns of assignments you might see under different types of blocked and clustered designs. (Colors indicate treatment assignment)\n\n\n\n\n\n\n5 You can randomize in a way that makes it easier to see if there are spillovers\nWhen designing your experiment, think critically about whether “spillovers” pose a threat to your ability to identify the causal effect of your treatment. Spillovers arise if one units outcome is affected by the treatment status of another unit. This can be tricky if units have the ability to interact with each other: one member of a village may learn of another villager’s receipt of a cash grant and may change their behavior accordingly.\nOne way to make spillovers more evident is to use double randomization. You would first randomly assign some clusters to treatment and others to control, and within clusters, you would assign some individuals to treatment and others to control. Comparing control individuals in your treatment cluster to individuals in your control cluster will enable you to assess the role of spillovers in your experiment.\n\n\n6 Different units can be assigned to treatment with different probabilities\nSometimes people think that “random” means that two events are equally likely, but in fact, random assignment is “random” so long as the probability of assignment to treatment is strictly between 0 and 1. If a subject has a 0 or a 100 percent chance of being assigned to treatment, that subject should be excluded from your experimental analysis because there is no randomization occurring. However, all subjects with a probability of assignment to treatment strictly between 0 and 1 may be included, even if their probabilities differ, so long as their probabilities are known.\nWhy might you want to assign different probabilities of assignment to treatment? Suppose you are working with an implementing partner to randomize the allocation of election observers in order to measure the effect on electoral fraud. Your implementing partner can afford to send only a few election observers to a rural part of the country. You could address this constraint by blocking on geographic area and assigning a higher probability of assignment to treatment to more proximate villages to which it is less costly to travel. So long as the probability of assignment to treatment for more accessible villages is less than 1, the probability of assignment to treatment for less accessible villages is greater than zero, and these probabilities are known, it is possible to estimate the effect of the treatment.\nWhen subjects have differing probabilities of assignment to treatment, however, you can no longer simply merge all subjects in the analysis of your data. If you do, then treatment assignment will be correlated with background characteristics on which you blocked. There are two ways of handling this.\nThe first way is to estimate the average treatment effect block by block and then to average the treatment effects, each weighted by the size of the block relative to the entire sample.\nThe second way is inverse probability weighting (IPW). In IPW, weights are defined as the 1/p for treated units and 1/(1-p) for control units, where p refers to the probability of assignment to treatment. This method allows you to run a weighted regression of your outcome on treatment assignment.\n\nN <- 100000\ndat <- tibble(Y0 = 1:N,\n              Y1 = Y0 + 10000)\ndat <-\n  dat %>%\n  mutate(\n    p = seq(0.25, 0.75, length.out = N),\n    Z = simple_ra(N, prob_unit = p),\n    Y = Z * Y1 + (1 - Z) * Y0)\n  \ndat %>%\n  summarise(\n    naive_estimate = mean(Y[Z == 1]) - mean(Y[Z == 0]),\n    ipw_estimate = weighted.mean(Y[Z == 1], 1 / p[Z == 1]) -\n      weighted.mean(Y[Z == 0], 1 / (1 - p[Z == 0]))\n  )\n\n\n\n7 Restricted randomization: If you don’t like what you get you can start over\nSometimes you might want to make sure that randomization does not produce particular types of pattern (for example, too many people who know each other all being in treatment). But the patterns you care about might be too hard to set up in advance. What you can do is take a random draw and then check whether the draw meets the criteria you care about. If it doesn’t, then draw again. Be warned, though, that if you do this, you create a couple of complications: (1) each unit will not necessarily be assigned to treatment with the same probability, (2) units may not be independently assigned to treatment. You need to take into account both of these facts in your analysis. You can do so by generating inverse probability weights as we did in point 6. Here, you will need to use the same restricted randomization code that you used to assign treatment to figure out how likely it is that each subject is assigned to treatment under these restrictions. You simply run the code a large number of times and calculate the proportion of times that a given unit is assigned to treatment across all repetitions. Next, you use the distribution of possible treatment assignments to implement randomization inference. These analyses are complex so proceed with caution.\n\n\n8 Write randomization code that lets you simulate many possible randomizations\nA benefit of using R code to randomize is that you can perform thousands of possible randomizations in seconds. Why is this beneficial?\n\nIt can be useful as a way to check whether your randomization code worked. For example, if one or more subjects in your experiment never received treatment over 10,000 possible random assignments, then you would suspect a flaw in your randomization code.\nYou can use re-randomization to calculate the exact probability of assignment to treatment for each individual in your experiment. This is especially helpful if your randomization code is more complex. Perhaps you employ both block and cluster randomization or a restricted randomization procedure, resulting probabilities of assignment to treatment that vary greatly across individuals in a large experiment. These probabilities would be difficult to calculate by hand, but an easy solution is to run your original randomization code many times and generate a variable representing each individual’s proportion of times they were assigned to treatment: this represents his or her individual probability of assignment to treatment. The inverse of this variable can then be used in a weighted regression when estimating the average treatment effect.\nSimulating possible randomizations is a design-based approach to calculating statistical significance. This approach, called randomization inference, generates an exact \\(p\\)-value by calculating possible average treatment effects that would be observed under hypothetical random assignments if in fact the treatment had no effect. The \\(p\\)-value is then the proportion of the estimated treatment effects that is at least as large in magnitude as the one that your experiment observed. Randomization inference avoids making distributional assumptions and instead uses the distribution of data observed in your experiment. This approach is preferable to standard calculations of statistical significance when the sampling distribution is not normal – a problem that is more likely to arise when your experimental sample is small and when your outcomes do not follow a normal distribution. For more information on randomization inference, including sample code, visit the 10 Things to Know About Randomization Inference Methods Guide.\n\n\n\n9 You can randomize as you go along\nIn many experiments, you may not know the entirety of your sample at the beginning of the experiment; some subjects may join over time. This presents a complication when we want to use a simple blocking algorithm because the addition of subjects to our pool may change the composition of our blocks and therefore their probabilities of assignment to treatment.\nTo maintain the ability to block and therefore the ability to assert control over the balance between treatment and control groups, you can use covariates to calculate a new subject’s similarity to other previously assigned subjects and assign the new subject to the treatment condition with fewer similar units.3 4\n\n\n10 Randomization can sometimes be an ethical way of assigning a treatment, but sometimes it isn’t\nRandomization is the key ingredient for isolating the causal effect of a treatment from a research design perspective, but it is also important to consider the ethical implications of randomization as well. When we think about the long-term effects of an experiment, randomization enables us to test which programs are most effective so that resources can be directed to programs that make the most difference in the lives of future populations. In the short term, randomizing access to a program (as opposed to distributing based on arbitrary characteristics) can be a particularly ethical way of distributing scarce goods that cannot be extended to everyone.\nHowever, sometimes, it is the neediest populations that need to be served by an intervention in an experiment. A randomized design that treats equal numbers of low-income and high-income participants with loans is letting resources flow to less rather than more needy individuals. If we believe there are beneficial effects of the loan, then this raises concerns about the ethics of allocating resources away from the neediest.5 One would need a strong case for social benefits of the research and would also seek designs that provide benefits ultimately to control groups.\nA wait list randomization design is one way of treating an entire subject pool while enabling the researcher to test the effectiveness of the treatment experimentally. In this design, the program could roll out the intervention in phases and randomly assign the units to the phase in which they will be treated. For example, if a program wanted to treat 90 villages in total, it could treat 30 villages each year, and measure outcomes at the end of each year. If you wanted to compare outcomes in treatment and control villages, you would compare the 30 treated villages to the 60 yet-untreated villages at the end of the first year. At the end of the second year, you could compare the 30 villages that were treated in the previous year to the 30 villages that are yet-untreated. Essentially, this creates two experiments, identical but for the year’s time that separates them. In the table below, you can see that in the first year, we could compare the dark blue treatment group to the two light blue control groups. In the second year, we could compare the dark red treatment group to the light red treatment group, but we would want to avoid pooling the two treatment groups because one has been treated for longer than the other. You can see that after the third year, no more comparisons may be made because all units have been treated.\n\nThe only requirement is that a subject’s assignment to treatment in a particular phase is randomly assigned and unrelated to their potential outcomes of interest. A design in which more eager participants received treatment earlier would violate this assumption and would not yield an unbiased estimate of the treatment effect, as unobserved factors that predispose them to seeking out treatment may be influencing their schedule of potential outcomes. The wait list design is an example of a creative randomization design that could address ethical concerns about limiting the distribution of a valuable treatment.\nEthics are often highly intertwined with randomized designs, especially in social science and medical research. As a researcher, you should carefully consider the possible implications of randomizing any given treatment. You will also need to solicit approval for your research design from your research institution’s Institutional Review Board (IRB).\n\n\n\n\n\nFootnotes\n\n\nRandom number generators are actually pseudo-random because they generate a vector of random numbers based on a small set of initial values, known as a seed state. Random number generators operate this way in order to improve computational speed. However, the series of random numbers generated is as random as you need to it to be for the purposes of random assignment because it is wholly unrelated to the potential outcomes of your subjects.↩︎\nAll code chunks updated by Alex Coppock on 25 November, 2020.↩︎\nFor more, see Moore, Ryan T., and Sally A. Moore. “Blocking for sequential political experiments.” Political Analysis 21.4 (2013): 507-523.↩︎\nFor a more detalied walkthrough on the randomization procedures available in the R package randomizr, see: https://declaredesign.org/r/randomizr/articles/randomizr_vignette.html↩︎\nBut if we are certain about the loan’s effects, then it’s also unclear why we are running an experiment to test it. In medical research, randomized controlled trials often stop if it becomes clear early on that a drug is undoubtedly curing life-threatening diseases, and therefore withholding it from control subjects is dangerous. (Similarly, a trial would also stop if it were clear early on that a drug is undoubtedly causing negative and harmful effects.)↩︎"
  },
  {
    "objectID": "guides/getting-started/survey-design_en.html",
    "href": "guides/getting-started/survey-design_en.html",
    "title": "10 Things to Know About Survey Design",
    "section": "",
    "text": "Surveys are the most frequently-used tool for collecting experimental data in social science research, and the design of these surveys can have a profound effect on the conclusions we draw about the treatments we study. Therefore, the stakes are high when designing surveys around your experimental projects.\nAt a minimum, all that is needed to estimate a treatment effect after an experiment has been conducted is a measure of the outcome, collected after the treatment has been delivered, with a sufficient number of observations across the treatment and control groups. If time and budget allow, baseline surveys, conducted before the implementation of an experiment, serve important functions as well and can improve analysis greatly.\n\n\nBaseline surveys should produce data that describe the experimental subject population as they were before the treatment is delivered. This is accomplished through the measurement of covariates, which are observed pre-treatment characteristics of experimental subjects. Using covariate data you can: 1) describe the subject population, 2) improve the precision with which you estimate treatment effects, 3) report balance, and 4) estimate heterogeneous treatment effects.\n\n\n\nCovariates improve the precision with which you can estimate treatment effects by reducing variance in three ways; covariates can be used to rescale your dependent variable, as controls when using regression to estimate treatment effects, and to construct blocks in order to conduct blocked random assignment.1 In order for covariate data to be used to reduce variance in our estimates of treatment effects, they need to be unaffected by treatment assignment, i.e. collected sometime before treatment is delivered. See the guide on covariate adjustment for more about how to use covariate data.\nThe greater the predictive power of included covariates, the greater increase in the power of your design and the precision with which you can estimate effects. If you believe covariates will likely predict outcomes in your experiment, then that is grounds to include them in your survey. For example, if the intervention involves providing a service at a cost to treated users, income will likely explain some variation in outcomes and is therefore a useful covariate to measure at the baseline stage.\nBecause pre-treatment covariates can improve precision, conducting a baseline becomes more important when the sample size is limited.\nCovariates also allow you to conduct sub-group analyses. Heterogeneous effects are not causal, and so interpretation is limited. Still, understanding how treatment effects vary by subejcts’ attributes can provide you with important clues about mechanisms. The implication for design is to include covariates for which you would like to report heterogeneous effects. Covariate data will also be used to show “balance”, or the extent to which the treatment and control populations resemble each other. Although random assignment alone ensures that outcomes across the treatment and control group are in expectation the same, it is standard practice to show that random assignment resulted in two groups that are “balanced” on covariates of interest. If, for example, the treatment group included 25% more men than the control group, we might worry that random assignment failed in some way. Collecting pre-treatment covariate data allows us to evaluate and report balance.\n\n\n\nThe baseline provides an opportunity to measure the outcome before the experiment was conducted, later allowing you to use change scores as your outcome and the difference-in-differences estimator. The difference-in-differences estimator will improve precision only when a covariate strongly predicts outcomes.2\n\n\n\nEndline surveys, conducted after the treatment is delivered, are primarily used to measure outcomes. Including questions about implementation can improve analysis and interpretation greatly.\nSurveys conducted after treatments are delivered are one way to understand if there were compliance issues or other implementation issues that may have consequences for analysis. Survey data can help to determine the scope of non-compliance with the assigned treatment, and the underlying causes. This is an opportunity to ask subjects directly about reasons for noncompliance. Subjects may have understood the treatment differently than the researchers, and survey data can be used to both show this and speculate why this may have happened.\nIn the endline, you can learn about spillover by asking subjects in the control condition about their knowledge and access to the treatment. Interviews with treated subjects are useful for understanding spillover as well, because survey data can be used to understand the networks through which the treatment could have “spilled-over” into the control group.\nDescribing the population is important here as well, but covariate data collected after implementation are less useful for improving precision. Ordinarily, covariates collected after treatment assignment are considered suspect, as they could conceivably be affected by treatment.\n\n\n\n\n\n\n\nBaseline checklist\nEndline checklist\n\n\n\n\nWill your data allow you to:\nWill your data allow you to:\n\n\n• Describe the population\n• Estimate effects\n\n\n• Adjust treatment effect estimates (are the covariates included likely to be prognostic of outcomes?)\n• Assess if spillover occurred, or if there was interference\n\n\n• Estimate heterogeneous treatment effects\n• Measure non-compliance (and the reasons for non-compliance if it occurred)\n\n\n• Design a blocked randomization procedure\n• Look for causal mechanisms (how are effects transmitted?)\n\n\n• Describe balance across treatment and control conditions"
  },
  {
    "objectID": "guides/getting-started/survey-design_en.html#gathering-behavioral-data-doesnt-have-to-be-expensive.-here-is-how-to-develop-low-cost-measures",
    "href": "guides/getting-started/survey-design_en.html#gathering-behavioral-data-doesnt-have-to-be-expensive.-here-is-how-to-develop-low-cost-measures",
    "title": "10 Things to Know About Survey Design",
    "section": "Gathering behavioral data doesn’t have to be expensive. Here is how to develop low-cost measures:",
    "text": "Gathering behavioral data doesn’t have to be expensive. Here is how to develop low-cost measures:\n\nBrainstorm a set of actions that subjects would do if the treatment had had an effect or would not do in the case that the treatment did not have an effect. It also works to think about behavior on a continuum—i.e., what would people be more likely to do if affected, and less likely to do if not? Local context matters a lot here; rely on your enumerators, local survey staff, or implementation partners to help you think through a set of possibilities. One nice way to think about this is to challenge yourself to think about “hints.” In the above example, we might think about a group of opposition activists wearing wristbands publicly as a “hint” that people are more likely to take risks. Keep in mind your eventual audience: what behaviors are frequently tracked in the literature you hope to speak to?\nIsolate the set of behaviors that are feasible to measure. This will most likely be the set of behaviors that can be immediately observed by the enumerator and involve minimal materials. What is the least expensive or costly action that would be associated with the behavioral change you want to detect?\nIdeally, pre-test the measures either with the rest of your survey, or in smaller focus groups. Learning why respondents did or did not behave a certain way will increase confidence in your results."
  },
  {
    "objectID": "guides/getting-started/survey-design_en.html#how-do-you-construct-questions-that-accomplish-these-goals",
    "href": "guides/getting-started/survey-design_en.html#how-do-you-construct-questions-that-accomplish-these-goals",
    "title": "10 Things to Know About Survey Design",
    "section": "How do you construct questions that accomplish these goals?",
    "text": "How do you construct questions that accomplish these goals?\n\nUse the simplest possible form of each question, using the most widely-understood words. Avoid jargon or technical terms, and be straightforward and brief.\nBe specific, such that if the question were to be lifted from the section and asked without context you would get the same response.\nIt helps to begin the question by providing a context. For example, you can prime a time period (“Thinking of the last year: has your income been better, the same, or worse?”), or a place (“Thinking of people in this village: have people earned more or less this year as compared to last year?”)\nAvoid measuring multiple things at once. For example, the following question measures attitudes about both the president and government concurrently, making it difficult to draw a clear conclusion from the data: “Do you think the president and the government are doing a good job in terms of protecting basic freedoms?”\nWhen constructing response categories, be as comprehensive as possible. Include all possible responses. You don’t want to record a lot of “don’t know” responses and miss important information. See below for a discussion of scales.\nKeep in mind the concerns with social desirability discussed above; the wording of the question shouldn’t lead the respondent towards a certain response."
  },
  {
    "objectID": "guides/getting-started/pap_en.html",
    "href": "guides/getting-started/pap_en.html",
    "title": "10 Things to Know About Pre-Analysis Plans",
    "section": "",
    "text": "Abstract\nThis guide introduces the idea of a pre-analysis plan (PAP), offers a model and guiding questions for writing pre-analysis plans for your studies, and explains the uses of a pre-analysis plan. Links to PAP registries with example plans are provided at the end of this document.\n\n\n1 What is a Pre-Analysis Plan (PAP)\nA PAP is a document that formalizes and declares the design and analysis plan for your study. It is written before the analysis is conducted and is generally registered on a third-party website.1\nThe objectives of the PAP are to improve research design choices, increase research transparency, and allow other scholars to replicate your analysis. As a result, we recommend focusing the PAP on analytic details that will help you analyze your study and allow other researchers to replicate your analysis. A brief section on theory should be included insofar as it helps articulate hypotheses, but a detailed theory and literature review need not be included. The PAP does not need to include the front-end of an academic paper if these sections do not help you think about your analysis or help readers replicate your analysis.\nIn the following sections, we provide guidelines for the details you should include in PAPs, including example text. We also recommend that you include as much code and analysis of simulated data as possible.2 Many PAPs will not be able to include everything on our list, but a PAP should, at a minimum, include the full list of hypotheses that you intend to test, how you will measure variables relevant to those hypotheses, and a verifiable time stamp.\n\n\n2 What is in a PAP? Study Design Overview\nThe first section of a PAP should provide a brief overview of your study design. If this study is an experiment, describe the randomization procedure and the intervention or experimental procedure. If the study is not an experiment, describe the data. These descriptions should include: (1) unit of analysis, population, and inclusion/exclusion criteria, (2) method (observational, experimental, quasi-experimental), (3) experimental intervention or explanatory variable, and (4) outcomes of interest.\n\nExample: Let us use a simplified version of an intervention in Malawi as a running example.  We use a block-randomized experiment to evaluate the effect of information and public service provision (explanatory variable) on tax compliance (outcome) in Malawi, where service delivery is low and tax noncompliance is high. Our unit of analysis is the owner-occupied household in a city in Malawi. We exclude renters because only homeowners pay the city taxes relevant to our study. We cluster households within 80 neighborhood-level units of approximately 20 households each. Each neighborhood is also a block such that 10 households within each neighborhood are assigned to treatment and 10 households are assigned to control. Our intervention is the provision of information and public services. We implement this intervention by providing two free waste pickups, a visit from a canvasser to discuss how tax payments fund city services like waste collection, and a pamphlet with more information about tax payments.  Our main outcome is tax compliance, which we study as city tax payments. We conduct a survey and collect administrative data on tax payment both before and after the intervention.\n\n\n\n3 What is in a PAP? Hypotheses\nThe PAP should specify your hypotheses – the relationship(s) you expect to observe between variables. The formulation of a hypothesis should make clear whether it involves one- or two-tailed tests (i.e. predict an increase, decrease, or change in the outcome variable).\nThere are two types of hypotheses to consider including in your PAP: confirmatory and exploratory. Confirmatory hypotheses are the main focus of most studies; these are the hypotheses your study is designed to test. Your analyses for these hypotheses will typically be well-powered and you will generally have a strong theory leading to these hypotheses a priori.\nExploratory hypotheses are hypotheses you may wish to test but are not the main focus of your study. They are often secondary hypotheses about mechanisms, subgroups, heterogeneous effects, or downstream outcomes. The analyses guided by these hypotheses may not be well-powered and your theory may not focus on these effects, but analysis of these hypotheses may lead to surprising discoveries.\nSome people prefer to list few hypotheses and others prefer to list many. As a rule, you should include as many hypotheses as relate to your theory or intervention.3 This may be a single outcome, but if your experimental intervention or theory makes predictions about 8 outcomes, list hypotheses for those 8 outcomes. If your experimental intervention or theory postulates specific mechanisms through which the explanatory variable affects an outcome or outcomes, those mechanisms should be clear in the hypothesis.\nNote that with more than one hypothesis you will need to specify a procedure for handling multiple hypotheses in the inference criteria section of your PAP, either by correcting for multiple tests or by aggregating all hypotheses into an index or an omnibus test.4 If using an omnibus test, you could list all outcomes under one hypothesis. See our section about inference criteria for more about correcting for multiple tests.\n\nExample: Hypotheses We specify our confirmatory hypothesis as:\n\n\nH: The treatment will increase tax compliance among treatment households vs. control households.\n\n\n In addition to overall improved attitudes toward government, we expect that residents may shift their attitudes about specific issues. We would like to explore which issues may be driving the overall attitudinal changes with the following exploratory hypotheses:\n\n\nH: The treatment will increase beliefs that the city deserves to collect taxes among treatment households vs. control households.\n\n\nH: The treatment will increase beliefs in city service capacity among treatment households vs. control households.\n\n\nH: The treatment will increase beliefs in city enforcement capacity among treatment households vs. control households.\n\n\n\n\n\n4 What is in a PAP? Measures and Index Construction\nThe PAP should specify the way you measure or operationalize variables of interest, including outcomes, covariates, and explanatory variables. These operationalizations can be included as its own section, or the operationalization of variables can be included after the hypothesis for which each variable is relevant.\nFor each variable, the PAP should list the way the variable is measured (such as survey or interview questions, administrative data, behavioral/observational measures, etc.) and to which hypothesis or hypotheses the variable relates. Details of these measures, such as precise wording for survey questions, should be included either in this section or in an appendix. If you are using indices or factors, or combining outcomes together in other ways, specify how the combined outcomes will be constructed. If you are manipulating or transforming the outcomes in some way (such as logging a variable), describe the manipulation or transformation process.\nWe recommend including code in your PAP to show how you plan to execute all data transformation.\n\nExample: Measures and Index Construction We measure our primary outcome, tax compliance, using administrative data on citizen tax payments. The tax compliance measure takes the value \\(0\\) if the household did not pay taxes and \\(1\\) if the household did pay taxes. Our explanatory variable is assignment to treatment, where individuals who are assigned to treatment are assigned \\(1\\) and individuals assigned to control are assigned \\(0\\). We also create a “government attitudes” index using inverse-covariance weights (ICW) of 6 survey questions where higher values mean more positive attitudes towards government. The ICW index weighs the baseline questions by the covariance of the responses in the control group at baseline and weighs the endline questions by the covariance of the responses in the control group at endline. We then standardize the ICW scale by standard deviation so that a 1 at baseline means 1 SD above the mean at baseline and a 1 at endline means a 1 SD above the mean at endline.\n\n\n\n5 What is in a PAP? Estimation Procedure\nNow that you have described your study design, hypotheses, and variables, you are ready to discuss your testing and estimation procedures.\nThis section should clearly specify what you are estimating (i.e. the estimand) and how you intend to estimate it (i.e. the estimator). For example, many studies estimate the average treatment effect of an experimental intervention using OLS linear regression as the estimator.5 Clearly specify your model specification, including your outcomes, explanatory variables, and covariates, and your test statistic.\nWe recommend including code for the statistical model or the functional form of the statistical model in the PAP.\n\nExample: Estimation Procedure We estimate the effect of the information campaign and waste collection service on the payment of city taxes among residents with an intent-to-treat analysis. Our estimand is the average treatment effect. If we have balance on baseline and endline outcomes, we will use the following estimator to estimate the average treatment effect: \\(Y_{i,j} = \\beta_0 + \\beta_1Z_{i,j} + X_{i,j}+ \\epsilon_{i,j}\\) where \\(i\\) is the individual in neighborhood \\(j\\), \\(Z\\) is the treatment indicator, and \\(Y\\) is the outcome. \\(X\\) is the baseline outcome for individual \\(i\\). We will use regression weights proportional to the size of the neighborhoods \\(j\\). If baseline and endline outcomes are not balanced, we will use the change score, \\(Y_i = Y_{i,endline} - Y_{i,baseline}\\) and we will not use \\(X\\).\n\n\n\n6 What is in a PAP? Inference Criteria\nInference criteria are decision rules for determining the detectability of effects (i.e. if an explanatory variable really affects an outcome variable). Establishing inference criteria requires making several choices about when to believe the estimated effect is “statistically significantly different” from the null hypothesis. We recommend clearly specifying and justifying answers to the following questions:\n\nWhat kind of standard errors will you use? Why are you using this kind of standard errors?6\nDo you plan to do a one-tailed or two-tailed test?\n\nAt what \\(\\alpha\\) level will you reject the null hypothesis from a \\(p\\)-value?\n\nDo you have multiple hypotheses? If so, what procedure will you use to adjust for multiple comparisons?\n\nYou may choose to use several procedures for inference criteria. For example, you may want to use both FWER and FDR adjustments for multiple comparisons and compare between the two. Or you may want to use \\(p\\)-values from randomization inference as a check on \\(p\\)-values from a null distribution assumed to be normal. If you choose to use several procedures, you should specify how you will interpret findings if different procedures come to different conclusions.\n\nExample: We use HC2 standard errors with our block-randomized experiment because it is equivalent to randomization-based Neyman variance estimator (Samii and Aronow 2012). We expect the treatment group to pay more city taxes than the control group, and therefore use a one-tailed test where \\(H_1 > H_0\\). We set \\(\\alpha = 0.05\\) and will reject the null when the \\(p\\)-value is less than 0.05. Because we only have one confirmatory hypothesis, we do not adjust for multiple comparisons.  As a check on the HC2 standard errors, we also calculate \\(p\\)-values directly using randomization inference, with the difference-in-means as our test statistic.\n\n\n\n7 What is in a PAP? Procedures for Data Issues\nExperiments and data collection often do not go the way that one expects or hopes. The PAP gives you an opportunity to think through what those issues might be, and specify how you plan to address them.\nTwo common data issues are (1) extreme data points and (2) missing data. Extreme points may represent a true outlier – a unit with outcomes much larger or smaller than other units – or may occur because of data collection errors. Survey tablets, recording tools, web scrapers, and other data collection tools may record extreme points due to technical glitches. It can be difficult to know whether extreme points represent true data collected or if they are due to data entry errors, but it is important to specify in the PAP when you expect to see data collection errors and the procedures to deal with extreme points.\nMissing data can come in two forms: missing covariates and missing outcomes. It is also important to specify when you expect to see missing outcomes or covariate and the procedures to deal with them in your PAP. Extreme points/missingness that are random will be less problematic than extreme points/missingess that seem to have a pattern.\nCommon procedures to address missing data or extreme points are 1) bounds analysis; 2) imputation; 3) dropping observations. We recommend considering the following questions when determining which procedure you would like to use:\n\nWhat issues may cause these extreme data points/missing covariates/missing outcomes? What can you do ahead of time to mitigate these data issues?\nHow would you assess if the extreme/missing data are plausibly random (i.e. do the extreme data points/missingness correlate with treatment, specific covariates/subgroups, or outcomes)?\nWhat procedures will you use to address extreme points/missingness and how do you justify using your procedure?\n\nIf extreme/missing data are not random, we recommend including some kind of bounds analysis to determine the bounds of your estimate with extreme points/missing data. For example, extreme value bounds can help you determine the range of an average treatment effect by setting all missing outcomes in the treatment group with largest possible outcome and setting all missing outcomes in the control group with the smallest possible outcome (Gerber and Green 2012).\n\nIf extreme/missing data are random, we recommend imputing the extreme points or missing data. You may also drop these data if the extreme/missing data are random, and you should specify conditions under which you choose to drop the data. For example, consider setting a threshold for a missing covariate such that if the percentage of data missing from the covariate exceeds the threshold, you drop the covariate.\n\n\nWe recommend including code to show your procedures to address data issues.\n\nExample: Missing Outcomes from Survey Questions Some respondents will not answer one or more questions that measure an outcome. If we notice that nonresponse rates for questions seem high (\\(\\geq 10\\%\\)) during the pilot session, we will ask for explanations from both respondents and interviewers so that we can change the questions.\nWhen respondents do not answer one or more questions that measure an outcome as we field our survey, our procedure is as follows:\n\n\nWe will assess the relationship between missing outcomes and treatment assignment using a hypothesis test and report these results.\n\n\nIf \\(p < .05\\) for the assessment of the relationship between treatment and missing outcomes, we will report an extreme value bounds analysis in which we set all of the missing outcomes for treatment to the block maximum and all missing outcomes for control to the block minimum.\n\n\nIf \\(p \\geq 0.5\\) for the assessment of the relationship between treatment and missing outcomes, we will impute the missing outcomes using the mean of the assignment-by-block subcategory.\n\n\nIf we still have outcome questions with especially high nonresponse rate of over 10 percent, we will:\n\n\nDescribe the relationship between non-response to this question and other data on the people via\n\n\n\nBivariate explorations of plots and/or tables\n\n\nVariable selection using adaptive elastic net models with tuning parameters chosen by 10-fold cross validation within design-subgroup.\n\n\n\nConsider dropping these survey outcomes and catalog the reason for our decision.\n\n\n\n\n\n8 What is in a PAP? Power Analysis\nA power analysis is commonly included in PAPs. Statistical power is the likelihood that your study will detect an effect, if there is an effect to be detected.\nThere are tools to help you calculate power, but you can also produce your own power analysis computationally. A benefit of computational power analysis is that the power analysis doubles as your final analysis code, or at minimum a template of the final analysis code.\nIn many ways, the computational power analysis implements all of the specifications you made in your PAP into code. In fact, you can combine code for a computational power analysis with the code written for other sections of the PAP, and create a computational PAP. Such a tool, which can be implemented with software like DeclareDesign, can then help you diagnose potential problems, teach you more about your design, and strengthen your PAP.\n\n\n9 Why make a PAP?\nYou may have heard arguments for and against PAPs. This discussion offers some thoughts that address the benefits and costs of PAPs.\n1. PAPs help counter the replication crisis.\nA lack of research transparency has led to several issues, one of which is the replication crisis.\nThe replication crisis is that many academic studies are difficult or impossible to replicate. PAPs reduce the number of studies that are not replicable due to bad data practices, such as data mining for spurious relationships and inadvertently hacking \\(p\\)-values. PAPs also increase the number of studies that elaborate the procedures necessary for replication, allowing attempts to replicate those studies.\n\n\n2. PAPs help counter publication bias.\nA lack of research transparency also contributes to publication bias.\nPublication bias occurs when journals publish studies based on the study’s results, rather than the quality of the research. This bias can lead to erroneous beliefs about a connection between variables \\(X\\) and \\(Y\\) because journals only publish studies that show \\(X\\) affecting \\(Y\\) and do not publish studies where \\(X\\) does not affect \\(Y\\), even if those studies are more numerous.\nPAP registries function as repositories of attempted studies, both published and unpublished. These registries allow scholars and practitioners to identify if published effects about a topic accurately represent the effects found in unpublished studies, or if published effects differ from unpublished studies.\nPre-registering studies as exploratory or confirmatory further allows researchers to know if future research should build off the study or if future research should corroborate the study. There is nothing wrong with exploratory research, and many important but unknown relationships can be uncovered through exploratory analyses. It is important to acknowledge when findings are exploratory and need to be confirmed in other studies. Details in PAP registries allow scholars to do so.\n3. PAPs encourage quality research.\nCreating a PAP forces the researcher to elaborate the many design decisions that need to be made while conducting a study. In the case of observational studies, changing these design decisions later is not a problem. But for experimental studies or other studies using original data collection, the researcher gets one chance to collect the data needed for the analysis. PAPs help ensure the researcher thinks through all decisions and collects the right data.\n4. PAPs encourage impactful research.\nPAPs increase research transparency, and transparent research should be more readily trusted and used than non-transparent research because the study’s decisions, and reasons for those decisions, are made before the study’s results are known. Transparency assures academic, policy, and other communities that the research findings can be used as the basis for more research, for policy programs, and for other real-world applications.\n5. PAPs can shorten the publication process.\nPAPs require more pre-research time-investment, but substantially shorten post-research analysis time because analytic decisions and code are written in advance. PAPs should also shorten the review process that leads to publication. Journals often require numerous robustness checks before accepting the results of a study. Reviewers may request fewer alternative analyses from pre-specified work because it is clear to reviewers that pre-specified analysis decisions were not influenced by the study’s results. This is especially useful when the researcher wants to use an unconventional but more powerful statistical test that may look suspicious without pre-specification, such as one-sided tests or test statistics other than the difference-in-means.\nWhy not make a PAP?\n1. Research is unpredictable and PAPs make research inflexible.\nSome people argue that a PAP locks the study into a particular design, intervention, and estimation strategy even though details of the design, intervention, and estimation strategy may change while conducting a study. In experimental studies, unforeseen difficulties often change aspects of the randomization or intervention, or a new outcome measure may fail validity tests. And in observational studies, future thought about how a theory applies to your data may reveal the need for new control, mediating, and/or moderating variables.\nResearchers should remember that any Pre-Analysis Plan can be revised! Your first PAP does not lock you into a specific research design, outcome variable, or model specification. The PAP makes the research process transparent, not inflexible. Revisions can be made either by submitting a new PAP or through an amendment describing changes from the previous PAP..  Exploratory analyses are okay! The point of a PAP is not to prevent these unanticipated analyses, but to formalize and explain the process that led to the analyses.\n\nMore discussion of the pros and cons of PAPs can be found in chapter 19 of Rosenbaum (2010) and in Olken (2015).\n\n\n10 When and where do you register a PAP?\nYour PAP is now complete and you are ready to register it! For experimental studies, the latest you should register the PAP is before final data are collected. For observational studies, the latest you should register the PAP is before any analyses are done, including looking at descriptive statistics. You can revise PAPs through an amendment at any time.\nThere are several third-party sites on which you can register your PAP. We list common sites for social science PAPs below. You can list your PAP on multiple sites, and certain journals require potential articles to register PAPs with a specific site.7\n\nEGAP Registry\nAEA Registry (for RCTs only)\nOSF Registry\n\n\nHappy Pre-Analysis Planning!\n\n\n\n\n\n\n\n\n\nReferences\n\nCaughey, Devin, Allan Dafoe, and Jason Seawright. 2017. “Nonparametric Combination: A Framework for Testing Elaborate Theories.” The Journal of Politics 79 (2): 688–701.\n\n\nGerber, Alan S, and Donald P Green. 2012. Field Experiments: Design, Analysis, and Interpretation. WW Norton.\n\n\nNosek, B. A., G. Alter, G. C. Banks, D. Borsboom, S. D. Bowman, S. J. Breckler, S. Buck, et al. 2015. “Promoting an Open Research Culture.” Science 348 (6242): 1422–25.\n\n\nOlken, Benjamin A. 2015. “Promises and Perils of Pre Analysis Plans.” Journal of Economic Perspectives 29 (3): 61–80.\n\n\nRosenbaum, Paul R. 2010. Design of Observational Studies. Springer Series in Statistics. Springer.\n\n\nSamii, Cyrus, and Peter M Aronow. 2012. “On Equivalencies Between Design Based and Regression Based Variance Estimators for Randomized Experiments.” Statistics and Probability Letters 82 (2): 365–70.\n\nFootnotes\n\n\nPAPs are encouraged as part of the Transparency and Openness Promotion (TOP) Guidelines (Nosek et al. 2015), published in Science, with leading social science journals committing to implementing TOP Guidelines.↩︎\nResources like the DeclareDesign project can assist you with simulating and analyzing fake data that mimics the real data your project will gather. Power analysis, an important component of a PAP, requires simulated data.↩︎\nAs R.A. Fisher said and has often been requoted, “Make your theories elaborate…when constructing a causal hypothesis one should envisage as many different consequences of the truth as possible,” (Cochran, 1965; cited in Rosenbaum (2010), pp. 327). Though this was said about determining causation in observational studies, the logic also applies to experimental studies.↩︎\nFor an example of an omnibus test, see Caughey, Dafoe, and Seawright (2017).↩︎\nYou may also be interested in estimating other types of effects. See this guide on types of treatment effects for more information about effect types.↩︎\nFor example, you could calculate standard errors and \\(p\\)-values using permutation-based randomization inference. Or you could closely approximate standard errors and \\(p\\)-values using analytic methods (Samii and Aronow 2012)↩︎\nNote that some organizations do not use third party sites. For example, the U.S. General Services Administration Office of Evaluation Sciences process uses Github, which has timestamps that verify the PAP was created before the analysis was conducted.↩︎"
  },
  {
    "objectID": "guides/getting-started/workflow_en.html",
    "href": "guides/getting-started/workflow_en.html",
    "title": "10 Things You Need to Know About Project Workflow",
    "section": "",
    "text": "1. People have limited memories\nThere are limits to human memory. Since most experimental research requires at least months if not years of design, monitoring, analysis, and reporting, few researchers can maintain mental oversight of all of a project’s moving pieces over time. Introduce additional investigators into the mix, and the questions of who did what, when, and why (if not how) multiply and become harder to answer. As replication becomes more important (by the original project team or outside researchers), maintaining a written record of decisions, actions, and questions becomes essential. Bowers and Voors (2016)1 provide a framework and steps for improving project’s workflow; this guide draws upon their paper and upon additional tools aimed at documenting the important choices made by researchers and effectively communicating those choices to the project team.\n\n\n2. Effective data analysis is aided by coding\nThe best way to leave an easy-to-retrace path (for your future self and others) in data analysis is to produce all outputs through code. Opening Excel to make one change to one table may seem the quickest way to complete the task, but when you are trying to remember what you did months later, you will be glad to have updated your R script or do file to make the desired edit. To maximize re-traceability, leave comments in your code file to explain the purpose of each line, as in the example below.\n\n# This file produces a plot relating the explanatory variable to the outcome.\n## Read the data\nthedata <- read.csv(“Data/thedata-15-03-2011.csv”)\n## begin writing to the pdf file\nplease-open-pdf(“g1.pdf”)\nplease-plot(outcome by explanatory using thedata. red lines\nplease.)\nplease-add-a-line(using model1)\n## Note to self: a quadratic term does not add to the substance\n## model2 <- please-fit(outcome by explanatory+explanatory^2\nusing thedata\n## summary(abs(fitted(model1)-fitted(model2)))\n## stop writing to the pdf file\nplease-close-pdf()\n\nCoding has other benefits. It saves time by systematically completing repetitive tasks that could be done manually but with greater time investment. It leads to smoother collaboration by communicating progress and next steps. Finally, it helps researchers avoid making mistakes by combining multiple steps into one file where the order of steps and the relationships among steps is clear.\n\n\n3. Coding is communication 1: to your self, to co-investigators, and to the future\nData analysis requires a series of decisions, large and small. These decisions will be made both by individual researchers and by research teams. The why and how questions that will arise when you or others attempt to reproduce your work can only be answered if you can remember and communicate about these decisions.\nOne way to record decisions made during the coding process is to use the comment feature of whichever analysis software you are using (for example, lines beginning with * in Stata, # in R are not executed but are understood as notes). Comments are unexecuted text within the script, and can explain the purpose of each line or section to your collaborators or future self. You can see some commented code in the chunk of R code in point 2.\n\n\n4. Coding is communication 2: to yourself, to co-investigators, and to the future\nCoding and communication can be linked even tighter by combining them in the same file. Using markup systems like R Markdown (see section 4 below) or R+LaTeX2, you can place a code chunk that produces a table or figure right where that table/figure will be in the document. If you make a change to the code that produces that table/figure, it appears in the document without any additional work. The R code in point 2 starts with ```{r codechunk1} which tells R+markdown that the next bit of text is R code to be executed and ends with ``` which tells R+markdown to go back to interpreting the text as prose. For example, in markdown one uses two asterisks to make text bold (**like this**) but two asterisks would confuse R (there is no R command with two asterisks), so the idea of a code chunk allows two or more kinds of text to be mixed in a single document. Coauthors and editors can also review the manuscript together with the tables and figures, rather than scrolling back and forth between the two. When it comes time to produce the final version of the document, the code chunks can be hidden to display only the relevant table/figure.3 The idea is to type the document in plain text but that external programs (including R) can compile so that they end up looking like nice, formatted PDFs. You can use Markdown on Windows and Mac OSX, and you can also edit the file in an ordinary text editor. Jupyter notebooks are another recent open source approach to mixing prose text and code text which emphasize the python language but which can also include R.4\n\n\n4. A useful tool for combining code and writeup: R Markdown\nR Markdown is a format for making documents within R using a plain text format. When compiled, these text formats end up looking like nice, formatted PDFs. You can use Markdown on Windows and Mac OSX, and you can also edit the file in an ordinary text editor.\nFor more information, see the Metaketa III committee R Markdown Manual.\n\n\n5. Data analysis requires clear routes between inputs and outputs\nNearly every script begins by calling in the relevant data for analysis. This should be done with a line that produces the same output on every computer. This, in turn, requires that collaborators need to have access to a shared file structure for storing project materials, about which more in section 7.\nWithin that shared file structure, the organization of files can be a very helpful map for identifying what went where. Nagler 5 outlines the principle of modularity, which suggest that you separate files by function (namely making data cleaning/processing/recoding/merging distinct from analysis). Storing the original data file in a folder labeled “raw” and the cleaned/recoded datafile in one marked “clean” tells you and your collaborators where to find each without searching too hard. If the folder structure starts to get overly complicated, consider using a README file (a brief text file) to explain where the relevant input and output files for your analysis are stored.\n\n\n6. Version control keeps track of who did what and when, and makes sure that work is not overwritten\nEven teams with perfect file structure in their shared folder can run into issues without effective version control. Version control allows teams to:\n\nDetermine what changed between different versions of the same document;\nExperiment within a document, delete the parts that did not work, and merge in the parts that did;\nProduce multiple versions of the same document (such as one for a journal and one for a conference);\nWork on a file without undoing the work that someone else on your team happened to do at the same time\n\nOne way to practice effective version control is to use the “track changes” function within word processors. This allows one author to make edits in a way that others can quickly identify, review, and “accept” or “reject.” In Google Docs6, this feature (which Google calls “suggestion” mode) can be combined with the ability for multiple authors to work on the document at once time (assuming that everyone has a reliable internet connection).\nIn situations with less reliable internet access, Dropbox7 can provide shared access to a folder or series of folders where project materials are stored. One author can make an edit and sync their account, and then another can view the latest version. Authors cannot make changes at the same time, but they can hand off to each other with greater ease. This can be aided by adopting a file naming structure that tells you when the latest changes were made. Naming a file document.docx does not communicate this; naming it yyyymmdd_document.docx does (it also allows you to sort the folder by name and see the most recently edited version first. If the versions start to pile up, consider creating an archive subfolder to store all but the most recent version.\n\n\n7. A useful tool for version control: GitHub\nWhen collaboration involves plain text files (i.e., code or files that combine code and prose), we recommend following the best practices of community of software developers (who make a living from working together in teams to write reliable code in an efficient way). Currently, the standard tool to enable this collaboration is Github.\nGitHub is a code hosting platform to streamline issues related to version control and working as a group. It lets you and others work together on projects from anywhere. The core GitHub workflow includes each user copying, or “cloning”, a version of a folder (or folder structure) to their own desktop, so that the latest version from the repository is on a local device, making changes to the desired files, and pushing those files to the shared repository. At the beginning of a work session one “pulls” or “synchronizes” any changes made by other team members online with the local copy. The git system tracks all changes, and contains functionality to revert back to previous versions as needed.\nFor more information, see Metaketa III committee GitHub Manual.\n\n\n8. Your future self–and others–will want to be able to access your data\nScience involves learning from the work of others in addition to remembering about the past work of oneself. If one works in such a way that a future self can remember, then, often that work will be easy to use by others as they themselves attempt to extend the science or even just learn how to apply your good ideas to their own context.8 There are a number of tools to make storing and sharing files for replication easier, including GitHub and the Center for Open Science’s Open Science Framework (OSF). OSF recently ran a replication project aimed at understanding “predictors of reproducibility of research results, and how low reproducibility may inhibit efficient accumulation of knowledge.”\nData should be stored in places like a Dataverse or the ICPSR or other places with a plan for daily backups band also a plan for data to be preserved and made available for many years into the future (in contrast to your favorite homemade website).\n\n\n9. To minimize error, build testing into the code\nTesting ensures that errors from bugs and typos are caught before it is too late, and including that testing in your coding makes sure that it is done.\nBelow is a simple example of this testing on a function designed to multiply a number by 2.\n\n##  The test function:\ntest.times.2.fn <- function(){\n    ##  This function tests times.2.fn\n    if (times.2.fn(thenumber = 4) == 8 &\n        times.2.fn(thenumber = -4) == -8) {\n        print(“It works!”)\n        } else { print(“It does not work!”)\n}\n}\n##  The actual function is:\ntimes.2.fn <- function(thenumber){\n    ##  This function multiplies a scalar number by 2\n    ##  thenumber is a scalar number\n    thenumber+2\n\nWhat happens when running the test function?\n\ntest.times.2.fn()\n[1] “It does not work!”\n\nThis tells you to go back and review the function, where we see a + instead of a *. Testing can be even more effective if we code the script to stop when an error occurs instead of generating an error message. In R, the stopifnot function will do just that.\n\n\n10. Research should remove speculation\nUltimately, the point of doing research is to move things from the realm of opinion into the realm of knowledge. Thus, effective research should have findings that are not a matter of opinion, and at which others can arrive. If, in the future, somebody disagrees with your analysis, you can hand them the raw data files and analysis scripts and watch them arrive at the same result.\n\n\n\n\n\nFootnotes\n\n\nBowers, J. & Voors, M. 2016. How to Improve Your Relationship With Your Future Self. Revista de Ciencia Política. Vol. 36, Issue 3.↩︎\nCalled, anachronistically, Sweave↩︎\nNote that all of the EGAP methods guides that produce figures or tables are produced this way (see https://github.com/egap/methods-guides)↩︎\nThere are many ways to type text that includes code such that parenthesis are automatically closed, spelling is checked, or even help is provided about the code itself. RStudio is an integrated development environment (IDE) that includes a nice editor. Some of us use older tools that are more widespread such as vim, or eMacs. Others any of the contemporary improvements on those old tools such as Atom.↩︎\nNagler, Jonathan. 1995. “Coding Style and Good Computing Practices”. PS: Political Science and Politics 28(3): 488-92.↩︎\nhttps://docs.google.com↩︎\nhttps://www.dropbox.com↩︎\nKing, Gary. 1995. “Replication, Replication”. PS: Political Science and Politics 28(3): 444-452.↩︎"
  },
  {
    "objectID": "guides/getting-started/causal-inference_en.html",
    "href": "guides/getting-started/causal-inference_en.html",
    "title": "10 Things You Need to Know About Causal Inference",
    "section": "",
    "text": "Abstract\nThe philosopher David Lewis described causation as “something that makes a difference, and the difference it makes must be a difference from what would have happened without it.” This is the interpretation given to causality by most experimentalists. Even though the definition seems simple, it has many subtle implications. Here are ten ideas implied by this notion of causality that matter for research design.1\n\n\n1. A causal claim is a statement about what didn’t happen.\nFor most experimentalists, the statement “\\(X\\) caused \\(Y\\)” means that \\(Y\\) is present and \\(Y\\) would not have been present if \\(X\\) were not present. This definition requires a notion of what could have happened, but did not happen (Holland (1986)). Similarly, the “effect” of \\(X\\) on \\(Y\\) is thought of as the difference between the value that \\(Y\\) would have taken given one value of \\(X\\) and the value that \\(Y\\) would have taken given another value of \\(X\\). Because of the focus on differences in outcomes, this approach is sometimes called the “difference making” or “counterfactual” approach to causation.\nTechnical Note: Statisticians employ the “potential outcomes” framework to describe counterfactual relations. In this framework, we let \\(Y_i(1)\\) denote the outcome for unit \\(i\\) that would be observed under one condition (e.g., if unit \\(i\\) received a treatment) and \\(Y_i(0)\\) the outcome that would be observed in another condition (e.g., if unit \\(i\\) did not receive the treatment). One causal effect of the treatment for unit \\(i\\) might be a simple difference of the potential outcomes \\(τ_i=Y_i(1)−Y_i(0)\\). A treatment has a (positive or negative) causal effect on \\(Y\\) for unit \\(i\\) if \\(Y_i(1)≠Y_i(0)\\).\n\n\n2. There is no causation without manipulation.\nThe “counterfactual” definition of causality requires one to be able to think through what outcomes may result in different conditions. How would things look if one party as opposed to another was elected? Everyday causal statements often fall short of this requirement in one of two ways.\n\nFirst, some statements do not specify clear counterfactual conditions. For example the claim that “the recession was caused by Wall Street” does not point to an obvious counterfactual— are we to consider whether there would have been a recession if Wall Street did not exist? Or is the statement really a statement about particular actions that Wall Street could have taken but did not. If so, which actions? The validity of such statements is hard to assess, and can depend on which counterfactual conditions are implied by a statement.\nSecond, some statements involve counterfactual conditions that cannot be imagined. For example, the claim that Peter got the job because he is Peter implies a consideration of what would have happened if Peter was not Peter. Alternatively, the claim that Peter got the job because he is a man requires considering Peter as other than a man. The problem is that the counterfactuals in these cases imply a change not just in the condition facing an individual but in the individual themselves.\n\nTo avoid such problems, some statisticians urge a restriction of causal claims to treatments that can conceivably (not necessarily practically) be manipulated (Holland (1986)). For example, while we might have difficulties with the claim that Peter got the job because he was a man, we have no such difficulties with the claim that Peter got the job because the hiring agency thought he was a man.\n\n\n3. Causes are non-rival.\nEven though we may focus on the effect of a single cause \\(X\\) on an outcome \\(Y\\), we generally do not expect that there is ever only a single cause of \\(Y\\).[^In some accounts this has been called the “Problem of Profligate Causes”.] Moreover, if you add up the causal effects of different causes, there is no reason to expect them to add up to 100%. Hence, there is not much point trying to “apportion” outcomes to different causal factors. In other words, causes are not rival. The National Rifle Association argues, for example, that guns don’t kill people, people kill people. That statement does not make much sense in the counterfactual framework. Take away guns and you have no deaths from gunshot wounds. So guns are a cause. Take away people and you also have no deaths from gunshot wounds, so people are also a cause. Put differently, these two factors are simultaneously causes of the same outcomes.\n\n\n4. \\(X\\) can cause \\(Y\\) even if \\(X\\) is not a necessary condition or a sufficient condition for \\(Y\\).\nWe often talk about causal relations in deterministic terms. Even the Lewis quote at the top of this page seems to suggest a deterministic relation between causes and effects. Sometimes causal relations are thought to entail necessary conditions (for \\(Y\\) to occur, \\(X\\) has to happen); sometimes such relations are thought to entail sufficient conditions (if \\(X\\) occurs, then \\(Y\\) occurs). But once we are talking about multiple units, there are at least two ways in which we can think of \\(X\\) causing \\(Y\\) even if \\(X\\) is neither a necessary nor a sufficient condition for \\(Y\\). The first is to reinterpret everything in probabilistic terms: by \\(X\\) causes \\(Y\\), we simply mean that the probability of \\(Y\\) is higher when \\(X\\) is present. Another is to allow for contingencies — for example, \\(X\\) may cause \\(Y\\) if condition \\(Z\\) is present, but not otherwise.2\n\n\n5. There is a fundamental problem of causal inference.\nIf causal effects are statements about the difference between what happened and what could have happened, then causal effects cannot be measured. That’s bad news. Prospectively, you can arrange things so that you can observe either what happens if someone gets a treatment or what happens if they do not get the treatment. Yet, for the same person, you will never be able to observe both of these outcomes and hence also not the difference between them. This inability to observe unit-level causal effects is often called the “fundamental problem of causal inference.”\n\n\n6. You can estimate average causal effects even though you cannot observe any individual causal effects.\nEven though you cannot observe whether \\(X\\) causes \\(Y\\) for any given unit, it can still be possible to figure out whether \\(X\\) causes \\(Y\\) on average. The key insight here is that the average causal effect equals the difference between the average outcome across all units if all units were in the control condition and the average outcome across all units if all units were in the treatment condition. Many strategies for causal identification (see 10 Strategies for Figuring Out If X Caused Y) focus on ways to learn about these average potential outcomes.3\n10 Things to Know About Hypothesis Testing describes how one can learn about individual causal effects rather than average causal effects given the fundamental problem of causal inference.\n\n\n7. Estimating average causal effects does not require that treatment and control groups are identical.\nOne strategy that people use to learn about average causal effects is to create treatment and control groups through randomization (see 10 Strategies for Figuring Out If X Caused Y). When doing so, researchers sometimes worry if they find that the resulting treatment and control groups do not look the same along relevant dimensions.\nThe good news is that the argument for why differences in average outcomes across randomly assigned treatment and control groups capture average treatment effects (in expectation across repeated randomizations within the same pool of units) does not rely on treatment and control groups being similar in their observed characteristics. It relies only on the idea that, on average, the outcomes in the treated and control groups will capture the average outcomes for all units in the experimental pool if they were, respectively, in treatment or in control. In practice actual treatment and control groups will not be identical.4\n\n\n8. Correlation is not causation.\nA correlation between \\(X\\) and \\(Y\\) is a statement about relations between actual outcomes in the world, not about the relation between actual outcomes and counterfactual outcomes. So statements about causes and correlations don’t have much to do with each other. Positive correlations can be consistent with positive causal effects, no causal effects, or even negative causal effects. For example taking cough medication is positively correlated with coughing but hopefully has a negative causal effect on coughing.5\n\n\n9. If you know that, on average, \\(A\\) causes \\(B\\) and \\(B\\) causes \\(C\\), this does not mean that, on average, \\(A\\) causes \\(C\\).\nYou might expect that if \\(A\\) causes \\(B\\) and \\(B\\) causes \\(C\\) that therefore \\(A\\) causes \\(C\\).6 But there is no reason to believe that average causal relations are transitive in this way. To see why, imagine \\(A\\) caused \\(B\\) for men but not women and \\(B\\) caused \\(C\\) for women but not men. Then on average \\(A\\) causes \\(B\\) and \\(B\\) causes \\(C\\) but there may still be no one for whom \\(A\\) causes \\(C\\) through \\(B\\).\n\n\n10. It’s easier to learn about the “effects of causes” than to learn about the “causes of effects.”\nThough it might sound like two ways of saying the same thing, there is a difference between understanding what the effect of \\(X\\) on \\(Y\\) is (the “effects of a cause”) and whether an outcome \\(Y\\) was due to cause \\(X\\) (the “cause of an effect”).7 Consider the following example. Suppose we run an experiment with a sample that contains an equal number of men and women. The experiment randomly assigns men and women to a binary treatment \\(X\\) and measures a binary outcome \\(Y\\). Further, suppose that \\(X\\) has a positive effect of 1 for all men, i.e. men’s control potential outcome is zero (\\(Y_i(0) = 0\\)) and their treated potential outcome is one (\\(Y_i(1) = 1\\)). For all women, \\(X\\) has a negative effect of \\(-1\\), i.e., women’s control potential outcome is one (\\(Y_i(0) = 1\\)) and their treated potential outcome is zero (\\(Y_i(1) = 0\\)). In this example, the average effect of \\(X\\) on \\(Y\\) is zero. But for all participants in the treatment group with \\(Y=1\\), it is the case that \\(Y=1\\) because \\(X=1\\). Similarly, for all participants in the treatment group with \\(Y=0\\), it is the case that \\(Y=0\\) because \\(X=1\\). Experimentation can get an exact answer to the question about the “effects of a cause”, but generally it is not possible to get an exact answer to the question about the “cause of an effect”.8\n\n\n\n\n\n\n\n\n\nReferences\n\nGelman, Andrew, and Guido Imbens. 2013. “Why Ask Why? Forward Causal Inference and Reverse Causal Questions.” NBER Working Paper No. 19614.\n\n\nHolland, Paul W. 1986. “Statistics and Causal Inference.” Journal of the American Statistical Association 81 (396): 945–60.\n\n\nMackie, John L. 1974. The Cement of the Universe. Oxford University Press.\n\n\nTian, Jin, and Judea Pearl. 2000. “Probabilities of Causation: Bounds and Identification.” Annals of Mathematics and Artificial Intelligence 28: 287–313.\n\nFootnotes\n\n\nOriginating author: Macartan Humphreys. Minor revisions: Winston Lin and Donald P. Green, 24 Jun 2016. Revisions MH 6 Jan 2020. Revisions Anna Wilke May 2021. The guide is a live document and subject to updating by EGAP members at any time; contributors listed are not responsible for subsequent edits.↩︎\nFollowing Mackie (1974), sometimes the idea of “INUS” conditions is invoked to capture the dependency of causes on other causes. Under this account, a cause may be an Insufficient but Necessary part of a condition which is itself Unnecessary but Sufficient. For example dialing a phone number is a cause of contacting someone since having a connection and dialing a number is sufficient (S) for making a phone call, whereas dialing alone without a connection alone would not be enough (I), nor would having a connection (N). There are of course other ways to contact someone without making phone calls (U).↩︎\nTechnical Note: The key technical insight is that the difference of averages is the same as the average of differences. That is, using the “expectations operator,” \\(𝔼(τ_i)=𝔼(Y_i(1)−Y_i(0))=𝔼(Y_i(1))−𝔼(Y_i(0))\\). The terms inside the expectations operator in the second quantity cannot be estimated, but the terms inside the expectations operators in the third quantity can be (Holland (1986)). See illustration here.↩︎\nFor this reason \\(t\\)-tests to check whether “randomization worked” do not make much sense, at least if you know that a randomized procedure was followed — just by chance 1 in 20 such tests will show statistically detectable differences between treated and control groups. If there are doubts about whether a randomized procedure was correctly implemented these tests can be used to test the hypothesis that the data was indeed generated by a randomized procedure. This later reason for randomization tests can be especially important in field experiments where chains of communication from the person creating random numbers and the person implementing treatment assignment may be long and complex.↩︎\nTechnical Note: Let \\(D_i\\) be an indicator for whether unit \\(i\\) has received a treatment or not. Then the difference in average outcomes between those that receive the treatment and those that do not can be written as \\(\\frac{∑_i D_i×Y_i(1)}{∑_iD_i}−\\frac{∑_i (1−D_i)×Y_i(0)}{∑_i (1−D_i)}\\). In the absence of information about how treatment was assigned, we can say little about whether this difference is a good estimator of the average treatment effect, i.e., of the difference in average treated and control potential outcomes across all units. What matters is whether \\(\\frac{∑_i D_i×Y_i(1)}{∑_iD_i}\\) is a good estimate of \\(\\frac{∑_i 1×Y_i(1)}{∑_i1}\\) and whether \\(\\frac{∑_i (1−D_i)×Y_i(0)}{∑_i (1−D_i)}\\) is a good estimate of \\(\\frac{∑_i 1×Y_i(0)}{∑_i1}\\). This might be the case if those who received treatment are a representative sample of all units, but otherwise there is no reason to expect that it would be.↩︎\nInterpret “\\(A\\) causes \\(B\\), on average” as “the average effect of \\(A\\) on \\(B\\) is positive.”↩︎\nSome reinterpret the “causes of effects” question to mean: what are the causes that have effects on outcomes. See Gelman and Imbens (2013).↩︎\nSee, for example, Tian and Pearl (2000)↩︎"
  },
  {
    "objectID": "guides/getting-started/heterogeneous-effects_en.html",
    "href": "guides/getting-started/heterogeneous-effects_en.html",
    "title": "10 Things to Know About Heterogeneous Treatment Effects",
    "section": "",
    "text": "Abstract\nThis guide1 discusses methods for analyzing heterogeneous treatment effects: testing for heterogeneity, estimating subgroup treatment effects and their differences, and addressing the pitfalls of multiple comparisons and ad hoc specification search.2\n\n\n1 What Is Treatment Effect Heterogeneity?\nAny given treatment might affect different experimental subjects in different ways. The study of treatment effect heterogeneity is the study of these differences across subjects: For whom are there big effects? For whom are there small effects? For whom does treatment generate beneficial or adverse effects? Research on such questions can help inform theories about the conditions under which treatments are especially effective or ineffective; it can also help inform ways of designing and deploying policies so as to maximize their effectiveness.\n\n\n2 Testing for Heterogeneity\nAs a first step, one might be interested in whether the variance of the treatment effect \\(\\tau_i\\) across subjects is statistically distinguishable from zero and seek to test the null hypothesis that \\(Var(\\tau_i) = 0\\) (which is equivalent to the hypothesis of a constant treatment effect, i.e., \\(\\tau_i = \\tau, \\forall i\\)). However, it is not possible to estimate \\(Var(\\tau_i)\\) in an experimental setting because you never get to see the treatment effect for any particular individual. Instead, you only get to see the outcome for each person either in the treatment or in the control condition.3\nTo illustrate this, we can rewrite \\(Var(\\tau_i)\\) as \\[\n\\begin{aligned}\nVar(\\tau_i) &= Var(Y_i(1) - Y_i(0)) \\\\\n&= Var(Y_i(1)) + Var(Y_i(0)) - 2Cov(Y_i(1),Y_i(0))\n\\end{aligned}\n\\] The term \\(Cov(Y_i(1),Y_i(0))\\) on the right-hand side cannot be estimated in an experiment because we only observe a subject’s treated potential outcome or untreated potential outcome, not both.\nAlthough we cannot estimate \\(Var(\\tau_i)\\), the hypothesis that \\(Var(\\tau_i) = 0\\) does have the testable implication that the distributions of the treated and untreated potential outcomes are identical except for a constant shift \\(\\tau\\). Randomization inference (Fisher 1935) allows us to test this implication without additional modeling assumptions, asymptotics, or regularity conditions.4\n1. Compare treatment and control outcome variances. Under the null hypothesis of a constant treatment effect, the variances of the treated and untreated potential outcomes are equal: \\(Var(Y_i(1)) = Var(Y_i(0))\\). This is because \\[\n\\begin{aligned}\nVar(Y_i(1)) &= Var(Y_i(0) + \\tau_i) \\\\\n&= Var(Y_i(0)) + Var(\\tau_i) + 2 \\cdot Cov(Y_i(0),\\tau_i)\n\\end{aligned}\n\\] and under the null hypothesis, \\(Var(\\tau_i) = 0\\) and \\(Cov(Y_i(0),\\tau_i) = 0\\). Thus, we can test the null by testing the implication that \\(Var(Y_i(1)) = Var(Y_i(0))\\).\nTo implement the test, first use the experimental data to estimate the average treatment effect (ATE) and the difference in variances \\(Var(Y_i(1)) - Var(Y_i(0))\\). Next, create a full hypothetical schedule of potential outcomes assuming that the true treatment effect is constant and equal to the estimated ATE. Finally, to obtain a \\(p\\)-value, simulate random assignment a large number of times and calculate how often the simulated estimate of the difference in variances is at least as large (in absolute value) as the actual estimate.\n\nrm(list = ls(all = TRUE))\nset.seed(1234567)\n# Sample data generating process\nn = 100\nY0 = rnorm(n)\nY1 = Y0 + 2*(1:n)/n  # Treatment effect ranging from 0.02 to 2\nt = sample( rep(c(TRUE, FALSE), each = n/2) )  # Randomly assign treatment\nY = t * Y1 + (1 - t) * Y0  # Observed outcome\n# Estimate ATE\ndiff.mean = function(treated, Y) {\n  mean(Y[treated]) - mean(Y[!treated])\n}\nest.ate = diff.mean(t, Y)\nest.ate\n\n[1] 1.282838\n\n# Estimate absolute value of the difference in variances\nabs.diff.var = function(treated, Y) {\n  abs( var(Y[treated]) - var(Y[!treated]) )\n}\nobserved.stat = abs.diff.var(t, Y)\nobserved.stat\n\n[1] 0.3654271\n\n# Create hypothetical schedule of potential outcomes assuming that\n# the true treatment effect is constant and equal to est.ate\nY0.hyp  =  Y - est.ate * t\nY1.hyp  =  Y + est.ate * (1 - t)\n# Calculate p-value\np.value = function(observed.stat, treated, Y1, Y0, sims = 1000) {\n  sim.stats = numeric(sims)\n  for (i in 1:sims) {\n    t.sim  =  sample(treated)  # Simulate random assignment\n    Y.sim  =  t.sim * Y1  +  (1 - t.sim) * Y0\n    sim.stats[i]  =  abs.diff.var(t.sim, Y.sim)\n  }\n  mean(sim.stats >= observed.stat)\n}\np.value(observed.stat = observed.stat, treated = t, Y1 = Y1.hyp, Y0 = Y0.hyp)\n\n[1] 0.203\n\n# Additional code to calculate power\nreps = 1000\nsim.p.values = numeric(reps)\nfor (i in 1:reps) {\n  t.sim  =  sample(t)\n  Y.sim  =  t.sim * Y1  +  (1 - t.sim) * Y0\n  est.ate.sim = diff.mean(t.sim, Y.sim)\n  Y0.hyp.sim  =  Y.sim - est.ate.sim * t.sim\n  Y1.hyp.sim  =  Y.sim + est.ate.sim * (1 - t.sim)    \n  sim.p.values[i] = p.value(observed.stat = abs.diff.var(t.sim, Y.sim), treated = t.sim,\n                            Y1 = Y1.hyp.sim, Y0 = Y0.hyp.sim)\n}\nmean(sim.p.values < 0.05)  # Estimated power\n\n[1] 0.181\n\n\nThis approach is limited because power for tests of differences in variances is weaker than power for tests of differences in means; thus you might often fail to reject the null hypothesis of a constant treatment effect even when there is real heterogeneity in effects. Another limitation of this method is that it is uninformative when heterogeneous treatment effects exist but the variances of \\(Y_i(0)\\) and \\(Y_i(1)\\) are equal. A third limitation, discussed by Ding, Feller, and Miratrix (2016), is that because the hypothetical schedule of potential outcomes is based on the estimated ATE instead of the unknown true ATE, the approach is not guaranteed to yield a valid test of the constant treatment effect hypothesis. To address this problem, they suggest a “Fisher randomization test confidence interval” (FRT CI) method, described below.\n2. Compare treatment and control marginal cumulative distribution functions. As an alternative to comparing variances, one can compare the marginal cumulative distribution functions (CDFs) of the outcome between treatment and control. Under the null hypothesis of a constant treatment effect, the two CDFs differ only by a constant shift.\nThe first key change to the randomization inference procedure detailed above is the use of a different test statistic. Ding, Feller, and Miratrix (2016) suggest using a Kolmogorov-Smirnov (KS) statistic to measure the maximum pointwise distance between the treatment and control CDFs after shifting the treatment CDF by a constant treatment effect \\(\\tau\\). The test statistic is \\[ t_{KS}(\\tau) = \\max_y | \\hat{F}_0(y) - \\hat{F}_1(y + \\tau) | \\] where \\(\\hat{F}_0(\\cdot)\\) and \\(\\hat{F}_1(\\cdot)\\) denotes the empirical CDFs of the outcome in the control group and treatment group, respectively.\nDing, Feller, and Miratrix suggest plugging in the estimated average treatment effect \\(\\hat{\\tau}\\) for \\(\\tau\\). Their “shifted” KS statistic \\[ t_{SKS} = \\max_y | \\hat{F}_0(y) - \\hat{F}_1(y + \\hat{\\tau}) | \\] is appropriate for testing the hypothesis that the true treatment effect is constant and equal to the estimated ATE.\nThe second key change that Ding, Feller, and Miratrix suggest is the FRT CI method, which addresses the problem that the true ATE may differ from the estimated ATE. The basic idea is that instead of using just one value for the hypothesized constant treatment effect to create the full schedule of potential outcomes for a randomization test, we can try a range of hypothesized constant treatment effects and find the maximum \\(p\\)-value over all the resulting randomization tests. They first construct a 99.9% confidence interval for the ATE (using the Neyman variance estimator), which becomes the range of hypothesized constant treatment effects. They then find the maximum \\(p\\)-value over all the resulting randomization tests and add an increment of 100% - 99.9% = 0.001. This method is guaranteed to yield a valid test of the constant treatment effect hypothesis if the confidence interval used is exactly valid. In practice, the CI is only approximately valid, but the FRT CI method with the shifted KS statistic still yields an exact or conservative test in their simulations.\n\n\n3 Conditional Average Treatment Effects (CATEs)\nA more structured, theory-driven inquiry of treatment effect heterogeneity involves pre-specifying and investigating conditional average treatment effects (CATEs). A CATE is an average treatment effect specific to a subgroup of subjects, where the subgroup is defined by subjects’ attributes (e.g., the ATE among female subjects) or attributes of the context in which the experiment occurs (e.g., the ATE among subjects at a specific site in a multi-site field experiment).\n\n\n4 Interaction Effects: Treatment-by-Covariate versus Treatment-by-Treatment\nIn addition to CATEs, researchers are also interested in treatment-by-covariate interaction effects, or the difference between two CATEs when the covariate partitioning subjects into subgroups is not experimentally manipulated. For example, one might estimate an ATE for female subjects and an ATE for male subjects but actually care about whether the difference in ATEs between the female and male subgroups is statistically distinguishable from zero. To ensure unbiased estimation of CATEs and of interaction effects, the covariate used to partition subjects into subgroups must be a pre-treatment covariate and must be measured using the same procedure for all subjects across experimental groups. A treatment-by-covariate interaction can be interpreted as a descriptive measure of association between the covariate and the treatment effect, but does not necessarily represent the causal effect of a change in the covariate value on the ATE if the covariate is not randomly assigned.\nIn contrast to treatment-by-covariate interactions, treatment-by-treatment interactions are differences in CATEs where the personal or contextual attribute partitioning subjects into subgroups is experimentally manipulated. Because the covariate is randomly assigned, treatment-by-treatment interactions may be interpreted causally. Factorial and partial factorial designs allow researchers to randomly assign subjects to different combinations of “cross-cutting” treatment conditions and to estimate treatment-by-treatment interactions as allowed by the design.\n\n\n5 Estimating CATEs and Interaction Effects\nEstimating CATEs and interaction effects is straightforward. Nonparametrically, the CATE may be estimated by calculating the ATE among subjects in the specific subgroup of interest. Interaction effects may be estimated by differencing relevant CATEs.\nCATEs and interaction effects may also be estimated in a regression framework. Here is an example for a hypothetical experiment evaluating the effect of a job training program on future earnings. Let \\(Y\\) be the outcome (future earnings), \\(Z\\) be the treatment variable (1=job training program, 0=control), and \\(X\\) be a pre-treatment covariate (1=scholarship receipt, 0=no scholarship). The model \\[\n\\begin{aligned}\nY_i &= \\alpha + \\beta Z_i + \\gamma X_i + \\varepsilon_i \\label{null}\n\\end{aligned}\n\\] allows us to estimate the ATE (\\(\\beta\\)) only. We can add an additional term interacting \\(Z\\) and \\(X\\), which yields \\[\n\\begin{aligned}\nY_i &= \\alpha + \\beta Z_i + \\gamma X_i + \\delta Z_iX_i + \\varepsilon_i \\label{alt}\n\\end{aligned}\n\\] where the coefficient \\(\\delta\\) is the interaction effect and is interpreted as the difference between the ATE of the job training program among subjects receiving a scholarship and the ATE of the job training program among subjects not receiving a scholarship. This has a causal interpretation (i.e., \\(\\delta\\) is a treatment-by-treatment interaction) when scholarship receipt is randomly assigned and a descriptive interpretation (i.e., \\(\\delta\\) is a treatment-by-covariate interaction) when scholarship receipt is not randomly assigned.\nThe model with the interaction also allows us to back out the values of the CATEs. The ATE of the job training program among subjects who do not receive a scholarship is \\(\\beta\\). The ATE of the job training program among subjects who receive a scholarship is \\((\\beta + \\delta)\\).\n\n\n6 Hypothesis Testing for Interaction Effects\nTo test whether the estimated interaction effect could have occurred by chance, one can use randomization inference: First generate a full schedule of potential outcomes under the null hypothesis that the true treatment effect is constant and equal to the estimated ATE. Then simulate random assignment a large number of times and calculate how often the simulated estimate of the interaction effect is at least as large (in absolute value) as the actual estimate.\nOne can also conduct randomization inference in a regression framework. One method suitable for two-sided tests involves using the \\(F\\)-statistic as the test statistic, where the null model is \\[\n\\begin{aligned}\nY_i &= \\alpha + \\beta Z_i + \\gamma X_i + \\varepsilon_i\n\\end{aligned}\n\\] and the alternative model is \\[\n\\begin{aligned}\nY_i &= \\alpha + \\beta Z_i + \\gamma X_i + \\delta Z_iX_i + \\varepsilon_i .\n\\end{aligned}\n\\]\n\n# Sample code for RI using F-stat as test statistic\n# Code adapted from: http://isps.its.yale.edu/isps/public/Gerber_Green_FEDAI_2012/\n#                    Chapter-9/GerberGreenBook_Chapter9_PlotandFtest_Figure_9_1.R\nrm(list = ls(all = TRUE))\nset.seed(1234567)\n# Let:\n# Y = observed outcome\n# Z = treatment assignment (complete randomization)\n# X = covariate\nn <- 1000\nZ <- sample( rep(c(1, 0), each = n/2) )\nX <- sample( rep(c(1, 0), each = n/2) )\nY <- rnorm(n)\nnumiter <- 1000 # No. of RI iterations (use more for greater precision, fewer for greater speed)\n# estimate ATE\nestate <- mean(Y[Z==1]) - mean(Y[Z==0])\n# construct hypothetical schedule of potential outcomes\n# using constant effects assumption where tau_i == estate\nY0 <- Y - estate*Z\nY1 <- Y + estate*(1-Z)\n# estimate CATEs\nestcate0 <- mean(Y[X==0 & Z==1]) - mean(Y[X==0 & Z==0])\nestcate1 <- mean(Y[X==1 & Z==1]) - mean(Y[X==1 & Z==0])\nlm1  <- lm(Y~Z*X)  # alternative model\nlm2  <- lm(Y~Z+X)  # null model\nFtest <- (sum(lm2$residuals^2) - sum(lm1$residuals^2)) / (sum(lm1$residuals^2) / (n - 4))\n# or alternatively\n# library(lmtest)\n# Ftest <- waldtest(lm1,lm2)$F[2]\nFdist <- rep(NA,numiter)\nfor (i in 1:numiter) {\n    Zri <- sample(Z)\n    Yri <- Y0*(1-Zri) + Y1*Zri\n    estcate0ri <- mean(Yri[X==0 & Zri==1]) - mean(Yri[X==0 & Zri==0]) \n    estcate1ri <- mean(Yri[X==1 & Zri==1]) - mean(Yri[X==1 & Zri==0])\n    \n    lm1ri  <- lm(Yri~Zri*X)\n    lm2ri  <- lm(Yri~Zri+X)\n    Fdist[i] <- (sum(lm2ri$residuals^2) - sum(lm1ri$residuals^2)) / (sum(lm1ri$residuals^2) / (n - 4))\n    # or alternatively\n    # Fdist[i] <- waldtest(lm1ri, lm2ri)$F[2]   \n    }\n#p-value\nmean(Fdist >= Ftest)\n\n[1] 0.525\n\n\nFor one-sided tests, the coefficient on the interaction term may be used as the test statistic, given the appropriate model.\n\n\n7 Multiple Comparisons\nResearchers interested in heterogeneous treatment effects are likely to encounter the problem of multiple comparisons: for example, when numerous subgroup analyses are conducted, the probability that at least one result looks statistically significant at the 5 percent level may be considerably greater than 5 percent even when the treatment has no effect on anyone.5\nOne way to mitigate the multiple comparisons problem is to reduce the number of tests conducted (e.g., by analyzing a small number of pre-specified subgroups). Another approach is to adjust the \\(p\\)-values to account for the fact that multiple hypotheses are being tested simultaneously.\n7.1 Familywise error rate (FWER) control methods\nFamilywise error rate (FWER) control methods limit the probability of making at least one type I error given the number of tests conducted. Suppose one is testing \\(K\\) hypotheses, \\(H_1, H_2, \\ldots, H_K\\), and \\(K_0\\) of the \\(K\\) hypotheses are true, where \\(K_0 \\le K\\). The familywise error rate is the probability that at least one of the \\(K_0\\) true hypotheses is falsely rejected. The FWER increases in the number of hypotheses tested. FWER control methods adjust the \\(p\\)-values so that, for example, if we reject a hypothesis only when the adjusted \\(p\\)-value is less than 0.05, the FWER will not exceed 5 percent.\nThe most conservative FWER control method is the Bonferroni correction, which multiplies the \\(p\\)-values by the number of tests conducted. (If the result exceeds 1, the adjusted \\(p\\)-value is set to 1.) For example, suppose we tested the significance of four interaction effects and found unadjusted \\(p\\)-values of 0.02, 0.04, 0.2, and 0.3. The adjusted \\(p\\)-values would then be 0.08, 0.16, 0.8, and 1. This approach has limitations because one quickly loses statistical power with just a few tests.\nThe Westfall–Young step-down procedure is an alternative FWER control method that can be more powerful than the Bonferroni correction because it takes into account correlations between the tests.6 The procedure involves the following steps:7\n\nGiven a family of \\(K\\) null hypotheses (where each hypothesis corresponds to a subgroup or interaction of interest), sort the hypotheses in order of decreasing statistical significance (increasing \\(p\\)-value): \\(p_1 \\leq p_2 \\leq \\ldots \\leq p_K\\).\nSimulate the sharp null hypothesis of no treatment effect by performing a large number \\(L\\) of replications of random assignment of treatment, leaving the outcome and covariate data unchanged.\nFor each replication, compute a set of simulated \\(p\\)-values, \\(p_1^*, \\ldots, p_K^*\\). (Do not sort the simulated \\(p\\)-values. Keep the ordering of hypotheses from step 1, so that, e.g., \\(p_1^*\\) corresponds to the same hypothesis as \\(p_1\\).)\nCompute the adjusted \\(p\\)-values as follows:\n\n\\[ p_1^{adj} = \\frac{\\mbox{No. of replications where } \\min (p_1^*, \\ldots, p_K^*) \\leq p_1}{L}\\]\n\\[ p_2^{adj} = \\max \\left(p_1^{adj}, \\ \\frac{\\mbox{No. of replications where } \\min (p_2^*, \\ldots, p_K^*) \\leq p_2}{L} \\right)\\]\n\\[ p_3^{adj} = \\max \\left(p_2^{adj}, \\ \\frac{\\mbox{No. of replications where } \\min (p_3^*, \\ldots, p_K^*) \\leq p_3}{L} \\right)\\]\n\\[ \\ldots \\]\n\\[ p_K^{adj} = \\max \\left(p_{K-1}^{adj} \\, , \\ \\frac{\\mbox{No. of replications where } p_K^* \\leq p_K}{L} \\right)\\]\nR functions to implement the Westfall–Young step-down procedure are available in Porter (2016)8 and the package multtest9.\n7.2 False discovery rate (FDR) control methods\nFalse discovery rate (FDR) control methods control the expected proportion of rejected null hypotheses that are type I errors. Formally, \\(FDR = E[V \\, / \\, R]\\) where \\(V\\) is the number of rejected nulls that are actually true, \\(R\\) is the total number of rejected nulls, and \\(V \\, / \\, R\\) is defined as \\(0\\) if \\(R = 0\\). An equivalent definition is \\(FDR = Pr[R > 0] \\times E[V \\, / \\, R \\mid R > 0]\\).\nThe basic procedure developed by Benjamini and Hochberg (1995)10 involves the following steps to control the FDR. As in the setup to control the FWER, specify \\(K\\) hypotheses \\(H_1, \\ldots, H_K\\) and index the hypotheses in order of decreasing statistical significance so that \\(p_1 \\leq p_2 \\leq \\ldots \\leq p_K\\). Let \\(q \\in (0,1)\\) be the desired upper limit on the FDR. Let \\(c\\) be the largest index for which \\(p_c \\leq (cq \\, / \\, K)\\). Reject \\(H_1, \\ldots, H_c\\) but do not reject any other hypotheses in the family. This procedure controls the FDR at level \\(q\\, (K_0 \\, / \\, K) \\leq q\\) where \\(K_0\\) is the number of true null hypotheses.11\nFDR control tends to be less conservative than FWER control and is popular in fields such as genomics, where, as Westfall et al. (2011, p. 14) write, “the number of hypotheses can easily be in the thousands or millions, [and] you usually do not expect that every significant result is real and replicable. Rather, you just want to ensure that a controlled high proportion (e.g., 0.95 or more) of the significant results is real and replicable.” FDR control methods have also been used in the social sciences—for example, Anderson (2008) uses FDR control for exploratory analyses and FWER control for confirmatory analyses. However, the concept of the FDR can be difficult to interpret, for several reasons:\n\nAs noted above, the FDR is equivalent to \\(Pr[R > 0] \\times E[V \\, / \\, R \\mid R > 0]\\). Thus, Westfall et al. (2011, p. 496) note that interpreting the FDR as “the expected proportion of false rejections” is reasonable when thousands or millions of null hypotheses are tested and \\(Pr[R > 0]\\) (the probability that at least one null is rejected) is close to 1, but “in cases where \\(R\\) can be \\(0\\) with reasonably high probability, the interpretation of FDR is unclear” (because then \\(E[V \\, / \\, R \\mid R > 0]\\) can be much higher than the FDR) and “it is better to use FWE-controlling methods in these cases” because they “have more straightforward interpretation.”\nThe FDR is defined as an expectation: the average value of \\(V \\, / \\, R\\) across an infinite number of hypothetical replications of the study. “Controlling the FDR” means keeping this expectation less than or equal to some threshold \\(q\\). But this says nothing about the variability of \\(V \\, / \\, R\\) across replications and thus does not by itself control the probability that \\(V \\, / \\, R\\) substantially exceeds \\(q\\) (Efron 2010, pp. 51, 55–57).12\nAs Gelman, Hill, and Yajima (2012) write: “Methods that control for the FDR may make particular sense in fields like genetics where one would expect to see a number of real effects amidst a vast quantity of zero effects such as when examining the effect of a treatment on differential gene expression. … They may be less useful in social science applications when we are less likely to be testing thousands of hypotheses at a time and when there are less likely to be effects that are truly zero (or at least the distinction between zero and not-zero may be more blurry).”\n\n\n\n8 Use a Pre-Analysis Plan To Reduce the Number of Hypothesis Tests\nYou can also reduce the numbers of CATEs and interactions under consideration for hypothesis testing by pre-specifying the tests of primary interest in a registered pre-analysis plan (PAP). Additional subgroup analyses can be conceptualized and specified as exploratory or descriptive analyses in the PAP. Another bonus is that if you prefer a one-sided test, you can commit to that choice in the PAP before seeing the outcome data, so that you “cannot be justly accused of cherry-picking the test after the fact” (Olken 2015).13 See our guide 10 Things to Know About Pre-Analysis Plans for more on pre-registration.\n\n\n9 Automate the Search for Interactions\nMachine learning methods are useful to automate the search for systematic variation in treatment effects. These automated approaches are attractive because they minimize researchers’ use of ad hoc discretion in selecting and testing interactions, and are useful for conducting exploratory analyses.\nPopular machine learning methods include support vector machines (R package FindIt),14 Bayesian additive regression trees (R package BayesTree),15 classification and regression trees (R package causalTree),16 random forests,17 and kernel regularized least squares (R package KRLS).18\nIn addition to single machine learning methods, ensemble methods may be used. Ensemble methods estimate a weighted average of multiple machine learning estimates of heterogeneous effects where the weights are a function of out-of-sample prediction performance.19\n\n\n10 A Note on Interactions between Treatment and Post-Treatment Covariates\nThe discussion thus far has assumed that the treatment effect heterogeneity of interest involves pre-treatment covariates, to ensure unbiased estimation of CATEs and treatment-by-covariate interaction effects.\nSome researchers may be interested in post-treatment effect modification, or the interaction between a treatment and a post-treatment covariate. For example, how do the effects of a job search assistance program vary with participants’ levels of depression during the followup period? Conditioning on a post-treatment covariate may lead to bias, because biased estimation of both the main effect and the interaction effects is possible when a post-treatment covariate is included as a regressor. This is especially likely when the covariate is affected by the treatment.\nThere is a burgeoning body of methodological research on the conditions under which CATEs involving post-treatment covariates are identified. These methods rely on model-based identification.20\n\n\n\n\n\nFootnotes\n\n\nOriginating author: Albert Fang, 3 Jun 2016. Revisions: Winston Lin and Don Green, 16 Jan 2017. The guide is a live document and subject to updating by EGAP members at any time; contributors listed are not responsible for subsequent edits.↩︎\nThis guide draws heavily from Alan S. Gerber and Donald P. Green (2012), Field Experiments: Design, Analysis, and Interpretation (New York: WW Norton), and from Don Green’s course notes for Experimental Methods at Columbia University.↩︎\nThis is known as the Fundamental Problem of Causal Inference. For more background, see 10 Things You Need to Know About Causal Inference.↩︎\nFor further reading, see Gerber and Green (2012) and Peng Ding, Avi Feller, and Luke Miratrix (2016), “Randomization Inference for Treatment Effect Variation,” Journal of the Royal Statistical Society, Series B 78: 655–671.↩︎\nFor more background and a range of views on the multiple comparisons problem, see, e.g.: 10 Things You Need to Know About Multiple Comparisons; Richard J. Cook and Vern T. Farewell (1996), “Multiplicity Considerations in the Design and Analysis of Clinical Trials,” Journal of the Royal Statistical Society, Series A 159: 93–110; Kenneth F. Schulz and David A. Grimes (2005), “Multiplicity in Randomised Trials I: Endpoints and Treatments,” Lancet 365: 1591–1595; Schulz and Grimes (2005), “Multiplicity in Randomised Trials II: Subgroups and Interim Analyses,” Lancet 365: 1657–1661; Michael L. Anderson (2008), “Multiple Inference and Gender Differences in the Effects of Early Intervention: A Reevaluation of the Abecedarian, Perry Preschool, and Early Training Projects,” Journal of the American Statistical Association 103: 1481–1495; Peter H. Westfall, Randall D. Tobias, and Russell D. Wolfinger (2011), Multiple Comparisons and Multiple Tests Using SAS, 2nd ed.; Andrew Gelman, Jennifer Hill, and Masanao Yajima (2012), “Why We (Usually) Don’t Have to Worry About Multiple Comparisons,” Journal of Research on Educational Effectiveness 5: 189–211.↩︎\nThe Westfall–Young method’s ability to control the FWER depends on a “subset pivotality” assumption that may be violated when outcomes are heteroskedastic or when there are multiple treatment arms. Westfall et al. (2011, p. 421) write: “However, this theoretical shortcoming is only rarely a practical one for continuously distributed data. Experience shows that this issue is most likely to arise in cases with extreme heteroscedasticity and unbalanced sample sizes. … These issues can become even more problematic when testing binary data.” See also Frank Bretz, Torsten Hothorn, and Peter Westfall (2011), Multiple Comparisons Using R, pp. 133–137. Bootstrap methods that relax the subset pivotality assumption are discussed in: Joseph P. Romano and Michael Wolf (2005), “Exact and Approximate Stepdown Methods for Multiple Hypothesis Testing,” Journal of the American Statistical Association 100: 94–108; Romano and Wolf (2005), “Stepwise Multiple Testing as Formalized Data Snooping,” Econometrica 73: 1237–1282; Romano and Wolf (2016), “Efficient Computation of Adjusted \\(p\\)-Values for Resampling-Based Stepdown Multiple Testing,” Statistics and Probability Letters 113: 38–40; John A. List, Azeem M. Shaikh, and Yang Xu (2016), “Multiple Hypothesis Testing in Experimental Economics,” NBER Working Paper 21875.↩︎\nThis description of the algorithm is adapted from Anderson (2008) and Daniel Gubits, Winston Lin, Stephen Bell, and David Judkins (2014), “BOND Implementation and Evaluation: First- and Second-Year Snapshot of Earnings and Benefit Impacts for Stage 2,” Abt Associates report submitted to the Social Security Administration.↩︎\nKristin E. Porter (2016), “Statistical Power in Evaluations That Investigate Effects on Multiple Outcomes: A Guide for Researchers,” MDRC working paper.↩︎\nKatherine S. Pollard, Sandrine Dudoit, and Mark J. van der Laan (2004), “Multiple Testing Procedures: R multtest Package and Applications to Genomics,” working paper; Sandrine Dudoit and Mark J. van der Laan (2008), Multiple Testing Procedures with Applications to Genomics.↩︎\nYoav Benjamini and Yosef Hochberg (1995), “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing,” Journal of the Royal Statistical Society, Series B 57: 289–300.↩︎\nStrictly speaking, to guarantee that the Benjamini–Hochberg procedure controls the FDR, we need to assume either that the \\(p\\)-values corresponding to the true null hypotheses are independent or that they obey a positive dependence condition. For a brief overview of work addressing dependence, see section 3.2 of Yoav Benjamini (2010), “Discovering the False Discovery Rate,” Journal of the Royal Statistical Society, Series B 72: 405–416.↩︎\nBradley Efron (2010), Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction.↩︎\nBenjamin A. Olken (2015), “Promises and Perils of Pre-Analysis Plans,” Journal of Economic Perspectives 29(3): 61–80.↩︎\nSee, for example, Kosuke Imai and Marc Ratkovic (2013), “Estimating Treatment Effect Heterogeneity in Randomized Program Evaluation,” Annals of Applied Statistics 7(1): 443–470.↩︎\nH. A. Chipman, E. I. George, and R.E. McCulloch (2010), “BART: Bayesian Additive Regression Trees,” Annals of Applied Statistics 4: 266–298; Jennifer L. Hill (2011), “Bayesian Nonparametric Modeling for Causal Inference,” Journal of Computational and Graphical Statistics 20(1): 217–240; Donald P. Green and Holger L. Kern (2012), “Modeling Heterogeneous Treatment Effects in Survey Experiments with Bayesian Additive Regression Trees,” Public Opinion Quarterly 76(3): 491–511.↩︎\nSusan Athey and Guido W. Imbens (2016), “Recursive Partitioning for Heterogeneous Causal Effects,” Proceedings of the National Academy of Sciences 113: 7353–7360.↩︎\nStefan Wager and Susan Athey (2016), “Estimation and Inference of Heterogeneous Treatment Effects using Random Forests,” arXiv.↩︎\nJens Hainmueller and Chad Hazlett (2013), “Kernel Regularized Least Squares: Reducing Misspecification Bias with a Flexible and Interpretable Machine Learning Approach,” Political Analysis.↩︎\nMark van der Laan, Eric Polley, and Alan Hubbard (2007), “Super Learner,” Statistical Applications in Genetics and Molecular Biology 6(1); Justin Grimmer, Solomon Messing, and Sean J. Westwood (2014), “Estimating Heterogeneous Treatment Effects and the Effects of Heterogeneous Treatments with Ensemble Methods,” working paper.↩︎\nFor further reading (at an advanced technical level), see S. Vansteelandt and E. Goetghebeur (2003), “Causal Inference with Generalized Structural Mean Models,” Journal of the Royal Statistical Society, Series B 65: 817–835; Vansteelandt and Goetghebeur (2004), “Using Potential Outcomes as Predictors of Treatment Activity via Strong Structural Mean Models,” Statistica Sinica 14: 907–925; S. Vansteelandt (2010), “Estimation of Controlled Direct Effects on a Dichotomous Outcome using Logistic Structural Direct Effect Models,” Biometrika 97: 921–934; Alisa Stephens, Luke Keele, and Marshall Joffe (2016), “Generalized Structural Mean Models for Evaluating Depression as a Post-Treatment Effect Modifier of a Jobs Training Intervention,” working paper. Footer↩︎"
  },
  {
    "objectID": "guides/getting-started/regression-table_en.html",
    "href": "guides/getting-started/regression-table_en.html",
    "title": "10 Things to Know About Reading a Regression Table",
    "section": "",
    "text": "Abstract\nThis guide1 gives basic information to help you understand how to interpret the results of ordinary least squares (OLS) regression in social science research. The guide focuses on regression but also discusses general concepts such as confidence intervals.\nThe table below that will be used throughout this methods guide is adapted from a study done by EGAP members Miriam Golden, Eric Kramon and their colleagues (J. Asunka et al., “Protecting the Polls: The Effect of Observers on Election Fraud”). The authors performed a field experiment in Ghana in 2012 to test the effectiveness of domestic election observers on combating two common electoral fraud problems: ballot stuffing and overvoting. Ballot stuffing occurs when more ballots are found in a ballot box than are known to have been distributed to voters. Overvoting occurs when more votes are cast at a polling station than the number of voters registered. This table reports a multiple regression (this is a concept that will be further explained below) from their experiment that explores the effects of domestic election observers on ballot stuffing. The sample consists of 2,004 polling stations.\n\n\n\n1 What is regression?\nRegression is a method for calculating the line of best fit. The regression line uses the “independent variables” to predict the outcome or “dependent variable.” The dependent variable represents the output or response. The independent variables represent inputs or predictors, or they are variables that are tested to see if they predict the outcome.\nIndependent and dependent variables have many synonyms, so it helps to be familiar with them. They are the explanatory and response variables, input and output variables, right hand side and left hand side variables, explanans and explanandum, regressor and regressand, predictor and criterion variable, among many others. The first thing you need to do when you see a regression table is to figure out what the dependent variable is—this is often written at the top of the column. Afterwards identify the most important independent variables. You will base your interpretation on these.\nA positive relationship in a regression means that high values of the independent variable are associated with high values of the dependent variable. A negative relationship means that units which have high values on the independent variable tend to have low values on the dependent variable, and vice versa. Regressions can be run to estimate or test many different relationships. You might run a regression to predict how much more money people earn on average for every additional year of education, or to predict the likelihood of success based on hours practiced in a given sport.\nUse the app below to get a feel for what a regression is and what it does. Below we will talk through the output of the regression table. Fill in values for x and for y and then look to see how the line of best fit changes to capture the average relationship between x and y. As the line changes, so too does the key information in the regression table.\n\n\n\n\n2 What is a regression equation?\nThis is the formula for a regression that contains only two variables:\n\\[Y=α+βX+ε\\]\nThe Y on the left side of the equation is the dependent variable. The α or Alpha coefficient represents the intercept, which is where the line hits the y-axis in the graph, i.e., the predicted value of Y when X equals 0. The β or Beta coefficient represents the slope, the predicted change in Y for each one-unit increase in X.\nIt’s really all about that Beta. The Beta coefficient represents either an increase or a decrease in the rate of ballot stuffing when the independent variable increases. For instance (see the Table), when the presence of observers increases by one unit, the occurrence of ballot stuffing decreases by .037 units, and for every one-unit increase in competition, there was a .019 unit increase in ballot stuffing. Note there is an assumed linear relationship (though different models can relax this): when X goes up by so much, Y goes up or down by so much. The ε is the epsilon or “error term,” representing the remaining variation in Y that cannot be explained by a linear relationship with X.\nWe observe Y and X in our data, but not ε. The coefficients α and β are parameters—unknown quantities that we use the data to estimate.\nA regression with one dependent variable and more than one independent variable is called a multiple regression. This type of regression is very commonly used. It is a statistical tool to predict the value of the dependent variable, using several independent variables. The independent variables can include quadratic or other nonlinear transformations: for example, if the dependent variable Y is earnings, we might include gender, age, and the square of age as independent variables, in which case the assumption of a “linear” relationship between Y and the three regressors actually allows the possibility of a quadratic relationship with age.\nThe example table above examines how the dependent variable, fraud in the form of ballot stuffing, is associated with the following factors/independent variables: election observers, how saturated the area is, the electoral competition in the area, and the density. The regression will show if any of these independent variables help to predict the dependent variable.\n\n\n3 What are the main purposes of regression?\nRegressions can be run for any of several distinct purposes, including (1) to give a descriptive summary of how the outcome varies with the explanatory variables; (2) to predict the outcome, given a set of values for the explanatory variables; (3) to estimate the parameters of a model describing a process that generates the outcome; and (4) to study causal relationships. As Terry Speed writes, the “core” textbook approach to regression “is unlikely to be the right thing in any of these cases. Sharpening the question is just as necessary when considering regression as it is with any other statistical analysis.”\nFor descriptive summaries, there’s a narrow technical sense in which ordinary least squares (OLS) regression gets the job done: OLS shows us the best-fitting linear relationship, where “best” is defined as minimizing the sum of squares of the residuals (the differences between the actual outcomes and the values predicted from the explanatory variables). Furthermore, if we have a sufficiently large sample that was randomly drawn from a much larger population, OLS estimates the best-fitting line in the population, and we can use the estimated coefficients and “robust” standard errors to construct confidence intervals (see section 5) for the coefficients of the population line.2 However, the summary provided by OLS may miss important features of the data, such as outliers or nonlinear relationships; see the famous graphs of Anscombe’s quartet.\nSimilarly, for prediction, OLS regression gives the best linear predictor in the sample, and if the sample is drawn randomly from a larger population, OLS is a consistent estimator of the population’s best linear predictor. However, (a) the best linear predictor from a particular set of regressors may not be the best predictor that can be constructed from the available data, and (b) a prediction that works well in our sample or in similar populations may not work well in other populations. Regression and many other methods for prediction are discussed in the freely downloadable book An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\nEstimating the parameters of a model is the purpose that receives the most discussion in traditional textbooks. However, studying causal relationships is often the real motivation for regression. Many researchers use regression for causal inference but are not interested in all the parameters of the regression model. To estimate an average causal effect of one particular explanatory variable (the treatment) on the outcome, the researchers may regress the outcome on a treatment indicator and other explanatory variables known as covariates. The covariates are included in the regression to reduce bias (in an observational study) or variance (in a randomized experiment), but the coefficients on the covariates are typically not of interest in themselves. Strong assumptions are needed for regression to yield valid inferences about treatment effects in observational studies, but weaker assumptions may suffice in randomized experiments.3\n\n\n4 What are the standard errors, t-statistics, p-values, and degrees of freedom?\nStandard Error\nThe standard error (SE) is an estimate of the standard deviation of an estimated coefficient.4 It is often shown in parentheses next to or below the coefficient in the regression table. It can be thought of as a measure of the precision with which the regression coefficient is estimated. The smaller the SE, the more precise is our estimate of the coefficient. SEs are of interest not so much for their own sake as for enabling the construction of confidence intervals (CIs) and significance tests. An often-used rule of thumb is that when the sample is reasonably large, the margin of error for a 95% CI is approximately twice the SE. However, explicit CI calculations are preferable. We discuss CIs in more detail in the next section.\nThe table above from Asunka et al. shows “robust” standard errors, which have attractive properties in large samples because they remain valid even when some of the regression model assumptions are violated. The key assumptions that “conventional” or “classical” SEs make and robust SEs relax are that (1) the expected value of Y, given X, is a linear function of X, and (2) the variance of Y does not depend on X (conditional homoskedasticity). Robust SEs do assume (unless they are “clustered”) either that the observations are statistically independent or that the treatment was randomly assigned to the units of observation (the polling stations in this example).5\nt-Statistic\nThe t-statistic (in square brackets in the example table) is the ratio of the estimated coefficient to its standard error. T-statistics usually appear in the output of regression procedures but are often omitted from published regression tables, as they’re just a tool for constructing confidence intervals and significance tests.\np-Values and Significance Tests\nIn the table above, if an estimated coefficient (in bold) is marked with one or more asterisks,6 that means the estimate is “statistically significant” at the 1%, 5%, or 10% level—in other words, the p-value (from a two-sided test7 of the null hypothesis that the true coefficient is zero) is below 0.01, 0.05, or 0.1.\nTo calculate a p-value, we typically assume that the data on which you run your regression are a random sample from some larger population. We then imagine that you draw a new random sample many times and run your regression for every new sample. (Alternatively, we may imagine randomly assigning some treatment many times. See our guide on hypothesis testing for more details.) This procedure would create a distribution of estimates and t-statistics. Given this distribution, the p-value captures the probability that the absolute value of the t-statistic would have been at least as large as the value that you actually observed if the true coefficient were zero. If the p-value is greater than or equal to some conventional threshold (such as 0.05 or 0.1), the estimate is “not statistically significant” (at the 5% or 10% level). According to convention, estimates that are not statistically significant are not considered evidence that the true coefficient is nonzero.\nIn the table, the only estimated coefficient that is statistically significant at any of the conventional levels is the intercept (which is labeled “Constant/Intercept” because in the algebra of regression, the intercept is the coefficient on the constant 1). The intercept is the predicted value of the outcome when the values of the explanatory variables are all zero. In this example, the question of whether the true intercept is zero is of no particular interest, but the table reports the significance test for completeness. The research question is about observer effects on ballot stuffing (as shown in the heading of the table). The estimated coefficient on “Observer Present (OP)” is of main interest, and it is not statistically significant.\nIt is easy to misinterpret p-values and significance tests. Many scholars believe that although significance tests can be useful as a restraining device, they are often overemphasized. Helpful discussions include the American Statistical Association’s 2016 statement on p-values; the invited comments on the statement, especially Sander Greenland et al.’s “Statistical Tests, P values, Confidence Intervals, and Power: A Guide to Misinterpretations” (republished here); and short posts by David Aldous and Andrew Gelman.\nF-Test and Degrees of Freedom\nThe bottom section of the table includes a row with the heading “F(5, 59)”, the value 1.43 (the F-statistic), and the p-value .223. This F-test is a test of the null hypothesis that the true values of the regression coefficients, excluding the intercept, are all zero. In other words, the null hypothesis is that none of the explanatory variables actually help predict the outcome. In this example, the p-value associated with the F-statistic is 0.223, so the null hypothesis is not rejected at any of the conventional significance levels. However, since our main interest is in the effects of observers, the F-test isn’t of much interest in this application. (We already knew that the estimated coefficient on “Observer Present” is not statistically significant, as noted above.)\nThe numbers 5 and 59 in parentheses are the degrees of freedom (df) associated with the numerator and denominator in the F-statistic formula. The numerator df (5) is the number of parameters that the null hypothesis claims are zero. In this example, those parameters are the coefficients on the 5 explanatory variables shown in the table. The denominator df (59) equals the sample size minus the total number of parameters estimated. (In this example, the sample size is 2,004 and there are only 6 estimated parameters shown in the table, but the regression also included many dummy variables for constituencies that were used in blocking.)\n\n\n5 What the confidence intervals mean\nConfidence intervals (CIs) are frequently reported in social science research papers and occasionally shown in regression tables. They communicate some of the uncertainty in estimation: for example, the point estimate of the coefficient on “Observer Present” is a specific value, –0.037, but the CI (calculated as the point estimate plus or minus a margin of error) is the range of values from –0.09 to 0.01, implying that any value in that range is compatible with the data. (In other words, having an observer present may have reduced the rate of ballot stuffing by 9 percentage points, or it may have actually increased the rate by 1 percentage point, or the effect may have been somewhere in between.) The coverage probability (or confidence level) of a CI is the probability that the CI contains the true value of the parameter. Reported CIs usually have a nominal (claimed) coverage probability of 95%, so they are called 95% confidence intervals.\nCoverage probabilities are easy to misinterpret. In the example table, (–0.09, 0.01) is the 95% CI for the observer effect on ballot stuffing. This does not mean that there’s a 95% probability that the true effect was in the range between –0.09 and 0.01. Statements of that nature can be made in Bayesian statistics (with posterior intervals, also known as credible intervals), but confidence intervals are a construct of frequentist statistics. The coverage probability answers the following question: Imagine that we can replicate the experiment a large number of times, and the only thing that varies from one replication to another is which units are randomly assigned to treatment.8 How often will the CI capture the true effect of election observers? In this framework, the observer effect is fixed, but the endpoints of the CI are random. For example, if the true effect is –0.02, then it is –0.02 on every replication. But because different units are randomly assigned to treatment on each replication, we could see the following CIs in three replications of the experiment: (–0.10, -0.01), (-0.03, 0.03), and (0.00, 0.10). The first and second CIs capture the true value of –0.02, but the third misses it. The nominal coverage probability of 95% means that in a million replications, about 950,000 of the CIs would capture the true value of –0.02. It’s a claim about the ex ante reliability of our method for reporting a range, not about the ex post probability that the true observer effect is in the range between –0.09 and 0.01.\nGreenland et al. give a helpful discussion of the benefits, limitations, and common misinterpretations of CIs. As they note, “many authors agree that confidence intervals are superior to tests and P values because they allow one to shift focus away from the null hypothesis, toward the full range of effect sizes compatible with the data—a shift recommended by many authors and a growing number of journals.” However, “the confidence interval is computed from many assumptions, the violation of which may have led to the results. Thus it is the combination of the data with the assumptions, along with the arbitrary 95% criterion, that are needed to declare an effect size outside the interval is in some way incompatible with the observations. Even then, judgements as extreme as saying the effect size has been refuted or excluded will require even stronger conditions.”\nThe CONSORT Explanation and Elaboration document notes that in medicine, “Many journals require or strongly encourage the use of confidence intervals.” In the social sciences, CIs aren’t always explicitly reported; some authors report only point estimates and standard errors. If the degrees of freedom for the distribution of the t-statistic are reported, readers with sufficient technical background can construct a CI on their own (although it would obviously be more helpful if authors reported CIs explicitly). In our example table, the df for the t-statistics is the same as the denominator df (59) for the F-statistic. To construct the margin of error for a 95% CI, we multiply the SE by the appropriate critical value, the 0.975 quantile of the t-distribution with 59 degrees of freedom, which is 2.001 (in R, use the command qt(.975, df = 59)). Thus, the rule of thumb that we mentioned in the section on SEs (“the margin of error for a 95% CI is approximately twice the SE”) works well here. However, if we had, say, only 20 degrees of freedom, the appropriate critical value would be about 2.09, and the 95% CI should be wider than the rule of thumb would suggest.\n\n\n6 Watch out for “researcher degrees of freedom”\nThe SEs, p-values, significance tests, and CIs reported in regression tables typically assume that the researchers would have made all the same analytic decisions (which observations and variables to include in the regression, which hypothesis to test, etc.) if the outcome data had shown different patterns or if (in a randomized experiment) different units had been randomly assigned to treatment. This assumption is credible if all those decisions were pre-specified before the researchers saw any data on outcomes or treatment assignments. Otherwise, researchers may make decisions that consciously or unconsciously tilt a study toward a desired result. This problem is known as “fishing”9, “researcher degrees of freedom,”10 or “the garden of forking paths.”11\nIn an instructive and entertaining paper, Joseph Simmons, Leif Nelson, and Uri Simonsohn use simulations as well as actual experiments to show how easy it is for researcher degrees of freedom to invalidate significance tests. In simulations, they show that when researchers have unlimited discretion about which outcome to analyze, when to stop recruiting subjects, how to model the effect of a covariate, and which treatment conditions to include in the analysis, a significance test that claims to have a Type I error probability (false-positive rate) of 5% can easily be made to have an actual Type I error probability as high as 61%. In other words (and as said in the paper’s title), “Undisclosed flexibility in data collection and analysis allows presenting anything as significant.” Allowing themselves unlimited flexibility in the data collection and analysis for an actual experiment, Simmons et al. manage to reach the necessarily false conclusion that listening to the Beatles’ song “When I’m Sixty-Four” made the subjects “nearly a year-and-a-half younger,” with a p-value of .04.\nOne remedy is for researchers to pre-specify and publicly archive their decisions about data collection and analysis (e.g., stopping rules, outcome measures, covariates, regression models, sample exclusions, and subgroup definitions) before they see the outcome data (and, ideally, before they assign treatments). Documents of these decisions are known as pre-analysis plans (PAPs). Critics of PAPs worry that they inhibit exploratory data analysis. Proponents argue that deviations from the plans are not prohibited, but should be fully disclosed and highlighted, to help readers distinguish between exploratory and confirmatory analyses. For valuable discussions, see the symposia in Political Analysis (Winter 2013) and Journal of Economic Perspectives (Summer 2015). Also take a look at our guide on pre-registration.\n\n\n7 Watch out for other possible biases\nJust because you use a regression to estimate a relationship does not mean that the relationship you estimate truly captures the type of relationship you are interested in. Here are some of the possible sources of bias to be aware of:\n\nSelection bias can arise when there are systematic, unmeasured differences in characteristics between the individuals who are selected into the sample or the treatment and those who are not selected. In other words, selection bias can refer to either of two concerns:\n\nIf treatment is determined by some process other than random assignment (e.g., if subjects self-select into treatment), then treated subjects may differ from untreated subjects in ways that affect the outcome. Such differences can easily lead to bias in a regression of the outcome on treatment, even if measured characteristics of the subjects are included as covariates, because treated and untreated subjects may differ in unmeasured ways.\nIf the sample that is included in the regression isn’t a random sample of the population of interest, then the regression may yield biased estimates of the population relationship between the outcome and the explanatory variables.\n\nAttrition bias is a form of selection bias that can occur when outcome data are missing for a nonrandom subset of the original sample. In studies of treatment effects, attrition bias can be especially challenging to address if the treatment may have affected attrition (the loss of outcome data): when the rates or patterns of attrition differ between treated and untreated subjects, even a randomized experiment may not yield unbiased treatment effect estimates for any population.12 See our guide on missing data for details.\nSimilarly, if the treatment affects the measurement of the outcome, the symmetry that random assignment created is threatened, and estimated treatment effects may be biased even in a randomized experiment.\nAdjustment for covariates that may have been affected by the treatment can lead to bias, as explained in 10 Things to Know About Covariate Adjustment.13\nPublication bias, also known as the file drawer problem, arises when entire studies go unpublished not because their quality is any lower than that of other studies on the same topic, but because of the nature of their results (e.g., because the results are considered unsurprising, or because they do not reach conventional thresholds for statistical significance). As Robert Rosenthal wrote in a classic article, “The extreme view of the ‘file drawer problem’ is that journals are filled with the 5% of the studies that show Type I errors, while the file drawers are filled with the 95% of the studies that show nonsignificant results.”14\n\n\n\n8 What the R² means\nR² is the squared multiple correlation coefficient, also known as the Coefficient of Determination. R² shows the proportion of the variance of the outcome that is “explained” by the regression. In other words, it is the variance of the outcome values predicted from the explanatory variables, divided by the variance of the actual outcome values. The larger the R² is, the better the fit of the regression model. And a model fits the data well if the differences between the actual values and the values predicted by the regression are small. The R² is generally of secondary importance, unless your main concern is using the regression equation to make accurate predictions. It is always between 0 and 1, so if the independent variables are strong predictors, the R² will be closer to 1. It is possible, however, that a statistically significant relationship between X and Y is found even though the R² is low; this just means we have evidence of a relationship between X and Y, but X does not explain a large proportion of the variation in Y.\nIn the example table, the R² value is .011, showing that in this case, the explanatory variables account for only a small portion of the variance of the outcome. If a model could explain all of the variance, the values predicted by the regression would always equal the actual values observed, so the regression line would fit the data perfectly and the R² would equal 1.\nAlthough R² summarizes how well the model fits the data, any single-number summary has limitations. In Anscombe’s quartet, all four regressions have the same R², but the four graphs look very different.\n\n\n9 Be careful when comparing coefficients\nIf one coefficient is bigger than another, does that mean the outcome is more sensitive to that explanatory variable? No—the interpretation of coefficients depends on the scales the variables are measured on. If you convert an explanatory variable from feet to miles, the coefficient will get a lot bigger, without any real change in the underlying relationship between the explanatory variable and the outcome.\n\n\n10 Meet the whole family\nSo far, this guide has focused on ordinary least squares regression, one of the most commonly used estimation methods in the social sciences. In fact, there are many other regression methods, including weighted least squares and generalized least squares, as well as all sorts of nonlinear models for limited dependent variables—outcomes that are limited to a particular range of values, such as binary (0/1), categorical (A,B,C,…), or count (0,1,2,…) outcomes.\n\nResearchers might use a weighted least squares regression when the variance of ε differs from one observation to another and can be modeled as a function of one or more predictors (this is called heteroskedasticity, which generally looks something like this).15\n\n\n\nYou might see a logit or a probit regression when the outcome is binary, meaning it has only two possible values: yes/no, 0/1, or True/False. Logit and probit differ in terms of the assumptions about the underlying data-generating process, but they often yield similar results.16\nOrdered logit and ordered probit models may be used for outcomes with multiple ordered categories (such as “strongly disagree,” “disagree,” “agree,” “strongly agree”).\nMultinomial logit or multinomial probit models may be used for outcomes with multiple unordered categories (“Labour,” “Conservative,” “Lib Dem”).\nPoisson or Negative Binomial models may be used when the outcome is a count (“how many riots this year”).\nTobit models are sometimes used for non-negative outcomes (“How much time spent working this month”).\nand many more …\n\nFor the simple linear case, the coefficient tells you the change in Y you get for each unit change in X, but for nonlinear regressions the interpretation can be much more difficult. For nonlinear models you should generally expect authors to provide substantive interpretations of the coefficients. The program Clarify by Gary King and colleagues helps with this for Stata users; the Zelig package for R (also by King and coauthors) supports analysis and interpretation of these models in R.\n\n\n\n\n\nFootnotes\n\n\nOriginating author: Abby Long. Revisions: Winston Lin, 21 July 2016. The guide is a live document and subject to updating by EGAP members at any time; contributors listed are not responsible for subsequent edits. Thanks to Don Green, Macartan Humphreys, and Tod Mijanovich for helpful discussions.↩︎\nFor in-depth discussions, see: Joshua D. Angrist & Jörn-Steffen Pischke (2009), Mostly Harmless Econometrics, chapters 3 and 8; Richard A. Berk et al. (2014), “Misspecified Mean Function Regression: Making Good Use of Regression Models That Are Wrong,” Sociological Methods and Research 43: 422–451; Andreas Buja et al., “Models as Approximations—A Conspiracy of Random Regressors and Model Misspecification Against Classical Inference in Regression,” working paper; Bruce Hansen, Econometrics, online textbook.↩︎\nOn regression in observational studies, see: 10 Strategies for Figuring out if X Caused Y; David A. Freedman (1991), “Statistical Models and Shoe Leather” (with discussion), Sociological Methodology 21: 291–358; Angrist & Pischke, Mastering ’Metrics: The Path from Cause to Effect (2015) and Mostly Harmless Econometrics; Guido W. Imbens & Jeffrey M. Wooldridge (2009), “Recent Developments in the Econometrics of Program Evaluation,” Journal of Economic Literature 47: 5–86; Imbens (2015), “Matching Methods in Practice: Three Examples,” Journal of Human Resources 50: 373–419. On regression adjustment in randomized experiments, see 10 Things to Know About Covariate Adjustment and Winston Lin’s Development Impact blog posts (here and here).↩︎\nStrictly speaking, the true SE is the standard deviation of the estimated coefficient, while what we see in the regression table is the estimated SE. However, in common parlance, people often say “standard error” when they mean the estimated SE, and we’ll do the same.↩︎\nRobust SEs are also known as Huber–White or sandwich SEs. On the properties of robust SEs, see: Mostly Harmless Econometrics, section 3.1.3 and chapter 8; Guido W. Imbens & Michal Kolesár (2016), “Robust Standard Errors in Small Samples: Some Practical Advice,” Review of Economics and Statistics 98: 701–712; Charles S. Reichardt & Harry F. Gollob (1999), “Justifying the Use and Increasing the Power of a t Test for a Randomized Experiment with a Convenience Sample,” Psychological Methods 4: 117–128; Cyrus Samii & Peter M. Aronow (2012), “On Equivalencies Between Design-Based and Regression-Based Variance Estimators for Randomized Experiments,” Statistics and Probability Letters 82: 365–370; Winston Lin (2013), “Agnostic Notes on Regression Adjustments to Experimental Data: Reexamining Freedman’s Critique,” Annals of Applied Statistics 7: 295–318; Alberto Abadie, Susan Athey, Guido W. Imbens, & Jeffrey M. Wooldridge (2014), “Finite Population Causal Standard Errors”, NBER Working Paper No. 20325.↩︎\nThe use of asterisks to flag statistically significant results is common but not universal. Our intention here is merely to explain what the asterisks mean, not to recommend that they should or should not be used.↩︎\nTwo-sided tests are the default in most software packages and in some research fields, so when tables do not explicitly note whether the p-values associated with regression coefficients are one- or two-sided, they are usually two-sided. Ben Olken (2015, pp. 67, 70) notes that since “convention typically dictates two-sided hypothesis tests,” researchers who prefer one-sided tests should commit to that choice in a pre-analysis plan so “they cannot be justly accused of cherry-picking the test after the fact.” Sander Greenland et al. (2016, p. 342) argue against the view that one should always use two-sided p-values, but write, “Nonetheless, because two-sided P values are the usual default, it will be important to note when and why a one-sided P value is being used instead.”↩︎\nThe framework where “the only thing that varies from one replication to another is which units are randomly assigned to treatment” is known as randomization-based inference. This isn’t the only framework for frequentist inference. In the classical regression framework, the only thing that varies is that on each replication, different values of ε are randomly drawn. And in the random sampling framework, on each replication a different random sample is drawn from the population. On randomization-based inference, see the Reichardt & Gollob, Samii & Aronow, Lin, and Abadie et al. references in note 5; on the random sampling framework, see the references in note 2.↩︎\nSee, e.g., Macartan Humphreys, Raul Sanchez de la Sierra, & Peter van der Windt (2013), “Fishing, Commitment, and Communication: A Proposal for Comprehensive Nonbinding Research Registration,” Political Analysis 21: 1–20.↩︎\nJoseph P. Simmons, Leif D. Nelson, & Uri Simonsohn (2011), “False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant,” Psychological Science 22: 1359–1366.↩︎\nAndrew Gelman & Eric Loken (2013), “The Garden of Forking Paths: Why Multiple Comparisons Can Be a Problem, Even When There Is No ‘Fishing Expedition’ or ‘P-Hacking’ and the Research Hypothesis Was Posited Ahead of Time,” preprint.↩︎\nFor more discussion of attrition bias in randomized experiments, see, e.g., Alan S. Gerber & Donald P. Green (2012), Field Experiments: Design, Analysis, and Interpretation, chapter 7.↩︎\nSee also: Mostly Harmless Econometrics, section 3.2.3; Paul R. Rosenbaum (1984), “The Consquences of Adjustment for a Concomitant Variable That Has Been Affected by the Treatment,” Journal of the Royal Statistical Society. Series A (General) 147: 656-666.↩︎\nRobert Rosenthal (1979), “The ‘File Drawer Problem’ and Tolerance for Null Results,” Psychological Bulletin 86: 638–641. On reforms to counter publication bias, see: Brendan Nyhan (2015), “Increasing the Credibility of Political Science Research: A Proposal for Journal Reforms,” PS: Political Science and Politics 48 (S1): 78–83; Michael G. Findley, Nathan M. Jensen, Edmund J. Malesky, and Thomas B. Pepinsky (forthcoming), “Can Results-Free Review Reduce Publication Bias? The Results and Implications of a Pilot Study,” Comparative Political Studies.↩︎\nWeighting by the inverse of the variance of ε is a form of generalized least squares (GLS). The classical argument is that GLS is more efficient (i.e., has lower variance) than OLS under heteroskedasticity. However, when the goal is to estimate an average treatment effect, some researchers question the relevance of the classical theory, because if treatment effects are heterogeneous, GLS and OLS are not just more efficient and less efficient ways to estimate the same treatment effect. Instead, they estimate different weighted average treatment effects. In other words, they answer different questions, and choosing GLS for efficiency is arguably like looking for your keys where the light’s better. In “The Credibility Revolution in Empirical Economics,” Angrist & Pischke (2010, pp. 11–12) write: “Today’s applied economists have the benefit of a less dogmatic understanding of regression analysis. Specifically, an emerging grasp of the sense in which regression and two-stage least squares produce average effects even when the underlying relationship is heterogeneous and/or nonlinear has made functional form concerns less central. The linear models that constitute the workhorse of contemporary empirical practice usually turn out to be remarkably robust, a feature many applied researchers have long sensed and that econometric theory now does a better job of explaining. Robust standard errors, automated clustering, and larger samples have also taken the steam out of issues like heteroskedasticity and serial correlation. A legacy of White’s (1980a) paper on robust standard errors, one of the most highly cited from the period, is the near death of generalized least squares in cross-sectional applied work. In the interests of replicability, and to reduce the scope for errors, modern applied researchers often prefer simpler estimators though they might be giving up asymptotic efficiency.” Similarly, Jim Stock (2010, p. 85) comments: “The 1970s procedure for handling potential heteroskedasticity was either to ignore it or to test for it, to model the variance as a function of the regressors, and then to use weighted least squares. While in theory weighted least squares can yield more statistically efficient estimators, modeling heteroskedasticity in a multiple regression context is difficult, and statistical inference about the effect of interest becomes hostage to the required subsidiary modeling assumptions. White’s (1980) important paper showed how to get valid standard errors whether there is heteroskedasticity or not, without modeling the heteroskedasticity. This paper had a tremendous impact on econometric practice: today, the use of heteroskedasticity-robust standard errors is standard, and one rarely sees weighted least squares used to correct for heteroskedasticity.” (Emphasis added in both quotations.)↩︎\nLogit, probit, and other limited dependent variable (LDV) models do not immediately yield estimates of the average treatment effect (ATE). To estimate ATE, one needs to compute an average marginal effect (or average predictive comparison) after estimating the LDV model (see, e.g., Andrew Gelman & Iain Pardoe [2007], “Average Predictive Comparisons for Models with Nonlinearity, Interactions, and Variance Components,” Sociological Methodology 37: 23–51). Some researchers argue that the complexity of marginal effect calculations for LDV models is unnecessary because OLS tends to yield similar ATE estimates (see Mostly Harmless Econometrics, section 3.4.2, and the debate between Angrist and his discussants in “Estimation of Limited Dependent Variable Models with Dummy Endogenous Regressors” [2001], Journal of Business and Economic Statistics 19: 2–28). In randomized experiments, the robustness of OLS is supported by both asymptotic theory and simulation evidence. For theory, see Lin, “Agnostic Notes on Regression Adjustments to Experimental Data.” For simulations, see Humphreys et al., “Fishing, Commitment, and Communication,” and David R. Judkins & Kristin E. Porter (2016), “Robustness of Ordinary Least Squares in Randomized Clinical Trials,” Statistics in Medicine 35: 1763–73. See also Lin’s comments on this MHE blog post.↩︎"
  },
  {
    "objectID": "guides/getting-started/multiple-comparisons_en.html",
    "href": "guides/getting-started/multiple-comparisons_en.html",
    "title": "10 Things You Need to Know About Multiple Comparisons",
    "section": "",
    "text": "Abstract\nThe “Multiple Comparisons Problem” is the problem that standard statistical procedures can be misleading when researchers conduct a large group of hypothesis tests. When a researcher does more than one test of a hypothesis (or set of closely related hypotheses), the chances are that some finding will appear “significant” even when there’s nothing going on.\nClassical hypothesis tests assess statistical significance by calculating the probability under a null hypothesis of obtaining estimates as large or larger as the observed estimate. When multiple tests are conducted, however, classical p-values can mislead — they no longer reflect the true probability under the null.\nThis guide 1 will help you guard against drawing false conclusions from your experiments. We focus on the big ideas and provide examples and tools that you can use in R.\n\n\n1: Almost every social science experiment faces a multiple comparisons problem\nTypically, researchers are not interested in just one treatment versus control comparison per experiment. There are three main ways that comparisons proliferate:\n\nMultiple treatment arms. When an experiment has \\(n\\) treatment arms, there are \\(n(n-1)/2\\) possible comparisons between the arms.\nHeterogeneous treatment effects. Often, we are interested in whether the treatment has different impacts on different subgroups. For example, a treatment might be more effective for women than for men.\nMultiple estimators. Often, experimenters will apply multiple estimators to the same dataset: for example, difference-in-means and covariate adjustment. There is of course nothing wrong with employing multiple treatment arms, exploring treatment effect heterogeneity, or using multiple estimators of the treatment effect. However, these design and analysis choices sometimes require that researchers correct their statistical tests to account for multiple comparisons.\nMultiple outcomes. Researchers often assess the effects of an intervention on multiple distinct outcomes or multiple operationalizations of the outcome variable.\n\nThese concerns are especially problematic when making a “family claim,” that is, when you are summarizing a series of results. For example, a family claim might be that treatments A, B, C, and D had no effect, but treatment E did. Or, similarly, the treatment had no effect among group 1, group 2, or group 3, but had a strong effect among group 4.\nThe multiple comparisons problem is related to, but different from, the problem of “fishing.” Fishing occurs when an unscrupulous analyst conducts many tests but only reports the “interesting” ones. In essence, fishing withholds the necessary information we would need in order to correct for multiple comparisons.\n\n\n2: Why multiple comparisons are a problem\nLet’s set up 1 test: we have 1 coin and we will flip it 10 times. We don’t know if the coin is fair but decide that if we flip over 9 heads and 1 tails, we will say that the coin is unfair. Let’s suppose that this is a fair coin; a coin that has a 50% chance of showing a head in a single flip. We are therefore not likely to flip over 9 heads and 1 tails and will most likely find that the coin is fair.\nNow, let’s set up 5 tests: we have 5 coins and we will flip each of them 10 times. Again, we decide that the coin is unfair if we flip 9 heads and 1 tails; again, let’s suppose that all 5 coins are fair. The chance that we see 9 heads and 1 tails for a single coin is still very low. However, after doing 5 tests, it is much more likely that at least 1 of the coins will show 9 heads and 1 tails purely by chance. This result may then lead us to think that some or all of the coins are unfair, even though in reality all the coins are fair.\nWe can see from the coin flipping example that the more tests we run, the more likely we are to see an effect or relationship and the more likely we are to mistakenly claim to have detected an effect when in reality there is no effect and what we observe occurred by chance; this is the multiple comparisons problem.\nIn this guide, we will describe three main approaches for addressing the multiple comparisons problem:\n\np-value adjustments. Statisticians have derived a number of corrections that can guard against multiple comparisons mistakes. As described in the next section, these corrections control either the Family-Wise Error Rate (FWER) or the False Discovery Rate (FDR). Most of these adjustments apply a simple formula to a series of “raw” p-values; we will also describe a simulation method that can take account of features of a specific research setting.\nPre-analysis plans. These plans are a powerful design-based tool that enables an analyst to pro-actively manage the multiple comparisons problem.\nReplication. If we are concerned that a finding is simply an artifact of sampling variability that we happened to discover because of a naive repeated application of classical hypothesis testing, then the best way to resolve the question is to conduct the experiment again.\n\n\n\n3: Don’t mix up the FWER and FDR!\nIn classical hypothesis testing, the “\\(\\alpha\\) level” describes how willing the researcher is to make a certain kind of mistake: a false positive or a “Type I error” where a researcher falsely concludes that an observed difference is “real,” when in fact there is no difference. From the coin flipping example, concluding that we have an unfair coin because we flipped 9 heads and 1 tails when in reality we have fair coins is an example of a Type I error or false positive error. After setting the \\(\\alpha\\) level, the researcher conducts a hypothesis test and if the p-value \\(\\leq \\alpha\\), we call the result “statistically significant”. In many social science applications, the alpha level, or Type I error rate, is set to 0.05. This means that the researcher is willing to commit a Type I error 5% of the time.\nIn the world of multiple testing, the Type I error rate is associated with one of two things: the Family-Wise Error Rate (FWER) or the False Discovery Rate (FDR). To explain their differences, let’s first consider Table 1 which shows different types of errors. After conducting our hypothesis tests, we observe \\(m\\) (the total number of hypothesis tests), \\(R\\) (the total number of hypothesis tests that are statistically significant), \\(m-R\\), (the total number of hypothesis tests that are not statistically significant). However, we do not know how many tests from \\(R\\) are false positives (FP, Type I error) or true positives (TP).\n\n\n\n\n\n\n\n\n\n \nFail to reject null hypothesis (p > 0.05)\nReject null hypothesis (p \\(\\leq\\) 0.05)\nTotal Hypotheses\n\n\n\n\nNull hypothesis is true\nTN  (True Negative)\nFP  (Type I error, False Positive)\n\\(m_0\\)\n\n\nNull hypothesis is false\nFN  (Type II error, False Negative)\nTP  (True Positive)\n\\(m - m_0\\)\n\n\nTotal Hypotheses\n\\(m - R\\)\n\\(R\\)\n\\(m\\)\n\n\n\nThe FWER is the probability of incorrectly rejecting even one null hypothesis, or \\(P(FP \\geq 1)\\) across all of our tests. Suppose we have three null hypotheses, all of which are true. When the null hypothesis is true, but we nevertheless reject it in favor of some alternative, we commit a Type I error. If we set \\(\\alpha\\) (the Type I error rate) to be 0.05, we have a [\\(1−(1−0.05)^3=0.142\\)] chance of rejecting at least one of them. In order to control the FWER (i.e., reduce it from 14.2% back down to 5%), we need to employ a correction. We’ll explore three ways to control the FWER (Bonferroni, Holm, and simulation) in the sections below.\nThe FDR is subtly different. It is the expected proportion of false discoveries among all discoveries, or \\(E[FP/R]\\)2. In the case where no discoveries are found (\\(R=0\\)), then the “share” of false discoveries is taken to be zero. There are some connections to the FWER. For example, if in fact all null hypotheses are true then the FWER and FDR are the same. To see this, note that in this case if no significant rejections of the null are found, then the share that are false discoveries is zero. But if some are found (no matter how many), then the share that are false is 100% (since all null hypotheses are true). So the false discovery rate in this case is just the probability that some true null hypothesis is falsely rejected—the FWER. Beyond this case however the FDR is less stringent than the FWER. We’ll also explore some ways to control FDR in the sections below.\n\n\n4: Control the FWER with Bonferroni-style corrections (including the Holm correction)\nThe Bonferroni correction is a simple and commonly-used approach for addressing the multiple comparisons problem although it usually understates how much information is available to detect an effect. If you conduct \\(m\\) tests but want to make sure that you make no more than \\(\\alpha\\) errors out of the total \\(m\\) tests, the target significance level should be \\(\\alpha/m\\), or, equivalently, you multiply your p-values by \\(m\\), and apply the standard \\(\\alpha\\) level. (The trouble with multiplying the p-values is sometimes you end up with values over one, rendering the interpretation of the p-values incoherent so software just replaces those p-values with the number 1.)\nFor example, suppose you conduct an experiment that has 3 dependent variables. You conduct three difference-in-means tests that yield the following classical p-values: 0.004, 0.020, and 0.122. If your \\(\\alpha\\) level is the standard 0.05 threshold, then you would usually declare the first two tests statistically significant and the last test insignificant. The Bonferroni correction, however, adjusts the target p-value to \\(0.05/3 = 0.016\\). We then declare only the first test to be statistically significant.\nThe Bonferroni correction works under the most extreme circumstances, that is, when all \\(m\\) tests are independent from one another. To see how this works, imagine we are testing three true null hypotheses using a classical \\(\\alpha\\) level of 0.05. Each test, therefore has a 5% chance of yielding the wrong answer that the null hypothesis is false.\nBut our chances of making at least one mistake are much greater than 5% because we have three chances to get it wrong. As above, this probability is in fact [\\(1 - (1 - 0.05)^3 = 0.142\\)]. If we use the Bonferroni correction, however, our chances of getting it wrong fall back to our target \\(\\alpha\\) value: [\\(1 - (1 - 0.05/3)^3 \\approx 0.05\\)] .\nThis correction works in the worst-case scenario that all tests are independent. But in most cases, tests are not independent. That is, if your treatment moves outcome A, it probably moves outcome B too, at least a little. So what tends to happen is, researchers report that their results “withstand” Bonferroni when they are extremely strong, but decry Bonferroni as too extreme when the results are weaker.\nInstead of using the Bonferroni correction, you can use the Holm correction. It is strictly more powerful than Bonferroni, and is valid under the same assumptions. It also controls the FWER. Suppose you have \\(m\\) p-values. Order them from smallest to largest. Find the smallest p-value that satisfies this condition: \\(p_{k}>\\frac{\\alpha}{m+1−k}\\), where \\(k\\) is the p-value’s index. This and all larger p-values are insignificant; all smaller p-values are significant.\nTaking our three p-values from above: 0.004, 0.020, and 0.122: \\[0.004<\\frac{0.05}{3+1−1}=0.017\\] \\[0.020<\\frac{0.05}{3+1−2}=0.025\\] \\[0.122>\\frac{0.05}{3+1−3}=0.050\\]\nUnder the Holm correction, the first two tests are significant, but the last test is not.\n\n\n5: Control the FDR with Benjamini-Hochberg\nThe Benjamini–Hochberg (B-H) procedure controls the FDR. Like the Holm correction, you also begin by ordering \\(m\\) p-values. Then you find the largest p-value that satisfies: \\(p_{k}≤\\frac{k}{m}\\alpha\\). This test, and all tests with smaller p-values are declared significant.\n\\[0.004<\\frac{1}{3}0.05=0.017\\] \\[0.020<\\frac{2}{3}0.05=0.033\\] \\[0.122>\\frac{3}{3}0.05=0.050\\]\nUsing the Benjamini–Hochberg procedure, the first two tests are significant, but the third is not. Notice that the FDR does not control the same error rate as the FWER. It is common for those controlling the FDR error rate to take “significant tests” to indicate places to direct more attention for confirmatory tests (which, would then be analyzed using the stricter FWER control). The idea that one uses the FDR to explore and FWER to confirm is also common in genetics and other applications of large scale hypothesis testing.\n\n\n6: It’s easy to implement these procedures\nIn R, the p.adjust() function contains many of the corrections devised by statisticians to address the multiple comparisons problem. The p.adjust() function is in base R, so no additional packages are required. The p.adjust() function gives adjusted p-values after implementing a correction.\n\n\nCode\n# Set seed for reproducibility\nset.seed(343)\n \n# Generate 50 test statistics\n# Half are drawn from a normal with mean 0\n# The other half are drawn from a normal with mean 3\nx <- rnorm(50, mean = c(rep(0, 25), rep(3, 25)))\n \n# Obtain 50 p-values\np <- round(2*pnorm(sort(-abs(x))), 3)\n \n# Choose alpha level\nalpha <- 0.05\n \n# Without any corrections\nsig <- p < alpha\n \n# Conduct three corrections\n# and compare to target alpha\nbonferroni_sig <- p.adjust(p, \"bonferroni\") < alpha\nholm_sig <- p.adjust(p, \"holm\") < alpha\nBH_sig <- p.adjust(p, \"BH\") <alpha\n\n\nThe results of this simulation are presented in the table and figure below.\n\n\n\n\n\n\n\n\n\n\nCorrection Type\nNo Correction\nBenjamini-Hochberg\nHolm\nBonferroni\n\n\n\n\nStatistically Significant\n25\n22\n11\n8\n\n\nNot Statistically Significant\n25\n28\n39\n42\n\n\n\n\n\n\n\n\nOf the 25 null hypotheses that would be rejected if no correction were made, the Bonferroni correction only rejects 8, the Holm procedure rejects 11, and the Benjamini–Hochberg procedure rejects 22 (or tags 22 hypotheses as promising for future exploration). Of these three corrections, Bonferroni is the most stringent while Benjamini–Hochberg is the most lenient.\nInstead of R, you can also use this calculator to adjust your p-values.\nThis calculator works best in Firefox. To use full-screen, go here.\n\n\n\n\n7: A better way to control the FWER is simulation\nThe trouble with the corrections above is that they struggle to address the extent to which the multiple comparisons are correlated with one another. A straightforward method of addressing this problem is simulation under the sharp null hypothesis of no effect for any unit on any dependent variable. Note that this is a family-wise sharp null.\nIf the treatment has no effect at all on any outcome, then we observe all potential outcomes for all subjects. We can re-randomize the experiment 1000 or more times and conduct all \\(m\\) hypothesis tests each time. We know for sure that all \\(m\\) null hypotheses are true, because the treatment has no effect by construction.\nThe next step is picking the right threshold value below which results are deemed statistically significant. If \\(\\alpha\\) is 0.05, we need to find the target p-value that, across all simulations under the sharp null, yields 5% significant hypothesis tests.\nOnce we have the right threshold value, it’s as easy as comparing the uncorrected p-values to the threshold value — those below the threshold are deemed significant.\n\n\nCode\n# Control the FWER through simulation\nrm(list=ls())\nlibrary(mvtnorm)\nlibrary(randomizr)\n# Helper functions\ndo_t_test <- function(Y, Z){\n  t.test(Y[Z==1], Y[Z==0])$p.value\n}\npermute_treat <- function(){\n  treatment_sim <- complete_ra(n, m=n/2)\n  ps_sim <- apply(outcomes, 2, do_t_test, Z = treatment_sim)\n  return(ps_sim)\n}\nthreshold_finder<- function(threshold){\n  mean(apply(many_ps, 2, x <- function(x) sum(x <= threshold) > 0 ))\n}\n# Set a seed\nset.seed(343)\n# Generate correlated outcomes\n# Outcomes are unrelated to treatment\n# All null hypotheses are true\nn <- 1000\nk <- 100; r <- .7; s <- 1\nsigma <- matrix(s*r, k,k)\ndiag(sigma) <- s\noutcomes <- rmvnorm(n=n, mean=rep(0, k), sigma=sigma)\n# Complete Random Assignment\ntreatment <- complete_ra(n, m=n/2)\n# Conduct k hypothesis tests\np_obs <- apply(outcomes, 2, do_t_test, Z = treatment)\n# Simulate under the sharp null\nmany_ps <- replicate(1000, permute_treat(), simplify = TRUE)\n# Obtain the Type I error rate for a series of thresholds\nthresholds <- seq(0, 0.05, length.out = 1000)\ntype_I_rate <- sapply(thresholds, threshold_finder)\n# Find the largest threshold that yields an alpha type I error rate\ntarget_p_value <- thresholds[max(which(type_I_rate <=0.05))]\n# Apply target p_value to observed p_values\nsig_simulated <- p_obs <= target_p_value\n# Compare to raw p-values\nsig <- p_obs <= 0.05\n\n\nThe target p-value obtained by the simulation is 0.002 — hypothesis tests with raw p-values below 0.002 are deemed significant. Compare this with the Bonferroni method, which would require a p-value below 0.05/100 = 0.0005, an order of magnitude smaller. The closer the correlation of the tests (the parameter “r” in the code above) is to zero, the closer the two methods will be.\nThe flexibility of the simulation method is both an advantage and a disadvantage. The advantage is that it can accommodate any set of testing procedures, returning a study-specific correction that will generally be more powerful than other methods to control the FWER. The disadvantage is that it requires the researcher to code up a simulation — there are no prewritten functions that will apply across research contexts.\nHere are some guidelines and tips for writing your own simulation.\n\nFollow the original random assignment procedure as exactly as possible. For example, if you block-randomized your experiment, make sure your simulations permute the treatment assignment according to the same blocks.\nEach simulation should return a set of p-values. (this was accomplished in the permute_treat() function above.)\nBe sure to count up the number of simulations in which at least one test was deemed significant, not the average number of tests across all simulations deemed significant.\n\n\n\n8: Alternative ways to control FDR and variants of FDR\nRecall from earlier that the Benjamini-Hochberg procedure to control FDR can also be too conservative when all null hypotheses are true. There are a number of different variants of the FDR and with each variant, their own procedures for control. In general, these are more powerful than the B-H procedure. Below is a table of a few types:\n\n\n\nFDR Variants\nMethods for Control\nSoftware\n\n\n\n\npFDR: positive FDR, \\(E[V/R|R>0]\\)\nThe Storey-BH procedure sets a rejection area and estimates the corresponding error rate. This contrasts the B-H procedure sets an error rate \\(\\alpha\\) and estimates its rejection area with the adjustments (Storey, 2002).\nSee R package qvalue\n\n\nmFDR or Fdr: marginal FDR, \\(E[FP]/E[R]\\).\n\\(\\alpha\\)-investing, a procedure where, after rejecting a null hypothesis, the researcher can “invest” in the \\(\\alpha\\) threshold by increasing the threshold for subsequent tests (Foster and Stine, 2008).\nSee R package  onlineFDR \n\n\nfdr: Local false discovery rate, \\(fdr(z)\\)\nAn empirical Bayes approach to estimate local false discovery rate as a function of the size of the test statistic \\(z\\) (Efron, 2004).\nSee R pacakge  locfdr \n\n\n\nThere is the increased chance of accepting more false discoveries with these other methods over the basic B-H approach. However, in cases with many, many tests, where the researcher might be willing to accept a few more false discoveries along with making more true discoveries, these methods for controlling FDR are good alternatives.\n\n\n9: Creating an index is a way to get a single comparison out of many\nSuppose3 a researcher measures \\(k>1\\) dependent variables. Indexing allows the researcher to reduce these \\(k\\) outcomes into a single measure (or several thematically-grouped measures). These indexing methods effectively condense the number of dependent variables that investigators test in order to address the multiple comparisons problem. There are a number of ancillary benefits of these indexing methods:\n\nUnlike the other methods of addressing the multiple comparisons problem, the indexing approach may reward researchers for increasing the number of dependent variables. Imagine that a researcher collects \\(k=50\\) outcome variables and that the treatment does not cause significant differences for any of them, but all the point estimates are in the same direction. If we were to apply a multiple comparisons correction to our 50 tests, our results would get even murkier. However, if we combine all 50 dependent variables into a single index, the resulting dependent variable may in fact exhibit significant differences.\nIn the presence of limited amounts of attrition across outcomes, these methods may provide some leverage for dealing with missingness on some dependent variables (but not for units for which outcome variables are entirely unobserved).\n\nThere are two principal indexing methods in the literature:\n\nMean Effects Index\nKling, Liebman, and Katz (2004) employ a mean effects index, constructed as follows:\n\nIf necessary, reorient some outcomes so that beneficial effects are consistently scored higher across all outcomes.\nCalculate a \\(z\\)-score, \\(\\tilde{y}_{ik}\\) by subtracting off the control group mean and dividing by the control group standard deviation, as follows, where \\(i\\) indexes individuals and \\(k\\) indexes outcomes:\n\n\\[\\tilde{y}_{ik}= \\frac{y_{ik}- \\bar{y}_k^{Z=0}}{\\sigma_{k}^{y,Z=0}}\\]\n\nSum the \\(z\\)-scores, \\(\\sum_{i=1}^K \\tilde{y}_{ik}\\) (optionally) divide by \\(K\\) to generate the index.\nOptional: It may be desirable to normalize the final index by the control group mean and standard deviation.\n\nIn the presence of missing outcomes, one of two approaches could be employed:\n\nImputation: Kling, Liebman, and Katz advocate a imputation approach for missing values on individual outcomes. Specifically, prior to constructing the index, compute the mean of each outcome variable for each experimental group, \\(\\bar{y}_{ik}^{Z=1}\\) and \\(\\bar{y}_{ik}^{Z=0}\\) using the above notation. Then, impute the mean corresponding to a unit’s assignment status (treatment or control) prior to constructing the index.\n“Greedy” Indexing: Instead of imputing values of missing outcome variables ex-ante as in method 1, calculate the \\(z\\)-scores as above. Where there are missing values for the “raw” outcome variables, there will be missing \\(z\\)-scores. For each unit, sum the non-missing \\(z\\)-scores and then divide by the number of non-missing outcomes. Hence, instead of dividing \\(\\sum_{i=1}^K \\tilde{y}_{ik}\\) by \\(K\\) as above, we calculate \\(K_{i}\\), the number of non-missing outcomes, for each unit.\n\n\n\nInverse Covariance Weighted Index\nAnderson (2008) provides a similar approach that constructs an index that employs inverse covariance weighting. This weighting scheme improves efficiency relative to the mean effects index above by affording less weight to highly correlated outcomes. The Anderson index can be constructed through the following procedure:\n\nIf necessary, reorient some outcomes so that beneficial effects are consistently scored higher across all outcomes.\nCalculate a \\(z\\)-score, \\(\\tilde{y}_{ik}\\) by subtracting off the control group mean and dividing by the control group standard deviation, as follows, where \\(i\\) indexes individuals and \\(k\\) indexes outcomes:\n\n\\[\\tilde{y}_{ik}= \\frac{y_{ik}- \\bar{y}_k^{Z=0}}{\\sigma_{k}^{y,Z=0}}\\]\n\nConstruct and invert the (variance)-covariance matrix of the resultant matrix of \\(z\\)-scores calculated in step 2. Call this \\(k \\times k\\) inverted (variance)-covariance matrix \\(\\hat{\\boldsymbol{\\Sigma}}^{-1}\\).\nThe weighted indexed outcome, \\(\\bar{s}_i\\) can be estimated via the following procedure, where \\(\\textbf{1}\\) is a \\(k \\times 1\\) vector of ones and \\(\\textbf{y}_{ik}\\) is the \\(n \\times k\\) matrix of \\(z\\)-scores calculated in step 2.\n\n\\[\\bar{s}_i = (\\textbf{1}^T \\hat{\\boldsymbol{\\Sigma}}^{-1} \\textbf{1})^{-1}(\\textbf{1}^T \\hat{\\boldsymbol{\\Sigma}}^{-1} \\textbf{y}_{ik})\\]\n\nOptional: As above, it may be desirable to normalize the final index by the control group mean and standard deviation.\n\nAs with the mean effects index, this varible \\(\\bar{s}_i\\) the serves as the dependent variable in your analysis. One potential drawback to the inverse covariance weighting index is that there is no guarantee that elements in the inverted covariance matrix (\\(\\boldsymbol{\\Sigma}^{-1}\\)) are positive. As such, it is possible to generate negative weights using this indexing method. Given that outcomes are oriented in the same direction, a negative weight effectively reverses the direction of the effect on negatively-weighted outcomes in the construction of the index.\nThe following functions implement both the mean effects and inverse covariance weighted index methods and evaluate both functions on a DGP with 50 outcome measures:\n\n\nCode\nstopifnot(require(mvtnorm))\nstopifnot(require(dplyr))\nstopifnot(require(randomizr))\nstopifnot(require(ggplot2))\nset.seed(1234)\ncalculate_mean_effects_index <- function(Z, outcome_mat, to_reorient, reorient = FALSE, greedy = TRUE,\n                                impute = FALSE){\n  if(length(Z) != nrow(outcome_mat)) stop(\"Error: Treatment assignment, outcome matrix require same n!\")\n  if(impute == TRUE){\n    R <- 1 * is.na(outcome_mat)\n    means_for_imputation <- rbind(apply(outcome_mat[Z==0,], MAR = 2, FUN = mean, na.rm = T),\n                                  apply(outcome_mat[Z==1,], MAR = 2, FUN = mean, na.rm = T))\n    to_impute <- R * means_for_imputation[Z+1,]\n    outcome_mat[is.na(outcome_mat)] <- 0\n    outcome_mat <- outcome_mat + to_impute\n  }\n  c_mean <- apply(X = outcome_mat[Z==0,], MARGIN = 2, FUN = mean, na.rm = T)\n  c_sd <- apply(X = outcome_mat[Z==0,], MARGIN = 2, FUN = sd, na.rm = T)\n  z_score <- t(t(sweep(outcome_mat, 2, c_mean))/ c_sd)\n  index_numerator <- rowSums(z_score)\n  if(greedy == TRUE){\n    n_outcomes <- rowSums(!is.na(z_score))\n  }\n  else if(greedy == FALSE){\n    n_outcomes <- ncol(outcome_mat)\n  }\n  index <- index_numerator/n_outcomes\n  index <-  (index - mean(index[Z==0], na.rm =T))/sd(index[Z==0], na.rm =T)\n  return(index)\n}\ncalculate_inverse_covariance_weighted_index <- function(Z, outcome_mat, to_reorient, reorient = FALSE){\n  if(length(Z) != nrow(outcome_mat)) stop(\"Error: Treatment assignment, outcome matrix require same n!\")\n  if(reorient == TRUE){\n    outcome_mat[, c(to_reorient)] <- -outcome_mat[, c(to_reorient)] \n  }\n  c_mean <- apply(X = outcome_mat[Z==0,], MARGIN = 2, FUN = mean, na.rm = T)\n  c_sd <- apply(X = outcome_mat[Z==0,], MARGIN = 2, FUN = sd, na.rm = T)\n  z_score <- t(t(sweep(outcome_mat, 2, c_mean))/ c_sd)\n  Sigma_hat <- solve(cov(z_score, y = z_score, use = \"complete.obs\"))\n  one_vec <- as.vector(rep(1, ncol(outcome_mat)))\n  if(sum(is.na(outcome_mat))>0){\n    z_score[is.na(z_score)] <- 0\n  }\n  w_ij <- t(solve(t(one_vec) %*% Sigma_hat %*% one_vec) %*% (t(one_vec) %*% Sigma_hat))\n  if(sum(w_ij < 0) > 0){warning('Warning, at least one weight is negative!')}\n  s_ij <- t(solve(t(one_vec) %*% Sigma_hat %*% one_vec) %*% (t(one_vec) %*% Sigma_hat %*% t(z_score)))\n  index <- (s_ij - mean(s_ij[Z==0], na.rm = T))/sd(s_ij[Z==0], na.rm = T)\n  return(s_ij)\n}\n\n\nWe can see how these indices perform in a setting with \\(k = 5\\) outcome variables.\n\n\n\n[Click to show code]\n\n\n\nCode\n# A DGP with K outcome variables\n# Untreated potential outcomes drawn from multivariate normal distribution\nK <- 5\nr <- runif(n = K, min = -.9, max = .9)\nsigma <- outer(r, r, FUN = \"*\")\ndiag(sigma) <- 1\nmat <- rmvnorm(n = 200, mean = rep(0, K), sigma = sigma)\n# Treatment assignment\nZ <- complete_ra(200)\n# Created observed potential outcomes\n# Assume that ATEs are all oriented in the same direction for the time being\nATEs <- rnorm(K, mean = .25, sd = 1)\nfor(i in 1:K){\n  mat[,i] <- mat[,i] + rnorm(n = 200, mean = Z * ATEs[i], sd = 1)\n}\nmean_effects_index <- calculate_mean_effects_index(Z = Z, outcome_mat = mat, reorient = F)\ninv_cov_weighted_index <- calculate_inverse_covariance_weighted_index(Z = Z, outcome_mat = mat,reorient = F)\n\n\nFirst, we can examine the properties of the indices alongside our five outcome variables by looking at the covariance matrix.\n\n\n\n[Click to show code]\n\n\n\nCode\nknitr::kable(cov(data.frame(mat, mean_effects_index, inv_cov_weighted_index)), digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nX5\nmean_effects_index\ninv_cov_weighted_index\n\n\n\n\nX1\n2.096\n0.247\n-0.330\n-0.394\n-0.443\n0.448\n0.249\n\n\nX2\n0.247\n2.059\n-0.195\n0.057\n-0.054\n0.747\n0.247\n\n\nX3\n-0.330\n-0.195\n2.614\n-0.172\n0.450\n0.641\n0.295\n\n\nX4\n-0.394\n0.057\n-0.172\n1.784\n0.249\n0.534\n0.242\n\n\nX5\n-0.443\n-0.054\n0.450\n0.249\n2.525\n0.755\n0.305\n\n\nmean_effects_index\n0.448\n0.747\n0.641\n0.534\n0.755\n0.999\n0.428\n\n\ninv_cov_weighted_index\n0.249\n0.247\n0.295\n0.242\n0.305\n0.428\n0.189\n\n\n\n\n\nWe can also plot the two indices to show their similarities (or differences). Note that with the final normalization included in the functions above, both indices are on the same scale.\n\n\n\n[Click to show code]\n\n\n\nCode\ndata.frame(Z, mean_effects_index, inv_cov_weighted_index) %>%\n  mutate(Group = ifelse(Z==1, \"Treatment\", \"Control\")) %>%\n  ggplot(aes(x = mean_effects_index, y = inv_cov_weighted_index, colour = Group)) + \n  geom_point() + theme_bw() + xlab(\"Mean Effects Index\") + ylab(\"Inverse Covariance Weighted Index\")\n\n\n\n\n\nWe can estimate the treatment effect on the indexed variable using OLS or a difference-in-means. Note that given the normalization of both indices, the coefficient estimates are on the same scale and thus directly comparable. The p-values here use the standard OLS test-statistic.\n\n\n\n[Click to show code]\n\n\n\nCode\ntable <- rbind(summary(lm(mean_effects_index ~ Z))$coef[\"Z\", c (1, 2, 4)], \n        summary(lm(inv_cov_weighted_index ~ Z))$coef[\"Z\", c(1, 2, 4)])\nrownames(table) <- c(\"Mean Effects Index\", \"Inverse Covariance Weighted Index\")\ncolnames(table) <- c(\"Estimate\", \"Std. Error\", \"p-value\")\nknitr::kable(table, digits = 3)\n\n\n\n\n\n\nEstimate\nStd. Error\np-value\n\n\n\n\nMean Effects Index\n0.304\n0.140\n0.031\n\n\nInverse Covariance Weighted Index\n0.155\n0.061\n0.011\n\n\n\n\n\n\n\n\n10: Use Design Based approaches\nThere are also two design based approaches for thinking about this problem. One is to use pre-analysis plans to describe in advance which comparisons will be made; the number of tests implicated in a Bonferroni or Bonferroni-style correction is specified before any data analysis is conducted. A pre-analysis plan, among its many benefits, helps to clarify the multiple comparisons problem.\nA good example is this pre-analysis plan by Gwyneth McClendon and Rachel Beatty Riedl. The authors specify 24 individual tests they plan to conduct and state ahead of time that they will employ both the Bonferroni and Benjamini-Hochberg corrections. The authors do not state beforehand how they will handle a situation in which the corrections disagree; presumably it will be a matter of judgment for both the authors and the readers of their study.\nAn additional benefit of pre-analysis plans is the ability to specify beforehand what the primary hypothesis is. There is disagreement among methodologists on this point, but some argue that because the primary hypothesis is not part of a “family claim” that a standardp-value is correct. For example, a researcher might have one primary hypothesis and 10 ancillary hypotheses. The uncertainty surrounding the primary hypothesis should not depend on the number of ancillary hypotheses. The advantage of a preanalysis plan is establishing beforehand which of the many hypotheses is the primary one.\nAn alternative perspective is to consider, from a theoretical and research design perspective, what is part of the “family claim.” For example, there may be a set of 5 hypotheses that are all used to support the primary hypothesis and the study was designed to focus on these 5 hypotheses. There may also be ancillary hypotheses that are used for exploration; the study was not developed with these ancillary hypotheses in mind, but the researcher thinks that it is interesting to explore these ancillary hypotheses. In a situation like this, the researcher might identify the 5 hypotheses as confirmatory hypotheses that are a family of hypotheses, and thus make adjustments to the p-values of those hypotheses, say controlling the FWER. The ancillary hypotheses would be considered exploratory hypotheses and do not need adjustments or could use more lenient adjustments compared to the confirmatory hypotheses, say controlling the FDR — allowing some false discoveries. Policy or action may be taken according to the confirmatory hypotheses, while future research may be developed to target the exploratory hypotheses.\nThe advantage of a preanalysis plan is establishing beforehand which of the many hypotheses is the primary one, which hypotheses are part of the same family of hypotheses or confirmatory, and which hypotheses are exploratory.\nA second approach is to turn to replication. Replication is the best guard against drawing false conclusions from a noisy experiment. In particular, replication helps to establish whether heterogeneous effects findings are reliable. Researchers have a large degree of freedom in choosing how to search for heterogeneity — they can explore the entire covariate space searching for interesting interactions. Such a procedure is likely to lead to a large number of false positives. Multiple comparisons corrections such as the ones discussed above might help — but researchers also have discretion as to which correction to apply. Replication addresses this problem directly by measuring the same covariates and looking for differential effects according to the previous experiment’s analysis.\nA good example comes from Coppock, Guess, and Ternovski (2015), in which the same experiment was conducted twice. The experiments measured the effects of two treatments on two dependent variables among four subgroups. In principle, this leads to 60 possible comparisons per experiment.\n\n3 pairwise comparisons between treatment 1, treatment 2, and control.\n4 subgroups means 4 comparisons against zero plus 6 pairwise comparisons = 10.\n2 dependent variables\n3 * 10 * 2 = 60\n\nThe figure below shows the results of the experiment and replication. Study 1 finds that “organization” accounts have smaller treatment effects than females, males, and unknown accounts on the “signed” DV but not the “tweeted” DV. The uncorrected p-value of the difference between “Organization” and “Male” conditional average treatment effects was 0.00003 for the “follower” treatment and 0.00632 for the “organizer” treatment. The Bonferroni correction would multiply both p-values by 60, suggesting that the “organizer” treatment does not work significantly differently for organizations versus men.\nThe replication, however, shows the same pattern of treatment effects: smaller effects for Organization accounts than for others on the “Signed” DV, but similar treatment effects on the “Tweeted” DV. Any doubt that the different response on the two different dependent variables was due to sampling variability are assuaged by the replication.\n\n\n\n\n\n\nFootnotes\n\n\nOriginating author: Alex Coppock, 18 Feb 2015. The guide is a live document and subject to updating by EGAP members at any time. Coppock is not responsible for subsequent edits to this guide. Updated by Lula Chen, 16 Feb 2021.↩︎\nFormally \\(E[FP/R|R>0]P(R>0)\\) to avoid dividing by 0.↩︎\nSection 9 updated by Tara Lyn Slough, 4 February 2016↩︎"
  },
  {
    "objectID": "guides/getting-started/measurement_en.html",
    "href": "guides/getting-started/measurement_en.html",
    "title": "10 Things to Know About Measurement in Experiments",
    "section": "",
    "text": "1. The validity of inferences we draw from an experiment depend on the validity of the measures used.\nWe typically experiment in order to estimate the causal effect of a treatment, \\(Z\\), on an outcome, \\(Y\\). Yet, the reason that we care about estimating this causal effect is, in principle, to understand characteristics of the relationship between two theoretical, unobserved, concepts measured by observed variables \\(Z\\) and \\(Y\\).\nFollowing Adcock and Collier (2001), consider the measurement process graphed in Figure 1 in three steps. First, researchers begin with systematized concept, a clearly-defined theoretical construct. From this concept, the researcher develops an indicator mapping the concept onto a scale or a set of categories. Finally, units or cases are scored on the indicator, yielding a measurement of treatment, \\(Z\\) and an outcome \\(Y\\). A measurement is valid if variation in the indicator closely approximates variation in the underlying concept of interest.\nAn experimental research design should allow a researcher to estimate the causal effect of \\(Z\\) on \\(Y\\) under standard assumptions. But if the ultimate goal is to make an inference about causal effect of the concept that \\(Z\\) measures on the concept that \\(Y\\) measures, the inferences that we can hope to make on the basis of our experimental evidence are valid if and only if both measures are valid.\n\n\n\n\n\nThe process of measurment in the context of an experimental research design.\n\n\n\n\n\n\n2. Measurement is the link between a researcher’s substantive and/or theoretical argument and an (experimental) research design.\nWhen we consider the design of an experiment, we tend to focus on the process by which the randomly assigned treatment, \\(Z\\) is assigned and the joint distribution of \\(Z\\) and an outcome, \\(Y\\). In other words, we tend to divorce scores of \\(Z\\) and \\(Y\\) from broader concepts when considering the statistical properties of a research design. In this telling, two completely separate experiments with the same distribution of \\(Z\\) and \\(Y\\) could have identical properties.\nFor example, a clinical trial on the efficacy of aspirin on headaches and an experiment that provides information on an incumbent politician’s level of corruption and then asks the respondent if she will vote for the incumbent could have identical sized and distributed samples, assignments, estimands, and realizations of outcomes (data). Yet, this characterization of two completely distinct research projects that seek to make completely distinct inferences as “equivalent” may strike us as quite strange or even unsettling.\nHowever, when we consider measurement as a fundamental component of research design, clearly these experiments are distinct. We observe measures of different concepts in the data for the two experiments. By considering the indicators and the broader concepts underlying the treatments and outcomes, we are forced to examine the researchers’ respective theories or arguments. In so doing, we can raise questions about the validity of the measures and the relationship between the validity of the measures and the validity of final, substantive, inferences.\n\n\n3. Measuring treatments includes the operationalization of treatment as well as compliance with treatment assignment.\nIn an experiment, treatments are typically designed, or at a minimum, described, by the researcher. Consumers of experimental research should be interested the characteristics of the treatment and how it manipulates a concept of interest. Most treatments in social science are compound, or include a bundle of attributes. We may be interested in the effect of providing voters with information on their elected officials’ performance. Yet, providing information also includes the mode of delivery and who was delivering the information. To understand the degree to which the treatment manipulates a concept, we must also understand what else the treatment could be manipulating.\nHowever, despite all the effort operationalizing a treatment, in experimental research, the link from the operationalization to the treatment indicator is fundamentally distinct from measurement of covariates or outcomes for two reasons. First, by assigning treatment, experimenters aim to control the values a given unit takes on. Second, for the treatment indicator, the score comes from assignment to treatment, which is a product of the randomization. A subject may or may not have received the treatment, but her score on the treatment indicator is simply the treatment that she was assigned to, not the treatment she received.\nWhen subjects receive treatments other than those to which they are assigned, we typically seek to measure compliance — whether the treatments were delivered and to what extent. To do so, we define what constitutes compliance with treatment assignment. In determining what constitutes compliance, researchers should consider the core aspect of how the treatment manipulates the concept of interest. At what point in the administration of the treatment does this manipulation occur? Once compliance is operationalized,we seek to code the compliance indicator in a manner faithful to this definition.\nFor example, consider a door-to-door canvassing campaign that distributes information about the performance of an incumbent politician. Households are assigned to receive a visit from a canvasser who shares the information (treatment) or no visit (control). The treatment indicator is simply whether a household was assigned to the treatment or not. However, if residents of a household are not home when the canvasser visits, they do not receive the information. Our definition of compliance should determine what constitutes “treated” on our (endogenous) measure of whether a household received the treatment, here the information. Some common definitions of compliance may be (a) that someone from the household answered the door; or (b) that someone from the household listened to the full information script.\n\n\n4. Most outcomes of interest in social science are latent.\nIn contrast to measuring treatment indicators and compliance, measuring outcomes in experimental research follows much more closely the process outlined in the figure above. We theorize how the treatment may influence an outcome concept. We then operationalize the concept and record scores or values to complete our measurement of the outcome.\nOne particular challenge in the measurement of outcomes is that many of the most common outcomes of interest in social science are latent. This means that we are unable to observe the true value of the outcome concept directly. In fact, the nature of the true value may itself be under debate (for example, the debate about the measurement of “democracy” is a classic case where the definition of the concept itself is contested). Outcomes including knowledge, preferences, and attitudes are latent. We thus record or score observable indicators assumed to be related to the latent outcome in an effort to infer characteristics of the latent variable. Even behavioral outcomes are often used as manifestations of larger latent concepts (i.e. assessed voting behavior is used to make inferences about “electoral accountability” in a place).\nBecause these variables are latent, it is challenging to devise appropriate indicators. Poor operationalization has rather drastic consequences for the validity of our inferences about the concept of interest for two reasons. As in section #1 above, if these indicators do not conceptually measure the concept of interest, then inferences we make about the relationship between \\(Z\\) and \\(Y\\) (even with a “perfect” design in terms of statistical power and missing data,etc.) may not teach us about the “ultimate inference” we are seeking to make. Furthermore, measurement error may undermine our ability to estimate the effect of \\(Z\\) and \\(Y\\) leading to incorrect inferences. The remainder of this guide focuses on the latter problem.\n\n\n5. There are two types of measurement error that we should consider.\nWe can formalize measurement challenges quite simply. Suppose a treatment, \\(Z_i\\) is hypothesized to change preferences for democratic norms, \\(\\nu_i\\). In principle, the quantity that we would like to estimate is \\(E[\\nu_i|Z_i = 1] - E[\\nu_i|Z_i =0]\\), the ATE of our treatment on preferences for democratic norms. However, \\(\\nu_i\\) is a latent variable: we cannot measure it directly. Instead we ask about support for various behaviors thought to correspond to these norms. This indicator, \\(Y_i\\), can be decomposed into the latent variable, \\(\\nu_i\\) and two forms of measurement error:\n\nNon-systematic measurement error, \\(\\delta_i\\): This error is independent of treatment assignment, \\(\\delta_i \\perp Z_i\\).\nSystematic measurement error, \\(\\kappa_i\\): This error is not independent of treatment assigniment, \\(\\kappa_i \\not\\perp Z_i\\).\n\n\\[Y_i = \\underbrace{\\nu_i}_{\\text{Latent outcome}} + \\underbrace{\\delta_i}_{\\substack{\\text{Non-systematic} \\\\ \\text{measurement error}}} + \\underbrace{\\kappa_i}_{\\substack{\\text{Systematic} \\\\ \\text{measurement error}}}\\]\n\n\n6. Measurement error reduces the power of your experiment.\nNon-systematic measurement error, represented by \\(\\delta_i\\) above, refers to the noise with which we are measuring the latent variable. In the absence of systematic measurement error, we measure:\n\\[Y_i = \\underbrace{\\nu_i}_{\\text{Latent outcome}} + \\underbrace{\\delta_i}_{\\substack{\\text{Non-systematic} \\\\ \\text{measurement error}}}\\]\nNow, consider the analytical power formula for a two-armed experiment. We can express \\(\\sigma\\), or the standard deviation of the outcome as \\(\\sqrt{Var(Y_i)}\\). Note that in the formula below, this term appears in the denominator of the first term. As \\(\\sqrt{Var(Y_i)}\\) increases, statistical power decreases.\n\\[\\beta = \\Phi \\left(\\frac{|\\mu_t− \\mu_c| \\sqrt{N}}{2 \\color{red}{\\sqrt{Var(Y_i)}}} − \\Phi^{−1}\\left(1 − \\frac{\\alpha}{2}\\right)\\right)\\]\nIn what way does non-systematic measurement error \\(\\delta_i\\) impact power? We can decompose \\(\\sqrt{Var(Y_i)}\\) as follows:\n\\[\\sqrt{Var(Y_i)} = \\sqrt{Var(\\nu_i) + Var(\\delta_i) + 2 Cov(\\nu_i, \\delta_i)}\\]\nSo long as \\(Cov(\\nu_i, \\delta_i)\\geq 0\\) (we often assume \\(Cov(\\nu_i, \\delta_i)= 0\\)), it must be the case that the \\(Var(Y_i)\\) is increasing as measurement error, or \\(Var(\\delta_i)\\) increases. This implies that power is decreasing as non-systematic measurement error increases. In other words, the noisier our measures of a latent variable, the lower our ability to detect effects of a treatment on a latent variable.\nWhat about the case in which \\(Cov(\\nu_i, \\delta_i) < 0\\)? While this reduces \\(Var(Y_i)\\) (holding \\(Var(\\nu_i)\\) and \\(Var(\\delta_i)\\) constant), it also attenuates the variation that we measure in \\(Y_i\\). In principle, this should attenuate the numerator \\(|\\mu_t-\\mu_c|\\), which, if sufficient relative to the reduction in variance, will also reduce power.\n\n\n7. Systematic measurement error biases estimates of causal effects of interests.\nIf we are estimating the Average Treatment Effect (ATE) of our treatment \\(Z_i\\), on preferences for democratic norms, \\(\\nu_i\\), we are trying to recover the ATE, or \\(E[\\nu_i|Z_i = 1] - E[\\nu_i|Z_i =0]\\). However, in the presence of systematic measurement error, where measurement error is related to the treatment assignment itself (say, the outcome is measured differently in the treatment group than in the control group) a difference-in-means estimator on the observed outcome, \\(Y_i\\), recovers a biased estimate of the ATE. The effect of the treatment now includes the measurement difference as well as the difference between treated and control groups:\n\\[E[Y_i|Z_i = 1]−E[Y_i|Z_i = 0] =  E[\\nu_i + \\delta_i + \\kappa_i |Z_i = 1] − E[\\nu_i + \\delta_i + \\kappa_i|Z_i =0]\\] Because non-systematic measurement error, \\(\\delta_i\\) is independent of treatment assignment, \\(E[\\delta_i|Z_i = 1] = E[\\delta_i |Z_i = 0]\\). Simplifying and rearranging, we can write:\n\\[E[Y_i|Z_i = 1]−E[Y_i|Z_i = 0] = \\underbrace{E[\\nu_i|Z_i = 1] − E[\\nu_i|Z_i =0]}_{ATE} +\n\\underbrace{E[\\kappa_i|Z_i = 1] - E[\\kappa_i|Z_i =0]}_{\\text{Bias}}\\]\nThere are various sources of non-systematic measurement error in experiments. Demand effects and Hawthorne effects can be motivated as sources of systematic measurement error. Moreover, designs that measure outcomes asymmetrically in treatment and control groups may be prone to systematic measurement error. In all cases, there exists asymmetry across treatment conditions in: (a) the way that subjects respond to being observed; or (b) the way that we observe outcomes that is distinct from any effect of the treatment on the latent variable of interest. The biased estimate of the ATE becomes the net of any effects on the latent variables (the ATE) and the non-systematic measurement error.\nWhen designing an experiment, researchers can take various steps to limit systematic measurement error. First and foremost, they can attempt to use identical measurement strategies across all experimental groups. To limit demand and Hawthorne effects, researchers often aim to design treatments and measurement strategies that are as naturalistic and unobtrusive as possible. For example, sometimes it can be beneficial to avoid baseline surveys at the outset of an experiment that would reveal the purpose of a study to participants. Researchers may also want to separate treatment from outcome measurement phases to limit the apparent connection between the two. Ensuring that study staff are blind towards the treatment status of study participants can also help maintain measurement symmetry across treatment conditions. The use of placebo treatments sometimes helps to hide their treatment status even from study participants themselves. Finally, researchers sometimes supplement outcome measures such as survey questions that are particularly susceptible to demand effects with behavioral measures for which experimenter demand may be less of a concern. See de Quidt et al. (2018) for a way to assess the robustness of your experimental results to demand effects.\n\n\n8. Leverage multiple indicators to assess the validity of a measure but be aware of the limitations of such tests.\nBeyond consideration of the quality of the mapping between a concept and a measure, we can often assess the quality of the measure by comparing it to measures from alternate operationalizations of the same concept, closely related concepts, or distinct concepts. In convergent tests of the validity of a measure, we assess the correlation between alternate measures of a concept. If they are coded in the same direction, we expect the correlation to be positive and validity of both measures increases as the magnitude of the correlation increases. One limitation of convergent tests of validity is if two measures are weakly correlated, absent additional information, we do not know whether one measure is valid (and which) or whether both measures are invalid.\nGathering multiple indicators may also allow for researchers to assess the predictive validity of a measure. To what extent does a measure of a latent concept predict behavior believed to be shaped by the concept? For example, does political ideology (the latent variable) predict reported vote choice for left parties? This provides an additional means of validating a measure. Here, the higher the ability of an indicator to predict behavior (or other outcomes), the stronger the predictive validity of the indicator. Yet, we believe that most behaviors are a result of a complex array of causes. Determining whether a measure is a “good enough” predictor is a somewhat arbitrary determination.\nFinally, we may want to determine whether we are measuring the concept of interest in isolation rather than a bundle of concepts. Tests of discriminant validity look at indicators of a concept and a related but distinct concept. In principle, we look for low correlations (correlations close to 0) between both indicators. One limitation of tests of discriminant validity is that we don’t know how underlying distinct concepts covary. It may be the case that we have valid indicators of both concepts, but they exhibit strong correlation (positive or negative) because units with high levels of \\(A\\) tend to have higher (resp. low) levels of \\(B\\).\nIn sum, the addition of more measures can help validate an indicator, but these validation tests are limited in what they tell us when they fail. To this extent, we should remain cognizant of the limitations in addition to the utility of collecting additional measures to simply validate an indicator.\n\n\n9. The use of multiple indicators often improves the power of your experiment, but may introduce a bias-efficiency tradeoff.\nGathering multiple indicators of a concept or outcome may also improve the power of your experiment. If multiple indicators measure the same concept but are measured with (non-systematic) error, we can improve the precision with which we measure the latent variable by leveraging multiple measures.\nThere are multiple ways to aggregate multiple outcomes into an index. “10 Things to Know about Multiple Comparisons” describes indices built from \\(z\\)-score and inverse covariance weighting of multiple outcomes. There are also many other structural models for estimating latent variables from multiple measures.\nBelow, we look at simple \\(z\\)-score index of two noisy measures of a latent variable. We assume that the latent variables and both indicators “Measure 1” and “Measure 2” are drawn from a multivariate normal distribution and are positively correlated with the latent variable and with each other. For the purposes of simulation, we assume that we know the latent variable, though in practice this is not possible. First, we can show that across many simulations of the data, the correlation between the \\(z\\)-score index of the two measures and the latent variable is, on average, higher than the correlation between either of the indicators and the latent variable. When graphing the correlation of the individual measures and the latent variable against (\\(x\\)-axes) the correlation of the index and the latent variable (\\(y\\)-axis), almost all points are above the 45-degree line. This shows that the index approximates the latent variable with greater precision.\n\nlibrary(mvtnorm)\nlibrary(randomizr)\nlibrary(dplyr)\nlibrary(estimatr)\nmake_Z_score <- function(data, outcome){\n  ctrl <- filter(data, Z == 0)\n  return(with(data, (data[,outcome] - mean(ctrl[,outcome]))/sd(ctrl[,outcome])))\n}\npull_estimates <- function(model){\n  est <- unlist(model)$coefficients.Z\n  se <- unlist(model)$std.error.Z\n  return(c(est, se))\n}\ndo_sim <- function(N, rhos, taus, var = c(1, 1, 1)){\n   measures <- rmvnorm(n = N, \n                       sigma = matrix(c(var[1], rhos[1], rhos[2], \n                                        rhos[1], var[2], rhos[3], \n                                        rhos[2], rhos[3], var[3]), nrow = 3))\n   df <- data.frame(Z = complete_ra(N = N),\n                      latent = measures[,1],\n                      Y0_1 = measures[,2],\n                      Y0_2 = measures[,3]) %>%\n            mutate(Yobs_1 = Y0_1 + Z * taus[1],\n                   Yobs_2 = Y0_2 + Z * taus[2])\n   df$Ystd_1 = make_Z_score(data = df, outcome = \"Yobs_1\")\n   df$Ystd_2 = make_Z_score(data = df, outcome = \"Yobs_2\")\n   df$index = (df$Ystd_1 + df$Ystd_2)/2\n   cors <- c(cor(df$index, df$latent), cor(df$Ystd_1, df$latent), cor(df$Ystd_2, df$latent))\n   ests <- c(pull_estimates(lm_robust(Ystd_1 ~ Z, data = df)),\n             pull_estimates(lm_robust(Ystd_2 ~ Z, data = df)),\n             pull_estimates(lm_robust(index ~ Z, data = df)))\n   output <- c(cors, ests)\n   names(output) <- c(\"cor_index\", \"cor_Y1\", \"cor_Y2\", \"est_Y1\", \"se_Y1\",\n                      \"est_Y2\", \"se_Y2\", \"est_index\", \"se_index\")\n   return(output)\n}\nsims <- replicate(n = 500, expr = do_sim(N = 200, \n                                         rhos = c(.6, .6, .6), \n                                         taus = c(.4, .4),\n                                         var = c(1, 3, 3)))\ndata.frame(measures = c(sims[\"cor_Y1\",], sims[\"cor_Y2\",]),\n           index = rep(sims[\"cor_index\",], 2),\n           variable = rep(c(\"Measure 1\", \"Measure 2\"), each = 500)) %>%\n  ggplot(aes(x = measures, y = index)) + geom_point() + \n  facet_wrap(~variable) + \n  geom_abline(a = 0, b = 1, col = \"red\", lwd = 1.25) + \n  scale_x_continuous(\"Correlation between measure and latent variable\", limits = c(0.1, .6)) +\n  scale_y_continuous(\"Correlation between index and latent variable\", limits = c(0.1, .6)) + \n  theme_minimal()\n\n\n\n\nNow, consider the implications for power. In the simulations, we estimate of the ATE of a treatment on Measure 1, Measure 2, and the index. The following graph visualizes the estimates. The blue lines show 95 percent confidence intervals. The smaller confidence intervals about the index visualize the precision gains from leveraging both measures. We see that this manifests in higher statistical power for the experiment.\n\ndata.frame(est = c(sims[\"est_index\",], sims[\"est_Y1\",], sims[\"est_Y2\",]),\n           se = c(sims[\"se_index\",], sims[\"se_Y1\",], sims[\"se_Y2\",]),\n           outcome = rep(c(\"Index\", \"Measure 1\", \"Measure 2\"), each = 500)) %>%\n  mutate(T = est/se, \n         sig = 1 * (abs(T) > 1.96)) %>%\n  group_by(outcome) %>%\n  mutate(power = sum(sig)/n(),\n          lab = paste0(\"Power = \", round(power, 2))) %>%\n  arrange(est) %>%\n  mutate(order = 1:500/500) %>%\n  ggplot(aes(x = order, y = est)) + \n  geom_errorbar(aes(ymin = est - 1.96 * se, ymax = est + 1.96 * se), width = 0,\n                col = \"light blue\", alpha = .25) +\n  geom_point() +\n  facet_wrap(~outcome) +\n  geom_text(x = 0.5, y = -.4, aes(label = lab), cex = 4) +\n  geom_hline(yintercept = 0, col = \"red\", lty = 3) + \n  theme_minimal() + xlab(\"Percentile of Estimate in 500 simulations\") + \n  ylab(\"ATE\")\n\n\n\n\nWe have examined an index composed of just two indicators. In principle, there are further efficiency gains to be made by incorporating more indicators into your index. Yet, as we increase the number of indicators, we should consider the degree to which the amalgamation of indicators adheres to the original concept. By adding measures to leverage efficiency gains, we may introduce bias into the measure of the latent concept. Researchers must navigate this tradeoff. Pre-registration of the components of an index provides one principled way to navigate the issue which forces thorough consideration of the concept in the absence of data. This also avoids the ex-post questions about the choice of indicators for an index.\n\n\n10. While concepts may be global, many indicators are context-specific.\nMany studies in the social sciences focus on concepts that are typically assumed to be latent including preferences, knowledge, and attitudes. To the extent that we work on common concepts, there is a tendency to draw from existing operationalizations from studies on related concepts in different contexts. In studies in multiple contexts, as in EGAP’s Metaketa Initiative, researchers aim to study the same causal relationship in multiple national contexts. But a desire to study common concepts need not imply that the same indicators should be used across contexts.\nFor example, consider a set of studies that seek to measure variation in the concept of political knowledge or sophistication. Political knowledge may be assessed through questions that ask subjects to recall a fact about politics. One question may ask subjects to recall the current executive’s (president/prime minister etc.) name, scoring answers as “correct” or “incorrect.” In country \\(A\\), 50% of respondents answer the question correctly. In Country \\(B\\), 100% of respondents answer the question correctly. In Country \\(B\\), we are unable to identify any variation in the indicator because everyone could answer the question. This does not imply that there is no variation in political knowledge in Country \\(B\\), just that this indicator is a poor measure of the variation that exists. In Country \\(A\\), however, this question may be a completely appropriate indicator of political knowledge. If political knowledge was the outcome of an experiment, the lack of variation in the outcome in Country \\(B\\) fails to allow us to identify any difference in political knowledge between treatment and control groups.\nFor this reason, while it may be useful to develop indicators based on existing work or instruments from other contexts, this is not necessarily the best way to develop measures in a new context. Pre-testing can provide insights into whether indicators are appropriate in a given setting. In sum, the mapping between concepts and indicators is site-specific in many cases. Researchers should consider these limitations when operationalizing common concepts in distinct settings.\n\n\nBibliography\nAdcock, Robert and David Collier. “Measurement Validity: A Shared Standard for Qualitative and Quantitative Research.” American Political Science Review. 95 (3): 529-546.\nde Quidt, Jonathan, Johannes Haushofer, and Christopher Roth. 2018. “Measuring and Bounding Experimenter Demand.” American Economic Review. 108 (11): 3266-3302."
  },
  {
    "objectID": "guides/getting-started/null-results_en.html",
    "href": "guides/getting-started/null-results_en.html",
    "title": "10 Things Your Null Result Might Mean",
    "section": "",
    "text": "Abstract\nAfter the excitement and hard work of running a field experiment is over, it’s not uncommon to hear policymakers and researchers express disappointment when they end up hearing that the intervention did not have a detectable impact. This guide explains that a null result rarely means “the intervention didn’t work,” even though that tends to be the shorthand many people use. Instead, a null result can reflect the myriad design choices that policy implementers and researchers make in the course of developing and testing an intervention. After all, people tend to label hypothesis tests with high p-values as “null results”, and hypothesis tests (as summaries of information about design and data) can produce large p-values for many reasons. Policymakers can make better decisions about what to do with a null result when they understand how and why they got that result.\nImagine you lead the department of education for a government and are wondering about how to boost student attendance. You decide to consider a text message intervention that offers individual students counseling. Counselors at each school can help students address challenges specifically related to school attendance. Your team runs a randomized trial of the intervention, and tells you there is a null result.\nHow should you understand the null result, and what should you do about it? It could be a result of unmet challenges at several stages of your work – in the way the intervention is designed, the way the intervention is implemented, or the way study is designed Below are 10 things to consider when interpreting your null result.\n\n\nIntervention Design\n\n1. Your intervention theory and approach are mismatched to the problem.\nYou delivered a counseling intervention because you thought that students needed support to address challenges in their home life. However, students who had the greatest needs never actually met with a counselor, in part because they did not trust adults at the school. The theory of change assumed that absenteeism was a function primarily of a student’s personal decisions or family circumstances and that the offer of counseling without changes to school climate would be sufficient; it did not account appropriately for low levels of trust in teacher-student relationships. Therefore, this null effect does not suggest that counseling per se cannot boost attendance, but that counseling in the absence of other structural or policy changes or in the context of low-trust schools may not be sufficient.\nHow can you tell if…you have a mismatch between your theory of change and the problem that needs to be solved? List all potential barriers and consider how they connect. Does the intervention as designed address only one of those barriers, and, if so, can it succeed without addressing others? Are there assumptions made about one source or one cause that may undermine the success of the intervention?\n\n\n2. Your intervention strength or dosage is too low for the problem or outcome of interest.\nAfter talking to experts, you learn that counseling interventions can build trust, but usually require meetings that are more frequent and regular than your intervention offered to have the potential for an effect. Maybe your “dose” of services is too small.\nHow can you tell if…you did not have a sufficient “dose”? Even if no existing services tackle your problem of interest, consider what is a minimum level, strength, or dose that is both feasible to implement and could yield an effect. When asking sites what they are willing to take on, beware of defaulting to the lowest dose. The more complex the problem or outcome is to move, the stronger or more comprehensive the intervention may need to be.\n\n\n3. Your intervention does not represent a large enough enhancement over usual services.\nIn your position at the state department of education, you learn that students at the target schools were already receiving some counseling and support services. Even though the existing services were not sufficient to boost attendance to the targeted levels, the new intervention did not add enough content or frequency of the counseling services to reach those levels either—the intervention yielded show-up rates that were about the same as existing services. So this null effect does not reflect that counseling has no effect, but rather that the version of counseling your intervention offered was not effective over and above existing counseling services.\nHow can you tell if…the relative strength of your intervention was not sufficient to yield an effect? Take stock of the structure and content of existing services, and consider if the extent or form in which clients respond to existing services indicates that the theory of change or approach needs to be revised. If the theory holds, use existing services as a benchmark and consider whether your proposed intervention needs to include something supplementary and/or something complementary.\n\n\n\nIntervention Implementation\nPrograms rarely rollout exactly as intended, but some variations are more problematic than others.\n\n4. Your implementation format was not reliable.\nIn the schools in your study, counseling interventions sometimes occurred in person, sometimes happened by text message, sometimes by phone. Anticipating and allowing for some variation and adaptation is important. Intervention dosage and strength is often not delivered as designed nor to as many people as expected.\nBut unplanned variations in format can reflect a host of selection bias issues, such that you cannot disentangle whether counseling as a concept does not work or whether certain formats of outreach did not work. This is especially important to guard against if you intend to test specific channels or mechanisms critical to your theory of change.\nHow can you tell if…an unreliable format is the reason for your null? Were you able to specify or standardize formats in a checklist? Could you leave enough discretion but still incentivize fidelity? Pre-specifying what the intervention should look like can help staff and researchers monitor along the way and correct inconsistencies or deviations that may affect the results. This could include a training protocol for those implementing the intervention. If nothing was specified or no one was trained, then the lack of consistency may be part of the explanation.\n\n\n5. Your intervention and outcome measure are mismatched to your randomization design.\nYou expected counseling to be more effective in schools with higher student-to-teacher ratios, but did not block randomize by class size (for more on block randomization, see our guide on 10 Things to Know About Randomization). then it may no longer have the potential to be more effective for students in high class size schools .\nHow can you tell if…you have a mismatch between your intervention and randomization design? Consider whether treatment effects could vary, or service delivery might occur in a cluster, or intervention concepts could spill over, and to what extent your randomization design accounted for that.\n\n\n\nStudy Design\n\n6. Your study sample includes people whose behavior could not be moved by the intervention.\nYou ask schools to randomize students into an intervention or control (business-as-usual) group. Some students in both your intervention and control groups will always attend school, while some students will rarely attend school, regardless of what interventions are or are not offered to them. Your intervention’s success depends on not just whether students actually receive the message and/or believe it, but also on whether it can shift behavior among such potential responders.\nIf the proportion of potential responders is too small, then it may be difficult to detect an effect. In addition, your intervention may need to be targeted and modified in some way to address the needs of potential responders.\nHow can you tell if…the proportion of potential responders may be too small? Take a look at the pre-intervention attendance rate. If it is extremely low, does that rate reflect low demand or structural barriers that may limit the potential for response? Is it so high that it tells us that most people who could respond have already done so (say, 85% or higher)? Even if there is a large proportion of hypothetical potential responders, is it lower when you consider existing barriers preventing students from using counseling services that your intervention is not addressing?\n\n\n7. Your measure is not validated or reliable: It varies too much and systematically across sites.\nAs a leader of the state’s department of education, you want to measure the effectiveness of your intervention using survey data on student attitudes related to attendance. You learn that only some schools administer a new survey measuring student attitudes, and those with surveys changed the survey items so that there is not the same wording across surveys or schools. If you observe no statistically significant difference on a survey measure that is newly developed or used by only select schools, it may be difficult to know whether the intervention “has no effect” or whether the outcome is measuring something different in each school because of different wording.\nHow can you tell if… outcome measurement is the problem? Check to see whether the outcome is (1) collected in the same way across your sites and (2) if it means the same thing to the participants as it means to you. In addition, check on any reporting bias and if your study participants or sites face any pressure from inside or outside of their organizations to report or answer in a particular way.\n\n\n8. Your outcome is not validated or reliable: It varies too little.\nGiven the problems with the survey measure, you then decide to use administrative data from student records to measure whether students show up on time to school. But it turns out that schools used a generic definition of “on time” such that almost every student looks like they arrive on time. An outcome that does not have enough variation in it to detect an effect between intervention and control groups can be especially limiting if your intervention potentially could have had different effects on different types of students, but the outcome measure used in the study lacks the precision to capture the effects on different subgroups.\nHow can you tell if…your null result arises from measures that are too coarse or subject to response biases? Pressures to report a certain kind of outcome faced by people at your sites could again yield this kind of problem with outcome measurement. So, it is again worth investigating the meaning of the outcomes as reported by the sites from the perspective of those doing the reporting. This problem differs from the kind of ceiling and floor effects discussed elsewhere in this guide; it arises more from the strategic calculations of those producing administrative data and less from the natural behavior of those students whose behavior you are trying to change.\n\n\n9. Your statistical power is insufficient to detect an effect for the intervention as implemented.\nThis may sound obvious to people with experience testing interventions at scale. But researchers and policymakers can fall into two traps:\n\nThinking about statistical significance rather than what represents a meaningful and feasible effect. Although a study with an incredibly large sample size can detect small effects with precision, one does not want to trade precision for meaning. Moreover, an intervention known to be weak during the intervention design is likely to be weaker when implemented, especially across multiple sites or months. So it may not be sufficient to simply enroll more subjects to study an intervention known to be weak (even though strong research design cannot compensate for a weak intervention in any easy or direct way);\nThinking that the only relevant test statistic for an experiment effect is a difference of means (even though we have long known that differences of means are valid but low-powered test statistics when outcomes do not neatly fit into a normal distribution).\n\nHow can you tell if…your null result arises mostly from low statistical power? Recall that statistical power depends on (a) effect size or intervention strength, (b) variability in outcomes, (c) the number of independent observations (often well measured with sample size), and (d) the test statistic you use. The previous discussions pointed out ways to learn whether an intervention you thought might be strong was weak, or whether an outcome that you thought might be clear could turn out to be very noisy.\nA formal power analysis could also tell you that, given the variability in your outcome and the size of your effect, you would have needed a larger sample size to detect this effect reliably. For example, if you had known about the variability in administration of the treatment or the variability in the outcome (let alone surprises with missing data) in advance, your pre-field power analysis would have told you to use a different sample size.\nA different test statistic can also change a null result into a positive result if, say, the effect is large but it is not an effect that shifts means as much as moves people who are extreme, or has the effect of making moderate students extreme. A classic example of this problem occurs with outcomes that have very long tails – such as those involving money, like annual earnings or auction spending. A t-test might produce a p-value of .20 but a rank-based test might produce a p-value of < .01. The t-test is using evidence of a shift in averages (means) to reflect on the null hypothesis of no effects. The rank-based test is merely asking whether the treatment group outcomes tend to be bigger than (or smaller than) the control group outcomes (whether or not they differ in means).\n\n\n\nNot all nulls are the result of a flaw in design or implementation!\n\n10. Your null needs to be published.\nIf you addressed all the issues above related to intervention design, sample size and research design, and have a precisely estimated, statistically significant null result, it is time to publish. Your colleagues and other researchers need to learn from this finding, so do not keep it to yourself.\nWhen you have a precise null, you do not have a gap in evidence–you are generating evidence.\nWhat can you do to convince editors and reviewers they should publish your null results? This guide should help you reason about your null results and thus explain their importance. If other studies on your topic exist, you can also contextualize your results; for example, follow some of the ideas from Abadie 2019.\nFor an example, see how Bhatti et al.–in their study of a Danish governmental voter turnout intervention–used previous work on face-to-face voter turnout (reported on as a meta-analysis here) to contextualize their own small effects.\nIf you are unable to find a publication willing to include a study with null results in their journal, you can still contribute to the evidence base on the policy area under examination by making your working paper, data, and/or analysis code publicly available. Many researchers choose to do so via their personal websites; in addition, there are repositories (such as the Open Science Framework) that provide a platform for researchers to share their in-progress and unpublished work."
  },
  {
    "objectID": "guides/getting-started/meta-analysis_en.html",
    "href": "guides/getting-started/meta-analysis_en.html",
    "title": "10 Things to Know About Conducting a Meta-Analysis",
    "section": "",
    "text": "1. What is meta-analysis?\nMeta-analysis is a method for summarizing the statistical findings from a research literature. For example, if five experiments have been conducted using the same intervention and outcome measure on the same population of people with five separate estimates of an average treatment effect, one might imagine pooling these five studies together into a single dataset and analyzing them jointly. In broad strokes, in such a case, we could act as though the studies came from five blocks within a single experiment rather than five separate experiments. The benefit of such an approach would be more statistical power in the estimation of one overall average treatment effect. In essence, a meta-analysis produces a weighted average of the five studies’ results. As explained below, this method is also used to summarize research literatures that comprise a diverse array of interventions and outcomes measured in diverse settings, under the assumption that the interventions are theoretically similar and the outcome measures tap into a shared underlying trait.\n\n\n2. How does a meta-analysis differ from a literature review?\nMeta-analysis is often characterized as a form of systematic review insofar as it involves a specific set of data collection and analysis procedures. These procedures are spelled out in great detail by the Campbell Collaboration and by the James Lind Library. Particular attention is paid to gathering both published and unpublished studies (see below). By comparison, most conventional literature reviews cite the most noteworthy theoretical or empirical contributions but rarely attempt to be comprehensive or to summarize the findings quantitatively. Critics of conventional reviews point out the possibility that the most noteworthy or memorable studies present findings that are unrepresentative of the broader research literature and therefore may be a poor guide for policy. On the other hand, critics of meta-analysis point out that the flaws of individual studies are often lost sight of when their estimates are blended together to generate an overarching conclusion. As Uri Simonsohn once quipped, “Meta-analysis is a sausage factory that uses sausages by other factories as inputs.”1\n\n\n3. What types of data are used as input for a meta-analysis?\nIn principle, meta-analysis could be applied to the original data from each study, but in practice such data are seldom available for all relevant studies. Instead, researchers typically cull estimated treatment effects from research papers or other reports. This process presents scholars conducting a meta-analysis with an array of decisions when the research papers present results involving multiple outcomes, treatments, and estimation approaches. Often meta-analysis focuses on the “main” results, but identifying the main or primary results can be a judgment call. The reproducibility of meta-analysis hinges on careful documentation of such decisions. In addition to locating the key estimates, the meta-analyst must also track down measures of statistical uncertainty (e.g., standard errors, confidence intervals), as these statistics will be used to assign weights to each study in the averaging process. As noted below, meta-analysis tends to assign more weight to studies with less sampling variability such as studies with larger sample sizes.\n\n\n4. What is the estimand in meta-analysis?\nSome meta-analyses are narrowly tailored to specific treatments and outcomes. For example, a vast literature dating back to the 1920s focuses on the extent to which mailings that encourage voting in fact cause people to vote. In this case, one could imagine a population parameter that represents the average causal effect of mailed voting encouragements on a population of people in a specified region over some specified time period.\nOther meta-analyses are more abstract, focusing on a broad class of treatments and outcomes. For example, the literature on prejudice reduction comprises hundreds of studies on the effects of interpersonal contact between people with different racial, ethnic, religious, age, or gender backgrounds (Pettigrew and Tropp 2006).2 Contact ranges from a brief conversation to a year of co-habitation in a college dormitory. Outcomes also range widely from overt behaviors to self-reported feelings about in-groups and out-groups. Since these interventions and outcomes are on different scales and may refer to different concepts, the underlying population parameter is ambiguous. Researchers try to sidestep this issue by standardizing the outcomes (e.g., by dividing the outcome by the standard deviation in the control group), but there remains the problem of what to make of treatments that vary in intensity and duration. In effect, the population parameter in such cases becomes the average extent to which an ad hoc collection of interventions changes putative measures of prejudice within some location and time period.\nMeta-analyses can also struggle when the underlying population represented by the component studies is vague or abstract. For example, in a literature dominated by laboratory experiments conducted in the United States, the meta-analysis will implicitly assess an underlying average treatment effect in which the “average” gives disproportionate weight to American undergraduates.\n\n\n5. Meta-analysis and Bayes’ Rule\nConsider the simple case in which two experiments are conducted using the same treatments and outcomes. Imagine that both studies draw their subject pool from the same population. In this case, if we had the individual data for the experiments, we could pool them together into a single dataset and analyze them as though they were part of the same block-randomized experiment. But suppose that we did not have the individual data; instead, we only have the estimated ATE and estimated standard error from each study. How might we combine the two studies to form our best guess of the average treatment effect in the population from which the subjects were drawn? With some simplifying assumptions, we could apply Bayes’ Rule. Let’s assume that the sampling distribution of each experimental estimate is normal. (This is a reasonable assumption under the Central Limit Theorem, since we are using an average to estimate the average treatment effect and we assume that each experiment has at least a few dozen subjects and that the outcome distribution is not too skewed.) Since these experiments are independent of one another, Bayes’ Rule takes a simple form: take a weighted average of the two estimates, where the weights are the inverse of each study’s squared standard error (\\(\\hat{\\sigma}_j^2\\) is the squared estimated standard error for study \\(j\\)).\n\\[ \\hat{ATE_{pooled}} = \\frac{\\frac{1}{\\hat{\\sigma}_1^2}}{\\frac{1}{\\hat{\\sigma}_1^2} + \\frac{1}{\\hat{\\sigma}_2^2}}\\hat{ATE_1} + \\frac{\\frac{1}{\\hat{\\sigma}_1^2}}{\\frac{1}{\\hat{\\sigma}_1^2} + \\frac{1}{\\hat{\\sigma}_2^2}}\\hat{ATE_2} \\]\nThis formula turns out to be the same as a so-called “fixed effects” meta-analysis. This formula is sometimes called a “precision-weighted average,” where the term “precision” refers to the inverse of the squared standard error. In a simple two-arm, completely randomized study, the standard error of the simple estimator of the average treatment effect is a function of sample size and variation in the outcome, and ratio of treated to control units. So, notice that in this case the study with the smaller standard error (i.e. larger sample, less variable outcome, more equal ratio of treated to control units) received more weight in the pooled meta-analytic result.\n\n\n6. Fixed effects versus random effects estimation\nMost meta-analysis software3 presents users with a choice between fixed effects estimation and random effects estimation. Fixed effects estimation is simply a precision-weighted average.4 And random effects estimation is a special case of more general Bayesian meta-analysis. In either case, the studies with the smallest standard errors are accorded the most weight. Random effects estimation applies a different set of weights depending on the extent to which the estimates vary more than would be expected by chance under a fixed effects model. Therefore, the weights for the random effects estimation not only consider the variance within each study but also an estimate of the between-study variance (\\(\\tau^2\\)).\n\\[ \\hat{ATE_{pooled}^*} = \\frac{\\frac{1}{\\hat{\\sigma}_1^2+\\tau^2}}{\\frac{1}{\\hat{\\sigma}_1^2+\\tau^2} + \\frac{1}{\\hat{\\sigma}_2^2+\\tau^2}}\\hat{ATE_1} + \\frac{\\frac{1}{\\hat{\\sigma}_1^2+\\tau^2}}{\\frac{1}{\\hat{\\sigma}_1^2+\\tau^2} + \\frac{1}{\\hat{\\sigma}_2^2+\\tau^2}}\\hat{ATE_2} \\]\nThe more heterogeneous the estimated effects–perhaps due to variations in experimental techniques, outcome measurement, or context–the more the resulting weighted average represents a simple average rather than a precision-weighted average. Typically, researchers start with a fixed effects meta-analysis, test whether the estimates are significantly overdispersed given the fixed effects model, and, if so, estimate and report a random effects meta-analysis.\n\n\n7. Is it okay to summarize both experimental and observational research findings?\nBeware of meta-analyses that combine experimental and observational estimates. When properly executed, experiments provide unbiased estimates of the average treatment effect. An observational study, on the other hand, is prone to bias insofar as the treatments are not randomly assigned. The nominal standard errors associated with observational studies ignore the potential for bias; the standard errors are biased downward because they assume the best-case scenario, namely, that nature assigned treatments in a manner that was as good as random. Gerber, Green, and Kaplan (2004)5 show that merely being uncertain about the bias of an observational study is equivalent to according it a larger standard error. Although many prominent meta-analyses include both experiments and observational studies (e.g., Lau and Sigelman6; Pettigrew and Tropp 2006), this practice is frowned upon by leading scholars conducting biomedical meta-analyses.\n\n\n8. Publication bias as a threat to meta-analysis\nBecause meta-analyses draw their data from reported results, publication bias presents a serious threat to the interpretability of meta-analytic results. If the only results that see the light of day are splashy or statistically significant, meta-analysis may simply amplify publication bias. Methodological guidance to meta-analytic researchers therefore places special emphasis on conducting and carefully documenting a broad-ranging search for relevant studies, whether published or not, including languages other than English. This task is, in principle, aided by pre-registration of studies in public archives; unfortunately, pre-registration in the social sciences is not sufficiently comprehensive to make this a dependable approach on its own.\nWhen assembling a meta-analysis, it is often impossible to know whether one has missed relevant studies. Some statistical methods have been developed in order to detect publication bias, but these tests tend to have low power and therefore may give more reassurance than is warranted. For example, one common approach is to construct a scatterplot to assess the relationship between study size (whether measured by the N of subjects or the standard error of the estimated treatment effect) and effect size. A telltale symptom of publication bias is a tendency for smaller studies to produce larger effects (as would be the case if studies were published only if they showed statistically significant results; to reach the significance bar, small studies (with large standard errors) would need to generate larger effect estimates. Unfortunately, this test often produces ambiguous results (Bürkner and Doebler 2014),7 and methods to correct publication bias in the wake of such diagnostic tests (e.g., the trim-and-fill method) may do little to reduce bias. Given growing criticism of statistical tests for publication bias and accompanying statistical correctives, there is an increasing sense that the quality of a meta-analysis hinges on whether research reports in a given domain can be assembled in a comprehensive manner.\n\n\n9. Modeling inter-study heterogeneity using meta-regression\nResearchers often seek to investigate systematic sources of treatment effect heterogeneity. These systematic sources may reflect differences among subjects (Do certain drugs work especially well for men or women?), contexts (Do lab studies of exposure to mass media produce stronger effects than field studies?), outcomes (Are treatment effects especially large when outcomes are measured via opinion surveys as opposed to direct observation of behavior?), or treatments (Are partisan messages more effective at mobilizing voters than nonpartisan messages?). Quite often, these investigations are best studied directly, via an experimental design. For example, variation in treatment may be studied by randomly assigning different treatment arms. Variation in effects associated with different outcome measures may also be studied in the context of a given experiment by gathering data on more than one outcome or by randomly assigning how outcomes are measured.\nA second-best approach is to compare studies that differ on one or more of these dimensions (subjects, treatments, context, or outcomes). The drawback of this approach is that it is essentially descriptive rather than causal – the researcher is basically characterizing the features of studies that contribute to especially large or small effect sizes. That said, this exercise can be conducted via meta-regression: the estimated effect size is the dependent variable, while study attributes (e.g., whether outcomes were measured through direct observation or via survey self-reports) constitute the independent variables. Note that meta-regression is a generalization of random effects meta-analysis, with measured predictors of effect sizes as well as unmeasured sources of heterogeneity.\nSince meta-analysis is a technique for combining information across different studies, we do not here discuss the detection or modeling of heterogeneous treatment effects within any single study. See our guide 10 Things to Know About Heterogeneous Treatment Effects for more on this topic.\n\n\n10. Methods for assessing the accuracy of meta-analytic results\nA skeptic might ask whether meta-analysis improves our understanding of cause-and-effect in any practical way. Do we learn anything from pooling existing studies via a weighted average versus presenting the studies one at a time and leaving the synthesis to the reader? To address this question EGAP conducted an experiment among the academics and policy experts attending a conference to reveal the results of the first round of EGAP’s Metaketa Initiative, which focused on conducting a coordinated meta-analysis on the impact of information and accountability programs on electoral outcomes. The round consisted of six studies measuring the impact of the same causal mechanism.\nTo test the idea that accumulated knowledge (in the form of meta-analysis) allows for better inferences about the effect of a given program, the Metaketa committee randomized the audience to hear a presentation of the meta-analysis, each component study, a placebo, and an external study of a similar intervention that was not part of the Metaketa round or the subsequent meta-analysis. Each group of participants was not exposed to one of the above group of studies. And the participants were asked to predict the results of the left out study. This allowed the committee to measure the effect of each study type on attendees’ predictive abilities. The event attendees were then asked to predict the findings of the one study they had not yet seen. The resulting analysis found that exposure to the meta-analysis led to greater accuracy in predicting the effect in the left-out study in comparison to the external study (which, as a reminder, was not part of the meta-analysis in any way). For more on this Metaketa round, along with a more substantial discussion of this “evidence summit” see the book Information, Accountability, and Cumulative Learning: Lessons from Metaketa I.8\n\n\n\n\n\nFootnotes\n\n\nhttps://twitter.com/uri_sohn/status/471318552470126592↩︎\nPettigrew, T.F. & Tropp, L.R. (2006). A Meta-Analytic Test of Intergroup Contact Theory. Journal of Personality and Social Psychology, 90(5), 751–783.↩︎\nfor a list of R packages useful in conducting meta-analysis, see here: https://cran.r-project.org/web/views/MetaAnalysis.html↩︎\nSee for example https://www.stata.com/support/faqs/statistics/meta-analysis/ and https://cran.r-project.org/web/views/MetaAnalysis.html↩︎\nGerber, A.S., Green, D.P., & Kaplan, E.H. (2004). The illusion of learning from observational research. In I. Shapiro, R.M. Smith, & T.E. Masoud (Eds.), Problems and Methods in the Study of Politics (251-273). Cambridge, England: Cambridge University Press.↩︎\nLau, R.R., Sigelman, L., & Rovner, I.B. (2007). The Effects of Negative Political Campaigns: A Meta‐Analytic Reassessment. The Journal of Politics, 69(4), 1176-1209.↩︎\nBürkner, P. C., & Doebler, P. (2014). Testing for publication bias in diagnostic meta‐analysis: a simulation study. Statistics in Medicine, 33(18), 3061-3077.↩︎\nDunning, T., Grossman, G., Humphreys, M., Hyde, S. D., McIntosh, C., & Nellis, G. (Eds.). (2019). Information, accountability, and cumulative learning: Lessons from Metaketa I. Cambridge: Cambridge University Press.↩︎"
  },
  {
    "objectID": "guides/getting-started/external-validity_en.html",
    "href": "guides/getting-started/external-validity_en.html",
    "title": "10 Things You Need to Know About External Validity",
    "section": "",
    "text": "Abstract\nAfter months or years under development and implementation, navigating the practical, theoretical and inferential pitfalls of experimental social science research, your experiment has finally been completed. Comparing the treatment and control groups, you find a substantively and statistically significant result on an outcome of theoretical interest. Before you can pop the champagne in celebration of an intervention well evaluated, a friendly colleague asks: “But what does this tell us about the world?”\n\n\n1. What is external validity?\nExternal validity is another name for the generalizability of results, asking “whether a causal relationship holds over variation in persons, settings, treatments and outcomes.”1 A classic example of an external validity concern is whether traditional economics or psychology lab experiments carried out on college students produce results that are generalizable to the broader public. In the political economy of development, we might consider how a community-driven development program in India might apply (or not) in West Africa, or Central America.\nExternal validity becomes particularly important when making policy recommendations that come from research. Extrapolating causal effects from one or more studies to a given policy context requires careful consideration of both theory and empirical evidence. This methods guide discusses some key concepts, pitfalls to avoid, and useful references to consider when going from a Local Average Treatment Effect to the larger world.\n\n\n2. How is this different than internal validity?\nInternal validity refers to the quality of causal inferences being made for a given subject pool. As originally posited by Campbell,2 internal validity asks, “did in fact the experimental stimulus make some significant difference in this specific instance.” This concept dovetails with the counterfactual approach to causality that experimentalists typically use, which asks whether outcomes change depending on the presence or absence of a treatment.3\nBefore you can extrapolate a causal effect to a distinct population, it is vital that the original Average Treatment Effect be based on a well-identified result. For most experimentalists, random assignment provides the requisite identifying variation, provided no attrition, interference, spillovers, or other threats to inference. For observational studies, additional identifying assumptions are needed, such as conditional independence of the treatment from potential outcomes.\n\n\n3. Navigating the trade-offs between internal and external validity\nThere has been an ongoing debate within the social sciences regarding the relative importance of identifying internally valid results, which by definition apply to a local sample, and generating results that can be extrapolated to broader populations of interest. It is helpful to be familiar with this discussion when considering design trade-offs that inevitably crop up in resource-limited interventions. That both sides of the argument include luminaries of econometrics attests to the importance of the topic.\nOn one side of the argument fall advocates of “identification first,” who argue that without internally valid results, a study simply does not contribute useful information, regardless of whether it is a local or general population or context. As put by Imbens,4 “without strong internal validity studies have little to contribute to policy debates, whereas [internally valid] studies with very limited external validity often are, and in my view should be, taken seriously in such discussions.”\nOthers argue that even without full identification of an internally valid result, useful information can be salvaged, especially if it is relevant for important questions that affect a broad context. Manski5 writes that “what matters is the informativeness of a study for policy making, which depends jointly on internal and external validity.” With data from a broad but a poorly identified study, Manski argues, bounds on the estimand of interest can be generated that, while not as useful as a precise point estimate, still moves science forward.\n\n\n4. Theory and generalization\nExtrapolating a result to a distinct context, outcome, population or treatment is not a mechanical process. As discussed by Samii6 and Rosenbaum,7 relevant theory should be used to guide generalization, taking the relevant existing evidence and making predictions for other contexts in a principled fashion. Theories boil down complex problems into more parsimonious representations, and help to elucidate what factors matter. Just as theory guides the content of interventions and research designs, theoretical propositions can tell you which scope conditions are relevant for extrapolating a result. What covariates matter? What contextual information matters?\n\n\n5. How can I determine where my results apply?\nThere are two primary means of generalizing results, one based on the covariates of units in the study and the other based on actual experimental manipulation of moderating variables. Observing how a treatment effect varies over a non-randomized pre-treatment variable can describe treatment effect heterogeneity, which can be highly suggestive about where or for whom the intervention is likely to be most effective, beyond the original sample. Note, however, that this type of analysis cannot pin down whether the treatment-effect heterogeneity is caused by that pre-treatment variable. The concern—endemic to observational research—is that the non-randomized covariate may be correlated with an unobserved variable, and it is this “unseen” factor that in fact is responsible for the heterogeneous impacts of the treatment.8 Ideally, therefore, we want to leverage exogenous variation in the moderator of interest, thereby ruling out the possibility of such confounding. A factorial experimental design in which the researcher assigns the moderator independently of the main treatment of interest can generate especially compelling evidence about a moderator’s role. Though, of course, considerations of cost and statistical power may preclude this approach in practice.\nBecause generalization is primarily a prediction exercise, asking where we can expect a causal relationship similar to one observed locally, extrapolating heterogeneous effects based on similar covariates is often reasonable, provided theory does not indicate sources of confounding.9 Nonetheless, the strongest evidence for the generalizability of a result comes from a well-identified interaction between an exogenous moderator and the treatment, then projected across the covariate profile of a target population. Indeed, with some strong assumptions extrapolation can provide as good or better results than carrying out a second experiment in situ.10 The calculation of an extrapolated estimate can often be best performed using machine learning, although linear regression also performs reasonably well.11\n\n\n6. Strategic behavior can scuttle your extrapolations\nExtrapolating a local result to a different context can prove challenging even with a compelling covariate profile to which you want to generalize effects. A randomized experimental manipulation in a local area generates a “partial equilibrium effect.” Strategic dynamics, including compensatory behavior or backlashes, outside the local context of an experimental intervention can complicate efforts to generalize a result. Suppose, for example, that an unconditional cash transfer intervention is shown to increase welfare, entrepreneurship, and employment in a sample of 200 villages. What would happen if the intervention were extended to encompass 1000 villages? At this point, one could imagine that regions excluded from the program are more likely to learn about it. Untreated units may start to demand other types of transfers from the government, giving rise to effects similar to those produced by the direct cash transfer. In a similar vein, sometimes causal relationships only work when they are applied to some people. For example, imagine a job skills program that functions very well (as compared to those who did not receive it), what would happen if it were extended to all workers? Even if there are positive effects across all participants, there could be reduced or no average effects as higher skilled jobs are already filled by the first batch and the second batch is forced to remain in their previous jobs, now overqualified. In short, under general equilibrium conditions we might expect different results even where the covariate profile matches.\n\n\n7. Don’t confuse external validity with construct validity or ecological validity\nInternal and external validity are not the only ‘validity’ concerns that can be leveled at experimental work, and though relevant, they are also distinct. Ecological validity, as defined by Shadish, Cook and Campbell12 concerns whether an intervention appears artificial or out of place when deployed in a new context. For example, does an information workshop in a rural town carried out by experimenters resemble the kinds of information sharing that the population may experience in regular life? Similarly, if the same workshop were held in a large city, would it appear out of place?\nConstruct validity considers whether a theoretical concept being tested in a study is appropriately operationalized by the treatment(s). If your experiment is testing the effect of anger on political reciprocity and you are in fact manipulating fear or trust in your treatment, construct validity may be violated. Both construct and ecological validity are relevant for generalizations, and thus useful for making claims about external validity.\n\n\n8. Extrapolation across treatments and outcomes\nWhile much of this guide has implicitly focused on porting a given treatment to a new place or time, external validity also considers variations in treatments and outcomes. That is, imagine we did the same experiment on the same sample, but with a variation on the treatment, would we predict the local causal effect to be similar? Similarly, can we predict if a given treatment will produce the same or different causal effects on a different outcome? Sometimes we can address these concerns by conducting experiments that assess alternative treatments and outcomes. When follow-up experiments are in short supply, such issues have to be settled analytically. Rather than considering the features of subjects, extrapolation in this case requires thinking through, aided by theory, the characteristics of the treatments or outcomes and making reasonable predictions.\n\n\n9. Replication is important\nNo single study represents the final word on a scholarly question. Following the logic of Bayesian updating, additional evidence in favor of or against a given theory allows the scientific and policy community to update their beliefs about the strength and validity of a causal relationship.\nReplication of studies is an important part of this: scholars should replicate studies in contexts that look very different, but also in some contexts that look very similar. The former allows us to identify local causal relationships that can be triangulated with existing evidence and generalized as appropriate. At the same time, it is important to directly replicate existing studies under conditions that are as close as possible to the original in order to verify that local effects one may be interested in extrapolating are indeed reliable. The Open Science Collaboration13 found, for example, that when reproducing 100 major psychology experiments, just 47% of the original reported effect sizes fell within the 95% confidence interval of the effect size shown in the replication.\n\n\n10. Don’t forget time\nWhen thinking about causal relationships of interest, it is important also to consider time: do things we learn about the past extend to the future? How do an individual’s potential outcomes change over time? Immutable laws govern the physical and chemical worlds; hence what we learn about these laws today will always remain true. By contrast, we understand far less about the underlying drivers of social behavior and whether they hold constant in the same way. The answer may well be no. When making decisions about the policy relevance and generalizability of results, these considerations can help scholars determine a reasonable level of uncertainty and help policy makers adjust accordingly.\n\n\n\n\n\nFootnotes\n\n\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference. Houghton, Mifflin and Company.↩︎\nCampbell, D. T. (1957). Factors relevant to the validity of experiments in social settings. Psychological bulletin, 54(4), 297.↩︎\nMore details can be found in the causal inference methods guide: https://egap.org/resource/10-things-to-know-about-causal-inference↩︎\nImbens, G. (2013). Book Review Feature: Public Policy in an Uncertain World: By Charles F. Manski. The Economic Journal,123(570), F401-F411.↩︎\nManski, C. F. (2013). Response to the review of ‘public policy in an uncertain world’. The Economic Journal 123: F412–F415.↩︎\nSamii, Cyrus. (2016). “Causal Empiricism in Quantitative Research.” Journal of Politics 78(3):941–955.↩︎\nRosenbaum, Paul R. (1999). “Choice as an Alternative to Control in Observational Studies” (with discussion). Statistical Science 14(3): 259–304.↩︎\nGerber, A. S., & Green, D. P. (2012). Field experiments: Design, analysis, and interpretation. WW Norton.↩︎\nBisbee, James; Rajeev Dehejia; Cristian Pop-Eleches & Cyrus Samii. (2016). “Local Instruments, Global Extrapolation: External Validity of the Labor Supply-Fertility Local Average Treatment Effect.” Journal of Labor Economics↩︎\nBisbee, James; Rajeev Dehejia; Cristian Pop-Eleches & Cyrus Samii. (2016). “Local Instruments, Global Extrapolation: External Validity of the Labor Supply-Fertility Local Average Treatment Effect.” Journal of Labor Economics↩︎\nKern, H. L., Stuart, E. A., Hill, J., & Green, D. P. (2016). Assessing methods for generalizing experimental impact estimates to target populations. Journal of Research on Educational Effectiveness, 9(1), 103-127.↩︎\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference. Houghton, Mifflin and Company.↩︎\nOpen Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716.↩︎"
  },
  {
    "objectID": "guides/getting-started/adaptive-design_en.html",
    "href": "guides/getting-started/adaptive-design_en.html",
    "title": "10 Things to Know About Adaptive Experimental Design",
    "section": "",
    "text": "1. What is an “adaptive”1 design?\nA static design applies the same procedures for allocating treatments and measuring outcomes throughout the trial. In contrast, an adaptive design may, based on interim analysis of the trial’s result, change the allocation of subjects to treatment arms or may change the allocation of resources to different outcome measures.\nOrdinarily, mid-course changes in experimental design are viewed with skepticism since they open the door to researcher interference in ways that could favor certain results. In recent years, however, statisticians have developed methods to automate adaptation in ways that either lessen the risk of interference or facilitate bias correction at the analysis stage.\n\n\n2. What are the potential advantages of an adaptive design?\nAdaptive designs have the potential to detect the best-performing experimental arm(s) more quickly than a static design (i.e., with fewer data-collection sessions and fewer subjects). When these efficiencies are realized, resources may be reallocated to achieve other research objectives.\nAdaptive designs also have the potential to lessen the ethical concerns that arise when subjects are allocated to inferior treatment arms. For therapeutic interventions, adaptive designs may reduce subjects’ exposure to inferior treatments; for interventions designed to further broad societal objectives, adaptive designs may hasten the discovery of superior interventions.\nTo illustrate the potential advantages of adaptive design, we simulate an RCT involving a control group and eight treatment arms. We administer treatments and gather 100 outcomes during each “period.” The simulation assumes that each subject’s outcome is binary (e.g., good versus bad). The adaptive allocation of subjects is based on interim analyses conducted at the end of each period. We allocate next period’s subjects according to posterior probabilities that a given treatment arm is best (see below). The simulation assumes that the probability of success is 0.10 for all arms except one, which is 0.20. The stopping rule is that the RCT is halted when one arm is found to have a 95% posterior probability of being best.\nIn the adaptive trial depicted below, the best arm (the red line) is correctly identified, and the trial is halted after 23 periods (total N=2300).\n\n\n\n3. What are the potential disadvantages of adaptive designs?\nThere is no guarantee that adaptive design will be superior in terms of speed or accuracy. For example, adaptive designs may result in a lengthy trial in cases where all of the arms are approximately equally effective. Even when one arm is truly superior, adaptive searches have some probability of resulting in long, circuitous searches (and considerable expense) if by chance they get off to a bad start (i.e., one of the inferior arms appears to be better than the other based on an initial round of results).\nFor instance, consider the following scenario in which all but one of the arms have a 0.10 probability of success, and the superior arm has a 0.12 probability of success (with the same trial design as in the previous example). The design eventually settles on the truly superior arm but only after more than 200 periods (N = 23,810). Even after 50 periods, the results provide no clear sense that any of the arms is superior.\n\nA further disadvantage of adaptive designs is they may produce biased estimates of the average treatment effect of the apparent best arm vis-à-vis the control group. Bias arises because the trial stops when the best arm crosses a threshold suggesting optimality; this stopping rule tends to favor lucky draws that suggest the efficacy of the winning arm. Conversely, when adaptive algorithms associate sampling probability with observed history, under-estimation for inferior arms, including the control group, may persist until stopping time (Nie et al. (2017)).\nFor example, in the first scenario described above in which all arms have a 0.10 probability of success except for the best arm, which is 0.20, the average estimated success probability for the truly best arm is 0.202 across 1000 simulated experiments, while the control group average is found to 0.083. The average estimated difference in success probabilities (i.e., the average treatment effect) is 0.119, as compared to the true value of 0.10.\nIn the second scenario, in which the best arm’s success probability is just 0.12, the average estimated success probability for the best arm is 0.121, and the average estimated ATE is 0.027, as compared to the true ATE of 0.02. Bias in this case is relatively small on a percentage point scale due to the very large size of the average experiment.\n\n\n4. What kinds of experiments lend themselves to adaptive design?\nAdaptive designs require multiple periods of treatment and outcome assessment.\nAdaptive designs are well suited to survey, on-line, and lab experiments, where participants are treated and outcomes measured in batches over time.\nSome field experiments are conducted in stages, although the logistics of changing treatment arms may be cumbersome, as discussed below. One possible opportunity for adaptive design in a field context occurs when a given experiment is to be deployed over time in a series of different regions. This allows for adaptation based on region-by-region interim analyses.\nAdaptive designs are ill-suited to one-shot interventions with outcomes measured at a single point in time. For example, experiments designed to increase voter turnout in a given election do not lend themselves to adaptive design because everyone’s outcome is measured at the same time, leaving no opportunity for adaptation.\n\n\n5. What is the connection between adaptive designs and “multi-arm bandit problems”?\nThe multi-arm bandit problem (Scott (2010)) is a metaphor for the following optimization problem. Imagine that you could drop a coin in one of several slot machines that may pay off at different rates. (Slot machines are sometimes nicknamed “one-arm bandits,” hence the name.) You would like to make as much money as possible. The optimization problem may be characterized as a trade off between learning about the relative merits of the various slot machines – exploration – and reaping the benefits of employing the best arm – exploitation. A static design may be viewed as an extreme case of allocating subjects solely for exploration.\nAs applied to RCTs, the aim is to explore the merits of the various treatment arms while at the same time reaping the benefits of the best arm or arms. Although the MAB problem is not specifically about estimating treatment effects, one could adjust the optimization objective so that the aim is to find the treatment arm with the greatest apparent superiority over the control group.\n\n\n6. What are some widely used algorithms for automating “adaptation”?\nThe most commonly used methods employ some form of “Thompson sampling” (Thompson (1933)). Interim results are assessed periodically, and in the next period subjects are assigned to treatment arms in proportion to the posterior probability that a given arm is best. The more likely an arm is to be “best,” the more subjects it receives.\nMany variations on this basic assignment routine have been proposed, and some are designed to make it less prone to bias. If an adaptive trial is rolled out during a period in which success rates tend to be growing, increasing allocation of subjects to the best arm will tend to exaggerate that arm’s efficacy relative to the other arms, which receive fewer subjects during the high-yield period. In order to assess bias and correct for it, it may be useful to allocate some subjects in every period according to a static design. In this case, inverse probability weights for each period may be used to obtain unbiased estimates of the average treatment effect. (See Gerber and Green 2012 on the use of inverse probability weights for estimation of average treatment effects when the probability of assignment varies from block to block.)\n\n\n7. What are the symptoms of futile search?\nAlthough it is impossible to know for sure whether a drawn out search reflects an unlucky start or an underlying reality in which no arm is superior, the longer an adaptive trial goes, the more cause for concern. The following graphs summarize the distribution of stopping times for three scenarios. Stopping was dictated by at 10% value remaining criterion. Specifically, the trial stopped when the top of the 95% confidence interval showed that no other arm was likely to offer at least a 10 percent (not percentage point) gain in success rate. The first two scenarios were described above; the third scenario considers a case in which there are two superior arms. The graph illustrates how adaptive trials tend to conclude faster when the superiority of the best arm(s) is more clear-cut.\n\n\n\n8. What implications do adaptive designs have for pre-analysis plans?\nThe use of adaptive designs introduces additional decisions, which ideally should be addressed ex ante so as to limit researcher bias. For example, the researcher should specify which algorithms will be used for allocation. It is especially important to specify the stopping rule. Depending on the researcher’s objectives, this rule may focus on achieving a desired posterior probability, or it may use a “value remaining” criterion that considers whether one or more arms have shown themselves to be good enough vis-à-vis the alternative arms. Other hybrid stopping criteria may also be specified. The pre-analysis plan should also describe the analytic steps that will be taken to correct for bias.\n\n\n9. Are multi-arm bandit trials frequently used in social science?\nMany of the applications to date have taken place in commercial settings, such as website design for high-volume e-commerce, or in settings such as on-line fundraising. Relatively few applications have been written up in detail for a scholarly audience. Surprisingly rare are applications in biomedical research. As Villar, Bowden, and Wason (2015) note, “Despite this apparent near-perfect fit between a real-world problem and a mathematical theory, the MABP has yet to be applied to an actual clinical trial.”\nPutting aside the use of multi-arm bandit approaches, the use of adaptive trials is gradually winning acceptance in biomedical research. For details, see Chin (2016).\n\n\n10. What other considerations should inform the decision to use adaptive design?\nAs noted above, adaptive designs add to the complexity of the research design and analysis. They also may increase the challenges of implementation, particularly in field settings where the logistical or training costs associated with different arms vary markedly. Even when one arm is clearly superior (inferior), the lead-time necessary to staff or outfit this arm may make it difficult to scale it up (down). Adaptive designs are only practical if adaptation is feasible.\nOn the other hand, funders and implementation partners may welcome the idea of an experimental design that responds to on-the-ground conditions such that problematic arms are scaled back. A middle ground between static designs and designs that envision adaptation over many periods are adaptive designs involving only two or three interim analyses and adjustments. Such trials are winning increased acceptance in biomedical research (Chow and Chang (2008)) and are likely to become more widely used in the social sciences too. The growing interest in replication and design-based extensions of existing experiments to aid generalization are likely to create opportunities for adaptive design.\n\n\n\n\n\n\n\n\n\nReferences\n\nChin, Richard. 2016. Adaptive and Flexible Clinical Trials. CRC Press.\n\n\nChow, Shein-Chung, and Mark Chang. 2008. “Adaptive Design Methods in Clinical Trials – a Review.” Orphanet Journal of Rare Diseases 3 (11).\n\n\nNie, Xinkun, Xiaoying Tian, Jonathan Taylor, and James Zou. 2017. “Why Adaptively Collected Data Have Negative Bias and How to Correct for It.”\n\n\nScott, Steven L. 2010. “A Modern Bayesian Look at the Multi-Armed Bandit.” Applied Stochastic Models in Business and Industry 26: 639–58.\n\n\nThompson, William R. 1933. “On the Likelihood That One Unknown Probability Exceeds Another in View of the Evidence of Two Samples.” Biometrika 25: 285–94.\n\n\nVillar, Sofı́a S., Jack Bowden, and James Wason. 2015. “Multi-Armed Bandit Models for the Optimal Design of Clinical Trials: Benefits and Challenges.” Statistical Science: A Review Journal of the Institute of Mathematical Statistics 30 (2): 199–215.\n\nFootnotes\n\n\nThis guide originally authored by Donald Green (Columbia University) and Molly Offer-Westort (Yale University) and published on February 23, 2018.↩︎"
  },
  {
    "objectID": "guides/getting-started/ri2_en.html",
    "href": "guides/getting-started/ri2_en.html",
    "title": "10 Randomization Inference Procedures with ri2",
    "section": "",
    "text": "Randomization inference is a procedure for conducting hypothesis tests that takes explicit account of a study’s randomization procedure. See 10 things about Randomization Inference for more about the theory behind randomization inference. In this guide, we’ll see how to use the ri2 package for r to conduct 10 different analyses. This package was developed with funding from EGAP’s innaugural round of standards grants, which are aimed at projects designed to improve the quality of experimental research.\nTo illustrate what you can do with ri2, we’ll use some data from a hypothetical experiment involving 200 students in 20 schools. We’ll consider how to do randomization inference using a variety of different designs, including complete random assignment, block random assignment, cluster random assignment, and a multi-arm trial. You can check the kinds of random assignment methods guide for more on the varieties of random assignment.\nFollow the links below to download the four datasets we’ll use in the examples:\n\ncomplete randomization assignment dataset\nblocked randomization assignment dataset\nclustered randomization assignment dataset\nthree-arm randomization assignment dataset\n\n\n1. Randomization inference for the Average Treatment Effect\nWe’ll start with the most common randomization inference task: testing an observed average treatment effect estimate against the sharp null hypothesis of no effect for any unit.\nIn ri2, you always “declare” the random assignment procedure so the computer knows how treatments were assigned. In the first design we’ll consider, exactly half of the 200 students were assigned to treatment using complete random assignment.\n\nlibrary(ri2)\ncomplete_dat <- read.csv(\"ri2_complete_dat.csv\")\ncomplete_dec <- declare_ra(N = 200)\n\nNow all that remains is a call to conduct_ri. The sharp_hypothesis argument is set to 0 by default corresponding to the sharp null hypothesis of no effect for any unit. We can see the output using the summary and plot commands.\n\nsims <- 10000\nri_out <-\n  conduct_ri(\n    Y ~ Z,\n    declaration = complete_dec,\n    sharp_hypothesis = 0,\n    data = complete_dat,\n    sims = sims\n  )\nsummary(ri_out)\n\n  term estimate two_tailed_p_value\n1    Z    41.98             0.1144\n\nplot(ri_out)\n\n\n\n\nYou can obtain one-sided p-values with a call to summary:\n\nsummary(ri_out, p = \"upper\")\n\n  term estimate upper_p_value\n1    Z    41.98        0.0564\n\nsummary(ri_out, p = \"lower\")\n\n  term estimate lower_p_value\n1    Z    41.98        0.9436\n\n\n\n\n2. Randomization inference for alternative designs\nThe answer that ri2 produces depends deeply on the randomization procedure. The next example imagines that the treatment was blocked at the school level.\n\nblocked_dat <- read.csv(\"ri2_blocked_dat.csv\")\nblocked_dec <- declare_ra(blocks = blocked_dat$schools)\nri_out <-\n  conduct_ri(\n    Y ~ Z,\n    declaration = blocked_dec,\n    data = blocked_dat,\n    sims = sims\n  )\nsummary(ri_out)\n\n  term estimate two_tailed_p_value\n1    Z    91.98              2e-04\n\nplot(ri_out)\n\n\n\n\nA very similar syntax accommodates a cluster randomized trial.\n\nclustered_dat <- read.csv(\"ri2_clustered_dat.csv\")\nclustered_dec <- declare_ra(clusters =  clustered_dat$schools)\nri_out <-\n  conduct_ri(\n    Y ~ Z,\n    declaration = clustered_dec,\n    data = clustered_dat,\n    sims = sims\n  )\nsummary(ri_out)\n\n  term estimate two_tailed_p_value\n1    Z    79.32             0.0111\n\nplot(ri_out)\n\n\n\n\n\n\n3. Randomization inference with covariate adjustment\nCovariate adjustment can often produce large gains in precision. To analyze an experiment with covariate adjustment, simply include the covariates in the formula argument of conduct_ri:\n\ncomplete_dec <- declare_ra(N = 200)\nri_out <-\n  conduct_ri(\n    Y ~ Z + PSAT,\n    declaration = complete_dec,\n    data = complete_dat,\n    sims = sims\n  )\nsummary(ri_out)\n\n  term estimate two_tailed_p_value\n1    Z 59.27132                  0\n\nplot(ri_out)\n\n\n\n\n\n\n4. Randomization inference for a balance test\nYou can use randomization inference to conduct a balance test (or randomization check). In this case, we write a function of data that return some balance statistic (the F-statistic from a regression of the treatment assignment on two covariates).\n\nbalance_fun <- function(data) {\n  summary(lm(Z ~ professionalism + PSAT, data = data))$f[1]\n}\nri_out <-\n  conduct_ri(\n    test_function = balance_fun,\n    declaration = complete_dec,\n    data = complete_dat,\n    sims = sims\n  )\n\nWarning in data.frame(est_sim = test_stat_sim, est_obs = test_stat_obs, : row\nnames were found from a short variable and have been discarded\n\nsummary(ri_out)\n\n                   term  estimate two_tailed_p_value\n1 Custom Test Statistic 0.2924994             0.7489\n\nplot(ri_out)\n\n\n\n\n\n\n5. Randomization inference for treatment effect heterogeneity by subgroups\nYou can assess whether the treatment engenders different treatment effects among distinct subgroups by comparing the model fit (using an F-statistic) of two nested models:\n\nA regression of the outcome on the treatment assignment and the subgroup indicator\nA regression of the outcome on the treatment assignment, subgroup indicator, and their interaction\n\nThe null hypothesis we’re testing against in this example is the sharp hypothesis that the true treatment effect for each unit is the observed average treatment effect, i.e., that effects are constant.\n\nate_obs <- with(complete_dat, mean(Y[Z == 1]) - mean(Y[Z == 0]))\nri_out <-\n    conduct_ri(\n      model_1 = Y ~ Z + high_quality,\n      model_2 = Y ~ Z + high_quality + Z * high_quality,\n      declaration = complete_dec,\n      sharp_hypothesis = ate_obs,\n      data = complete_dat, \n      sims = sims\n    )\nsummary(ri_out)\n\n         term estimate two_tailed_p_value\n1 F-statistic 1.095294             0.7015\n\nplot(ri_out)\n\n\n\n\n\n\n6. Randomization inference for unmodeled treatment effect heterogeneity\nAnother way to investigate treatment effect heterogeneity is to consider whether the variance in the treatment and control groups are different. We can therefore test whether the difference-in-variances is larger in magnitude than what we would expect under the sharp null hypothesis of no effect for any unit.\n\nd_i_v <- function(dat) {\n    with(dat, var(Y[Z == 1]) - var(Y[Z == 0]))\n  }\nri_out <-\n    conduct_ri(\n      test_function = d_i_v,\n      declaration = complete_dec,\n      data = complete_dat, \n      sims = sims\n    )\nsummary(ri_out)\n\n                   term  estimate two_tailed_p_value\n1 Custom Test Statistic -8408.684             0.2824\n\nplot(ri_out)\n\n\n\n\n\n\n7. Randomization inference for multi-arm trials\nIn a three-arm trial, the research might wish to compare each treatment to control separately. To do this, we must change the null hypothesis in a subtle way: we are going to assume the sharp null for each pairwise comparison. For example, when comparing treatment 1 to control, we exclude the subjects assigned to treatment 2 and pretend we simply have a two arm trial conducted among the subjects assigned to control and treatment 1.\n\nthree_arm_dat <- read.csv(\"ri2_three_arm_dat.csv\")\nthree_arm_dec <- declare_ra(N = 200, \n                            conditions = c(\"Control\", \"Treatment 1\", \"Treatment 2\"))\nri_out <-\n    conduct_ri(\n      formula = Y ~ Z,\n      declaration = three_arm_dec,\n      data = three_arm_dat,\n      sims = sims\n    )\nsummary(ri_out)\n\n          term  estimate two_tailed_p_value\n1 ZTreatment 1  26.72546                 NA\n2 ZTreatment 2 -48.52827                 NA\n\n## plot(ri_out)\n\n\n\n8. Randomization inference for joint significance\nIn that same three-arm trial that compares two treatments to a control, we might be interested in testing whether, jointly, the treatments appear to change outcomes relative to the control. This is analogous to a joint F-test, conducted via randomization inference. We assume the sharp null that a unit would express their observed outcome in any of the three conditions.\n\nF_statistic <- function(data) {\n  summary(lm(Y ~ Z, data = data))$f[1]\n}\nri_out <-\n   conduct_ri(\n     test_function = F_statistic,\n     declaration = three_arm_dec,\n     data = three_arm_dat,\n     sims = sims\n   )\n\nWarning in data.frame(est_sim = test_stat_sim, est_obs = test_stat_obs, : row\nnames were found from a short variable and have been discarded\n\nsummary(ri_out)\n\n                   term estimate two_tailed_p_value\n1 Custom Test Statistic 2.802927             0.0664\n\nplot(ri_out)\n\n\n\n\n\n\n9. Randomization inference under noncompliance\nSome experiments encounter noncompliance, the slippage between treatment as assigned and treatment as delivered. The Complier Average Causal Effect (\\(CACE\\)) can be shown (under standard assumptions plus monotonicity) to be the ratio of the effect of assignment on the outcome – the “Intention-to-Treat” (\\(ITT_y\\)) and the effect of assignment on treatment receipt the (\\(ITT_D\\)). (The \\(CACE\\) is also called Local Average Treatment Effect. See our guide 10 Things to Know About the Local Average Treatment Effect for more details.) Because the \\(CACE\\) is just a rescaled, \\(ITT_y\\), a hypothesis test with respect to the \\(ITT_y\\) is a valid test for the \\(CACE\\). In practice, researchers can simply conduct a randomization inference test exactly as they would for the ATE, ignoring noncompliance altogether.\n\nITT_y = with(complete_dat, mean(Y[Z == 1]) - mean(Y[Z == 0])) \nITT_d = with(complete_dat, mean(D[Z == 1]) - mean(D[Z == 0])) \nCACE <- ITT_y / ITT_d\n           \nri_out <-\n  conduct_ri(\n    Y ~ Z, # notice we do inference on the ITT_y\n    declaration = complete_dec,\n    data = complete_dat,\n    sims = sims\n  )\nsummary(ri_out)\n\n  term estimate two_tailed_p_value\n1    Z    41.98             0.1156\n\nplot(ri_out)\n\n\n\n\n\n\n10. Randomization inference for arbitrary test statistics\nri2 can accommodate any scalar test statistic. A favorite among some analysts is the Wilcox rank-sum statistic, which can be extracted from the wilcox.test() function:\n\nwilcox_fun <- function(data){\n  wilcox_out <- with(data, wilcox.test(Y ~ Z))\n  wilcox_out$statistic\n}\nri_out <-\n  conduct_ri(\n    test_function = wilcox_fun,\n    declaration = complete_dec,\n    data = complete_dat,\n    sims = sims\n  )\n\nWarning in data.frame(est_sim = test_stat_sim, est_obs = test_stat_obs, : row\nnames were found from a short variable and have been discarded\n\nsummary(ri_out)\n\n                   term estimate two_tailed_p_value\n1 Custom Test Statistic     4320             0.9565\n\nplot(ri_out)"
  },
  {
    "objectID": "guides/getting-started/covariates_en.html",
    "href": "guides/getting-started/covariates_en.html",
    "title": "10 Things to Know About Covariate Adjustment",
    "section": "",
    "text": "Abstract\nThis guide1 will help you think through when it makes sense to try to “control for other things” when estimating treatment effects using experimental data. We focus on the big ideas and provide examples in R.\n\n\n1 What is covariate adjustment?\n“Covariates” are baseline characteristics of your experimental subjects. When you run an experiment, you are primarily interested in collecting data on outcome variables that your intervention may affect, e.g. expenditure decisions, attitudes toward democracy, or contributions for a public good in a lab experiment. But it’s also a good idea to collect data on baseline characteristics of subjects before treatment assignment occurs, e.g. gender, level of education, or ethnic group. If you do this you can explore how treatment effects vary with these characteristics (see 10 Things to Know About Heterogeneous Treatment Effects). But doing this also lets you perform covariate adjustment.\nCovariate adjustment is another name for controlling for baseline variables when estimating treatment effects. Often this is done to improve precision. Subjects’ outcomes are likely to have some correlation with variables that can be measured before random assignment. Accounting for variables like gender will allow you to set aside the variation in outcomes that is predicted by these baseline variables, so that you can isolate the effect of treatment on outcomes with greater precision and power.\nCovariate adjustment can be a cheaper route to improved precision than increasing the number of subjects in the experiment. Partly for that reason, researchers often collect extensive data on covariates before random assignment. Pre-tests (measures that are analogous to the outcome variable but are restricted to time periods before random assignment) may be especially valuable for predicting outcomes, and baseline surveys can ask subjects about other background characteristics.\n\n\n2 Controlling for covariates at the design stage (blocking)\nThe best way to control for covariates is to use block randomization to do it at the design stage even before you start your experiment. Block randomization enables you to create treatment and control groups that are balanced on certain covariates. For example, you might expect that gender and income help predict the outcome variable. Block randomization can ensure that the treatment and control groups have equal proportions of female/high-income, female/low-income, male/high-income, and male/low-income populations. When the blocking variables help predict outcomes, blocking improves precision by preventing chance correlations between treatment assignment and baseline covariates.\nFor more information on blocking and how to implement it in R, see 10 Things You Need to Know About Randomization. The precision gains from blocking (relative to covariate adjustment without blocking) tend to be greatest when sample sizes are small (Miratrix, Sekhon, and Yu (2013)).\nWhen blocking is done to improve precision, estimated standard errors should take the blocking into account. (Otherwise, the SEs will tend to be conservative because they won’t give you credit for the precision improvement that blocking achieved.) One simple and commonly used method is to regress the outcome on the treatment assignment dummy variable as well as block dummies. When the probability of assignment to treatment is constant across blocks, including the block dummies in the regression doesn’t change the estimated treatment effect, but tends to give a more accurate estimate of the SE.2\nIf the probability of assignment to treatment varies by block, then you need to control for these unequal probabilities in order to get unbiased estimates of average treatment effects. 10 Things You Need to Know About Randomization discusses ways to do this.\n\n\n3 How to do it in a regression\nSometimes you do not have the opportunity to implement a blocked experimental design (for example, if you join a project after random assignment occurs) or you would prefer to simplify your randomization scheme to reduce opportunities for administrative error. You can still adjust for covariates on the back end by using multiple regression. Remember that in a bivariate regression—when you regress your outcome on just your treatment indicator—the coefficient on treatment is just a difference-in-means. This simple method gives an unbiased estimate of the average treatment effect (ATE). When we add baseline covariates that are correlated with outcomes to the model, the coefficient on treatment is an approximately unbiased estimate of the ATE that tends to be more precise than bivariate regression.\nTo adjust for covariates through multiple regression, use the model:\n\\[Y_i = \\alpha + \\beta Z_i + \\gamma X_i + \\epsilon_i\\]\nwhere \\(Y_i\\) is the outcome variable, \\(Z_i\\) is the treatment indicator, and \\(X_i\\) is a vector of one or more covariates. The remainder \\(\\epsilon_i\\) is your disturbance term—the leftover unexplained noise.\nWhen the treatment and control groups are of unequal size, the precision gains from covariate adjustment may be greater if you include interactions between treatment and the covariates (see this blog post for more discussion). For ease of interpretation, recenter the covariates to have zero mean:\n\\[Y_i = \\alpha + \\beta Z_i + \\gamma W_i + \\delta Z_i*W_i + \\epsilon_i\\]\nwhere \\(W_i = X_i - \\overline{X}\\) and \\(\\overline{X}\\) is the mean value of \\(X_i\\) for the entire sample.\nIf subjects receive different probabilities of assignment to treatment based on their covariates, then our estimation method needs to account for this (again, see 10 Things You Need to Know About Randomization for details).\n\n\n4 Why to do it\nIt isn’t absolutely necessary to control for covariates when estimating the average treatment effect in an RCT that assigns every subject the same probability of receiving the treatment. The unadjusted treatment–control difference in mean outcomes is an unbiased estimator of the ATE. However, covariate adjustment tends to improve precision if the covariates are good predictors of the outcome.3\nIn large samples, random assignment tends to produce treatment and control groups with similar baseline characteristics. Still, by the “luck of the draw,” one group may be slightly more educated, or one group may have slightly higher voting rates in previous elections, or one group may be slightly older on average. For this reason, the estimated ATE is subject to “sampling variability,” meaning you’ll get estimates of the ATE that were produced by an unbiased method but happened to miss the mark.4 A high sampling variability contributes to noise (imprecision), not bias.\nControlling for these covariates tends to improve precision if the covariates are predictive of potential outcomes. Take a look at the following example, which is loosely based on Giné and Mansuri (2012), an experiment on female voting behavior in Pakistan. In this experiment, the authors randomized an information campaign to women in Pakistan to study its effects on their turnout behavior, the independence of their candidate choice, and their political knowledge. They carried out a baseline survey which provided them with several covariates.\nThe following code imitates this experiment by creating fake data for four of the covariates they collect: whether the woman owns an identification card, whether the woman has formal schooling, the woman’s age, and whether the woman has access to TV. It also creates two potential outcomes (the outcomes that would occur if she were assigned to treatment and if not) for a measure of the extent to which a woman’s choice of candidate was independent of the opinions of the men in her family. The potential outcomes are correlated with all four covariates, and the built-in “true” treatment effect on the independence measure here is 1. To figure out whether our estimator is biased or not, we simulate 10,000 replications of our experiment. On each replication, we randomly assign treatment and then regress the observed outcome \\(Y\\) on the treatment indicator \\(Z\\), with and without controlling for covariates. Thus, we are simulating two methods (unadjusted and covariate-adjusted) for estimating the ATE. To estimate the bias of each method, we take the difference between the average of the 10,000 simulated estimates and the “true” treatment effect.\n\nrm(list=ls())\nset.seed(20140714)\nN = 2000\nN.treated = 1000\nReplications = 10000\ntrue.treatment.effect = 1\n# Create pre-treatment covariates\nowns.id.card = rbinom(n = N, size = 1, prob = .18)\nhas.formal.schooling = rbinom(n = N, size = 1, prob = .6)\nage = round(rnorm(n = N, mean = 37, sd = 16))\nage[age<18] = 18\nage[age>65] = 65\nTV.access = rbinom(n = N, size = 1, prob = .7)\nepsilon = rnorm(n = N, mean = 0, sd = 2)\n# Create potential outcomes correlated with pre-treatment covariates\nY0 = round(owns.id.card + 2*has.formal.schooling + 3*TV.access + log(age) + epsilon)\nY1 = Y0 + true.treatment.effect\n# Assign treatment repeatedly\nZ.mat = replicate(Replications, ifelse(1:N %in% sample(1:N, N.treated), 1, 0))\n# Generate observed outcomes\nY.mat = Y1 * Z.mat + Y0 * (1 - Z.mat)\ndiff.in.means = function(Y, Z) {\n  coef(lm(Y ~ Z))[2]\n}\nols.adjust = function(Y, Z) {\n  coef(lm(Y ~ Z + owns.id.card + has.formal.schooling + age + TV.access))[2]\n}\nunadjusted.estimates = rep(NA, Replications)\nadjusted.estimates   = rep(NA, Replications)\nfor (i in 1:Replications) {\n  unadjusted.estimates[i]  =  diff.in.means(Y.mat[,i], Z.mat[,i])\n  adjusted.estimates[i]    =  ols.adjust(Y.mat[,i], Z.mat[,i])\n}\n# Estimated variability (standard deviation) of each estimator\nsd.of.unadj = sd(unadjusted.estimates)\nsd.of.unadj\nsd.of.adj   = sd(adjusted.estimates)\nsd.of.adj\n# Estimated bias of each estimator\nmean(unadjusted.estimates) - true.treatment.effect\nmean(adjusted.estimates) - true.treatment.effect\n# Margin of error (at 95% confidence level) for each estimated bias\n1.96 * sd.of.unadj / sqrt(Replications)\n1.96 * sd.of.adj   / sqrt(Replications)\n\nBoth methods—with and without covariates—yield the true treatment effect of 1 on average. When we ran the regression without covariates, our estimated ATE averaged 1.0008 across the 10,000 replications, and with covariates, it averaged 1.0003. Notice that the regression-adjusted estimate is essentially unbiased even though our regression model is misspecified—we control for age linearly when the true data generating process involves the log of age.5\nThe real gains come in the precision of our estimates. The standard error (the standard deviation of the sampling distribution) of our estimated ATE when we ignore covariates is 0.121. When we include covariates in the model, our estimate becomes a bit tighter: the standard error is 0.093. Because our covariates were prognostic of our outcome, including them in the regression explained some noise in our data so that we could tighten our estimate of ATE.\n\n\n5 When will it help?\nWhen is adjusting for covariates most likely to improve precision?\nCovariate adjustment will be most helpful when your covariates are strongly predictive (or “prognostic”) of your outcomes. Covariate adjustment essentially enables you to make use of information about relationships between baseline characteristics and your outcome so that you can better identify the relationship between treatment and the outcome. But if the baseline characteristics are only weakly correlated with the outcome, covariate adjustment won’t do you much good. The covariates you will want to adjust for are the ones that are strongly correlated with outcomes.\nThe following graph demonstrates the relationship between how prognostic your covariate is and the gains you get from adjusting for it. On the x-axis is the sample size, and on the y-axis is the root mean squared error (RMSE), the square root of the average squared difference between the estimator and the true ATE. We want our RMSE to be small, and covariate adjustment should help us reduce it.\n\nrm(list=ls())\nlibrary(MASS)  # for mvrnorm()\nset.seed(1234567)\nnum.reps = 10000\n# True treatment effect is 0 for every unit\nadj.est = function(n, cov.matrix, treated) {\n    Y.and.X  =  mvrnorm(n, mu = c(0, 0), Sigma = cov.matrix)\n    Y   =  Y.and.X[, 1]  \n    X   =  Y.and.X[, 2]\n    coef(lm(Y ~ treated + X))[2]\n}\nunadj.est = function(n, treated) {\n    Y = rnorm(n)\n    coef(lm(Y ~ treated))[2]\n}\nrmse = function(half.n, rho = 0, control = TRUE) {\n    treated  =  rep(c(0, 1), half.n)\n    n = 2 * half.n\n    if (control) {\n        cov.matrix  =  matrix(c(1, rho, rho, 1), nrow = 2, ncol = 2)\n        return( sqrt(mean(replicate(num.reps, adj.est(n, cov.matrix, treated)) ^ 2)) )\n    }\n    else {\n        return( sqrt(mean(replicate(num.reps, unadj.est(n, treated)) ^ 2)) )\n    }\n}\nhalf.n = c(5, 7, 11, 19, 35, 67, 131)\nn = 2 * half.n \nE  = sapply(half.n, rmse, control = FALSE)\nE0 = sapply(half.n, rmse, rho = 0)\nE1 = sapply(half.n, rmse, rho = 0.5)\nE2 = sapply(half.n, rmse, rho = 0.9)\nplot(n, E, type = \"l\", ylab = \"RMSE\", xlim = c(min(n),max(n)), ylim = c(0,.75))\nlines(n, E0, col = \"yellow\")\nlines(n, E1, col = \"orange\")\nlines(n, E2, col = \"red\")\nlegend(x = 'topright',\n       c(\"No controls\",\n         expression(paste(rho, \"=0\")), expression(paste(rho, \"=0.5\")),\n         expression(paste(rho, \"=0.9\"))),\n         col=c(\"black\", \"yellow\",\"orange\", \"red\"), lty = 1, lwd=2)\n\n\nThe black line shows the RMSE when we don’t adjust for a covariate. The red line shows the RMSE when we adjust for a highly prognostic covariate (the correlation between the covariate and the outcome is 0.9). You can see that the red line is always below the black line, which is to say that the RMSE is lower when you adjust for a prognostic covariate. The orange line represents the RMSE when we adjust for a moderately prognostic covariate (the correlation between the covariate and the outcome is 0.5). We still are getting gains in precision relative to the black line, but not nearly as much as we did with the red line. Finally, the yellow line shows what happens if you control for a covariate that is not at all predictive of the outcome. The yellow line is almost identical to the black line. You received no improvement in precision by controlling for a non-prognostic covariate; in fact, you paid a slight penalty because you wasted a degree of freedom, which is especially costly when the sample size is small. This exercise demonstrates that you’ll get the most gains in precision by controlling for covariates that strongly predict outcomes.\nHow can you know which covariates are likely to be prognostic before launching your experiment? Prior experiments or even observational studies can offer guidance about which baseline characteristics best predict outcomes.\n\n\n6 Control for prognostic covariates regardless of whether they show imbalances\nCovariates should generally be chosen on the basis of their expected ability to help predict outcomes, regardless of whether they show “imbalances” (i.e., regardless of whether there are any noteworthy differences between the treatment group and control group in average values or other aspects of covariate distributions). There are two reasons for this recommendation:\n\nFrequentist statistical inference (standard errors, confidence intervals, p-values, etc.) assumes that the analysis follows a pre-specified strategy. Choosing covariates on the basis of observed imbalances makes it more difficult to obtain inferences that reflect your actual strategy. For example, suppose you choose not to control for gender because the treatment and control groups have similar gender composition, but you would have controlled for gender if there’d been a noticeable imbalance. Typical methods for estimating standard errors will incorrectly assume that you’d never control for gender no matter how much imbalance you saw.\nAdjusting for a highly prognostic covariate tends to improve precision, as we explained above. To receive due credit for this precision improvement, you should adjust for the covariate even if there’s no imbalance. For example, suppose gender is highly correlated with your outcome, but it happens that the treatment group and control group have exactly the same gender composition. In this case, the unadjusted estimate of the ATE will be exactly the same as the adjusted estimate from a regression of the outcome on treatment and gender, but their standard errors will differ. The SE of the unadjusted estimate tends to be larger because it assumes that even if the treatment and control groups had very different gender compositions, you’d still use the unadjusted treatment–control difference in mean outcomes (which would likely be far from the true ATE in that case). If you pre-specify that you’ll adjust for gender regardless of how much or how little imbalance you see, you’ll tend to get smaller SEs, tighter confidence intervals, and more powerful significance tests.\n\nAssuming that random assignment was implemented correctly, should examination of imbalances play any role in choosing which covariates to adjust for? Here’s a sampling of views:\n\nMutz, Pemantle, and Pham (2017) argue that, unless there is differential attrition, the practice of selecting covariates on the basis of observed imbalances is “not only unnecessary” but “not even helpful … and may in fact be damaging,” because it invalidates confidence intervals, worsens precision (relative to pre-specified adjustment for prognostic covariates), and opens the door to fishing.\nPermutt (1990), using theory and simulations to study specific scenarios, finds that when a balance test is used to decide whether to adjust for a covariate, the significance test for the treatment effect is conservative (i.e., it has a true Type I error probability below its nominal level). He writes, “Greater power can be achieved by always adjusting for a covariate that is highly correlated with the response regardless of its distribution between groups.” However, he doesn’t completely rule out considering observed imbalances: “Choosing covariates on the basis of the difference between the means in the treatment and control groups is not irrational. After all, some type I errors may be more serious than others. Reporting a significant difference in outcome which can be explained away as the effect of a covariate may be a more embarrassing error than reporting one that happens to go away on replication but without an easy explanation. Similar considerations may apply to type II errors. A positive result that depends on adjustment for a covariate may be seen as less convincing than a positive two-sample test anyway, so that the error of failing to draw such a positive conclusion may be less serious. These justifications, however, come from outside the formal theory of testing hypotheses.”\nAltman (2005) writes, “It seems far preferable to choose which variables to adjust for without regard to the actual data set to hand.” He recommends controlling for highly prognostic covariates, as well as any that were used in blocking. However, he also discusses a dilemma: “In practice, imbalance may arise when the possible need for adjustment has not been anticipated. What should the researchers do? They might choose to ignore the imbalance; as noted, this would be entirely proper. The difficulty then is one of credibility. Readers of their paper (including reviewers and editors) may question whether the observed finding has been influenced by the unequal distribution of one or more baseline covariates. It is still possible, and arguably advisable, to carry out an adjusted analysis, but now with the explicit acknowledgment that this is an exploratory rather than definitive analysis, and that the unadjusted analysis should be taken as the primary one. Obviously, if the simple and adjusted analyses yield substantially the same result, then there is no difficulty of interpretation. This will usually be the case. However, if the results of the two analyses differ, then there is a real problem. The existence of such a discrepancy must cast some doubt on the veracity of the overall (unadjusted) result. The situation is similar to the difficulties of interpretation that arise with unplanned subgroup comparisons. One suggestion in such circumstances is to try to mimic what would have been done if the problem had been anticipated, namely to adjust not for variables that are observed to be unbalanced, but for all variables that would have been identified in advance as prognostic. An independent source could be used to identify such variables. Alternatively, the trial data could be used to determine which variables are prognostic. This strategy too could be prespecified in the study protocol. Because this analysis would be performed conditionally on the observed imbalance, it does not remove bias and thus cannot be considered fully satisfactory.”\nTukey (1991) notes that observed imbalances may justify adjustment as a robustness check: Although “most statisticians” would accept an analysis of a randomized clinical trial that doesn’t adjust for covariates, “Some clinicians, and some statisticians it would seem, would like to be more sceptical, (perhaps as a supplemental analysis) asking for an analysis that takes account of observed imbalances in these recorded covariates. Feeling more secure about the results of such an analysis is indeed appropriate, since the degree of protection against either the consequences of inadequate randomization or the (random) occurrence of an unusual randomization is considerably increased by adjustment. Greater security, rather than increased precision … will often be the basic reason for covariance adjustment in a randomized trial. … The main purpose of allowing [adjusting] for covariates in a randomized trial is defensive: to make it clear that analysis has met its scientific obligations.”\nSome statisticians argue that our inferences should be conditional on a measure of covariate imbalance—in other words, when assessing the bias, variance, and mean squared error of a point estimate or the coverage probability of a confidence interval, instead of considering all possible randomizations, it may be more relevant to consider only those randomizations that would yield a covariate imbalance similar to the one we observe. From this perspective, observed imbalances may be relevant to the choice of estimator.6\nLin, Green, and Coppock (2016) write: “Covariates should generally be chosen on the basis of their expected ability to help predict outcomes, regardless of whether they appear well-balanced or imbalanced across treatment arms. But there may be occasions when the covariate list specified in the PAP [pre-analysis plan] omitted a potentially important covariate (due to either an oversight or the need to keep the list short when N is small) with a nontrivial imbalance. Protection against ex post bias (conditional on the observed imbalance) is then a legitimate concern.” However, they recommend that if observed imbalances are allowed to influence the choice of covariates, “the balance checks and decisions about adjustment should be finalized before we see unblinded outcome data,” “the direction of the observed imbalance (e.g., whether the treatment group or the control group appears more advantaged at baseline) should not be allowed to influence decisions about adjustment,” and the originally pre-specified estimator should “always be reported and labeled as such, even if alternative estimates are also reported.”7\n\n\n\n7 When not to do it\nIt is a bad idea to adjust for covariates when you think those covariates could have been influenced by your treatment. This is one of the reasons that many covariates are collected from baseline surveys; sometimes covariates that are collected from surveys after intervention could reflect the effects of the treatment rather than underlying characteristics of the subject. Adjusting for covariates that are affected by the treatment—“post-treatment” covariates—can cause bias.\nSuppose, for example, that Giné and Mansuri had collected data on how many political rallies a woman attended after receiving the treatment. In estimating the treatment effect on independence of political choice, you may be tempted to include this variable as a covariate in your regression. But including this variable, even if it strongly predicts the outcome, may distort the estimated effect of the treatment.\nLet’s create this fake variable, which is correlated (like the outcome measure) with baseline covariates and also with treatment. Here, by construction, the treatment effect on number of political rallies attended is 2. When we included the rallies variable as a covariate, the estimated average treatment effect on independence of candidate choice averaged 0.54 across the 10,000 replications. Recall that the true treatment effect on this outcome is 1. This is severe bias, all because we controlled for a post-treatment covariate!8 This bias results from the fact that the covariate is correlated with treatment.\n\n# Create post-treatment covariate that's correlated with pre-treatment covariates\nrallies0 = round(.5*owns.id.card + has.formal.schooling + 1.5*TV.access + log(age))\nrallies1 = rallies0 + 2\nrallies.mat = rallies1 * Z.mat + rallies0 * (1-Z.mat)\n \n# Estimate ATE with new model that includes the post-treatment covariate\nadjust.for.post = function(Y, Z, X) {\n  coef(lm(Y ~ Z + X + owns.id.card + has.formal.schooling + age + TV.access))[2]\n}\npost.adjusted.estimates = rep(NA, Replications)\nfor (i in 1:Replications) {\n  post.adjusted.estimates[i]  =  adjust.for.post(Y.mat[,i], Z.mat[,i], rallies.mat[,i])\n}\n# Estimated bias of the new estimator\nmean(post.adjusted.estimates) - true.treatment.effect\n# Margin of error (at 95% confidence level) for the estimated bias\n1.96 * sd(post.adjusted.estimates) / sqrt(Replications)\n\nJust because you should not adjust for post-treatment covariates does not mean you cannot collect covariate data post-treatment, but you must exercise caution. Some measures could be collected post-treatment but are unlikely to be affected by treatment (e.g., age and gender). Be careful about measures that may be subject to evaluation-driven effects, though: for example, treated women may be more acutely aware of the expectation of political participation and may retrospectively report that they were more politically active than they actually were several years prior.\n\n\n8 Concerns about small-sample bias\nIn small samples, regression adjustment may produce a biased estimate of the average treatment effect.9 Some simulations have suggested that this bias tends to be negligible when the number of randomly assigned units is greater than twenty.[^@green_aronow_2011] If you’re working with a small sample, you may want to use an unbiased covariate adjustment method such as post-stratification (splitting the sample into subgroups based on the values of one or more baseline covariates, computing the treatment–control difference in mean outcomes for each subgroup, and taking a weighted average of these subgroup-specific treatment effect estimates, with weights proportional to sample size).10\n\n\n9 How to make your covariate adjustment decisions transparent\nIn the interests of transparency, if you adjust for covariates, pre-specify your models and report both unadjusted and covariate-adjusted estimates.\nThe simulations above have demonstrated that results may change slightly or not-so-slightly depending on which covariates you choose to include in your model. We’ve highlighted some rules of thumb here: include only pre-treatment covariates that are predictive of outcomes. Deciding which covariates to include, though, is often a subjective rather than an objective enterprise, so another rule of thumb is to be totally transparent about your covariate decisions. Always include the simplest model—the simple regression of outcome on treatment without controlling for covariates—in your paper or appendix to supplement the findings of your model including covariates.\nAnother way to minimize your readers’ concern that you went fishing for the particular combination of covariates that gave results favorable to your hypotheses is to pre-specify your models in a pre-analysis plan.11 This gives you the opportunity to explain before you see the findings which pre-treatment covariates you expect to be predictive of the outcome. You can even write these regressions out in R using fake data, as done here, so that when your results from the field arrive, all you need to do is run your code on the real data. These efforts are a useful way of binding your own hands as a researcher and improving your credibility.\n\n\n10 Covariates can help you investigate the integrity of the random assignment\nSometimes it is unclear whether random assignment actually occurred (or whether it occurred using the procedure that the researcher envisions). For example, when scholars analyze naturally occurring random assignments (e.g., those conducted by a government agency), it is useful to assess statistically whether the degree of imbalance between the treatment and control groups is within the expected margin of error. One statistical test is to regress treatment assignment on all of the covariates and calculate the F-statistic. The significance of this statistic can be assessed by simulating a large number of random assignments and for each one calculating the F-statistic; the resulting distribution can be used to calculate the p-value of the observed F-statistic. For example, if 10,000 simulations are conducted, and just 30 simulations generate an F-statistic larger than what one actually obtained from the data, the p-value is 0.003, which suggests that the observed level of imbalance is highly unusual. In such cases, one may wish to investigate the randomization procedure more closely.\n\n\nFor further reading\nAthey, Susan, and Guido W. Imbens (2017). “The Econometrics of Randomized Experiments.” In Handbook of Economic Field Experiments, vol. 1 (E. Duflo and A. Banerjee, eds.). arXiv DOI\nGerber, Alan S., and Donald P. Green (2012). Field Experiments: Design, Analysis, and Interpretation, chapter 4.\nHennessy, Jonathan, Tirthankar Dasgupta, Luke Miratrix, Cassandra Pattanayak, and Pradipta Sarkar (2016). “A Conditional Randomization Test to Account for Covariate Imbalance in Randomized Experiments.” Journal of Causal Inference 4: 61–80.\nJudkins, David R., and Kristin E. Porter (2016). “Robustness of Ordinary Least Squares in Randomized Clinical Trials.” Statistics in Medicine 35: 1763–1773.\nLin, Winston (2012). “Regression Adjustment in Randomized Experiments: Is the Cure Really Worse than the Disease?” Development Impact blog post, part I and part II.\nRaudenbush, Stephen W. (1997). “Statistical Analysis and Optimal Design for Cluster Randomized Trials.” Psychological Methods 2: 173–185.\nWager, Stefan, Wenfei Du, Jonathan Taylor, and Robert Tibshirani (2016). “High-Dimensional Regression Adjustments in Randomized Experiments.” Proceedings of the National Academy of Sciences 113: 12673–12678. arXiv DOI\n\n\n\n\n\n\n\n\n\nReferences\n\nAltman, Douglas G. 2005. “Covariate Imbalance, Adjustment For.” In Encyclopedia of Biostatistics.\n\n\nBruhn, Miriam, and David McKenzie. 2013. “In Pursuit of Balance: Randomization in Practice in Development Field Experiments.” American Economic Journal: Applied Economics 1 (4): 200–232.\n\n\nCox, D. R., and N. Reid. 2000. The Theory of the Design of Experiments.\n\n\nEfron, Bradley. 1978. “Controversies in the Foundations of Statistics.” American Mathematical Monthly 85: 231–46.\n\n\nFreedman, David A. 2008. “On Regression Adjustments in Experiments with Several Treatments.” Annals of Applied Statistics 2: 176–96.\n\n\nGiné, Xavier, and Ghazala Mansuri. 2012. “Together We Will: Experimental Evidence on Female Voting Behavior in Pakistan.” American Economic Journal: Applied Economics 10 (1): 207–35.\n\n\nHolt, D., and T. M. F. Smith. 1979. “Post Stratification.” Journal of the Royal Statistical Society, Series A (General) 142: 33–46.\n\n\nLin, Winston, Donald P. Green, and Alexander Coppock. 2016. Standard Operating Procedures for Don Green’s Lab at Columbia. Version 1.05.\n\n\nLohr, Sharon. 2010. Sampling: Design and Analysis. 2nd ed. Cengage Learning.\n\n\nMiratrix, Luke W., Jasjeet S. Sekhon, and Bin Yu. 2013. “Adjusting Treatment Effect Estimates by Post-Stratification in Randomized Experiments.” Journal of the Royal Statistical Society, Series B 75: 369–96.\n\n\nMutz, Diana C., Robin Pemantle, and Philip Pham. 2017. “The Perils of Balance Testing in Experimental Design: Messy Analyses of Clean Data.”\n\n\nOlken, Benjamin A. 2015. “Promises and Perils of Pre-Analysis Plans.” Journal of Economic Perspectives 29 (3): 61–80.\n\n\nPermutt, Thomas. 1990. “Testing for Imbalance of Covariates in Controlled Experiments.” Statistics in Medicine 9: 1455–62.\n\n\nRoyall, Richard M. 1976. “Current Advances in Sampling Theory: Implications for Human Observational Studies.” American Journal of Epidemiology 104: 463–74.\n\n\nTukey, John W. 1991. “Use of Many Covariates in Clinical Trials.” International Statistical Review 59 (123-137).\n\nFootnotes\n\n\nOriginating author: Lindsay Dolan. Revisions: Don Green and Winston Lin, 1 Nov 2016. The guide is a live document and subject to updating by EGAP members at any time; contributors listed are not responsible for subsequent edits. Thanks to Macartan Humphreys and Diana Mutz for helpful discussions.↩︎\nSee, e.g., pages 217–219 of Bruhn and McKenzie (2013).↩︎\nA brief review of bias and precision: Imagine replicating the experiment many times (without changing the experimental sample and conditions, but re-doing random assignment each time). An unbiased estimator may overestimate or underestimate the ATE on any given replication, but its expected value (the average over all possible replications) will equal the true ATE. We usually prefer unbiased or approximately unbiased estimators, but we also value precision (which is formally defined as the inverse of the variance). Imagine you’re throwing a dart at a dartboard. If you hit the center of the dartboard on average but your shots are often far from the mark, you have an unbiased but imprecise estimator. If you hit close to the center every time, your estimator is more precise. A researcher may choose to accept a small bias in return for a large improvement in precision. One possible criterion for evaluating estimators is the mean squared error, which equals the variance plus the square of the bias. See, e.g., Lohr (2010), pp. 31-32↩︎\n“Sampling variability” refers to the spread of estimates that will be produced just because of the different random assignments that could have been drawn. When the luck of the draw of random assignment produces a treatment group with more As and a control group with more Bs, it is more difficult to separate background characteristics (A and B) from treatment assignment as the predictor of the observed outcomes.↩︎\nThe estimated bias is 0.0003 with a margin of error (at the 95% confidence level) of 0.0018.↩︎\nSee, e.g.: Cox and Reid (2000), (pp. 29-32), Holt and Smith (1979), and Royall (1976). For an introduction to philosophical disagreements about statistical inference, see Efron (1978).↩︎\nLin, Green, and Coppock (2016). Italics in the original.↩︎\nThe estimated bias is \\(-\\) 0.459 with a margin of error (at the 95% confidence level) of 0.002.↩︎\nFreedman (2008). See also Winston Lin’s blog posts (part I and part II) discussing his response to Freedman.↩︎\nMiratrix, Sekhon, and Yu (2013).↩︎\nFor more discussion of pre-analysis plans, see, e.g., Olken (2015).↩︎"
  },
  {
    "objectID": "guides/getting-started/sampling_en.html",
    "href": "guides/getting-started/sampling_en.html",
    "title": "10 Things to Know About Sampling",
    "section": "",
    "text": "Abstract\nResearchers are rarely able to collect measurements on all units that make up the target population of a study. Time and budget constraints typically require the selection of a subset of units — a process called sampling. This guide provides an overview of different ways to sample and how these affect what can be learned from a study. Particular attention is paid to questions about sampling and randomized experiments."
  },
  {
    "objectID": "guides/getting-started/sampling_en.html#simple-random-sampling",
    "href": "guides/getting-started/sampling_en.html#simple-random-sampling",
    "title": "10 Things to Know About Sampling",
    "section": "Simple Random Sampling",
    "text": "Simple Random Sampling\nSimple random sampling is the most basic survey design. In this design, each sample of size \\(n\\) and hence each unit has the same probability of being sampled. One way of drawing a simple random sample of size \\(n\\) from a sampling frame with \\(N\\) units is to enumerate all possible samples of size \\(n\\) and randomly select one of those samples. This is the procedure described in the example above. However, this approach tends to be impractical in real world applications, since actual populations are typically much larger than \\(N=6\\), and the number of possible samples will thus be vast. An alternative procedure is to number all units from 1 to \\(N\\), to generate \\(n\\) random numbers, ideally after setting a random seed (like set.seed() in R), and to select the corresponding units.3"
  },
  {
    "objectID": "guides/getting-started/sampling_en.html#stratification",
    "href": "guides/getting-started/sampling_en.html#stratification",
    "title": "10 Things to Know About Sampling",
    "section": "Stratification",
    "text": "Stratification\nSuppose we know upfront that the characteristic in which we are interested varies across sub-populations. For example, if we aim to estimate average income, we may suspect that men earn more than women. If we draw a simple random sample, it is possible that we end up with a sample that contains more women than men. In this case, our average income estimate will be subject to a large sampling error. A way to guard against this possibility is to divide the population into subgroups, also called strata, and draw an independent random sample in each stratum. For example, we may draw an independent random sample of women and and one of men. This procedure fixes the proportion of women and men in the sample, thereby avoiding “bad” samples and improving the precision of our estimates. Being able to ensure that the sample contains enough units from a particular sub-population is also helpful if estimates among the sub-population are of independent interest. If we would like to estimate the gender pay gap, for example, we need a sample that contains enough men and women to obtain sufficiently precise estimates of each group’s average income. This latter use of stratification is especially important for learning about rare subgroups."
  },
  {
    "objectID": "guides/getting-started/sampling_en.html#clustering",
    "href": "guides/getting-started/sampling_en.html#clustering",
    "title": "10 Things to Know About Sampling",
    "section": "Clustering",
    "text": "Clustering\nSuppose we would like to estimate the average income among residents of a city. Our sample frame may not identify individual residents but we may have access to a list of households. Instead of directly sampling residents, we can randomly select households and interview all members of the selected households. In this case, the primary sampling units — the units that can be selected — differ from the observation units on which measurements are taken. Households serve as PSUs or clusters, while household members serve as observation units. The core downside of cluster sampling is that it typically leads to a loss in precision. For an analogous problem with cluster random assignment and for more on clustering and information reduction see “10 Things to Know About Cluster Randomization”. This loss in precision will be greater when units within the same cluster are more similar to each other (for example, members of the same household may have similar incomes or views). This problem of similarity or dependence within clusters can be severe in studies of politics where all members of a place have the same representative or share similar attitudes (Stoker and Bowers 2002). Nonetheless, cluster sampling may be necessary if sampling frames of individual observation units cannot be obtained. Cluster sampling may also save survey costs. For example, suppose we would like to estimate household-level income. Using households as the primary sampling unit may lead to a sample of households that is dispersed throughout the city, which increases transportation costs. Instead, we may select entire city blocks and interview all households within the selected blocks. Doing so may make it possible to interview more households with a smaller budget. Whether or not the gain in precision from a bigger sample size outweighs the loss in precision from clustering will depend on the degree to which households within the same city block have similar incomes."
  },
  {
    "objectID": "guides/getting-started/sampling_en.html#multi-stage-sampling",
    "href": "guides/getting-started/sampling_en.html#multi-stage-sampling",
    "title": "10 Things to Know About Sampling",
    "section": "Multi-stage sampling",
    "text": "Multi-stage sampling\nInstead of sampling all units in a cluster, one may draw a sub-sample of units. For example, instead of interviewing all members, one could sample two members in each sampled household. Additional stages can be added to this approach. For instance, we could first draw a sample of city blocks, then a sub-sample of households within each sampled city block and finally a sub-sample of household members within each sampled household. In this example, households would be referred to as secondary sampling units and household members as tertiary sampling units. One advantage of multi-stage sampling is that it allows researchers to navigate the trade-off between precision and cost-effectiveness. Increasing the number of clusters and sub-sampling fewer units per cluster can yield a more diverse sample and hence reduce sampling variability. Yet, doing so may also increase survey costs."
  }
]