[
  {
    "objectID": "guides/causal-inference/causal-inference_en.html",
    "href": "guides/causal-inference/causal-inference_en.html",
    "title": "10 Things You Need to Know About Causal Inference",
    "section": "",
    "text": "The philosopher David Lewis described causation as “something that makes a difference, and the difference it makes must be a difference from what would have happened without it.” This is the interpretation given to causality by most experimentalists. Even though the definition seems simple, it has many subtle implications. Here are ten ideas implied by this notion of causality that matter for research design."
  },
  {
    "objectID": "guides/causal-inference/causal-inference_en.html#footnotes",
    "href": "guides/causal-inference/causal-inference_en.html#footnotes",
    "title": "10 Things You Need to Know About Causal Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFollowing Mackie (1974), sometimes the idea of “INUS” conditions is invoked to capture the dependency of causes on other causes. Under this account, a cause may be an Insufficient but Necessary part of a condition which is itself Unnecessary but Sufficient. For example dialing a phone number is a cause of contacting someone since having a connection and dialing a number is sufficient (S) for making a phone call, whereas dialing alone without a connection alone would not be enough (I), nor would having a connection (N). There are of course other ways to contact someone without making phone calls (U).↩︎\nTechnical Note: The key technical insight is that the difference of averages is the same as the average of differences. That is, using the “expectations operator,” \\(𝔼(τ_i)=𝔼(Y_i(1)−Y_i(0))=𝔼(Y_i(1))−𝔼(Y_i(0))\\). The terms inside the expectations operator in the second quantity cannot be estimated, but the terms inside the expectations operators in the third quantity can be (Holland 1986). See illustration here.↩︎\nFor this reason \\(t\\)-tests to check whether “randomization worked” do not make much sense, at least if you know that a randomized procedure was followed — just by chance 1 in 20 such tests will show statistically detectable differences between treated and control groups. If there are doubts about whether a randomized procedure was correctly implemented these tests can be used to test the hypothesis that the data was indeed generated by a randomized procedure. This later reason for randomization tests can be especially important in field experiments where chains of communication from the person creating random numbers and the person implementing treatment assignment may be long and complex.↩︎\nTechnical Note: Let \\(D_i\\) be an indicator for whether unit \\(i\\) has received a treatment or not. Then the difference in average outcomes between those that receive the treatment and those that do not can be written as \\(\\frac{∑_i D_i×Y_i(1)}{∑_iD_i}−\\frac{∑_i (1−D_i)×Y_i(0)}{∑_i (1−D_i)}\\). In the absence of information about how treatment was assigned, we can say little about whether this difference is a good estimator of the average treatment effect, i.e., of the difference in average treated and control potential outcomes across all units. What matters is whether \\(\\frac{∑_i D_i×Y_i(1)}{∑_iD_i}\\) is a good estimate of \\(\\frac{∑_i 1×Y_i(1)}{∑_i1}\\) and whether \\(\\frac{∑_i (1−D_i)×Y_i(0)}{∑_i (1−D_i)}\\) is a good estimate of \\(\\frac{∑_i 1×Y_i(0)}{∑_i1}\\). This might be the case if those who received treatment are a representative sample of all units, but otherwise there is no reason to expect that it would be.↩︎\nInterpret “\\(A\\) causes \\(B\\), on average” as “the average effect of \\(A\\) on \\(B\\) is positive.”↩︎\nSome reinterpret the “causes of effects” question to mean: what are the causes that have effects on outcomes. See Gelman and Imbens (2013).↩︎\nSee, for example, Tian and Pearl (2000)↩︎"
  },
  {
    "objectID": "guides/causal-inference/x-cause-y_en.html",
    "href": "guides/causal-inference/x-cause-y_en.html",
    "title": "10 Strategies for Figuring out if X Causes Y",
    "section": "",
    "text": "1 Randomization\nThe strategy used in randomized control trials (or randomized interventions, randomized experiments) is to use some form of a lottery to determine who, among some group, will or won’t get access to a treatment or program (or perhaps who will get it first and who will get it later, or who will get one version and who will get another). The elegance of the approach is that it uses randomness to work out what the systematic effects of a program are. The randomness reduces the chance that an observed relationship between treatment and outcomes is due to “confounders”—other things that are different between groups (for example one might be worried that things look better in treatment areas precisely because programs choose to work in well-functioning areas, but knowing that the selection was random completely removes this concern). It is powerful because it guarantees that there is no systematic relationship between treatment and all other features that can affect outcomes, whether you are aware of them or not. For this reason it is often considered to be the gold standard. Randomization cannot be used always and everywhere however, both for ethical and practical reasons. But it can be used in many more situations than people think. See Humphreys and Weinstein (2009) for a discussion of strengths and limitations of the approach for research in the political economy of development.\n\n\n2 Experimental Control (induced unit homogeneity)\nA second strategy used more in lab settings and also in the physical sciences is to use experimental control to ensure that two units are identical to each other in all relevant respects except for treatment. For example if you wanted to see if a heavy ball falls faster than a lighter ball you might make sure that they have the same shape and size and drop them both at the same time, under the same weather conditions, and so on. You then attribute any differences in outcomes to the feature that you did not keep constant between the two units. This strategy is fundamentally different to that used in randomized trials. In randomized trials you normally give up on the idea of keeping everything fixed and seek instead to make sure that natural variation—on variables that you can or cannot observe—does not produce bias in your estimates; in addition you normally seek to assess average effects across a range of background conditions rather than for a fixed set of background conditions. The merits of the control approach depend on your confidence that you can indeed control all relevant factors; if you cannot, then a randomized approach may be superior.\n\n\n3 Natural experiments (as-if randomization)\nSometimes researchers are not able to randomize, but causal inference is still possible because nature has done the randomization for you. The key feature of the “natural experiment” approach is that you have reason to believe that variation in some natural treatment is “as-if random.” For example say that seats in a school are allocated by lottery. Then you might be able to analyze the effects of school attendance as if it were a randomized control trial. One clever study of the effects of conflict on children by Blattman and Annan (2010) used the fact that the Lord’s Resistance Army (LRA) in Uganda abducted children in a fairly random fashion. Another clever study on Disarmament, Demobilization, and Reintegration (DDR) programs by Gilligan, Samii, and Mvukiyehe (2012) used the fact that an NGO’s operations were interrupted because of a contract dispute, which resulted in a “natural” control group of ex-combatants that did not receive demobilization programs. See Dunning (2012) for a guide to finding and analyzing natural experiments.\n\n\n4 Before/after comparisons\nOften the first thing that people look to in order to work out causal effects is the comparison of units before and after treatment. Here you use the past as a control for the present. The basic idea is very intuitive: you switch the lightswitch off and you see the light switch off; attributing the light change to the action seems easy even in the absence of any randomization or control. But for many social interventions the approach is not that reliable, especially in changing environments. The problem is that things get better or worse for many reasons unrelated to treatments or programs you are interested in. In fact it is possible that because of all the other things that are changing, things can get worse in a program area even if the programs had a positive effect (so they get worse but are still not as bad as they would have been without the program!). A more sophisticated approach than simple before/after comparison is called “difference in differences” – basically you compare the before/after difference in treatment areas with those in control areas. This is a good approach but you still need to be sure that you have good control groups and in particular that control and treatment groups are not likely to change differently for reasons other than the treatment.\n\n\n5 Ex Post Controlling I: Regression\nPerhaps the most common approach to causal identification in applied statistical work is the use of multiple regression to control for possible confounders. The idea is to try to use whatever information you have about why treatment and control areas are not readily comparable and adjust for these differences statistically. This approach works well to the extent that you can figure out and measure the confounders and how they are related to treatment, but is not good if you don’t know what the confounders are. In general we just don’t know what all the confounders are and that exposes this approach to all kinds of biases (indeed if you control for the wrong variables it is possible to introduce bias where none existed previously).\n\n\n6 Ex Post Controlling II: Matching and Weighting\nA variety of alternative approaches seek to account for confounding variables by carefully matching treatment units to one or many control units. Matching has some advantages over regression (for example, estimates can be less sensitive to choices of functional form), but the basic idea is nevertheless similar, and indeed matching methods can be implemented in a regression framework using appropriate weights. Like regression, at its core, this strategy depends on a conviction that there are no important confounding variables that the researcher is unaware of or is unable to measure. Specific methods include:\n\nOptimal full- and pair-matching (Hansen 2004), and see the optmatch package\nOptimal pair-matching with fine-balance via mixed integer programming (Zubizarreta, Paredes, and Rosenbaum 2014). See also the designmatch package and the paper comparing approaches (Los Angeles Resa and Zubizarreta 2016)\nOptimal multi-level matching (for designs with schools and students) (Pimentel et al. 2018)\nSparse optimal matching\nGeneralized full matching (Sävje, Higgins, and Sekhon 2017)\nCoarsened exact matching\nGenetic matching (Diamond and Sekhon 2013)\nEntropy balancing (Hainmueller 2012)\nInverse propensity weighting (Glynn and Quinn 2010)\nStable balancing weights (Zubizarreta 2015), and the use of\nSynthetic controls (Abadie, Diamond, and Hainmueller 2015).\n\n\n\n7 Instrumental variables (IV)\nAnother approach to identifying causal effects is to look for a feature that explains why a given group got a treatment but which is otherwise unrelated to the outcome of interest. Such a feature is called an instrument. For example say you are interested in the effect of a livelihoods program on employment, and say it turned out that most people who got access to the livelihoods program did so because they were a relative of a particular program officer. Now suppose that being a relative of the program officer does not affect job prospects in any way other than through its effect on getting access to the livelihoods program. If so, then you can work out the effect of the program by understanding the effect of being a relative of the program officer on job prospects. This has been a fairly popular approach but the enthusiasm for it has died a bit, basically because it is hard to find a good instrument. One smart application are studies on the effects of poverty on conflict which use rainfall in Africa as an instrument for income/growth. While there are worries that the correlation between conflict and poverty may be due to the fact that conflict causes poverty, it does not seem plausible that conflict causes rainfall! So using rainfall as an instrument here gave a lot more confidence that really there is a causal, and not just correlational, relationship between poverty and conflict (Miguel, Satyanath, and Sergenti 2004).\n\n\n8 Regression discontinuity designs (RDD)\nThe regression discontinuity approach works as follows. Say that some program is going to be made available to a set of potential beneficiaries. These potential beneficiaries are all ranked on a set of relevant criteria, such as prior education levels, employment status, and so on. These criteria can be quantitative; but they can also include qualitative information such as assessments from interviews. These individual criteria are then aggregated into a single score and a threshold is identified. Candidates scoring above this threshold are admitted to the program, while those below are not. “Project” and “comparison” groups are then identified by selecting applicants that are close to this threshold on either side. Using this method we can be sure that treated and control units are similar, at least around the threshold. Moreover, we have a direct measure of the main feature on which they differ (their score on the selection criteria). This information provides the key to estimating a program effect from comparing outcomes between these two groups. The advantage of this approach is that all that is needed is that the implementing agency uses a clear set of criteria (which can be turned into a score) upon which they make treatment assignment decisions. The disadvantage is that really reliable estimates of impact can only be made for units right around the threshold. For overviews of RDD, see Skovron and Titiunik (2015) and Lee and Lemieux (2013); for two interesting applications, see Manacorda, Miguel, and Vigorito (2011) on Uruguay and Samii (2013) on Burundi.\n\n\n9 Process tracing\nIn much qualitative work researchers try to establish causality by looking not just at whether being in a program is associated with better outcomes but (a) looking for steps in the process along the way that would tell you whether a program had the effects you think it had and (b) looking for evidence of other outcomes that should be seen if (or perhaps: if and only if) the program was effective. For example not just whether people in a livelihoods program got a job but whether they got trained in something useful, got help from people in the program to find an employer in that area, and so on. If all these steps are there, that gives confidence that the relationship is causal and not spurious. If a program was implemented but no one actually took part in it, this might give grounds to suspect that any correlation between treatment and outcomes is spurious. The difficulty with this approach is that it can be hard to know whether any piece of within-case evidence has probative value. For example a program may have positive (or negative) effects through lots of processes that you don’t know anything about and processes that you think are important, might not be. See Humphreys and Jacobs (2015) for a description of the Bayesian logic underlying process tracing and illustrations of how to combine it with other statistical approaches.\n\n\n10 Front Door Strategies (Argument from mechanisms)\nA final approach, conceptually close to process tracing, is to make use of mechanisms. Say you know, as depicted in the picture below, that \\(A\\) can cause \\(C\\) only through \\(B\\). Say moreover that you know that no third variable causes both \\(B\\) and \\(C\\) (other than, perhaps, via \\(A\\)) and no third variable causes both \\(A\\) and \\(B\\). Then covariation between \\(A\\) and \\(B\\) and between \\(B\\) and \\(C\\) can be used to assess the effect of \\(A\\) on \\(C\\). The advantage is that causality can be established even in the presence of confounders — for example even if, as in the picture below, unobserved variables cause both \\(A\\) and \\(C\\). The difficulty however is that the strategy requires a lot of confidence in your beliefs about the structure of causal relations. For more see Pearl (2000).\n\n\n\n\n\n\n\n\n\n11 References\n\nAbadie, Alberto, Alexis Diamond, and Jens Hainmueller. 2015. “Comparative Politics and the Synthetic Control Method.” American Journal of Political Science 59 (2): 495–510.\n\n\nBlattman, Christopher, and Jeannie Annan. 2010. “The Consequences of Child Soldiering.” The Review of Economics and Statistics 92 (4): 882–98.\n\n\nDiamond, Alexis, and Jasjeet S. Sekhon. 2013. “Genetic Matching for Estimating Causal Effects: A General Multivariate Matching Method for Achieving Balance in Observational Studies.” The Review of Economics and Statistics 95 (3): 932–45.\n\n\nDunning, Thad. 2012. Natural Experiments in the Social Sciences Natural Experiments in the Social Sciences: A Design-Based Approach. Cambridge University Press.\n\n\nGilligan, Michael J., Cyrus Samii, and Eric N. Mvukiyehe. 2012. “Reintegrating Rebels into Civilian Life: Quasi-Experimental Evidence from Burundi.” Journal of Conflict Resolution 57 (4).\n\n\nGlynn, Adam N., and Kevin M. Quinn. 2010. “An Introduction to the Augmented Inverse Propensity Weighted Estimator.” Political Analysis 18 (1): 36–56.\n\n\nHainmueller, Jens. 2012. “Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies.” Political Analysis 20: 25–46.\n\n\nHansen, Ben B. 2004. “Full Matching in an Observational Study of Coaching for the SAT.” Journal of the American Statistical Association 99 (467): 609–18.\n\n\nHumphreys, Macartan, and Alan Jacobs. 2015. “Mixing Methods: A Bayesian Approach.” American Political Science Review 109 (4): 653–73.\n\n\nHumphreys, Macartan, and Jeremy M. Weinstein. 2009. “Field Experiments and the Political Economy of Development.” Annual Review of Political Science 12: 367–78.\n\n\nLee, David S., and Thomas Lemieux. 2013. “The SAGE Handbook of Regression Analysis and Causal Inference.” In. SAGE Publications Ltd.\n\n\nLos Angeles Resa, Marı́a de, and José R. Zubizarreta. 2016. “Evaluation of Subset Matching Methods and Forms of Covariate Balance.” Statistics in Medicine 35 (27): 4961–79.\n\n\nManacorda, Marco, Edward Miguel, and Andrea Vigorito. 2011. “Government Transfers and Political Support.” American Economic Journal: Applied Economics 3 (3): 1–28.\n\n\nMiguel, Edward, Shanker Satyanath, and Ernest Sergenti. 2004. “Economic Shocks and Civil Conflict: An Instrumental Variables Approach.” Journal of Political Economy 112 (4): 725–53.\n\n\nPearl, Judea. 2000. Causality: Models, Reasoning, and Inference. Cambridge University Press.\n\n\nPimentel, Samuel D., Lindsay C. Page, Matthew Lenard, and Luke Keele. 2018. “Optimal Multilevel Matching Using Network Flows: An Application to a Summer Reading Intervention.” Ann. Appl. Stat. 12 (3): 1479–1505.\n\n\nSamii, Cyrus. 2013. “Perils or Promise of Ethnic Integration? Evidence from a Hard Case in Burundi.” American Political Science Review 107 (3): 558–73.\n\n\nSävje, Fredrik, Michael J. Higgins, and Jasjeet S. Sekhon. 2017. “Generalized Full Matching and Extrapolation of the Results from a Large-Scale Voter Mobilization Experiment.”\n\n\nSkovron, Christopher, and Rocı́o Titiunik. 2015. “A Practical Guide to Regression Discontinuity Designs in Political Science.” In.\n\n\nZubizarreta, José R. 2015. “Stable Weights That Balance Covariates for Estimation with Incomplete Outcome Data.” Journal of the American Statistical Association 110 (511): 910–22.\n\n\nZubizarreta, José R., Ricardo D. Paredes, and Paul R. Rosenbaum. 2014. “Matching for Balance, Pairing for Heterogeneity in an Observational Study of the Effectiveness of for-Profit and Not-for-Profit High Schools in Chile.” Ann. Appl. Stat. 8 (1): 204–31."
  },
  {
    "objectID": "guides/causal-inference/causal-inference_fr.html",
    "href": "guides/causal-inference/causal-inference_fr.html",
    "title": "10 choses à savoir sur l’inférence causale",
    "section": "",
    "text": "Le philosophe David Lewis a décrit la causalité comme “quelque chose qui fait une différence, et cette différence doit être une différence par rapport à ce qui se serait passé sans elle”. Ceci est l’interprétation de la causalité pour la plupart des expérimentalistes. Même si la définition semble simple, elle a de nombreuses implications subtiles. Voici dix idées impliquées par cette notion de causalité qui importent pour la conception de recherche. 1"
  },
  {
    "objectID": "guides/causal-inference/causal-inference_fr.html#footnotes",
    "href": "guides/causal-inference/causal-inference_fr.html#footnotes",
    "title": "10 choses à savoir sur l’inférence causale",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAuteur d’origine : Macartan Humphreys. Révisions mineures : Winston Lin et Donald P. Green, 24 juin 2016. Révisions MH 6 janvier 2020. Révisions Anna Wilke mai 2021. Le guide est un document vivant et peut être mis à jour par les membres de EGAP à tout moment ; les contributeurs répertoriés ne sont pas responsables des modifications ultérieures.↩︎\nHolland, Paul W. “Statistics and causal inference.” Journal of the American Statistical Association 81.396 (1986): 945-960.↩︎\nHolland, Paul W. “Statistics and causal inference.” Journal of the American Statistical Association 81.396 (1986): 945-960.↩︎\nCertains appellent cela le “problème des causes de prodigalité”.↩︎\nMackie a présenté l’idée de conditions dites “INSS” (“INUS” en anglais) pour capturer la dépendance des causes sur d’autres causes. Une cause peut être une partie Insuffisante mais Nécessaire d’une condition qui est elle-même Superflue mais Suffisante. Par exemple, composer un numéro de téléphone est une cause de “contacter quelqu’un” car avoir une connexion et composer un numéro est suffisant (S) pour passer un appel téléphonique, alors que composer seul sans connexion ne suffirait pas (I), ni avoir un connexion (N). Il existe bien sûr d’autres moyens de contacter quelqu’un sans passer d’appels téléphoniques (S). Mackie, John L. “The cement of the universe.” London: Oxford Uni (1974).↩︎\nNote technique : La principale idée technique est que la différence des moyennes est la même que la moyenne des différences. C’est-à-dire, en utilisant “l’opérateur d’espérance”, \\(𝔼(τ_i)=𝔼(Y_i(1)−Y_i(0))=𝔼(Y_i(1))−𝔼(Y_i(0))\\). Les termes à l’intérieur de l’opérateur d’espérance dans la deuxième quantité ne peuvent pas être estimés, mais les termes à l’intérieur de l’opérateur d’espérance dans la troisième quantité peuvent l’être.6Voir l’illustration ici.↩︎\nPour cette raison, les tests-\\(t\\) pour vérifier si “la randomisation a fonctionné” n’ont pas beaucoup de sens, du moins si vous savez qu’une procédure randomisée a été suivie — simplement par hasard, 1 test sur 20 montrera des différences statistiquement détectables entre les groupes de traitement et de contrôle. En cas de doute sur la mise en œuvre correcte d’une procédure randomisée, ces tests peuvent être utilisés pour tester l’hypothèse selon laquelle les données ont bien été générées par une procédure randomisée. Ces tests peuvent alors être particulièrement importants pour des expériences de terrain où les chaînes de communication entre la personne randomisant et la personne mettant en œuvre l’assignation du traitement peuvent être longues et complexes.↩︎\nNote technique: soit \\(D_i\\) un indicateur pour savoir si l’unité \\(i\\) a reçu un traitement ou non. Alors, la différence des résultats moyens entre ceux qui reçoivent le traitement et ceux qui ne le reçoivent pas peut s’écrire \\(\\frac{∑_i D_i×Y_i(1)}{∑_iD_i}−\\frac{∑_i (1−D_i)× Y_i(0)}{∑_i (1−D_i)}\\). En l’absence d’informations sur la manière dont le traitement a été assigné, nous ne pouvons pas dire si cette différence est un bon estimateur de l’effet moyen du traitement, c’est-à-dire de la différence entre les résultats potentiels moyens pour les groupes de traitement et de contrôle pour toutes les unités. Ce qui importe est de savoir si \\(\\frac{∑_i D_i×Y_i(1)}{∑_iD_i}\\) est une bonne estimation de \\(\\frac{∑_i 1×Y_i(1)}{∑_i1}\\) et si \\(\\frac {∑_i (1−D_i)×Y_i(0)}{∑_i (1−D_i)}\\) est une bonne estimation de \\(\\frac{∑_i 1×Y_i(0)}{∑_i1}\\). Cela pourrait être le cas si ceux qui ont reçu un traitement sont un échantillon représentatif de toutes les unités, mais sinon il n’y a aucune raison de s’attendre à ce qu’il le soit.↩︎\nInterprétez “\\(A\\) cause \\(B\\), en moyenne” comme “l’effet moyen de \\(A\\) sur \\(B\\) est positif”.↩︎\nCertains réinterprètent la question des “causes des effets” comme suit : quelles sont les causes qui ont des effets sur les résultats. Voir Andrew Gelman and Guido Imbens, “Why ask why? Forward causal inference and reverse causal questions”, NBER Working Paper No. 19614 (Nov. 2013).↩︎\nVoir, par exemple, Tian, J., Pearl, J. 2000. “Probabilities of Causation: Bounds and Identification.” Annals of Mathematics and Artificial Intelligence 28:287–313.↩︎"
  },
  {
    "objectID": "guides/causal-inference/causal-inference_esp.html",
    "href": "guides/causal-inference/causal-inference_esp.html",
    "title": "10 cosas que debe saber sobre la inferencia causal",
    "section": "",
    "text": "El filósofo David Lewis describió la causalidad como “algo que marca la diferencia, y esa diferencia que hace, debe ser la diferencia entre lo que fue y lo que hubiera sido sin ese algo”. Esta es la interpretación que dan a la causalidad la mayoría de los experimentalistas. Aunque la definición parece simple, tiene muchas implicaciones sutiles. Aquí les presentamos diez ideas implícitas en esta noción de causalidad que son importantes para el diseño de investigación.1"
  },
  {
    "objectID": "guides/causal-inference/causal-inference_esp.html#footnotes",
    "href": "guides/causal-inference/causal-inference_esp.html#footnotes",
    "title": "10 cosas que debe saber sobre la inferencia causal",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAutor: Macartan Humphreys. Revisiones menores: Winston Lin y Donald P. Green, 24 de junio de 2016. Revisiones MH 6 de enero de 2020. Revisiones Anna Wilke de mayo de 2021. Esta guía es un documento dinámico y está sujeta a actualización por parte de los miembros de EGAP; los colaboradores enumerados no son responsables de las ediciones posteriores.↩︎\nHolland, Paul W. “Statistics and causal inference.” Journal of the American Statistical Association 81.396 (1986): 945-960.↩︎\nHolland, Paul W. “Statistics and causal inference.” Journal of the American Statistical Association 81.396 (1986): 945-960.↩︎\nEsto se conoce a veces como el “Problema de las causas excesivas”.↩︎\nDe acuerdo a Mackie, a veces se invoca la idea de condiciones “INUS” para capturar la dependencia de las causas de otras causas. Según esta explicación, una causa puede ser parte Insuficiente pero Necesaria de una condición que en sí misma es Innecesaria pero Suficiente. Por ejemplo, marcar un número de teléfono es una causa de contacto con alguien, ya que tener una conexión y marcar un número es suficiente (S) para hacer una llamada telefónica, mientras que marcar solo sin una conexión no sería suficiente (I), ni tener una conexión (N). Por supuesto, hay otras formas de contactar a alguien sin hacer llamadas telefónicas (U). Mackie, John L. “El cemento del universo”. Londres: Oxford Uni (1974).↩︎\nNota técnica: La idea técnica clave es que la diferencia de promedios es la misma que el promedio de diferencias. Es decir, usando el “operador de expectativas”, \\(\\text{E}(\\tau_i) = \\text{E}(Y_i (1) -Y_i (0)) = \\text{E}(Y_i (1)) - \\text{E}(Y_i (0))\\). Los términos dentro del operador de esperanzas en la segunda cantidad no se pueden estimar, pero los términos dentro de los operadores de expectativas en la tercera cantidad si se pueden ser estimados6 Vea la ilustración [aquí] (https://raw.githubusercontent.com/egap/ guías-métodos / maestro / inferencia-causal / PO.jpg).↩︎\nPor esta razón usar las pruebas \\(t\\) para verificar si “la asignación aleatoria funcionó bien” no tiene mucho sentido, al menos si se sabe que se siguió una procedimiento aleatorio: por simple chance, 1 de cada 20 de esas pruebas mostrará diferencias estadísticamente detectables entre los grupos tratados y de control. Si existen dudas sobre si la asignación aleatoria se realizó correctamente, estas pruebas se pueden utilizar para probar la hipótesis de que los datos se generaron efectivamente mediante un procedimiento aleatorio. Esta última razón para las pruebas de aleatorización puede ser especialmente importante en experimentos de campo donde las cadenas de comunicación entre la persona que crea los números aleatorios y la persona que implementa la asignación del tratamiento son largas y complejas.↩︎\nNota técnica: Sea \\(D_i\\) un indicador de si la unidad \\(i\\) ha recibido un tratamiento o no. Entonces la diferencia en los resultados promedio entre los que reciben el tratamiento y los que no lo reciben se puede escribir como \\(\\frac{\\sum_i D_i × Y_i (1)} {\\sum_iD_i} - \\frac {\\sum_i(1 - D_i) \\times Y_i (0)}{\\sum_i (1 - D_i)}\\). Sin información sobre cómo se asignó el tratamiento, no hay mucho por decir sobre si esta diferencia es un buen estimador del efecto promedio del tratamiento. Es decir, de la diferencia en los resultados potenciales promedio de las unidades en el grupo de tratamiento y control para todas las unidades. Lo que importa es si \\(\\frac{\\sum_i D_i × Y_i (1)} {\\sum_iD_i}\\) es una buena estimación de \\(\\frac{\\sum_i 1 × Y_i (1)} {\\sum_i1}\\) y si \\(\\frac{\\sum_i (1 - D_i) × Y_i (0)}{\\sum_i(1 - D_i)}\\) es una buena estimación de \\(\\frac{\\sum_i 1 × Y_i (0)} {\\sum_i1}\\). Este puede ser el caso si los que recibieron tratamiento son una muestra representativa de todas las unidades, pero de lo contrario no hay razón para esperar que así sea.↩︎\nEntiéndase la expresión “\\(A\\) causa \\(B\\), en promedio” como “el efecto promedio de \\(A\\) sobre \\(B\\) es positivo”.↩︎\nA veces se reinterpreta la pregunta “causas de los efectos” en el sentido de: ¿cuáles son las causas que tienen efectos sobre las variable de resultado? Véase Andrew Gelman and Guido Imbens, “Why ask why? Forward causal inference and reverse causal questions”, NBER Working Paper No. 19614 (Nov. 2013).↩︎\nVer, por ejemplo, Tian, J., Pearl, J. 2000. “Probabilities of Causation: Bounds and Identification.” Annals of Mathematics and Artificial Intelligence 28:287–313.↩︎"
  },
  {
    "objectID": "guides/data-strategies/spillovers_en.html",
    "href": "guides/data-strategies/spillovers_en.html",
    "title": "10 Things to Know About Spillovers",
    "section": "",
    "text": "This guide helps you think through how to design and analyze experiments when there is a risk of “interference” between units. This has been an important area of research in recent years and there have been real gains in our understanding of how to detect spillover effects. Spillovers arise whenever one unit is affected by the treatment status of another unit. Spillovers make it difficult to work out causal effects (we say why below). Experimentalists worry a lot about them, but the complications that spillovers create are not unique to randomized experiments."
  },
  {
    "objectID": "guides/data-strategies/spillovers_en.html#footnotes",
    "href": "guides/data-strategies/spillovers_en.html#footnotes",
    "title": "10 Things to Know About Spillovers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote though, it is generally very difficult to guess the direction of the bias that would be induced by spillover. Claims like, “spillover would only make our treatment effects appear stronger” usually depend on assumptions of treatment (and spillover) effect homogeneity.↩︎"
  },
  {
    "objectID": "guides/data-strategies/adaptive-design_en.html",
    "href": "guides/data-strategies/adaptive-design_en.html",
    "title": "10 Things to Know About Adaptive Experimental Design",
    "section": "",
    "text": "What is an “adaptive” design?\nA static design applies the same procedures for allocating treatments and measuring outcomes throughout the trial. In contrast, an adaptive design may, based on interim analysis of the trial’s result, change the allocation of subjects to treatment arms or may change the allocation of resources to different outcome measures.\nOrdinarily, mid-course changes in experimental design are viewed with skepticism since they open the door to researcher interference in ways that could favor certain results. In recent years, however, statisticians have developed methods to automate adaptation in ways that either lessen the risk of interference or facilitate bias correction at the analysis stage.\n\n\nWhat are the potential advantages of an adaptive design?\nAdaptive designs have the potential to detect the best-performing experimental arm(s) more quickly than a static design (i.e., with fewer data-collection sessions and fewer subjects). When these efficiencies are realized, resources may be reallocated to achieve other research objectives.\nAdaptive designs also have the potential to lessen the ethical concerns that arise when subjects are allocated to inferior treatment arms. For therapeutic interventions, adaptive designs may reduce subjects’ exposure to inferior treatments; for interventions designed to further broad societal objectives, adaptive designs may hasten the discovery of superior interventions.\nTo illustrate the potential advantages of adaptive design, we simulate an RCT involving a control group and eight treatment arms. We administer treatments and gather 100 outcomes during each “period.” The simulation assumes that each subject’s outcome is binary (e.g., good versus bad). The adaptive allocation of subjects is based on interim analyses conducted at the end of each period. We allocate next period’s subjects according to posterior probabilities that a given treatment arm is best (see below). The simulation assumes that the probability of success is 0.10 for all arms except one, which is 0.20. The stopping rule is that the RCT is halted when one arm is found to have a 95% posterior probability of being best.\nIn the adaptive trial depicted below, the best arm (the red line) is correctly identified, and the trial is halted after 23 periods (total N=2300).\n\n\n\nWhat are the potential disadvantages of adaptive designs?\nThere is no guarantee that adaptive design will be superior in terms of speed or accuracy. For example, adaptive designs may result in a lengthy trial in cases where all of the arms are approximately equally effective. Even when one arm is truly superior, adaptive searches have some probability of resulting in long, circuitous searches (and considerable expense) if by chance they get off to a bad start (i.e., one of the inferior arms appears to be better than the other based on an initial round of results).\nFor instance, consider the following scenario in which all but one of the arms have a 0.10 probability of success, and the superior arm has a 0.12 probability of success (with the same trial design as in the previous example). The design eventually settles on the truly superior arm but only after more than 200 periods (N = 23,810). Even after 50 periods, the results provide no clear sense that any of the arms is superior.\n\nA further disadvantage of adaptive designs is they may produce biased estimates of the average treatment effect of the apparent best arm vis-à-vis the control group. Bias arises because the trial stops when the best arm crosses a threshold suggesting optimality; this stopping rule tends to favor lucky draws that suggest the efficacy of the winning arm. Conversely, when adaptive algorithms associate sampling probability with observed history, under-estimation for inferior arms, including the control group, may persist until stopping time (Nie et al. 2017).\nFor example, in the first scenario described above in which all arms have a 0.10 probability of success except for the best arm, which is 0.20, the average estimated success probability for the truly best arm is 0.202 across 1000 simulated experiments, while the control group average is found to 0.083. The average estimated difference in success probabilities (i.e., the average treatment effect) is 0.119, as compared to the true value of 0.10.\nIn the second scenario, in which the best arm’s success probability is just 0.12, the average estimated success probability for the best arm is 0.121, and the average estimated ATE is 0.027, as compared to the true ATE of 0.02. Bias in this case is relatively small on a percentage point scale due to the very large size of the average experiment.\n\n\nWhat kinds of experiments lend themselves to adaptive design?\nAdaptive designs require multiple periods of treatment and outcome assessment.\nAdaptive designs are well suited to survey, on-line, and lab experiments, where participants are treated and outcomes measured in batches over time.\nSome field experiments are conducted in stages, although the logistics of changing treatment arms may be cumbersome, as discussed below. One possible opportunity for adaptive design in a field context occurs when a given experiment is to be deployed over time in a series of different regions. This allows for adaptation based on region-by-region interim analyses.\nAdaptive designs are ill-suited to one-shot interventions with outcomes measured at a single point in time. For example, experiments designed to increase voter turnout in a given election do not lend themselves to adaptive design because everyone’s outcome is measured at the same time, leaving no opportunity for adaptation.\n\n\nWhat is the connection between adaptive designs and “multi-arm bandit problems”?\nThe multi-arm bandit problem (Scott 2010) is a metaphor for the following optimization problem. Imagine that you could drop a coin in one of several slot machines that may pay off at different rates. (Slot machines are sometimes nicknamed “one-arm bandits,” hence the name.) You would like to make as much money as possible. The optimization problem may be characterized as a trade off between learning about the relative merits of the various slot machines – exploration – and reaping the benefits of employing the best arm – exploitation. A static design may be viewed as an extreme case of allocating subjects solely for exploration.\nAs applied to RCTs, the aim is to explore the merits of the various treatment arms while at the same time reaping the benefits of the best arm or arms. Although the MAB problem is not specifically about estimating treatment effects, one could adjust the optimization objective so that the aim is to find the treatment arm with the greatest apparent superiority over the control group.\n\n\nWhat are some widely used algorithms for automating “adaptation”?\nThe most commonly used methods employ some form of “Thompson sampling” (Thompson 1933). Interim results are assessed periodically, and in the next period subjects are assigned to treatment arms in proportion to the posterior probability that a given arm is best. The more likely an arm is to be “best,” the more subjects it receives.\nMany variations on this basic assignment routine have been proposed, and some are designed to make it less prone to bias. If an adaptive trial is rolled out during a period in which success rates tend to be growing, increasing allocation of subjects to the best arm will tend to exaggerate that arm’s efficacy relative to the other arms, which receive fewer subjects during the high-yield period. In order to assess bias and correct for it, it may be useful to allocate some subjects in every period according to a static design. In this case, inverse probability weights for each period may be used to obtain unbiased estimates of the average treatment effect. (See Gerber and Green 2012 on the use of inverse probability weights for estimation of average treatment effects when the probability of assignment varies from block to block.)\n\n\nWhat are the symptoms of futile search?\nAlthough it is impossible to know for sure whether a drawn out search reflects an unlucky start or an underlying reality in which no arm is superior, the longer an adaptive trial goes, the more cause for concern. The following graphs summarize the distribution of stopping times for three scenarios. Stopping was dictated by at 10% value remaining criterion. Specifically, the trial stopped when the top of the 95% confidence interval showed that no other arm was likely to offer at least a 10 percent (not percentage point) gain in success rate. The first two scenarios were described above; the third scenario considers a case in which there are two superior arms. The graph illustrates how adaptive trials tend to conclude faster when the superiority of the best arm(s) is more clear-cut.\n\n\n\nWhat implications do adaptive designs have for pre-analysis plans?\nThe use of adaptive designs introduces additional decisions, which ideally should be addressed ex ante so as to limit researcher bias. For example, the researcher should specify which algorithms will be used for allocation. It is especially important to specify the stopping rule. Depending on the researcher’s objectives, this rule may focus on achieving a desired posterior probability, or it may use a “value remaining” criterion that considers whether one or more arms have shown themselves to be good enough vis-à-vis the alternative arms. Other hybrid stopping criteria may also be specified. The pre-analysis plan should also describe the analytic steps that will be taken to correct for bias.\n\n\nAre multi-arm bandit trials frequently used in social science?\nMany of the applications to date have taken place in commercial settings, such as website design for high-volume e-commerce, or in settings such as on-line fundraising. Relatively few applications have been written up in detail for a scholarly audience. Surprisingly rare are applications in biomedical research. As Villar, Bowden, and Wason (2015) note, “Despite this apparent near-perfect fit between a real-world problem and a mathematical theory, the MABP has yet to be applied to an actual clinical trial.”\nPutting aside the use of multi-arm bandit approaches, the use of adaptive trials is gradually winning acceptance in biomedical research. For details, see Chin (2016).\n\n\nWhat other considerations should inform the decision to use adaptive design?\nAs noted above, adaptive designs add to the complexity of the research design and analysis. They also may increase the challenges of implementation, particularly in field settings where the logistical or training costs associated with different arms vary markedly. Even when one arm is clearly superior (inferior), the lead-time necessary to staff or outfit this arm may make it difficult to scale it up (down). Adaptive designs are only practical if adaptation is feasible.\nOn the other hand, funders and implementation partners may welcome the idea of an experimental design that responds to on-the-ground conditions such that problematic arms are scaled back. A middle ground between static designs and designs that envision adaptation over many periods are adaptive designs involving only two or three interim analyses and adjustments. Such trials are winning increased acceptance in biomedical research (Chow and Chang 2008) and are likely to become more widely used in the social sciences too. The growing interest in replication and design-based extensions of existing experiments to aid generalization are likely to create opportunities for adaptive design.\n\n\n\n\n\n\n\n\nReferences\n\nChin, Richard. 2016. Adaptive and Flexible Clinical Trials. CRC Press.\n\n\nChow, Shein-Chung, and Mark Chang. 2008. “Adaptive Design Methods in Clinical Trials – a Review.” Orphanet Journal of Rare Diseases 3 (11).\n\n\nNie, Xinkun, Xiaoying Tian, Jonathan Taylor, and James Zou. 2017. “Why Adaptively Collected Data Have Negative Bias and How to Correct for It.”\n\n\nScott, Steven L. 2010. “A Modern Bayesian Look at the Multi-Armed Bandit.” Applied Stochastic Models in Business and Industry 26: 639–58.\n\n\nThompson, William R. 1933. “On the Likelihood That One Unknown Probability Exceeds Another in View of the Evidence of Two Samples.” Biometrika 25: 285–94.\n\n\nVillar, Sofı́a S., Jack Bowden, and James Wason. 2015. “Multi-Armed Bandit Models for the Optimal Design of Clinical Trials: Benefits and Challenges.” Statistical Science: A Review Journal of the Institute of Mathematical Statistics 30 (2): 199–215."
  },
  {
    "objectID": "guides/data-strategies/missing-data_en.html",
    "href": "guides/data-strategies/missing-data_en.html",
    "title": "10 Things to Know About Missing Data",
    "section": "",
    "text": "What is missing data?\nWhen variables are missing some data values, we say that there is “missing data.” Depending on your software and the coding of the dataset, missing values may be coded as NA, ., an empty cell (\"\"), or a common numeric code (often -99 or 99).\nThe consequences of missing data for estimation and interpretation depend on the type of variable missing the data. For our purposes, we will consider three types of variables: pretreatment covariates, treatment indicator(s), and outcome (or dependent) variables. Pretreatment covariates, often known simply as “covariates,” are variables that we observe and measure before treatment is assigned. Outcome (or dependent) variables refer to outcomes that are measured after the assignment of treatment.\nMissing data emerges for different reasons. In survey data, a respondent could decline to answer a question or quit the survey without completing all questions. In a panel survey, some subjects may skip the second or later waves. With administrative data, records may be lost at some point in the process of collecting or recording data. To the extent that we can know the process by which data becomes missing, we can better understand the consequences of missing data for our analysis and inferences.\n\n\nMissing treatment or outcome data can bias our ability to describe empirical patterns and estimate causal effects.\nMissing data can induce bias in our estimates of descriptive patterns and causal effects. Consider a researcher trying to describe the income distribution in a country with survey data. Some individuals’ incomes are missing but the researcher describes the non-missing data at hand. Suppose low-income individuals are less likely to report their income than high-income individuals, thus missingness concentrates in the lower portion of the distribution. Then, the researcher’s characterization of the income distribution is apt to be biased. For example, the researcher’s estimate of median income is bound to be higher than the true (unknown) median income because more data is missing from the lower portion of the distribution. Since missingness is correlated with the variable that we are trying to describe, our characterization of the median of the distribution is biased.\nWe can illustrate two types of missingness by considering a variable \\(x\\) – for example, income, as above. The left panel shows the full distribution of the variable \\(x\\) without missing data. The middle panel depicts a scenario in which simulated missingness in \\(x\\) is not independent of \\(x\\) – low values of \\(x\\) are much more likely to be missing. The right panel depicts a scenario in which simulated missingness in \\(x\\) is independent of \\(x\\) (often called “missing at random”). The red line represents the median of the distribution without missingness. The blue lines represent the median of the observed data in each simulation. Where missingness is not independent of \\(x\\), we observe that the median of the nonmissing data is, in this instance, higher than the true median. This illustrates that missing data can bias our descriptions of single variables.\n\nlibrary(rmutil)\nlibrary(dplyr)\nlibrary(ggplot2)\ndata.frame(x = rep(rchisq(n = 1000, df = 5), 3),\n           panel = rep(c(\"No missing data\", \"Missingness is not\\nindependent of x\", \"Missingness is\\nindependent of x\"), each = 1000)) %&gt;%\n  mutate(panel = factor(panel, levels  = unique(panel)),\n         missing_cor = rbinom(n = 3000, size = 1,  prob = (1 - pchisq(x, df = 5))),\n         missing_ind = rbinom(n = 3000, size = 1, prob = .5),\n         obs = ifelse(missing_cor == 1 & panel == \"Missingness is not\\nindependent of x\", NA, \n                      ifelse(missing_ind == 1 & panel == \"Missingness is\\nindependent of x\", NA, x))) %&gt;%\n  group_by(panel) %&gt;%\n  mutate(med_x = median(x),\n         med_obs = median(obs, na.rm = T),\n         med_obs = ifelse(panel == \"No missing data\", NA, med_obs)) %&gt;%\n  ggplot(aes(x = obs)) +\n  geom_histogram(bins = 50) +\n  facet_grid(~panel) +\n  geom_vline(aes(xintercept = med_x), col = \"red\", lwd = 1) +\n  geom_vline(aes(xintercept = med_obs), col = \"blue\", lty = 2, lwd = 1) +\n  scale_x_continuous(\"x\") +\n  theme_minimal()\n\n\n\n\nSimilarly, when we seek to estimate causal effects, some patterns of missing data can lead to biased estimates of causal effects. In particular, missingness of the treatment indicator or the outcome variable of interest can induce bias in estimates of the ATE. First, consider missingness of an outcome variable \\(Y_i(Z)\\). Adopting some notation from Gerber and Green (2012), define “reporting” as a potential outcome of a treatment, \\(Z\\), as \\(R_i(Z) \\in \\{0, 1\\}\\). In this notation, \\(R_i(Z) = 0\\) indicates that \\(Y_i(Z)\\) is missing and \\(R_i(Z)=1\\) indicates that the outcome is non-missing. Using this notation, we can express the ATE as:\n\\[\\begin{align}\n\\underbrace{E[Y_i(1)]-E[Y_i(0)]}_{ATE} =& \\underbrace{E[R_i(1)]E[Y_i(1)|R_i(1) = 1]}_{Z = 1\\text{ and }Y_i \\text{ is not missing}} + \\underbrace{(1-E[R_i(1)])(E[Y_i(1)|R_i(1) = 0])}_{Z = 1 \\text{ and } Y_i \\text{ is missing}} - \\\\\n&\\underbrace{E[R_i(0)]E[Y_i(0)|R_i(0) = 1]}_{Z = 0 \\text { and } Y_i \\text{ is not missing }} - \\underbrace{(1-E[R_i(0)])E[Y_i(0)|R_i(0) = 0]}_{Z = 0 \\text { and } Y_i \\text{ is missing }}\n\\end{align}\\]\nIf we condition our analysis on on non-missing outcomes, our estimator of the ATE is:\n\\[E[Y_i(1)|R_i(1)=1] - E[Y_i(0) |R_i(0)=1]\\]\nExamining the preceding equations, the estimator using only only the non-missing outcomes is only (necessarily) equivalent to the ATE if:\n\nThere is no missingness, \\(E[R_i(1)] = 1\\) and \\(E[R_i(0)] = 1\\).\nMissingness is independent of potential outcomes:\n\n\\[\n\\begin{align}\n\\underbrace{E[Y_i(1)|R_i(1) = 1]}_{E[Y_i(1)] \\text{ if } Y_i(1) \\text{ is not missing }} &=  \\underbrace{E[Y_i(1)|R_i(1) = 0]}_{E[Y_i(1)] \\text{ if } Y_i(1) \\text{ is missing }}\\\\\n&\\text{and } \\\\\n\\underbrace{E[Y_i(0)|R_i(0) = 1]}_{E[Y_i(0)] \\text{ if } Y_i(0) \\text{ is not missing }} &= \\underbrace{E[Y_i(0)|R_i(0) = 0]}_{E[Y_i(0)] \\text{ if } Y_i(0) \\text{ is missing }}\n\\end{align}\n\\]\nOtherwise, the analysis conditional upon observing \\(Y_i(Z)\\) can induce an unknowable (but boundable) amount of bias in our estimate of the ATE.\nWe often do not think of missingness of the treatment indicator in experiments. Indeed, competent administration of an experiment generally ensures against missing treatment values. Nevertheless, it is important to note that missingness of the treatment indicator can also produce bias if missingness is not independent of potential outcomes.\nThe following simulation shows the consequences of two types of missingness for estimation of the ATE. We set the true ATE to 0.5 (the red vertical lines) in all cases. We simulate missingness through two types of data generating processes. In both cases, all missingness occurs among subjects in treatment (\\(Z = 1\\)). In the top panel, missingness is most likely among subjects in treatment with higher values of the outcome \\(Y_i(1)\\). In the bottom pannel, missingness is independent of the value of \\(Y_i(1)\\). If missingness is correlated with potential outcomes, the estimator of the ATE is biased (top row). This occurs whether we are missing values of the outcome (left column) or the treatment indicator (right column). In contrast, when missing data is independent of potential outcomes, the estimator is unbiased (bottom row).\n\nlibrary(randomizr)\nlibrary(estimatr)\n  \nextract_ests &lt;- function(estimatr_model){summary(estimatr_model)$coef[2,1]}\nsimulation &lt;- function(){\n  Y0 &lt;- rnorm(2000)\n  Z &lt;- complete_ra(2000)\n  Yobs &lt;- Y0 + Z * .5 # .5 standard deviation treatment effect\n  Z_miss &lt;- rbinom(n = 2000, prob = Z * pnorm(Yobs), size = 1)\n  Z_miss_ind &lt;- rbinom(n = 2000, prob = sum(Z_miss)/2000, size = 1)\n  Y_miss &lt;- rbinom(n = 2000, prob = Z * pnorm(Yobs), size = 1)\n  Y_miss_ind &lt;- rbinom(n = 2000, prob = sum(Y_miss)/2000, size = 1)\n  \n  m1 &lt;- lm_robust(Yobs ~ Z, subset = Z_miss == 0)\n  m2 &lt;- lm_robust(Yobs ~ Z, subset = Z_miss_ind == 0)\n  m3 &lt;- lm_robust(Yobs ~ Z, subset = Y_miss == 0)\n  m4 &lt;- lm_robust(Yobs ~ Z, subset = Y_miss_ind == 0)\n  return(sapply(list(m1, m2, m3, m4), FUN = extract_ests))\n}\nreps &lt;- replicate(n = 500, expr = simulation())\ndata.frame(ests = as.vector(t(reps)),\n           missing = rep(c(\"Missing Treatment Indicator\", \"Missing Outcome\"), each = 1000),\n           pos = rep(c(\"Missing is\\nNot Independent of POs\", \"Missingness is\\nIndependent of POs\"), each = 500)) %&gt;%\n  ggplot(aes(x = ests)) + \n  facet_grid(pos ~ missing) +\n  geom_histogram(bins = 100) +\n  geom_vline(xintercept = .5, col = \"red\") +\n  theme_minimal() +\n  xlab(\"ATE Estimates\")\n\n\n\n\n\n\nThe potential for bias increases in the proportion of treatment or outcome data that is missing.\nReturning to the equations in #2, it is useful to identify the terms that we can estimate (know) as analysts of a dataset. Rewriting the expression for the ATE in the presence of missing data, we can estimate:\n\nThe proportion of missingness in treatment and control: \\(\\color{red}{E[R_i(1)]}\\) and \\(\\color{red}{E[R_i(0)]}\\).\nThe expectation of the outcome variable among reporters (non-missing data) in treatment and control: \\(\\color{red}{E[Y_i(1)|R_i(1) = 1]}\\) and \\(\\color{red}{E[Y_i(0)|R_i(0)=1]}\\)\n\nThese expressions are colored in the following equation.\n\\[\\begin{align}\n\\underbrace{E[Y_i(1)]-E[Y_i(0)]}_{ATE} =& \\underbrace{\\color{red}{E[R_i(1)]E[Y_i(1)|R_i(1) = 1]}}_{Z = 1\\text{ and }Y_i \\text{ is not missing}} + \\underbrace{(1-E[R_i(1)])(E[Y_i(1)|R_i(1) = 0])}_{Z = 1 \\text{ and } Y_i \\text{ is missing}} - \\\\\n&\\underbrace{\\color{red}{E[R_i(0)]E[Y_i(0)|R_i(0) = 1]}}_{Z = 0 \\text { and } Y_i \\text{ is not missing }} - \\underbrace{(1-E[R_i(0)])E[Y_i(0)|R_i(0) = 0]}_{Z = 0 \\text { and } Y_i \\text{ is missing }}\n\\end{align}\\]\nWe are ultimately interested in estimating the ATE, \\(E[Y_i(1)]-E[Y_i(0)]\\). One straightforward implication of this expression is that the magnitude of bias possible for an estimator of the ATE on non-missing observations increases in the amount of missingness. As \\(E[R_i(1)] \\rightarrow 1\\) and \\(E[R_i(0)]\\rightarrow 1\\), bias stemming from missing outcome data converges to 0. In contrast, when we are missing a large proportion of the data, the magnitude of possible bias increases. Thus, our concerns about missing data should increase as the amount (proportion) of missingness increases.\n\n\nThe consequences of missing data for bias in estimates of causal effects depend on the type of variable that is missing.\nMissingness of pretreatment covariates need not induce bias in our estimates of the ATE. However, researchers can actually induce bias through improper treatment of missing pretreatment covariate data. If treatment is randomly assigned, treatment assignment should be orthogonal to pre-treatment missingness. In other words, pre-treatment missingness should be balanced across treatment assignment conditions.\nHowever, we should avoid “dropping” (excluding) observations based on pretreatment missingness for two reasons. First, it is possible to induce bias in our estimate of an ATE by dropping observations with pre-treatment missingness. After dropping these observations, we can estimate an unbiased estimate the local average treatment effect (LATE) among observations with no missing pretreatment data. However, if treatment effects vary with missingness of pretreatment variables, this LATE may be quite different than the ATE. Second, as we drop observations the number of observations decreases, reducing our power to detect a given ATE. In sum, we should refrain from dropping observations based on pre-treatment covariates to avoid inducing bias or efficiency loss in our estimates of the ATE.\nIn contrast, missingness of the treatment indicator or outcome variable(s) can induce bias in our estimates of causal effects, as demonstrated in #2. This categorization informs the strategies that we adopt to address the consequences of missing data.\n\n\nWhat assumptions do we invoke when we “ignore” treatment or outcome missingness in estimation?\nSuppose that we analyze a dataset with missing treatment or outcome data while ignoring the missingness. If we “ignore” the missingness, we drop observations for which we lack a value for one of these variables. If we proceed to estimate the ATE on the subsample of data for which we have full data for both treatment and outcomes, we estimate:\n\\[E[Y_i(1)|R_i(1)]- E[Y_i(0)|R_i(1)]\\]\nThis is necessarily equivalent to the ATE estimand \\(E[Y_i(1)]-E[Y_i(0)]\\) if missingness is independent of potential outcomes (MIPO), i.e. \\(Y_i(Z) \\perp R_i(Z)\\) (Gerber and Green 2012). Thus, to interpret \\(E[Y_i(1)|R_i(1)]- E[Y_i(0)|R_i(1)]\\) as an unbiased estimator of the ATE, we must impose an assumption that MIPO.\nThe assumption that MIPO is most plausible when missingness occurs by some random process. Under what conditions might this assumption be plausible? Perhaps we were only able to gather a random subset of administrative outcome data. In this case, missingness would be independent of both potential outcomes and pretreatment covariates. Alternatively, idiosyncratic behavior in data collection (enumerator absence or error) that is plausibly unrelated to potential outcomes may also be consistent with MIPO. On the other hand, survey non-response or other forms of outcome measurement dependent on subject reporting are often much harder to justify under MIPO. Because we cannot validate MIPO, we depend on researchers’ consideration of whether the assumption is plausible under a given data generating process. Where MIPO is not plausible, we should not simply “ignore” missing data in estimation. In these cases, researchers should consider the methods described in #6-#10.\n\n\nWhy should we assess whether missingness of outcomes is related to treatment assignment?\nWhen researchers encounter missingness in an experiment, they often examine the relationship between missingness of outcomes and treatment assignment. We have denoted reporting (or non-missingness) as a potential outcome of treament assignment, \\(R_i(Z)\\). For a binary treatment, we can denote four “types” of subjects in the experiment, as denoted in the following table.\n\n\n\nType\nProportion\n\\(R_i(1)\\)\n\\(R_i(0)\\)\n\n\n\n\nAlways Reporter\n\\(\\pi_A\\)\n1\n1\n\n\nIf Treated Reporter\n\\(\\pi_T\\)\n1\n0\n\n\nIf Untreated Reporter\n\\(\\pi_U\\)\n0\n1\n\n\nNever Reporter\n\\(\\pi_N\\)\n0\n0\n\n\n\nIn the case in which missingness is related to potential outcomes but not to treatment assignment, we are generally not able to identify an unbiased estimate of the ATE. However, we may be interested in identifying the LATE among always reporters – those subjects for which we observe the outcome regardless of treatment assignment. This effect can be a main estimand of interest and even the most policy-relevant estimand in certain settings. However, to estimate this LATE, we want to be sure that the outcomes that we observe are those of always reporters, not if-treated or if-untreated reporters.\nTo assess the plausibility of ths conjecture, we often examine the relationship between treatment assignment and reporting (non-missingness). Using the notation from the table, we can estimate:\n\\[\\begin{align}\nE[R_i(1)-R_i(0)] &= \\pi_A + \\pi_T - (\\pi_A + \\pi_U)\\\\\n&=\\pi_T - \\pi_U\n\\end{align}\\]\nIf we find no difference in reporting across treatment groups, this test provides no evidence that \\(\\pi_T \\neq \\pi_U\\). However, to further justify that the complete observations in the data are those of “always reporters,” we further must know that \\(\\pi_T = \\pi_U = 0\\). We cannot test this by examining the relationship between reporting and treatment assignment. As such, we generally complement this test with a justification of why reporting should not be endogenous to treatment assignment: for example by assessing covariate balance between treated and control units who are not missing, or by assessing covariate balance between units with missing vs non-missing outcomes. If reporting is not endogenous, then in principle the assumption that \\(\\pi_T = \\pi_U = 0\\) holds. As such, if we find no evidence of differential levels of reporting and see no way that reporting should be endogenous to treatment, we can justify that the \\(E[Y_i(1)|R_i(1) = 1] - E[Y_i(0)|R_i(0) = 1]\\) estimates the LATE among always reporters.\n\n\nWhat is imputation?\nThe most common approaches used to deal with missing data involve the imputation or “filling in” of missing values. In the following dataset with missingness, imputation procedures “fill in” the missing data – the NAs.\n\n\n\n\n\nXobs\nZ\nYobs\n\n\n\n\n-1.2070\n0\n3\n\n\nNA\n0\n2\n\n\n1.0840\n1\n3\n\n\n-2.3460\n1\n4\n\n\n0.4291\n1\n2\n\n\n0.5061\n0\nNA\n\n\nNA\n1\nNA\n\n\nNA\n1\n3\n\n\n-0.5645\n0\n2\n\n\nNA\n0\n1\n\n\n-0.4772\n0\n2\n\n\n-0.9984\n1\n4\n\n\n\n\n\nJust as the consequences of missingness vary by the type of variable that is missing, the imputation methods advocated to address missingness also vary. Importantly, these methods vary in the strength of the assumptions about missingness that are invoked to identify causal effects in the presence of missingness. We review three common approaches to imputation in #8-#10.\n\n\nHow do we address missingness of pre-treatment covariates and why does this matter?\nAs mentioned in #4, we should never “drop” observations on account of missing pre-treatment data. In order to estimate a model with covariate adjustment, thus, we need to “fill in” missing values to avoid dropping observations. We outline two forms of imputation advocated for missing pre-treatment covariates. The most common approach to address missingness of pre-treatment covariates is to create indicators for missingness and include these as covariates. To do this form of imputation:\n\nSubstitute a numerical value for the NA (as necessary). In the dataset below, we impute a 0 for all values of Xobs that are NAs. The new variable is named Ximputed.\nCreate an indicator for the missingness in each pretreatment variable. This indicator, Xmissing takes the value 1 whenever Xobs is NA, and a 1 otherwise.\n\nOur imputed dataset is thus:\n\n\n\n\n\nXobs\nXimputed\nXmissing\nZ\nYobs\n\n\n\n\n-1.2070\n-1.2070\n0\n0\n3\n\n\nNA\n0.0000\n1\n0\n2\n\n\n1.0840\n1.0840\n0\n1\n3\n\n\n-2.3460\n-2.3460\n0\n1\n4\n\n\n0.4291\n0.4291\n0\n1\n2\n\n\n0.5061\n0.5061\n0\n0\nNA\n\n\nNA\n0.0000\n1\n1\nNA\n\n\nNA\n0.0000\n1\n1\n3\n\n\n-0.5645\n-0.5645\n0\n0\n2\n\n\nNA\n0.0000\n1\n0\n1\n\n\n-0.4772\n-0.4772\n0\n0\n2\n\n\n-0.9984\n-0.9984\n0\n1\n4\n\n\n\n\n\nConsider how this imputation changes the specification of a regression model. Instead of estimating:\n\nmodel1 &lt;- lm(Yobs ~ Z + Xobs)\n\nwe estimate:\n\nmodel2 &lt;- lm(Yobs ~ Z + Ximputed + Xmissing)\n\nThe estimator model2 does not drop observations on the basis of the missing pre-treatment variable Xobs like model1 does.\nAlternatively, if pre-treatment missingness is minimal (less than 10% of observations), or when working with very small datasets with few observations relative to the number of covariates and missingness indicators, Lin, Green, and Coppock (2016) advocate imputing the (unconditional) mean of the observed pre-treatment covariate. Call this variable Ximputed_mean.\n\n\n\n\n\nXobs\nXimputed_mean\nZ\nYobs\n\n\n\n\n-1.2070\n-1.2070000\n0\n3\n\n\nNA\n-0.4467375\n0\n2\n\n\n1.0840\n1.0840000\n1\n3\n\n\n-2.3460\n-2.3460000\n1\n4\n\n\n0.4291\n0.4291000\n1\n2\n\n\n0.5061\n0.5061000\n0\nNA\n\n\nNA\n-0.4467375\n1\nNA\n\n\nNA\n-0.4467375\n1\n3\n\n\n-0.5645\n-0.5645000\n0\n2\n\n\nNA\n-0.4467375\n0\n1\n\n\n-0.4772\n-0.4772000\n0\n2\n\n\n-0.9984\n-0.9984000\n1\n4\n\n\n\n\n\nIf we are simply imputing the pretreatment mean, then instead of estimating model1 (as above) our regression model should be:\n\nmodel3 &lt;- lm(Yobs ~ Z + Ximputed_mean)\n\nSince we have imputed missing values in Xobs when constructing Ximputed_mean, we will not lose observations on the basis of the missing pretreatment covariate when estimating model3.\n\n\nWe can bound ATEs to account for missing outcome data without making assumptions about the distribution of missing outcomes.\nIf we know the maximum and minimum values of the outcome variable, we can construct bounds on the ATE even in the presence of missing values of the dependent variable. For example, the range of the variable Yobs is 1 to 4. While we do not know what values missing values of Yobs would have been, we can take advantage of the fact that we know the maximum and minimum possible value to construct bounds on the ATE. This allows us to construct an interval estimate of the ATE instead of the point estimate we would construct if we had no missing data. These bounds are known as “extreme value bounds” or “Manski bounds” and do not invoke any additional assumptions about the value of Yobs.\nManski bounds consist of a maximum and minimum bound (estimate) of the possible ATE. To create these bounds, we impute the maximum or minimum value of the outcome variable. as a function of treatment assignment. Thus we construct:\n\nA maximum bound by imputing the maximum value of the outcome for missing values in treatment (\\(Z=1\\)) and imputing the minimum value of the outcome for missing values in control (\\(Z=0\\)).\nA minimum bound by imputing the minimum value of the outcome for missing values in treatment (\\(Z=1\\)) and imputing the maximum value of the outcome for missing values in control (\\(Z=0\\)).\n\nUsing the above dataset, we can construct two variables Y_maxbound and Y_minbound as follows:\n\n\n\n\n\nXobs\nZ\nYobs\nY_maxbound\nY_minbound\n\n\n\n\n-1.2070\n0\n3\n3\n3\n\n\nNA\n0\n2\n2\n2\n\n\n1.0840\n1\n3\n3\n3\n\n\n-2.3460\n1\n4\n4\n4\n\n\n0.4291\n1\n2\n2\n2\n\n\n0.5061\n0\nNA\n1\n4\n\n\nNA\n1\nNA\n4\n1\n\n\nNA\n1\n3\n3\n3\n\n\n-0.5645\n0\n2\n2\n2\n\n\nNA\n0\n1\n1\n1\n\n\n-0.4772\n0\n2\n2\n2\n\n\n-0.9984\n1\n4\n4\n4\n\n\n\n\n\nWithout covariate adjustment, we can obtain our interval estimate of the ATE by estimating:\n\nupper &lt;- lm(Y_maxbound ~ Z)\nlower &lt;- lm(Y_minbound ~ Z)\ncoef(upper)[2]\n\n  Z \n1.5 \n\ncoef(lower)[2]\n\n  Z \n0.5 \n\n\nOur interval estimate of the ATE using Manski bounds is thus [0.5, 1.5].\n\n\nMultiple imputation for missing outcomes allows for point estimation of ATEs, but relies on stronger assumptions than bounding.\nThe methods in #8 and #9 describe methods of single imputation, where a single value is substituted for missing values. In multiple imputation, we impute missing values of the dataset multiple times according to an assumed stochastic data generating process. Different methods for multiple imputation impose different structures and assumptions about the probability distributions governing the data generating processes used to impute missing values. In general, multiple imputation proceeds via three stages:\n\nImputation: Missing values are imputed via a random draw of plausible values under the specified data generating process. This creates a full dataset without missing values. Typically, researchers will impute at least five complete datasets. The only differences across these imputed datasets are the values that were missing in the original data.\nEstimation: Estimate the ATE (or other estimand of interest) using each imputed dataset. This generates as many estimates and standard errors as there are imputed datasets.\nPooling estimates: Finally, researchers combine estimates from the different imputed datasets to generate a point estimate of the ATE and its stanard error. Typically, this point estimate can be calculated using rules laid out by Rubin (2004).\n\nMultiple imputation is implemented in many software packages and relatively straightforward to implement. However, the specification of a data generating process from which datasets are imputed relies on additional assumptions about the correctness of the model of missingness (the data generating process). These assumptions are generally untestable and stronger than the standard experimental assumptions invoked to identify an interval estimate of the ATE using Manski bounds.\n\n\n\n\n\n\n\n\nReferences\n\nGerber, Alan S., and Donald P. Green. 2012. Field Experiments: Design, Analysis, and Interpretation. W.W. Norton.\n\n\nLin, Winston, Donald P. Green, and Alexander Coppock. 2016. “Standard Operating Procedures for Don Green’s Lab at Columbia.”\n\n\nRubin, Donald B. 2004. Multiple Imputation for Nonresponse in Surveys. New York: John Wiley; Sons."
  },
  {
    "objectID": "guides/data-strategies/survey-experiments_en.html",
    "href": "guides/data-strategies/survey-experiments_en.html",
    "title": "10 Things to Know About Survey Experiments",
    "section": "",
    "text": "This guide discusses techniques for using randomization to create experiments within the text of a survey (i.e. survey experiments). These survey experiments are distinct from studies that use surveys to gather information related to an experiment that occurs outside of the survey. The guide distinguishes between survey experiments that are used mainly for measuring sensitive attitudes, like list experiments, and those that are mainly used to learn about causal effects, like conjoint experiments. Survey experiments for measurement attempt to ensure honest responses to sensitive questions by providing anonymity to respondents. Survey experiments for causal identification randomize images and text to learn how the image or text influences respondents. Both types of survey experiments face challenges, such as respondents not perceiving anonymity or not interpreting images and text as the researcher intended. New experimental techniques seek to address these challenges."
  },
  {
    "objectID": "guides/data-strategies/survey-experiments_en.html#footnotes",
    "href": "guides/data-strategies/survey-experiments_en.html#footnotes",
    "title": "10 Things to Know About Survey Experiments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Krosnick (1991) and Simon and March (2006).↩︎\nSee Kramon and Weghorst (2012) and Schwarz (1999).↩︎\nSee Zigerell (2011, 544)↩︎\nSee Glynn (2013) and Droitcour et al. (2004).↩︎\nSee Kramon and Weghorst (2012) and Kramon and Weghorst (2019).↩︎\nSee Warner (1965), Boruch (1971), D. Gingerich (2015), and D. W. Gingerich (2010).↩︎\nSee Blair, Imai, and Zhou (2015) and D. W. Gingerich (2010).↩︎\nSee Edgell, Himmelfarb, and Duchan (1982) and Yu, Tian, and Tang (2008).↩︎\nSee Yu, Tian, and Tang (2008) and Jann, Jerke, and Krumpal (2011).↩︎\nSee Macrae et al. (1994) and Schwarz and Clore (1983).↩︎\nSee Macrae et al. (1994) and Schwarz and Clore (1983).↩︎\nSee Gaines, Kuklinski, and Quirk (2007) and Druckman and Leeper (2012).↩︎\nSee Cohen (2003) and Kam (2005).↩︎\nSee Bullock, Imai, and Shapiro (2011) and Lyall, Blair, and Imai (2013).↩︎\nSee Blair, Imai, and Zhou (2015), Rosenfeld, Imai, and Shapiro (2016), and Lensvelt-Mulders et al. (2005).↩︎\nSee Auspurg and Hinz (2014) and Sniderman et al. (1991).↩︎\nSee Hainmueller, Hopkins, and Yamamoto (2014) and Green and Rao (1971).↩︎\nSee Abramson, Koçak, and Magazinnik (2019, 1)↩︎\nSee Rankin and Campbell (1955) and Figner, Murphy, et al. (2011).↩︎"
  },
  {
    "objectID": "guides/data-strategies/sampling_en.html",
    "href": "guides/data-strategies/sampling_en.html",
    "title": "10 Things to Know About Sampling",
    "section": "",
    "text": "Researchers are rarely able to collect measurements on all units that make up the target population of a study. Time and budget constraints typically require the selection of a subset of units — a process called sampling. This guide provides an overview of different ways to sample and how these affect what can be learned from a study. Particular attention is paid to questions about sampling and randomized experiments."
  },
  {
    "objectID": "guides/data-strategies/sampling_en.html#simple-random-sampling",
    "href": "guides/data-strategies/sampling_en.html#simple-random-sampling",
    "title": "10 Things to Know About Sampling",
    "section": "Simple Random Sampling",
    "text": "Simple Random Sampling\nSimple random sampling is the most basic survey design. In this design, each sample of size \\(n\\) and hence each unit has the same probability of being sampled. One way of drawing a simple random sample of size \\(n\\) from a sampling frame with \\(N\\) units is to enumerate all possible samples of size \\(n\\) and randomly select one of those samples. This is the procedure described in the example above. However, this approach tends to be impractical in real world applications, since actual populations are typically much larger than \\(N=6\\), and the number of possible samples will thus be vast. An alternative procedure is to number all units from 1 to \\(N\\), to generate \\(n\\) random numbers, ideally after setting a random seed (like set.seed() in R), and to select the corresponding units.3"
  },
  {
    "objectID": "guides/data-strategies/sampling_en.html#stratification",
    "href": "guides/data-strategies/sampling_en.html#stratification",
    "title": "10 Things to Know About Sampling",
    "section": "Stratification",
    "text": "Stratification\nSuppose we know upfront that the characteristic in which we are interested varies across sub-populations. For example, if we aim to estimate average income, we may suspect that men earn more than women. If we draw a simple random sample, it is possible that we end up with a sample that contains more women than men. In this case, our average income estimate will be subject to a large sampling error. A way to guard against this possibility is to divide the population into subgroups, also called strata, and draw an independent random sample in each stratum. For example, we may draw an independent random sample of women and and one of men. This procedure fixes the proportion of women and men in the sample, thereby avoiding “bad” samples and improving the precision of our estimates. Being able to ensure that the sample contains enough units from a particular sub-population is also helpful if estimates among the sub-population are of independent interest. If we would like to estimate the gender pay gap, for example, we need a sample that contains enough men and women to obtain sufficiently precise estimates of each group’s average income. This latter use of stratification is especially important for learning about rare subgroups."
  },
  {
    "objectID": "guides/data-strategies/sampling_en.html#clustering",
    "href": "guides/data-strategies/sampling_en.html#clustering",
    "title": "10 Things to Know About Sampling",
    "section": "Clustering",
    "text": "Clustering\nSuppose we would like to estimate the average income among residents of a city. Our sample frame may not identify individual residents but we may have access to a list of households. Instead of directly sampling residents, we can randomly select households and interview all members of the selected households. In this case, the primary sampling units — the units that can be selected — differ from the observation units on which measurements are taken. Households serve as PSUs or clusters, while household members serve as observation units. The core downside of cluster sampling is that it typically leads to a loss in precision. For an analogous problem with cluster random assignment and for more on clustering and information reduction see “10 Things to Know About Cluster Randomization”. This loss in precision will be greater when units within the same cluster are more similar to each other (for example, members of the same household may have similar incomes or views). This problem of similarity or dependence within clusters can be severe in studies of politics where all members of a place have the same representative or share similar attitudes (see Stoker and Bowers (2002)). Nonetheless, cluster sampling may be necessary if sampling frames of individual observation units cannot be obtained. Cluster sampling may also save survey costs. For example, suppose we would like to estimate household-level income. Using households as the primary sampling unit may lead to a sample of households that is dispersed throughout the city, which increases transportation costs. Instead, we may select entire city blocks and interview all households within the selected blocks. Doing so may make it possible to interview more households with a smaller budget. Whether or not the gain in precision from a bigger sample size outweighs the loss in precision from clustering will depend on the degree to which households within the same city block have similar incomes."
  },
  {
    "objectID": "guides/data-strategies/sampling_en.html#multi-stage-sampling",
    "href": "guides/data-strategies/sampling_en.html#multi-stage-sampling",
    "title": "10 Things to Know About Sampling",
    "section": "Multi-stage sampling",
    "text": "Multi-stage sampling\nInstead of sampling all units in a cluster, one may draw a sub-sample of units. For example, instead of interviewing all members, one could sample two members in each sampled household. Additional stages can be added to this approach. For instance, we could first draw a sample of city blocks, then a sub-sample of households within each sampled city block and finally a sub-sample of household members within each sampled household. In this example, households would be referred to as secondary sampling units and household members as tertiary sampling units. One advantage of multi-stage sampling is that it allows researchers to navigate the trade-off between precision and cost-effectiveness. Increasing the number of clusters and sub-sampling fewer units per cluster can yield a more diverse sample and hence reduce sampling variability. Yet, doing so may also increase survey costs."
  },
  {
    "objectID": "guides/data-strategies/sampling_en.html#footnotes",
    "href": "guides/data-strategies/sampling_en.html#footnotes",
    "title": "10 Things to Know About Sampling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhile only random sampling allows for the characterization of sampling uncertainty, random sampling does not guarantee the ability to obtain uncertainty estimates (see Kish (1965, 24) for a discussion). A design that samples only one cluster at random, for example, would not allow for the construction of standard errors and confidence intervals.↩︎\nOn a deeper level, however, random assignment has much in common of with random sampling. The process of randomly assigning units to a treatment group can be thought of as a process of drawing a random sample of treated potential outcomes in order to estimate the average treated potential outcome in the entire experimental subject pool (see Neyman (1990)). ↩︎\nNote that when talking about random assignment, we refer to the analogous procedure that assigns a fixed number of units to treatment with a a given probability as complete random assignment. The term simple random assignment refers to a procedure where the researcher, say, flips a coin for every unit to decide whether it should be assigned to treatment or control. The number of units assigned to treatment under simple random assignment thus remains a random variable. ↩︎\nSee Lohr (2009, chap. 14) for an overview of other approaches and Tran et al. (2015) for a comparison of RDS to time-location sampling as a major alternative.↩︎"
  },
  {
    "objectID": "guides/data-strategies/measurement_en.html",
    "href": "guides/data-strategies/measurement_en.html",
    "title": "10 Things to Know About Measurement in Experiments",
    "section": "",
    "text": "1. The validity of inferences we draw from an experiment depend on the validity of the measures used.\nWe typically experiment in order to estimate the causal effect of a treatment, \\(Z\\), on an outcome, \\(Y\\). Yet, the reason that we care about estimating this causal effect is, in principle, to understand characteristics of the relationship between two theoretical, unobserved, concepts measured by observed variables \\(Z\\) and \\(Y\\).\nFollowing Adcock and Collier (2001), consider the measurement process graphed in Figure 1 in three steps. First, researchers begin with systematized concept, a clearly-defined theoretical construct. From this concept, the researcher develops an indicator mapping the concept onto a scale or a set of categories. Finally, units or cases are scored on the indicator, yielding a measurement of treatment, \\(Z\\) and an outcome \\(Y\\). A measurement is valid if variation in the indicator closely approximates variation in the underlying concept of interest.\nAn experimental research design should allow a researcher to estimate the causal effect of \\(Z\\) on \\(Y\\) under standard assumptions. But if the ultimate goal is to make an inference about causal effect of the concept that \\(Z\\) measures on the concept that \\(Y\\) measures, the inferences that we can hope to make on the basis of our experimental evidence are valid if and only if both measures are valid.\n\n\n\n\n\nThe process of measurment in the context of an experimental research design.\n\n\n\n\n\n\n2. Measurement is the link between a researcher’s substantive and/or theoretical argument and an (experimental) research design.\nWhen we consider the design of an experiment, we tend to focus on the process by which the randomly assigned treatment, \\(Z\\) is assigned and the joint distribution of \\(Z\\) and an outcome, \\(Y\\). In other words, we tend to divorce scores of \\(Z\\) and \\(Y\\) from broader concepts when considering the statistical properties of a research design. In this telling, two completely separate experiments with the same distribution of \\(Z\\) and \\(Y\\) could have identical properties.\nFor example, a clinical trial on the efficacy of aspirin on headaches and an experiment that provides information on an incumbent politician’s level of corruption and then asks the respondent if she will vote for the incumbent could have identical sized and distributed samples, assignments, estimands, and realizations of outcomes (data). Yet, this characterization of two completely distinct research projects that seek to make completely distinct inferences as “equivalent” may strike us as quite strange or even unsettling.\nHowever, when we consider measurement as a fundamental component of research design, clearly these experiments are distinct. We observe measures of different concepts in the data for the two experiments. By considering the indicators and the broader concepts underlying the treatments and outcomes, we are forced to examine the researchers’ respective theories or arguments. In so doing, we can raise questions about the validity of the measures and the relationship between the validity of the measures and the validity of final, substantive, inferences.\n\n\n3. Measuring treatments includes the operationalization of treatment as well as compliance with treatment assignment.\nIn an experiment, treatments are typically designed, or at a minimum, described, by the researcher. Consumers of experimental research should be interested the characteristics of the treatment and how it manipulates a concept of interest. Most treatments in social science are compound, or include a bundle of attributes. We may be interested in the effect of providing voters with information on their elected officials’ performance. Yet, providing information also includes the mode of delivery and who was delivering the information. To understand the degree to which the treatment manipulates a concept, we must also understand what else the treatment could be manipulating.\nHowever, despite all the effort operationalizing a treatment, in experimental research, the link from the operationalization to the treatment indicator is fundamentally distinct from measurement of covariates or outcomes for two reasons. First, by assigning treatment, experimenters aim to control the values a given unit takes on. Second, for the treatment indicator, the score comes from assignment to treatment, which is a product of the randomization. A subject may or may not have received the treatment, but her score on the treatment indicator is simply the treatment that she was assigned to, not the treatment she received.\nWhen subjects receive treatments other than those to which they are assigned, we typically seek to measure compliance — whether the treatments were delivered and to what extent. To do so, we define what constitutes compliance with treatment assignment. In determining what constitutes compliance, researchers should consider the core aspect of how the treatment manipulates the concept of interest. At what point in the administration of the treatment does this manipulation occur? Once compliance is operationalized,we seek to code the compliance indicator in a manner faithful to this definition.\nFor example, consider a door-to-door canvassing campaign that distributes information about the performance of an incumbent politician. Households are assigned to receive a visit from a canvasser who shares the information (treatment) or no visit (control). The treatment indicator is simply whether a household was assigned to the treatment or not. However, if residents of a household are not home when the canvasser visits, they do not receive the information. Our definition of compliance should determine what constitutes “treated” on our (endogenous) measure of whether a household received the treatment, here the information. Some common definitions of compliance may be (a) that someone from the household answered the door; or (b) that someone from the household listened to the full information script.\n\n\n4. Most outcomes of interest in social science are latent.\nIn contrast to measuring treatment indicators and compliance, measuring outcomes in experimental research follows much more closely the process outlined in the figure above. We theorize how the treatment may influence an outcome concept. We then operationalize the concept and record scores or values to complete our measurement of the outcome.\nOne particular challenge in the measurement of outcomes is that many of the most common outcomes of interest in social science are latent. This means that we are unable to observe the true value of the outcome concept directly. In fact, the nature of the true value may itself be under debate (for example, the debate about the measurement of “democracy” is a classic case where the definition of the concept itself is contested). Outcomes including knowledge, preferences, and attitudes are latent. We thus record or score observable indicators assumed to be related to the latent outcome in an effort to infer characteristics of the latent variable. Even behavioral outcomes are often used as manifestations of larger latent concepts (i.e. assessed voting behavior is used to make inferences about “electoral accountability” in a place).\nBecause these variables are latent, it is challenging to devise appropriate indicators. Poor operationalization has rather drastic consequences for the validity of our inferences about the concept of interest for two reasons. As in section #1 above, if these indicators do not conceptually measure the concept of interest, then inferences we make about the relationship between \\(Z\\) and \\(Y\\) (even with a “perfect” design in terms of statistical power and missing data,etc.) may not teach us about the “ultimate inference” we are seeking to make. Furthermore, measurement error may undermine our ability to estimate the effect of \\(Z\\) and \\(Y\\) leading to incorrect inferences. The remainder of this guide focuses on the latter problem.\n\n\n5. There are two types of measurement error that we should consider.\nWe can formalize measurement challenges quite simply. Suppose a treatment, \\(Z_i\\) is hypothesized to change preferences for democratic norms, \\(\\nu_i\\). In principle, the quantity that we would like to estimate is \\(E[\\nu_i|Z_i = 1] - E[\\nu_i|Z_i =0]\\), the ATE of our treatment on preferences for democratic norms. However, \\(\\nu_i\\) is a latent variable: we cannot measure it directly. Instead we ask about support for various behaviors thought to correspond to these norms. This indicator, \\(Y_i\\), can be decomposed into the latent variable, \\(\\nu_i\\) and two forms of measurement error:\n\nNon-systematic measurement error, \\(\\delta_i\\): This error is independent of treatment assignment, \\(\\delta_i \\perp Z_i\\).\nSystematic measurement error, \\(\\kappa_i\\): This error is not independent of treatment assigniment, \\(\\kappa_i \\not\\perp Z_i\\).\n\n\\[Y_i = \\underbrace{\\nu_i}_{\\text{Latent outcome}} + \\underbrace{\\delta_i}_{\\substack{\\text{Non-systematic} \\\\ \\text{measurement error}}} + \\underbrace{\\kappa_i}_{\\substack{\\text{Systematic} \\\\ \\text{measurement error}}}\\]\n\n\n6. Measurement error reduces the power of your experiment.\nNon-systematic measurement error, represented by \\(\\delta_i\\) above, refers to the noise with which we are measuring the latent variable. In the absence of systematic measurement error, we measure:\n\\[Y_i = \\underbrace{\\nu_i}_{\\text{Latent outcome}} + \\underbrace{\\delta_i}_{\\substack{\\text{Non-systematic} \\\\ \\text{measurement error}}}\\]\nNow, consider the analytical power formula for a two-armed experiment. We can express \\(\\sigma\\), or the standard deviation of the outcome as \\(\\sqrt{Var(Y_i)}\\). Note that in the formula below, this term appears in the denominator of the first term. As \\(\\sqrt{Var(Y_i)}\\) increases, statistical power decreases.\n\\[\\beta = \\Phi \\left(\\frac{|\\mu_t− \\mu_c| \\sqrt{N}}{2 \\color{red}{\\sqrt{Var(Y_i)}}} − \\Phi^{−1}\\left(1 − \\frac{\\alpha}{2}\\right)\\right)\\]\nIn what way does non-systematic measurement error \\(\\delta_i\\) impact power? We can decompose \\(\\sqrt{Var(Y_i)}\\) as follows:\n\\[\\sqrt{Var(Y_i)} = \\sqrt{Var(\\nu_i) + Var(\\delta_i) + 2 Cov(\\nu_i, \\delta_i)}\\]\nSo long as \\(Cov(\\nu_i, \\delta_i)\\geq 0\\) (we often assume \\(Cov(\\nu_i, \\delta_i)= 0\\)), it must be the case that the \\(Var(Y_i)\\) is increasing as measurement error, or \\(Var(\\delta_i)\\) increases. This implies that power is decreasing as non-systematic measurement error increases. In other words, the noisier our measures of a latent variable, the lower our ability to detect effects of a treatment on a latent variable.\nWhat about the case in which \\(Cov(\\nu_i, \\delta_i) &lt; 0\\)? While this reduces \\(Var(Y_i)\\) (holding \\(Var(\\nu_i)\\) and \\(Var(\\delta_i)\\) constant), it also attenuates the variation that we measure in \\(Y_i\\). In principle, this should attenuate the numerator \\(|\\mu_t-\\mu_c|\\), which, if sufficient relative to the reduction in variance, will also reduce power.\n\n\n7. Systematic measurement error biases estimates of causal effects of interests.\nIf we are estimating the Average Treatment Effect (ATE) of our treatment \\(Z_i\\), on preferences for democratic norms, \\(\\nu_i\\), we are trying to recover the ATE, or \\(E[\\nu_i|Z_i = 1] - E[\\nu_i|Z_i =0]\\). However, in the presence of systematic measurement error, where measurement error is related to the treatment assignment itself (say, the outcome is measured differently in the treatment group than in the control group) a difference-in-means estimator on the observed outcome, \\(Y_i\\), recovers a biased estimate of the ATE. The effect of the treatment now includes the measurement difference as well as the difference between treated and control groups:\n\\[E[Y_i|Z_i = 1]−E[Y_i|Z_i = 0] =  E[\\nu_i + \\delta_i + \\kappa_i |Z_i = 1] − E[\\nu_i + \\delta_i + \\kappa_i|Z_i =0]\\] Because non-systematic measurement error, \\(\\delta_i\\) is independent of treatment assignment, \\(E[\\delta_i|Z_i = 1] = E[\\delta_i |Z_i = 0]\\). Simplifying and rearranging, we can write:\n\\[E[Y_i|Z_i = 1]−E[Y_i|Z_i = 0] = \\underbrace{E[\\nu_i|Z_i = 1] − E[\\nu_i|Z_i =0]}_{ATE} +\n\\underbrace{E[\\kappa_i|Z_i = 1] - E[\\kappa_i|Z_i =0]}_{\\text{Bias}}\\]\nThere are various sources of non-systematic measurement error in experiments. Demand effects and Hawthorne effects can be motivated as sources of systematic measurement error. Moreover, designs that measure outcomes asymmetrically in treatment and control groups may be prone to systematic measurement error. In all cases, there exists asymmetry across treatment conditions in: (a) the way that subjects respond to being observed; or (b) the way that we observe outcomes that is distinct from any effect of the treatment on the latent variable of interest. The biased estimate of the ATE becomes the net of any effects on the latent variables (the ATE) and the non-systematic measurement error.\nWhen designing an experiment, researchers can take various steps to limit systematic measurement error. First and foremost, they can attempt to use identical measurement strategies across all experimental groups. To limit demand and Hawthorne effects, researchers often aim to design treatments and measurement strategies that are as naturalistic and unobtrusive as possible. For example, sometimes it can be beneficial to avoid baseline surveys at the outset of an experiment that would reveal the purpose of a study to participants. Researchers may also want to separate treatment from outcome measurement phases to limit the apparent connection between the two. Ensuring that study staff are blind towards the treatment status of study participants can also help maintain measurement symmetry across treatment conditions. The use of placebo treatments sometimes helps to hide their treatment status even from study participants themselves. Finally, researchers sometimes supplement outcome measures such as survey questions that are particularly susceptible to demand effects with behavioral measures for which experimenter demand may be less of a concern. See Quidt, Haushofer, and Roth (2018) for a way to assess the robustness of your experimental results to demand effects.\n\n\n8. Leverage multiple indicators to assess the validity of a measure but be aware of the limitations of such tests.\nBeyond consideration of the quality of the mapping between a concept and a measure, we can often assess the quality of the measure by comparing it to measures from alternate operationalizations of the same concept, closely related concepts, or distinct concepts. In convergent tests of the validity of a measure, we assess the correlation between alternate measures of a concept. If they are coded in the same direction, we expect the correlation to be positive and validity of both measures increases as the magnitude of the correlation increases. One limitation of convergent tests of validity is if two measures are weakly correlated, absent additional information, we do not know whether one measure is valid (and which) or whether both measures are invalid.\nGathering multiple indicators may also allow for researchers to assess the predictive validity of a measure. To what extent does a measure of a latent concept predict behavior believed to be shaped by the concept? For example, does political ideology (the latent variable) predict reported vote choice for left parties? This provides an additional means of validating a measure. Here, the higher the ability of an indicator to predict behavior (or other outcomes), the stronger the predictive validity of the indicator. Yet, we believe that most behaviors are a result of a complex array of causes. Determining whether a measure is a “good enough” predictor is a somewhat arbitrary determination.\nFinally, we may want to determine whether we are measuring the concept of interest in isolation rather than a bundle of concepts. Tests of discriminant validity look at indicators of a concept and a related but distinct concept. In principle, we look for low correlations (correlations close to 0) between both indicators. One limitation of tests of discriminant validity is that we don’t know how underlying distinct concepts covary. It may be the case that we have valid indicators of both concepts, but they exhibit strong correlation (positive or negative) because units with high levels of \\(A\\) tend to have higher (resp. low) levels of \\(B\\).\nIn sum, the addition of more measures can help validate an indicator, but these validation tests are limited in what they tell us when they fail. To this extent, we should remain cognizant of the limitations in addition to the utility of collecting additional measures to simply validate an indicator.\n\n\n9. The use of multiple indicators often improves the power of your experiment, but may introduce a bias-efficiency tradeoff.\nGathering multiple indicators of a concept or outcome may also improve the power of your experiment. If multiple indicators measure the same concept but are measured with (non-systematic) error, we can improve the precision with which we measure the latent variable by leveraging multiple measures.\nThere are multiple ways to aggregate multiple outcomes into an index. 10 Things to Know about Multiple Comparisons describes indices built from \\(z\\)-score and inverse covariance weighting of multiple outcomes. There are also many other structural models for estimating latent variables from multiple measures.\nBelow, we look at simple \\(z\\)-score index of two noisy measures of a latent variable. We assume that the latent variables and both indicators “Measure 1” and “Measure 2” are drawn from a multivariate normal distribution and are positively correlated with the latent variable and with each other. For the purposes of simulation, we assume that we know the latent variable, though in practice this is not possible. First, we can show that across many simulations of the data, the correlation between the \\(z\\)-score index of the two measures and the latent variable is, on average, higher than the correlation between either of the indicators and the latent variable. When graphing the correlation of the individual measures and the latent variable against (\\(x\\)-axes) the correlation of the index and the latent variable (\\(y\\)-axis), almost all points are above the 45-degree line. This shows that the index approximates the latent variable with greater precision.\n\nlibrary(mvtnorm)\nlibrary(randomizr)\nlibrary(dplyr)\nlibrary(estimatr)\nmake_Z_score &lt;- function(data, outcome){\n  ctrl &lt;- filter(data, Z == 0)\n  return(with(data, (data[,outcome] - mean(ctrl[,outcome]))/sd(ctrl[,outcome])))\n}\npull_estimates &lt;- function(model){\n  est &lt;- unlist(model)$coefficients.Z\n  se &lt;- unlist(model)$std.error.Z\n  return(c(est, se))\n}\ndo_sim &lt;- function(N, rhos, taus, var = c(1, 1, 1)){\n   measures &lt;- rmvnorm(n = N, \n                       sigma = matrix(c(var[1], rhos[1], rhos[2], \n                                        rhos[1], var[2], rhos[3], \n                                        rhos[2], rhos[3], var[3]), nrow = 3))\n   df &lt;- data.frame(Z = complete_ra(N = N),\n                      latent = measures[,1],\n                      Y0_1 = measures[,2],\n                      Y0_2 = measures[,3]) %&gt;%\n            mutate(Yobs_1 = Y0_1 + Z * taus[1],\n                   Yobs_2 = Y0_2 + Z * taus[2])\n   df$Ystd_1 = make_Z_score(data = df, outcome = \"Yobs_1\")\n   df$Ystd_2 = make_Z_score(data = df, outcome = \"Yobs_2\")\n   df$index = (df$Ystd_1 + df$Ystd_2)/2\n   cors &lt;- c(cor(df$index, df$latent), cor(df$Ystd_1, df$latent), cor(df$Ystd_2, df$latent))\n   ests &lt;- c(pull_estimates(lm_robust(Ystd_1 ~ Z, data = df)),\n             pull_estimates(lm_robust(Ystd_2 ~ Z, data = df)),\n             pull_estimates(lm_robust(index ~ Z, data = df)))\n   output &lt;- c(cors, ests)\n   names(output) &lt;- c(\"cor_index\", \"cor_Y1\", \"cor_Y2\", \"est_Y1\", \"se_Y1\",\n                      \"est_Y2\", \"se_Y2\", \"est_index\", \"se_index\")\n   return(output)\n}\nsims &lt;- replicate(n = 500, expr = do_sim(N = 200, \n                                         rhos = c(.6, .6, .6), \n                                         taus = c(.4, .4),\n                                         var = c(1, 3, 3)))\ndata.frame(measures = c(sims[\"cor_Y1\",], sims[\"cor_Y2\",]),\n           index = rep(sims[\"cor_index\",], 2),\n           variable = rep(c(\"Measure 1\", \"Measure 2\"), each = 500)) %&gt;%\n  ggplot(aes(x = measures, y = index)) + geom_point() + \n  facet_wrap(~variable) + \n  geom_abline(a = 0, b = 1, col = \"red\", lwd = 1.25) + \n  scale_x_continuous(\"Correlation between measure and latent variable\", limits = c(0.1, .6)) +\n  scale_y_continuous(\"Correlation between index and latent variable\", limits = c(0.1, .6)) + \n  theme_minimal()\n\n\n\n\nNow, consider the implications for power. In the simulations, we estimate of the ATE of a treatment on Measure 1, Measure 2, and the index. The following graph visualizes the estimates. The blue lines show 95 percent confidence intervals. The smaller confidence intervals about the index visualize the precision gains from leveraging both measures. We see that this manifests in higher statistical power for the experiment.\n\ndata.frame(est = c(sims[\"est_index\",], sims[\"est_Y1\",], sims[\"est_Y2\",]),\n           se = c(sims[\"se_index\",], sims[\"se_Y1\",], sims[\"se_Y2\",]),\n           outcome = rep(c(\"Index\", \"Measure 1\", \"Measure 2\"), each = 500)) %&gt;%\n  mutate(T = est/se, \n         sig = 1 * (abs(T) &gt; 1.96)) %&gt;%\n  group_by(outcome) %&gt;%\n  mutate(power = sum(sig)/n(),\n          lab = paste0(\"Power = \", round(power, 2))) %&gt;%\n  arrange(est) %&gt;%\n  mutate(order = 1:500/500) %&gt;%\n  ggplot(aes(x = order, y = est)) + \n  geom_errorbar(aes(ymin = est - 1.96 * se, ymax = est + 1.96 * se), width = 0,\n                col = \"light blue\", alpha = .25) +\n  geom_point() +\n  facet_wrap(~outcome) +\n  geom_text(x = 0.5, y = -.4, aes(label = lab), cex = 4) +\n  geom_hline(yintercept = 0, col = \"red\", lty = 3) + \n  theme_minimal() + xlab(\"Percentile of Estimate in 500 simulations\") + \n  ylab(\"ATE\")\n\n\n\n\nWe have examined an index composed of just two indicators. In principle, there are further efficiency gains to be made by incorporating more indicators into your index. Yet, as we increase the number of indicators, we should consider the degree to which the amalgamation of indicators adheres to the original concept. By adding measures to leverage efficiency gains, we may introduce bias into the measure of the latent concept. Researchers must navigate this tradeoff. Pre-registration of the components of an index provides one principled way to navigate the issue which forces thorough consideration of the concept in the absence of data. This also avoids the ex-post questions about the choice of indicators for an index.\n\n\n10. While concepts may be global, many indicators are context-specific.\nMany studies in the social sciences focus on concepts that are typically assumed to be latent including preferences, knowledge, and attitudes. To the extent that we work on common concepts, there is a tendency to draw from existing operationalizations from studies on related concepts in different contexts. In studies in multiple contexts, as in EGAP’s Metaketa Initiative, researchers aim to study the same causal relationship in multiple national contexts. But a desire to study common concepts need not imply that the same indicators should be used across contexts.\nFor example, consider a set of studies that seek to measure variation in the concept of political knowledge or sophistication. Political knowledge may be assessed through questions that ask subjects to recall a fact about politics. One question may ask subjects to recall the current executive’s (president/prime minister etc.) name, scoring answers as “correct” or “incorrect.” In country \\(A\\), 50% of respondents answer the question correctly. In Country \\(B\\), 100% of respondents answer the question correctly. In Country \\(B\\), we are unable to identify any variation in the indicator because everyone could answer the question. This does not imply that there is no variation in political knowledge in Country \\(B\\), just that this indicator is a poor measure of the variation that exists. In Country \\(A\\), however, this question may be a completely appropriate indicator of political knowledge. If political knowledge was the outcome of an experiment, the lack of variation in the outcome in Country \\(B\\) fails to allow us to identify any difference in political knowledge between treatment and control groups.\nFor this reason, while it may be useful to develop indicators based on existing work or instruments from other contexts, this is not necessarily the best way to develop measures in a new context. Pre-testing can provide insights into whether indicators are appropriate in a given setting. In sum, the mapping between concepts and indicators is site-specific in many cases. Researchers should consider these limitations when operationalizing common concepts in distinct settings.\n\n\n\n\n\n\n\n\nReferences\n\nAdcock, Robert, and David Collier. 2001. “Measurement Validity: A Shared Standard for Qualitative and Quantitative Research.” American Political Science Review 95 (3): 529–46.\n\n\nQuidt, Jonathan de, Johannes Haushofer, and Christopher Roth. 2018. “Measuring and Bounding Experimenter Demand.” American Economic Review 108 (11): 3266–3302."
  },
  {
    "objectID": "guides/data-strategies/randomization_en.html",
    "href": "guides/data-strategies/randomization_en.html",
    "title": "10 Things You Need to Know About Randomization",
    "section": "",
    "text": "This guide will help you design and execute different types of randomization in your experiments. We focus on the big ideas and provide examples and tools that you can use in R. For why to do randomization see this methods guide."
  },
  {
    "objectID": "guides/data-strategies/randomization_en.html#footnotes",
    "href": "guides/data-strategies/randomization_en.html#footnotes",
    "title": "10 Things You Need to Know About Randomization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRandom number generators are actually pseudo-random because they generate a vector of random numbers based on a small set of initial values, known as a seed state. Random number generators operate this way in order to improve computational speed. However, the series of random numbers generated is as random as you need to it to be for the purposes of random assignment because it is wholly unrelated to the potential outcomes of your subjects.↩︎\nFor more, see Moore and Moore (2013).↩︎\nFor a more detalied walkthrough on the randomization procedures available in the R package randomizr, see here.↩︎\nBut if we are certain about the loan’s effects, then it’s also unclear why we are running an experiment to test it. In medical research, randomized controlled trials often stop if it becomes clear early on that a drug is undoubtedly curing life-threatening diseases, and therefore withholding it from control subjects is dangerous. (Similarly, a trial would also stop if it were clear early on that a drug is undoubtedly causing negative and harmful effects.)↩︎"
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html",
    "href": "guides/data-strategies/multisite-experiments_en.html",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "",
    "text": "A multisite or block-randomized trial is a randomized experiment “in which sample members are randomly assigned to a program or a control group within each of a number of sites” (S. W. Raudenbush and Bloom 2015).\nThis guide focuses on multisite educational trials for illustration, although multisite trials are not unique to education. Multisite trials are a subset of multilevel randomized controlled trials (RCTs), in which units are nested within hierarchical structures, such as students nested within schools nested within districts. This guide uses as an illustrative example the case where each site is a school, although they could also be districts or classrooms; thus the term “site” and “school” are used interchangeably.\nAn advantage of multisite trials is that they allow a researcher to study average impact across units or sites, while also getting a sense of heterogeneity across sites (S. W. Raudenbush and Bloom 2015). However, the opportunities provided by multisite trials also come with their own challenges. Much of the rest of this guide will discuss the choices that researchers must make when analyzing multisite trials, and the consequences of these choices.\n\n\nBefore diving in, let’s introduce the definitions of estimand, estimator, and estimate. These concepts are sometimes conflated, but disentangling them increases clarity and understanding. The main distinction is that the estimand is the goal, while the estimator is the analysis we do in order to reach that goal.\nAn estimand is an unobserved quantity of interest about which the researcher wishes to learn. In this guide, the only type of estimand considered is the overall average treatment effect (ATE). Other options include focusing on treatment effect for only a subgroup, or calculating a different summary, such as an odds ratio. After choosing an estimand, the researcher chooses an estimator, which is a method used to calculate the final estimate which should tell the researcher something about the estimand. Finally, the researcher must also choose a standard error estimator if she wants to summarize how the estimates might vary if the research design or underlying data generating process were repeated.\nFirst, to provide context, let’s consider an example. The researcher decides their estimand will be the average treatment effect for the pool of subjects in the experiment. In this example, the researchers observe all of the subjects for whom they want to estimate an effect. As with any causal analysis, the researchers do not observe the control outcomes of the subjects assigned to the active treatment, or the treated outcomes of the subjects assigned to the control treatment. Thus, causal inference is sometimes referenced as a missing data problem, because it is impossible to observe both potential outcomes (the potential outcome given active treatment and the potential outcome given control treatment). See 10 Things to Know About Causal Inference and 10 Types of Treatment Effect You Should Know About for a discussion of other common estimands.\nGiven an estimand, the researchers choose their estimator to be the coefficient from an OLS regression of the observed outcome on site-specific fixed effects and the treatment indicator. To calculate standard errors, they use Huber-White robust standard errors. All these choices result in a point estimate (e.g. the program increased reading scores by \\(5\\) points) and a measure of uncertainty (e.g. a standard error of \\(2\\) points).\nWe’ll also need some notation. This guide follows the Neyman-Rubin potential outcomes notation (Splawa-Neyman, Dabrowska, and Speed (1923/1990), Imbens and Rubin (2015)). The observed outcomes are \\(Y_{ij}\\) for unit \\(i\\) in site \\(j\\). The potential outcomes are \\(Y_{ij}(1)\\), the outcome given active treatment, and \\(Y_{ij}(0)\\), the outcome given control treatment. The quantity \\(B_{ij}\\) is the unit-level intention-to-treat effect (ITT) \\(B_{ij} = Y_{ij}(1) - Y_{ij}(0)\\). If there is no noncompliance, the ITT is the ATE, as defined above. Then \\(B_j\\) is the average impact at site \\(j\\), \\(B_j = 1/N_j \\sum_{i = 1}^{N_j} B_{ij}\\) where \\(N_j\\) is the number of units at site \\(j\\). Finally, \\(N = \\sum_{j = 1}^{J} N_j\\).\nThis guide is structured around the choices an analyst must make concerning estimand and estimators, and the resulting consequences. The choice of estimand impacts the substantive conclusion that a researcher makes. The choice of estimator and standard error estimator results in different statistical properties, including a potential trade off between bias and variance. This guide summarizes material using the framework provided by Miratrix, Weiss, and Henderson (2021)."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#preliminaries-estimands-estimators-and-estimates",
    "href": "guides/data-strategies/multisite-experiments_en.html#preliminaries-estimands-estimators-and-estimates",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "",
    "text": "Before diving in, let’s introduce the definitions of estimand, estimator, and estimate. These concepts are sometimes conflated, but disentangling them increases clarity and understanding. The main distinction is that the estimand is the goal, while the estimator is the analysis we do in order to reach that goal.\nAn estimand is an unobserved quantity of interest about which the researcher wishes to learn. In this guide, the only type of estimand considered is the overall average treatment effect (ATE). Other options include focusing on treatment effect for only a subgroup, or calculating a different summary, such as an odds ratio. After choosing an estimand, the researcher chooses an estimator, which is a method used to calculate the final estimate which should tell the researcher something about the estimand. Finally, the researcher must also choose a standard error estimator if she wants to summarize how the estimates might vary if the research design or underlying data generating process were repeated.\nFirst, to provide context, let’s consider an example. The researcher decides their estimand will be the average treatment effect for the pool of subjects in the experiment. In this example, the researchers observe all of the subjects for whom they want to estimate an effect. As with any causal analysis, the researchers do not observe the control outcomes of the subjects assigned to the active treatment, or the treated outcomes of the subjects assigned to the control treatment. Thus, causal inference is sometimes referenced as a missing data problem, because it is impossible to observe both potential outcomes (the potential outcome given active treatment and the potential outcome given control treatment). See 10 Things to Know About Causal Inference and 10 Types of Treatment Effect You Should Know About for a discussion of other common estimands.\nGiven an estimand, the researchers choose their estimator to be the coefficient from an OLS regression of the observed outcome on site-specific fixed effects and the treatment indicator. To calculate standard errors, they use Huber-White robust standard errors. All these choices result in a point estimate (e.g. the program increased reading scores by \\(5\\) points) and a measure of uncertainty (e.g. a standard error of \\(2\\) points).\nWe’ll also need some notation. This guide follows the Neyman-Rubin potential outcomes notation (Splawa-Neyman, Dabrowska, and Speed (1923/1990), Imbens and Rubin (2015)). The observed outcomes are \\(Y_{ij}\\) for unit \\(i\\) in site \\(j\\). The potential outcomes are \\(Y_{ij}(1)\\), the outcome given active treatment, and \\(Y_{ij}(0)\\), the outcome given control treatment. The quantity \\(B_{ij}\\) is the unit-level intention-to-treat effect (ITT) \\(B_{ij} = Y_{ij}(1) - Y_{ij}(0)\\). If there is no noncompliance, the ITT is the ATE, as defined above. Then \\(B_j\\) is the average impact at site \\(j\\), \\(B_j = 1/N_j \\sum_{i = 1}^{N_j} B_{ij}\\) where \\(N_j\\) is the number of units at site \\(j\\). Finally, \\(N = \\sum_{j = 1}^{J} N_j\\).\nThis guide is structured around the choices an analyst must make concerning estimand and estimators, and the resulting consequences. The choice of estimand impacts the substantive conclusion that a researcher makes. The choice of estimator and standard error estimator results in different statistical properties, including a potential trade off between bias and variance. This guide summarizes material using the framework provided by Miratrix, Weiss, and Henderson (2021)."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#a-multisite-trial-is-fundamentally-a-blocked-or-stratified-rct.",
    "href": "guides/data-strategies/multisite-experiments_en.html#a-multisite-trial-is-fundamentally-a-blocked-or-stratified-rct.",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "A multisite trial is fundamentally a blocked or stratified RCT.",
    "text": "A multisite trial is fundamentally a blocked or stratified RCT.\nA multisite trial is a blocked RCT with 2 levels: randomization occurs at the student level (level 1) within blocks defined by sites/schools (level 2). For example, in a study of a new online math tool for high school students, randomization occurs at the student level within blocks defined by sites/schools. Perhaps half of students at each school are assigned to the status quo / control treatment (no additional math practice), and half are assigned to the active treatment (an offer of additional math practice at home using an online tool).\nBecause of the direct correspondence between multisite trials and blocked experiments, statistical properties of blocked experiments also translate directly to multisite experiments. The main difference between a traditional blocked RCT and a multisite experiment is that in many blocked RCTs, the researcher is able to choose the blocks. For example, in a clinical trial, a researcher may decide to block based on gender or specific age categories. Blocking can help increase statistical power overall or ensure statistical power to assess effects within subgroups (such as those defined by time of entering the study, or defined by other important covariates that might predict the outcome) (Moore (2012), Moore and Moore (2013), Bowers (2011)). Pashley and Miratrix (2021) makes the distinction between fixed blocks, where the number and covariate distribution of blocks is chosen by the researcher, and structural blocks, where natural groupings determine the number of blocks and their covariate distributions. Multisite experiments have structural blocks, such as districts, schools, or classrooms. The type of block can impact variance estimation, as shown in Pashley and Miratrix (2021) and Pashley and Miratrix (2022).\nThe EGAP Metaketa Projects are also multisite trials: the 5 to 7 countries that contain sites for each study are fixed and chosen in advance by the different research teams."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#a-multisite-trial-is-not-a-cluster-randomized-trial",
    "href": "guides/data-strategies/multisite-experiments_en.html#a-multisite-trial-is-not-a-cluster-randomized-trial",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "A multisite trial is not a cluster-randomized trial",
    "text": "A multisite trial is not a cluster-randomized trial\nA different type of RCT is a cluster-randomized design, in which entire schools are assigned to either the active treatment or control treatment. This video explains the difference between cluster and block-randomized designs. In a multisite trial, treatment is assigned within a block to individual units. In a cluster-randomized trial, treatment is assigned to groups of units. Some designs combine cluster- and block-randomization.\nAnother design that is not a multisite or block-randomized trial is an experiment that takes place in only one school and assigns individual students to active treatment and control treatment. This type of study has only one site and thus differences between sites do not matter in this design."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#why-choose-a-multisite-or-block-randomized-trial-design",
    "href": "guides/data-strategies/multisite-experiments_en.html#why-choose-a-multisite-or-block-randomized-trial-design",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "Why choose a multisite or block-randomized trial design?",
    "text": "Why choose a multisite or block-randomized trial design?\nIn most contexts, blocking reduces estimation error over an unblocked (completely randomized) experiment (Moore (2012), Gerber and Green (2012)). Thus, blocked experiments generally offer higher statistical power than unblocked experiments. Blocking is most helpful in increasing precision and statistical power in the setting where there is variation in the outcome, and where the blocks are related to this variation.\nIn multisite trials as compared to block-randomized trials, the researcher typically cannot purposely construct blocks to reduce variation, because they are defined by pre-existing sites. However, the researcher can hope, and often expect, that sites naturally explain some between-site variation. For example, if some schools tend to have higher outcomes than others, then blocked randomization using the school as a block improves efficiency over complete randomization.\nRandomizing with purposefully created blocks or pre-existing sites also helps analysts learn about how treatment effects may vary across the sites or groups of people categorized into the blocks. If a new treatment should help the lowest performing students most, but in any given study most students are not the lowest performing, then researchers may prefer to create blocks of students within schools with the students divided by their previous performance. This blocking within site would allow comparisons of the treatment effects on the relatively rare lowest performing students with the treatment effects on the relatively rare highest performing students."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#why-not-block",
    "href": "guides/data-strategies/multisite-experiments_en.html#why-not-block",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "Why not block?",
    "text": "Why not block?\nOften, in a multisite trial with treatment administered by site administrators (like principals of schools), an analyst has no choice but to randomize within site. In other studies, the construction and choice of blocking criteria is a choice. Pashley and Miratrix (2022) shows that blocking is generally beneficial, but also explores settings in which it may be harmful. Blocking does result in fewer degrees of freedom, but in practice this reduction is rarely an issue, unless an experiment is very small (Imai, King, and Stuart 2008). Any use of blocking requires that an analyst keep track of the blocks and also that an analyst reflect the blocks in subsequent analysis: in many circumstances estimating average treatment effects from a block-randomized experiment while ignoring the blocks will yield biased estimates of the underlying targeted estimands (see “The trouble with ‘controlling for blocks’” and “Estimating Average Treatment Effects in Block Randomized Experiments” for demonstrations of bias arising from different approaches to weighting by blocks)."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#common-linear-regression-models",
    "href": "guides/data-strategies/multisite-experiments_en.html#common-linear-regression-models",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "Common linear regression models",
    "text": "Common linear regression models\nFixed effects with a constant treatment (FE)\nWith this model, the researcher assumes that there are site-specific fixed effects (intercepts), but a common overall ATE. The assumed model is \\(Y_{ij} = \\sum_{k = 1}^{J} \\alpha_k \\text{Site}_{k,ij} + \\beta T_{ij} + e_{ij}\\), where \\(\\text{Site}_{k,ij}\\) is an indicator for unit \\(ij\\) being in site \\(k\\) (out of \\(J\\) sites), \\(T_{ij}\\) is a treatment indicator, and \\(e_{ij}\\) is an \\(iid\\) error term. For more discussion, see S. W. Raudenbush and Bloom (2015).\nFixed effects with interactions (FE-inter)\nWith this model, the researcher assumes site-specific heterogeneous treatment effects, so in addition to fitting a separate fixed effect for the intercepts for each site, a separate treatment impact coefficient is found for each site. \\[Y_{ij} = \\sum_{k = 1}^{J} \\alpha_k \\text{Site}_{k,ij} +\n\\sum_{k = 1}^{J} \\beta_k \\text{Site}_{k,ij} T_{ij} + e_{ij}\\]\nGiven a series of site-specific treatment estimates \\(\\hat{\\beta}_j\\), these estimates are then averaged, with weights by either simple weighting (see Clark and Silverberg (2011)) or by site size."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#common-multilevel-models",
    "href": "guides/data-strategies/multisite-experiments_en.html#common-multilevel-models",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "Common multilevel models",
    "text": "Common multilevel models\nOnce an analyst selects multilevel modeling, for site intercepts and site impacts they must decide: what is considered random, and what is considered fixed?\nFixed intercept, random treatment coefficient (FIRC)\nThis model is similar to the fixed effects models above, but assumes that the site impact \\(\\beta_j\\) is drawn from a shared distribution. The FIRC model was more recently designed to handle bias issues that arise when the proportion of units treated varies across sites.\n\\[\\begin{align*}\n\\text{Level 1}\\qquad & Y_{ij} = \\sum_{k = 1}^{J} \\alpha_k\n\\text{Site}_{k,ij} + \\beta_j T_{ij} + e_{ij}\\\\\n\\text{Level 2}\\qquad & \\beta_j = \\beta + b_j\n\\end{align*}\\] See S. W. Raudenbush and Bloom (2015) and Bloom and Porter (2017).\nRandom intercept, random treatment coefficient (RIRC)\nThis model is an older version of multilevel models, and assumes that both the site intercept and site impact are drawn from shared distributions. \\[\\begin{align*}\n\\text{Level 1}\\qquad & Y_{ij} = A_j + \\beta_j T_{ij} + e_{ij}\\\\\n\\text{Level 2}\\qquad & \\beta_j = \\beta + b_j\\\\\n& A_j = \\alpha + a_j\n\\end{align*}\\]\nRandom intercept, constant treatment coefficient (RICC)\nFinally, this model assumes that the site intercepts are drawn from a shared distribution, but the treatment impact is shared. \\[\\begin{align*}\n\\text{Level 1}\\qquad & Y_{ij} = A_j + \\beta T_{ij} + e_{ij}\\\\\n\\text{Level 2}\\qquad & A_j = \\alpha + a_j\\\\\n\\end{align*}\\] As noted previously, the multilevel framework generally naturally corresponds to the super population perspective. However, for RICC models, the site impacts are not assumed to be drawn from a super population; only the site intercepts are assumed to be random. Thus, when it comes to estimating treatment impacts, RICC models actually take a finite population perspective.\nThere are also weighted versions of both traditional regressions and multilevel models. For example, a fixed-effects model can weigh each person by their inverse chance of treatment to help increase precision. Weighted regression for traditional regression is discussed in Miratrix, Weiss, and Henderson (2021), and weighted regression for multilevel models is discussed in Raudenbush S. W. and Schwartz (2020)."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#design-based-estimators",
    "href": "guides/data-strategies/multisite-experiments_en.html#design-based-estimators",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "Design-based estimators",
    "text": "Design-based estimators\nDesign-based estimators are the most straightforward, as they are composed of simple weighted combinations of means. First, the site-specific treatment impact estimates \\(\\hat{B_j}\\) are calculated by taking differences in means between the active treatment and control treatment groups for each site. Then, the overall estimate is a weighted combination of these estimates, weighted by either person or site weighting.\nThe design-based estimators are \\[\\begin{align*}\n\\hat{\\beta}_{DB-persons} &= \\sum_{j = 1}^{J} \\frac{N_j}{N} \\hat{B_j} \\\\\n\\hat{\\beta}_{DB-sites} &= \\sum_{j = 1}^{J} \\frac{1}{J} \\hat{B_j}.\n\\end{align*}\\] Design-based estimators are generally unbiased for their corresponding estimands (person-weighted or site-weighted). Unbiasedness does not hold for one super population model; see Pashley and Miratrix (2022) for more details."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#linear-regression-estimators",
    "href": "guides/data-strategies/multisite-experiments_en.html#linear-regression-estimators",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "Linear regression estimators",
    "text": "Linear regression estimators\nConsider the FE model (fixed effects with a constant treatment). This regression model results in a precision-weighted estimate, in which each site impact is weighted by the estimated precision of estimating that site’s impact. The estimator is \\(\\hat{\\beta}_{FE} = \\sum_{j = 1}^{J} \\frac{N_j p_j (1 - p_j)}{Z} \\hat{B_j}\\), where \\(p_j\\) is the proportion treated at site \\(j\\). The quantity \\(Z\\) is a normalizing constant, so \\(Z\\) is defined as \\(\\sum_{j = 1}^{J} N_j p_j (1-p_j)\\) to ensure the weights sum to one. The weights are \\(N_j p_j (1 - p_j)\\), which is the inverse of \\(Var(\\hat{\\beta_j})\\), so the weights are related to the precision of the estimate for each site. This expression shows that sites with larger \\(N_j\\), or that have \\(p_j\\) closer to \\(0.5\\), have larger weights.\nThe FE estimator is not generally unbiased for either person-weighted or site-weighted estimands. If the impact size \\(B_j\\) is related to the weights (\\(N_j p_j (1 - p_j)\\)), then the estimator could be biased. For example, if sites that treat a higher proportion of treated units also experience a larger treatment impact, then \\(B_j\\) can be related to \\(p_j (1- p_j)\\). This setting is plausible for example if sites with more resources to intervene on more students also implement the intervention more effectively. If larger sites are more effective, then \\(B_j\\) can be related to \\(N_j p_j (1- p_j)\\).\nInstead, the FE estimator is unbiased for an estimand that weights the site impacts by \\(N_j p_j (1- p_j)\\). However, this estimand does not have a natural substantive interpretation. Although the FE estimator is generally biased for the estimands of interest, it may have increased precision and thus a lower mean squared error.\nIn contrast, the FE-inter model ends up with weights identical to the design-based estimators, depending on if the estimated site impacts are weighted equally or by size."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#multilevel-model-estimators",
    "href": "guides/data-strategies/multisite-experiments_en.html#multilevel-model-estimators",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "Multilevel model estimators",
    "text": "Multilevel model estimators\nMultilevel models also result in precision weighting, but in these models the estimated precision also takes into account the assumed underlying variance in site impacts. For example, the FIRC model can be expressed roughly as: \\[\\hat{\\beta}_{ML-FIRC*} = \\sum_{j = 1}^{J} \\frac{1}{Z}\n\\left(\\frac{\\sigma^2}{N_j p_j ( 1 - p_j)} + \\tau^2\\right)^{-1} \\hat{B_j}\\], where \\(Z\\) is again a normalizing constant, \\(Z = \\sum_{j = 1}^{J} \\left(\\frac{\\sigma^2}{N_j p_j ( 1 - p_j)} + \\tau^2\\right)^{-1}\\). This equation assumes that the \\(b_j\\) have known variance \\(\\tau^2\\), and the \\(e_{ij}\\) have known variance \\(\\sigma^2\\). In general, we do not know these quantities, and instead must estimate them. However, we can see that the implied precision weights incorporate the additional uncertainty assumed in the value of \\(b_j\\).\nThe RIRC model imposes the same structure on the site impacts, and thus the weights are similar to the FIRC model. The RICC model assumes a constant treatment impact, and thus is essentially equivalent to the precision-weighted fixed effects with constant treatment model (FE) when it comes to estimating the site impacts.\nWe summarize the weights in the table below. The following table includes additional estimators that are not discussed in this guide; for more information about these additional estimators, see Miratrix, Weiss, and Henderson (2021).\n\n\n\n\n\n\n\n\nWeight name\nWeight\nEstimators\n\n\n\n\nUnbiased person-weighting\n\\(w_j \\propto N_j\\)\n\\(\\hat{\\beta}_{DB-FP-person}\\), \\(\\hat{\\beta}_{DB-SP-person}\\), \\(\\hat{\\beta}_{FE-weight-person}\\), \\(\\hat{\\beta}_{FE-inter-person}\\)\n\n\nFixed-effect precision-weighting\n\\(w_j \\propto N_j p_j (1 - p_j)\\)\n\\(\\hat{\\beta}_{FE}\\), \\(\\hat{\\beta}_{FE-HW}\\), \\(\\hat{\\beta}_{FE-CR}\\), \\(\\hat{\\beta}_{ML-RICC}\\) (approximately)\n\n\nRandom-effect precision-weighting\n\\(w_j \\propto \\left[\\hat{\\tau} + N_j p_j (1 - p_j)\\right]^{-1}\\)  (approximately)\n\\(\\hat{\\beta}_{ML-FIRC}\\), \\(\\hat{\\beta}_{ML-RIRC}\\)\n\n\nUnbiased site-weighting\n\\(w_j \\propto 1\\)\n\\(\\hat{\\beta}_{DB-FP-site}\\), \\(\\hat{\\beta}_{DB-SP-site}\\), \\(\\hat{\\beta}_{FE-weight-site}\\), \\(\\hat{\\beta}_{FE-inter-site}\\)"
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#point-estimates",
    "href": "guides/data-strategies/multisite-experiments_en.html#point-estimates",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "Point estimates",
    "text": "Point estimates\nFirst, they consider the impact of choices on point estimates. The authors ask, “to what extent can the choice of estimator of the overall average treatment effect result in a different impact estimate?” In general, the authors find that the choice of estimator can substantially impact the point estimates, although the degree of impact depends on the choice. The authors reach the following conclusions.\nPerson-weighted estimands can result in a different conclusion than site-weighted estimands.\nIn some trials, estimates resulting from person-weighted estimands differed substantially from estimates resulting from site-weighted estimands. These discrepancies could be due to a difference in the true underlying values of the estimands, but they could also be due to estimation error from the estimation procedure. Through empirical exploration, they found that the difference is likely due to the estimands themselves being different. They found that “the range of estimates across all estimators is rarely meaningfully larger than the range between the person- and site-weighted estimates alone.”\nFor person-weighted estimands, the choice of estimator generally does not matter.\nThe unbiased design-based estimator and the precision-weighted fixed effect estimate both target the person-weighted estimand. There was little difference in estimates between these estimators. Most likely, “this implies that the potential bias in the bias-precision trade off to the fixed effect estimators is negligible in practice.” Other authors have been able to create situations in which the bias-precision trade off is more severe.\nFor site-weighted estimands, the choice of estimator can matter.\nFIRC estimates did differ from the unbiased design-based site estimator. FIRC can be seen as an adaptive estimator: when there is little estimated variation in impacts between sites, it tends to be more similar to the person-weighted estimate instead of the site-weighted estimate.\nDifferent estimators have different bias-variance trade offs.\nFinally, the authors consider the empirical bias-variance trade off of different estimators, and find:\n\nFE estimators have little bias, but also do not improve precision much over design-based estimators.\nFIRC tends to have lower mean squared error than design-based estimators.\nLarger site impact heterogeneity results in more biased estimates for FIRC.\nEven with more site impact heterogeneity, the mean squared error for FIRC estimators is still generally lower.\nCoverage for design-based estimators is more reliable, especially when site size is variable and site size is correlated with impact."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#standard-errors",
    "href": "guides/data-strategies/multisite-experiments_en.html#standard-errors",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "Standard errors",
    "text": "Standard errors\nThe second question concerns the choice of standard error estimators. The authors ask, “to what extent can the choice of estimator of the standard error of the overall average treatment effect result in a different estimated standard error?”\nThe choice of standard error estimator can substantially impact the estimated standard error. The authors reach the following conclusions.\nThe choice of estimand impacts the standard error.\nSuper population estimators generally have larger standard errors than finite population estimators. Site-weighted estimators generally have larger standard errors than person-weighted estimators.\nGiven a particular estimand, the choice of estimator matters in some contexts and not others.\nFor finite population estimands (including both person and site-weighted estimands) or super population person-weighted estimands, the choice of standard error estimator generally does not matter. In practice, Miratrix, Weiss, and Henderson (2021) found that estimators that attempt to improve precision by trading bias may not actually result in gains in precision in practice. The use of robust standard errors also does not differ much from non-robust standard errors in practice.\nFor super population site-weighted estimands, the choice of standard error estimator can matter a lot. In most cases, standard error estimates differed substantially between the design-based super population estimator and FIRC. The authors further conclude that for super population site-weighted estimands, the wide-ranging standard error estimates stem from instability in estimation. Through a simulation study, they find that super population standard errors can underestimate the true error. The design-based super population standard error estimator is particularly prone to underestimate the standard error compared to multilevel models, and can be unstable, in that it estimates a wide range of different values across simulations."
  },
  {
    "objectID": "guides/data-strategies/multisite-experiments_en.html#footnotes",
    "href": "guides/data-strategies/multisite-experiments_en.html#footnotes",
    "title": "10 Things to Know About Multisite or Block-Randomized Trials",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Linear regression can be used as a tool in both design-based approaches (to calculate the difference in means) and model-based approaches (to estimate the parameters of a Normal data-generating process). In general, this guide considers linear regression as used in a model-based approach.↩︎"
  },
  {
    "objectID": "guides/data-strategies/cluster-randomization_en.html",
    "href": "guides/data-strategies/cluster-randomization_en.html",
    "title": "10 Things to Know About Cluster Randomization",
    "section": "",
    "text": "1. What clustering is\nCluster randomized experiments allocate treatments to groups, but measure outcomes at the level of the individuals that compose the groups. It is this divergence between the level at which the intervention is assigned and the level at which outcomes are defined that classifies an experiment as cluster randomized.\nConsider a study that randomly assigns villages to receive different development programs, where the well-being of households in the village is the outcome of interest. This is a clustered design because, while the treatment is assigned to the village as a whole, we are interested in outcomes defined at the household level. Or consider a study that randomly assigns certain households to receive different get-out-the-vote messages, where we are interested in the voting behavior of individuals. Because the unit of assignment for the message is the household, but the outcome is defined as individual behavior, this study is cluster randomized.\nNow consider a study in which villages are selected at random, and 10 people from each village are assigned to treatment or control, and we measure the well-being of those individuals. In this case, the study is not cluster randomized, because the level at which treatment is assigned and the level at which outcomes are defined is the same. Suppose that a study randomly assigned villages to different development programs and then measured social cohesion in the village. Though it contains the same randomization procedure as our first example, it is not clustered because we are interested here in village-level outcomes: the level of treatment assignment and of outcome measurement is the same.\nClustering matters for two main reasons. On the one hand, clustering reduces the amount of information in an experiment because it restricts the number of ways that the treatment and control groups can be composed, relative to randomization at the individual level. If this fact is not taken into account, we often underestimate the variance in our estimator, leading to over-confidence in our the results of the study. On the other hand, clustering raises the question of how to combine information from different parts of the same experiment into one quantity. Especially when clusters are not of equal sizes and the potential outcomes of units within them are very different, conventional estimators will systematically produce the wrong answer due to bias. However, several approaches at the design and analysis phases can overcome the challenges posed by cluster randomized designs.\n\n\n2. Why clustering can matter I: information reduction\nWe commonly think of the information contained in studies in terms of the sample size and the characteristics of the units within the sample. Yet two studies with exactly the same sample size and the same participants could in theory contain very different amounts of information depending on whether units are clustered. This will greatly affect the precision of the inferences we make based on the studies.\nImagine an impact evaluation with 10 people, where 5 are assigned to the treatment group and 5 to control. In one version of the experiment, the treatment is assigned to individuals - it is not cluster randomized. In another version of the experiment, the 5 individuals with black hair and the 5 individuals with some other color of hair are assigned to treatment as a group. This design has two clusters: ‘black hair’ and ‘other color’.\nA simple application of the n choose k rule shows why this matters. In the first version, the randomization procedure allows for 252 different combinations of people as the treatment and control groups. However, in the second version, the randomization procedure assigns all the black-haired subjects either to treatment or to control, and thus only ever produces two ways of combining people to estimate an effect.\nThroughout this guide, we will illustrate points using examples written in R code. You can copy and paste this into your own R program to see how it works.\n\n[Click to show code]\n\n\n\nCode\nset.seed(12345)\n# Define the sample average treatment effect (SATE)\ntreatment_effect     &lt;- 1\n# Define the individual ids (i)\nperson               &lt;- 1:10\n# Define the cluster indicator (j)\nhair_color           &lt;- c(rep(\"black\",5),rep(\"brown\",5))\n# Define the control outcome (Y0)\noutcome_if_untreated &lt;- rnorm(n = 10)\n# Define the treatment outcome (Y1)\noutcome_if_treated   &lt;- outcome_if_untreated + treatment_effect\n# Version 1 - Not cluster randomized\n# Generate all possible non-clustered assignments of treatment (Z)\nnon_clustered_assignments &lt;- combn(x = unique(person),m = 5)\n# Estimate the treatment effect\ntreatment_effects_V1 &lt;-\n     apply(\n          X = non_clustered_assignments,\n          MARGIN = 2,\n          FUN = function(assignment) {\n               treated_outcomes   &lt;- outcome_if_treated[person %in% assignment]\n               untreated_outcomes &lt;- outcome_if_untreated[!person %in% assignment]\n               mean(treated_outcomes) - mean(untreated_outcomes)\n          }\n     )\n# Estimate the true standard error\nstandard_error_V1 &lt;- sd(treatment_effects_V1)\n# Plot the histogram of all possible estimates of the treatment effect\nhist(treatment_effects_V1,xlim = c(-1,2.5),breaks = 20)\n\n\n\n\n\n\n[Click to show code]\n\n\n\nCode\n# Version 2 - Cluster randomized\n# Generate all possible assignments of treatment when clustering by hair color (Z)\nclustered_assignments     &lt;- combn(x = unique(hair_color),m = 1)\n# Estimate the treatment effect\ntreatment_effects_V2 &lt;-\n     sapply(\n          X = clustered_assignments,\n          FUN = function(assignment) {\n               treated_outcomes   &lt;- outcome_if_treated[person %in% person[hair_color==assignment]]\n               untreated_outcomes &lt;- outcome_if_untreated[person %in% person[!hair_color==assignment]]\n               mean(treated_outcomes) - mean(untreated_outcomes)\n          }\n     )\n# Estimate the true standard error\nstandard_error_V2 &lt;- sd(treatment_effects_V2)\n# Plot the histogram of all possible estimates of the treatment effect\nhist(treatment_effects_V2,xlim = c(-1,2.5),breaks = 20)\n\n\n\n\n\nAs the histograms show, clustering provides a much ‘coarser’ view of the treatment effect. Irrespective of the number of times one randomizes the treatment and of the number of subjects one has, in a clustered randomization procedure the number of possible estimates of the treatment effect will be strictly determined by the number of clusters assigned to the different treatment conditions. This has important implications for the standard error of the estimator.\n\n[Click to show code]\n\n\n\nCode\n# Compare the standard errors\nkable(round(data.frame(standard_error_V1,standard_error_V2),2))\n\n\n\n\n\nstandard_error_V1\nstandard_error_V2\n\n\n\n\n0.52\n1.13\n\n\n\n\n\nWhile the sampling distribution for the non-clustered estimate of the treatment effect has a standard error of about .52, that of the clustered estimate is more than twice as large, at 1.13. Recall that the data going into both studies is identical, the only difference between the studies resides in the way that the treatment assignment mechanism reveals the information.\nRelated to this issue of information is the question of how much units in our study vary within and between clusters. Two cluster randomized studies with \\(J=10\\) villages and \\(n_j=100\\) people per village may have different information about the treatment effect on individuals if, in one study, differences between villages are much greater than the differences in outcomes within them. If, say, all of the individuals in any village acted exactly the same but different villages showed different outcomes, then we would have on the order of 10 pieces of information: all of the information about causal effects in that study would be at the village level. Alternatively, if the individuals within a village acted more or less independently of each other, then we would have on the order of 10 \\(\\times\\) 100=1000 pieces of information.\nWe can formalize the idea that the highly dependent clusters provide less information than the highly independent clusters with the intracluster correlation coefficient. For a given variable, \\(y\\), units \\(i\\) and clusters \\(j\\), we can write the intracluster correlation coefficient as follows:\n\\[ \\text{ICC} \\equiv \\frac{\\text{variance between clusters in } y}{\\text{total variance in } y} \\equiv \\frac{\\sigma_j^2}{\\sigma_j^2 + \\sigma_i^2} \\]\nwhere \\(\\sigma_i^2\\) is the variance between units in the population and \\(\\sigma_j^2\\) is the variance between outcomes defined at the cluster level. Kish (1965) uses this description of dependence to define his idea of the “effective N” of a study (in the sample survey context, where samples may be clustered):\n\\[\\text{effective N}=\\frac{N}{1+(n_j -1)\\text{ICC}}=\\frac{Jn}{1+(n-1)\\text{ICC}},\\]\nwhere the second term follows if all of the clusters are the same size (\\(n_1 \\ldots n_J \\equiv n\\)).\nIf 200 observations arose from 10 clusters with 20 individuals within each cluster, where ICC = .5, such that 50% of the variation could be attributed to cluster-to-cluster differences (and not to differences within a cluster), Kish’s formula would suggest that we have an effective sample size of about 19 observations, instead of 200.\nAs the foregoing discussion suggests, clustering reduces information most when it a) greatly restricts the number of ways in which subjects can be assigned to treatment and control groups, and b) produces units whose outcomes are strongly related to the cluster they are a member of (i.e. when it increases the ICC).\n\n\n3. What to do about information reduction\nOne way to limit the extent to which clustering reduces the information contained in our design would be to form clusters in such a way that the intracluster correlation is low. For example, if we were able to form clusters randomly, there would be no systematic relationship between units within a cluster. Hence, using cluster random assignment to assign these randomly formed clusters to experimental conditions would not result in any information reduction. Yet, opportunities to create clusters are typically rare. In most cases, we have to rely on naturally formed clusters (e.g., villages, classrooms, cities). In most scenarios that involve clustered assignment, we hence will have to make sure that our estimates of uncertainty about treatment effects correctly reflect the resulting information loss from clustering.\nIn order to characterize our uncertainty about treatment effects, we typically want to calculate a standard error: an estimate of how much our treatment effect estimates would vary, were we able to repeat the experiment a very large number of times and, for each repetition, observe units in their resulting treated or untreated state.\nHowever, we are never able to observe the true standard error of an estimator, and therefore must use statistical procedures to infer this unknown quantity. Conventional methods for calculating standard errors do not take into account clustering. Thus, in order to avoid over-confidence in experimental findings, we need to modify the way in which we calculate uncertainty estimates.\nIn this section we limit our attention to so-called ‘design-based’ approaches to calculating the standard error. In the design-based approach we simulate repetitions of the experiment to derive and check ways of characterizing the variance of the estimate of the treatment effect, accounting for clustered randomization. We contrast these with ‘model-based’ approaches further on in the guide. In the model-based approach we state that the outcomes were generated according to a probability model and that the cluster-level relationships also follow a probability model.\nTo begin, we will create a function which simulates a cluster randomized experiment with fixed intracluster correlation, and use it to simulate some data from a simple cluster-randomized design.\n\n[Click to show code]\n\n\n\nCode\nmake_clustered_data &lt;- function(J = 10, n = 100, treatment_effect = .25, ICC = .1){\n     ## Inspired by Mathieu et al, 2012, Journal of Applied Psychology\n     if (J %% 2 != 0 | n %% 2 !=0) {\n          stop(paste(\"Number of clusters (J) and size of clusters (n) must be even.\"))\n     }\n     Y0_j         &lt;- rnorm(J,0,sd = (1 + treatment_effect) ^ 2 * sqrt(ICC))\n     fake_data    &lt;- expand.grid(i = 1:n,j = 1:J)\n     fake_data$Y0 &lt;- rnorm(n * J,0,sd = (1 + treatment_effect) ^ 2 * sqrt(1 - ICC)) + Y0_j[fake_data$j]\n     fake_data$Y1 &lt;- with(fake_data,mean(Y0) + treatment_effect + (Y0 - mean(Y0)) * (2 / 3))\n     fake_data$Z  &lt;- ifelse(fake_data$j %in% sample(1:J,J / 2) == TRUE, 1, 0)\n     fake_data$Y  &lt;- with(fake_data, Z * Y1 + (1 - Z) * Y0)\n     return(fake_data)\n}\nset.seed(12345)\npretend_data &lt;- make_clustered_data(J = 10,n = 100,treatment_effect = .25,ICC = .1)\n\n\nBecause we have created the data ourselves, we can calculate the true standard error of our estimator. We firstly generate the true sampling distribution by simulating every possible permutation of the treatment and calculating the estimate each time. The standard deviation of this distribution is the standard error of the estimator.\n\n[Click to show code]\n\n\n\nCode\n# Define the number of clusters\nJ &lt;- length(unique(pretend_data$j))\n# Generate all possible ways of combining clusters into a treatment group\nall_treatment_groups &lt;- with(pretend_data,combn(x = 1:J,m = J/2))\n# Create a function for estimating the effect\nclustered_ATE &lt;- function(j,Y1,Y0,treated_clusters) {\n     Z_sim    &lt;- (j %in% treated_clusters)*1\n     Y        &lt;- Z_sim * Y1 + (1 - Z_sim) * Y0\n     estimate &lt;- mean(Y[Z_sim == 1]) - mean(Y[Z_sim == 0])\n     return(estimate)\n}\nset.seed(12345)\n# Apply the function through all possible treatment assignments\ncluster_results &lt;- apply(\n  X = all_treatment_groups, MARGIN = 2,\n                         FUN = clustered_ATE,\n                         j  = pretend_data$j,Y1 = pretend_data$Y1,\n  Y0 = pretend_data$Y0\n)\ntrue_SE &lt;- sd(cluster_results)\ntrue_SE\n\n\n[1] 0.2567029\n\n\nThis gives a standard error of 0.26. We can compare the true standard error to two other kinds of standard error commonly employed. The first ignores clustering and assumes that treatment effect estimates are identically and independently distributed according to a normal distribution. We will refer to this as the I.I.D. standard error. To take clustering into account, we can use the following formula for the standard error:\n\\[\\text{Var}_\\text{clustered}(\\hat{\\tau})=\\frac{\\sigma^2}{\\sum_{j=1}^J \\sum_{i=1}^{n_j} (Z_{ij}-\\bar{Z})^2} (1-(n-1)\\rho)\\]\nwhere \\(\\sigma^2=\\sum_{j=1}^J \\sum_{i=1}^{n_j} (Y_{ij} - \\bar{Y}_{ij})^2\\) (following Arceneaux and Nickerson (2009) ). This adjustment to the IID standard error is commonly known as the “Robust Clustered Standard Error” or RCSE.\n\n[Click to show code]\n\n\n\nCode\nATE_estimate &lt;- lm(Y ~ Z,data = pretend_data)\nIID_SE &lt;- function(model) {\n     return(sqrt(diag(vcov(model)))[[\"Z\"]])\n}\nRCSE &lt;- function(model, cluster,return_cov = FALSE){\n  require(sandwich)\n  require(lmtest)\n  M &lt;- length(unique(cluster))\n  N &lt;- length(cluster)\n  K &lt;- model$rank\n  dfc &lt;- (M/(M - 1)) * ((N - 1)/(N - K))\n  uj &lt;- apply(estfun(model), 2, function(x) tapply(x, cluster, sum))\n  rcse.cov &lt;- dfc * sandwich(model, meat = crossprod(uj)/N)\n  rcse.se &lt;- as.matrix(coeftest(model, rcse.cov))\n  if(return_cov){\n    return(rcse.cov)\n  }else{\n    return(rcse.se)\n  }\n}\nIID_SE_estimate &lt;- IID_SE(model = ATE_estimate)\nRCSE_estimate   &lt;- RCSE(model = ATE_estimate,cluster = pretend_data$j)\nknitr::kable(round(\n  data.frame(\n     true_SE         = true_SE,\n     IID_SE_estimate = IID_SE_estimate,\n    RCSE_estimate   = RCSE_estimate[\"Z\", \"Std. Error\"]\n  ),\n  2\n))\n\n\n\n\n\ntrue_SE\nIID_SE_estimate\nRCSE_estimate\n\n\n\n\n0.26\n0.08\n0.26\n\n\n\n\n\nWhen we ignore the clustered-assignment, the standard error is too small: we are over-confident about the amount of information provided to us by the experiment. The RCSE is slightly more conservative than the true standard error in this instance, but is very close. The discrepancy is likely because the RCSE is not a good approximation of the true standard error when the number of clusters is as small as it is here. To illustrate the point further, we can compare a simulation of the true standard error generated through random permutations of the treatment to the IID and RC standard errors.\n\n[Click to show code]\n\n\n\nCode\ncompare_SEs &lt;- function(data) {\n     simulated_SE &lt;- sd(replicate(\n          5000,\n          clustered_ATE(\n               j = data$j,\n               Y1 = data$Y1,\n               Y0 = data$Y0,\n               treated_clusters = sample(unique(data$j),length(unique(data$j))/2)\n    )\n  ))\n     ATE_estimate &lt;- lm(Y ~ Z,data)\n     IID_SE_estimate &lt;- IID_SE(model = ATE_estimate)\n     RCSE_estimate &lt;- RCSE(model = ATE_estimate,cluster = data$j)[\"Z\", \"Std. Error\"]\n     return(round(c(\n          simulated_SE = simulated_SE,\n          IID_SE = IID_SE_estimate,\n          RCSE = RCSE_estimate\n     ),3))\n}\nJ_4_clusters    &lt;- make_clustered_data(J = 4)\nJ_10_clusters   &lt;- make_clustered_data(J = 10)\nJ_30_clusters   &lt;- make_clustered_data(J = 30)\nJ_100_clusters  &lt;- make_clustered_data(J = 100)\nJ_1000_clusters &lt;- make_clustered_data(J = 1000)\nset.seed(12345)\nknitr::kable(rbind(\n  c(J = 4,compare_SEs(J_4_clusters)),\n  c(J = 30,compare_SEs(J_30_clusters)),\n  c(J = 100,compare_SEs(J_100_clusters)),\n  c(J = 1000,compare_SEs(J_1000_clusters))\n  ))\n\n\n\n\n\nJ\nsimulated_SE\nIID_SE\nRCSE\n\n\n\n\n4\n0.270\n0.127\n0.260\n\n\n30\n0.161\n0.047\n0.146\n\n\n100\n0.085\n0.027\n0.088\n\n\n1000\n0.027\n0.008\n0.027\n\n\n\n\n\nAs these simple examples illustrate, the clustered estimate of the standard error (RCSE) gets closer to the truth (the simulated standard error) as the number of clusters increases. Meanwhile, the standard error ignoring clustering (assuming IID) tends to be smaller than either of the other standard errors. The smaller the estimate of the standard error is, the more precise the estimates seem to us, and the more likely we will find results that appear ‘statistically significant’. This is problematic: in this case, the IID standard error is leads us to be over confident in our results because it ignores intra-cluster correlation, the extent to which differences between units can be attributed to the cluster they are a member of. If we estimate standard errors using techniques that understate our uncertainty, we are more likely to falsely reject null hypotheses when we should not.\nAnother way to approach the problems that clustering introduces into the calculation of standard errors is to analyze the data at the level of the cluster. In this approach, we take averages or sums of the outcomes within the clusters, and then treat the study as though it only took place at the level of the cluster. Hansen and Bowers (2008) show that we can characterize the distribution of the difference of means using what we know about the distribution of the sum of the outcome in the treatment group, which varies from one assignment of treatment to another.\n\n[Click to show code]\n\n\n\nCode\n# Aggregate the unit-level data to the cluster level\n# Sum outcome to cluster level\nYj &lt;- tapply(pretend_data$Y,pretend_data$j,sum)\n# Aggregate assignment indicator to cluster level\nZj &lt;- tapply(pretend_data$Z,pretend_data$j,unique)\n# Calculate unique cluster size\nn_j &lt;- unique(as.vector(table(pretend_data$j)))\n# Calculate total sample size (our units are now clusters)\nN &lt;- length(Zj)\n# Generate cluster id\nj &lt;- 1:N\n# Calculate number of clusters treated\nJ_treated &lt;- sum(Zj)\n# Make a function for the cluster-level difference in means estimator (See Hansen & Bowers 2008)\ncluster_difference &lt;- function(Yj,Zj,n_j,J_treated,N){\n     ones &lt;- rep(1, length(Zj))\n     ATE_estimate &lt;- crossprod(Zj,Yj)*(N/(n_j*J_treated*(N-J_treated))) -\n          crossprod(ones,Yj)/(n_j*(N-J_treated))\n     return(ATE_estimate)\n}\n# Given equal sized clusters and no blocking, this is identical to the\n# unit-level difference in means\nATEs &lt;- colMeans(data.frame(\n  cluster_level_ATE =\n               cluster_difference(Yj,Zj,n_j,J_treated,N),\n          unit_level_ATE =\n    with(pretend_data, mean(Y[Z == 1]) - mean(Y[Z == 0]))\n))\nknitr::kable(data.frame(ATEs),align = \"c\")\n\n\n\n\n\n\nATEs\n\n\n\n\ncluster_level_ATE\n0.3417229\n\n\nunit_level_ATE\n0.3417229\n\n\n\n\n\nIn order to characterize uncertainty about the cluster-level ATE, we can exploit the fact that the only random element of the estimator is now the cross-product between the cluster-level assignment vector and the cluster-level outcome, \\(\\mathbf{Z}^\\top\\mathbf{Y}\\), scaled by some constant. We can estimate the variance of this random component through permutation of the assignment vector or through an approximation of the variance, assuming that the sampling distribution follows a normal distribution.\n\n[Click to show code]\n\n\n\nCode\n# Approximating variance using normality assumptions\nnormal_sampling_variance &lt;-\n     (N/(n_j*J_treated*(N-J_treated)))*(var(Yj)/n_j)\n# Approximating variance using permutations\nset.seed(12345)\nsampling_distribution &lt;- replicate(10000,cluster_difference(Yj,sample(Zj),n_j,J_treated,N))\nses &lt;- data.frame(\n  sampling_variance = c(sqrt(normal_sampling_variance), sd(sampling_distribution)),\n  p_values = c(\n    2 * (1 - pnorm(abs(ATEs[1]) / sqrt(normal_sampling_variance), mean = 0)),\n                               2*min(mean(sampling_distribution&gt;=ATEs[1]),mean(sampling_distribution&lt;=ATEs[1]))\n  )\n)\nrownames(ses) &lt;- c(\"Assuming Normality\",\"Permutations\")\nknitr::kable(ses)\n\n\n\n\n\n\nsampling_variance\np_values\n\n\n\n\nAssuming Normality\n0.2848150\n0.2302145\n\n\nPermutations\n0.2825801\n0.2792000\n\n\n\n\n\nThis cluster-level approach has the advantage of correctly characterizing uncertainty about effects when randomization is clustered, without having to use the RCSE standard errors for the unit-level estimates, which are overly permissive for small N. Indeed, the false positive rate of tests based on RCSE standard errors tend to be incorrect when the number of clusters is small, leading to over-confidence. As we shall see below, however, when the number of clusters is very small (\\(J=4\\)) the cluster-level approach is overly conservative, rejecting the null with a probability of 1. Another drawback of the cluster-level approach is that it does not allow for the estimation of unit-level quantities of interest, such as heterogeneous treatment effects.\n\n\n4. Why clustering can matter II: different cluster sizes and bias\nWhen clusters are of different sizes, this can pose a unique class of problems related to the estimation of the treatment effect. Especially when the size of the cluster is in some way related to the potential outcomes of the units within it many conventional estimators of the sample average treatment effect (SATE) can be biased.\nTo fix ideas, imagine an intervention targeted at firms of different sizes, which seeks to increase worker productivity. Due to economies of scale, the productivity of employees in big firms is increased much more proportional to that of employees in smaller firms. Imagine that the experiment includes 20 firms ranging in size from one-person entrepreneurs to large outfits with over 500 employees. Half of the firms are assigned to the treatment, and the other half are assigned to control. Outcomes are defined at the employee level.\n\n[Click to show code]\n\n\n\nCode\nset.seed(1000)\n# Number of firms\nJ &lt;- 20\n# Employees per firm\nn_j &lt;- rep(2^(0:(J/2-1)),rep(2,J/2))\n# Total number of employees\nN &lt;- sum(n_j)\n# 2046\n# Unique employee (unit) ID\ni &lt;- 1:N\n# Unique firm (cluster) ID\nj &lt;- rep(1:length(n_j),n_j)\n# Firm specific treatment effects\ncluster_ATE &lt;- n_j^2/10000\n# Untreated productivity\nY0 &lt;- rnorm(N)\n# Treated productivity\nY1 &lt;- Y0 + cluster_ATE[j]\n# True sample average treatment effect\n(true_SATE &lt;- mean(Y1-Y0))\n\n\n[1] 14.9943\n\n\n\n[Click to show code]\n\n\n\nCode\n# Correlation between firm size and effect size\ncor(n_j,cluster_ATE)\n\n\n[1] 0.961843\n\n\nAs we see, there is high correlation in the treatment effect and cluster size. Now let us simulate 1000 analyses of this experiment, permuting the treatment assignment vector each time, and taking the unweighted difference in means as an estimate of the sample average treatment effect.\n\n[Click to show code]\n\n\n\nCode\nset.seed(1234)\n# Unweighted SATE\nSATE_estimate_no_weights &lt;- NA\nfor(i in 1:1000){\n     # Clustered random assignment of half of the firms\n     Z &lt;- (j %in% sample(1:J,J/2))*1\n     # Reveal outcomes\n     Y &lt;- Z*Y1 + (1-Z)*Y0\n     # Estimate SATE\n     SATE_estimate_no_weights[i] &lt;- mean(Y[Z==1])-mean(Y[Z==0])\n     }\n# Generate histogram of estimated effects\nhist(SATE_estimate_no_weights,xlim = c(true_SATE-2,true_SATE+2),breaks = 100)\n# Add the expected estimate of the SATE using this estimator\nabline(v=mean(SATE_estimate_no_weights),col=\"blue\")\n# And add the true SATE\nabline(v=true_SATE,col=\"red\")\n\n\n\n\n\nThe histogram shows the sampling distribution of the estimator, with the true SATE in red and the unweighted estimate thereof in blue. The estimator is biased: in expectation, we do not recover the true SATE, instead underestimating it. Intuitively, one might correctly expect that the problem is related to the relative weight of the clusters in the calculation of the treatment effect. However, in this situation, taking the difference in the weighted average of the outcome among treated and control clusters is not enough to provide an unbiased estimator (see the code below).\n\n[Click to show code]\n\n\n\nCode\nset.seed(1234)\n# Weighted cluster-averages\nSATE_estimate_weighted &lt;- NA\nfor(i in 1:1000){\n     # Define the clusters put into treatment\n     treated_clusters &lt;- sample(1:J,J/2,replace = F)\n     # Generate unit-level assignment vector\n     Z &lt;- (j %in% treated_clusters)*1\n     # Reveal outcomes\n     Y &lt;- Z*Y1 + (1-Z)*Y0\n     # Calculate the cluster weights\n     treated_weights &lt;- n_j[1:J%in%treated_clusters]/sum(n_j[1:J%in%treated_clusters])\n     control_weights &lt;- n_j[!1:J%in%treated_clusters]/sum(n_j[!1:J%in%treated_clusters])\n     # Calculate the means of each cluster\n     treated_means &lt;- tapply(Y,j,mean)[1:J%in%treated_clusters]\n     control_means &lt;- tapply(Y,j,mean)[!1:J%in%treated_clusters]\n     # Calculate the cluster-weighted estimate of the SATE\n     SATE_estimate_weighted[i] &lt;-\n          weighted.mean(treated_means,treated_weights) -\n          weighted.mean(control_means,control_weights)\n}\n# Generate histogram of estimated effects\nhist(SATE_estimate_weighted,xlim = c(true_SATE-2,true_SATE+2),breaks = 100)\n# Add the expected estimate of the unweighted SATE\nabline(v=mean(SATE_estimate_no_weights),col=\"blue\")\n# Add the expected estimate of the weighted SATE\nabline(v=mean(SATE_estimate_weighted),col=\"green\")\n# And add the true SATE\nabline(v=true_SATE,col=\"red\")\n\n\n\n\n\nThe histogram shows the sampling distribution of the weighted estimator, with the true SATE in red and the unweighted estimate in blue, and the weighted estimate in green. In expectation, the weighted version of the estimator in fact gives the same estimate of the SATE as the non-weighted version. What is the nature of the bias?\nInstead of assigning treatment to half of the clusters and comparing outcomes at the level of the ‘treatment’ and ‘control’ groups, imagine that we paired each cluster with one other cluster, and assigned one to treatment within each pair. Our estimate of the treatment effect is then the aggregate of the pair-level estimates. This is analogous to the complete random assignment procedure employed above, in which \\(J/2\\) firms were assigned to treatment. Now, we will instead refer to the \\(k\\)’th of the \\(m\\) pairs, where \\(2m = J\\).\nGiven this setup, Imai, King, and Nall (2009) give the following formal definition of the bias in the cluster-weighted difference-in-means estimator\n\\[\\frac{1}{n}\n\\sum^m_{k = 1}\n\\sum^2_{l = 1}\n\\left[\n\\left(\n\\frac{n_{1k} + n_{2k}}{{2} - n_{lk}}\n\\right)\n\\times\n\\sum^{n_{lk}}_{i = 1}\n\\frac{Y_{ilk}(1) - Y_{ilk}(0)}{n_{lk}}\n\\right],\\]\nwhere \\(l = 1,2\\) indexes the clusters within each pair. Thus, \\(n_{1k}\\) refers to the number of units in the first of the \\(k\\)’th pair of clusters.\nThis expression indicates that bias from unequal cluster sizes arises if and only if two conditions are met. Firstly, the sizes of at least one pair of clusters must be unequal: when \\(n_{1k}=n_{2k}\\) for all \\(k\\), the bias term is reduced to 0. Secondly, the weighted effect sizes of at least one pair of clusters must be unequal: when \\(\\sum_{i = 1}^{n_{1k}}(Y_{i1k}(1)-Y_{i1k}(0))/n_{1k} = \\sum_{i = 1}^{n_{2k}}(Y_{i2k}(1)-Y_{i2k}(0))/n_{2k}\\) for all \\(k\\), the bias is also reduced to 0.\n\n\n5. What to do about different cluster sizes\nTwo other approaches work to alleviate this problem of bias from the correlation of cluster size and treatment effect: (1) condition on cluster size directly and (2) use the Horvitz-Thompson estimator. This DeclareDesign blog post discusses both options. Here we demonstrate a pairing approach to conditioning on cluster-size given the example data that we generated above.\nAs the above expression suggests, in order to reduce the bias from unequal cluster sizes to almost 0, it is sufficient to put clusters into pairs that either are of equal size or have almost identical potential outcomes.\nWe demonstrate this approach below using the same data as we examined in the example of a hypothetical firm-randomized employee productivity experiment.\n\n\n\n[Click to show code]\n\n\n\nCode\nset.seed(1234)\n# Make a function that matches pairs based on size\npair_sizes &lt;- function(j,n_j){\n     # Find all of the unique sizes\n     unique_sizes &lt;- unique(n_j)\n     # Find the number of unique sizes\n     N_unique_sizes &lt;- length(unique_sizes)\n     # Generate a list of candidates for pairing at each cluster size\n  possible_pairs &lt;- lapply(unique_sizes, function(size) {\n    which(n_j == size)\n  })\n     # Find the number of all possible pairs (m)\n     m_pairs &lt;- length(unlist(possible_pairs))/2\n     # Generate a vector with unique pair-level identifiers\n     pair_indicator &lt;- rep(1:m_pairs,rep(2,m_pairs))\n     # Randomly assign units of the same cluster size into pairs\n     pair_assignment &lt;-\n          unlist(lapply(\n               possible_pairs,\n               function(pair_list){\n        sample(pair_indicator[unlist(possible_pairs) %in% pair_list])\n      }\n    ))\n     # Generate a vector indicating the k'th pair for each i unit\n     pair_assignment &lt;- pair_assignment[match(x = j,table = unlist(possible_pairs))]\n     return(pair_assignment)\n}\npair_indicator &lt;- pair_sizes(j = j , n_j = n_j)\nSATE_estimate_paired &lt;- NA\nfor(i in 1:1000){\n     # Now loop through the vector of paired assignments\n     pair_ATEs &lt;- sapply(unique(pair_indicator),function(pair){\n          # For each pair, randomly assign one to treatment\n          Z &lt;- j[pair_indicator==pair] %in% sample(j[pair_indicator==pair],1)*1\n          # Reveal the potential outcomes of the pair\n          Y &lt;- Z*Y1[pair_indicator==pair] + (1-Z)*Y0[pair_indicator==pair]\n          clust_weight &lt;- length(j[pair_indicator==pair])/N\n          clust_ATE &lt;- mean(Y[Z==1])-mean(Y[Z==0])\n          return(c(weight = clust_weight, ATE = clust_ATE))\n     })\n     SATE_estimate_paired[i] &lt;- weighted.mean(x = pair_ATEs[\"ATE\",],w = pair_ATEs[\"weight\",])\n}\n# Generate histogram of estimated effects\nhist(SATE_estimate_paired,xlim = c(true_SATE-2,true_SATE+2),breaks = 100)\n# Add the expected estimate of the paired SATE\nabline(v=mean(SATE_estimate_paired),col=\"purple\")\n# And add the true SATE\nabline(v=true_SATE,col=\"red\")\n\n\n\n\n\n\n\n\n[Click to show code]\n\n\n\nCode\n# The paired estimator is much closer to the true SATE\nkable(round(data.frame(\n  true_SATE = true_SATE,\n  paired_SATE = mean(SATE_estimate_paired),\n  weighted_SATE = mean(SATE_estimate_weighted),\n  unweighted_SATE = mean(SATE_estimate_no_weights)\n),2))\n\n\n\n\n\ntrue_SATE\npaired_SATE\nweighted_SATE\nunweighted_SATE\n\n\n\n\n14.99\n14.99\n13.45\n13.45\n\n\n\n\n\nIn spite of unequal cluster sizes, the bias is completely eliminated by this technique: in expectation, the paired estimator recovers the true Sample Average Treatment Effect, whereas the cluster-weighted and non-weighted difference-in-means estimators are biased.\nNote also that the variance in the sampling distribution is much lower for the pair-matched estimator, giving rise to much more precise estimates. Thus, pair-matching not only promises to reduce bias, but can also greatly mitigate the problem of information reduction that clustering induces.\nSuch pre-randomization pair matching, however, does impose some constraints on the study, some of which may be difficult to meet in practice. For example, it may be difficult or even impossible to find perfectly-matched pairs for every cluster size, especially when there are multiple treatments (such that, instead of pairs, treatment is randomized over triplets or quadruplets). In such cases, researchers may adopt several other solutions, such as creating pairs by matching on observed covariates prior to the randomization, whereby, for example, the within-pair similarity of observed covariates is maximized. Imai, King, and Nall (2009) recommend a mixture model for post-randomization pair-matched estimation, and spell out some of the assumptions that must be made for such estimates to be valid. And this DeclareDesign blog post demonstrates conditioning on cluster-size without strict pairing.\n\n\n6. Why clustering can matter III: within-cluster spillovers\nIn many, or most experiments, we would like to estimate the average causal effect of the treatment within a population or a sample. Denoting \\(Y_{z_i}\\) the outcome \\(Y\\) of unit \\(i\\) when assigned to the treatment status \\(z_i \\in \\{1,0\\}\\), we can define this quantity – the ATE (Average Treatment Effect) – as the expected value of the difference between the sample when assigned to treatment, \\(Y_1\\) and the sample when assigned to control \\(Y_0\\): \\(E[Y_1 - Y_0]\\).\nHowever, it may be the case that a unit’s outcome depends on the treatment status \\(z_j\\) of another unit, \\(j\\), inside the same cluster. In that case, we denote potential outcomes \\(Y_{z_j,z_i} \\in \\{ Y_{00}, Y_{10}, Y_{01}, Y_{11} \\}\\), where an untreated unit with an untreated cluster neighbor is defined as \\(Y_{00}\\), an untreated unit with a treated cluster neighbor as \\(Y_{10}\\), a treated unit with an untreated cluster neighbor as \\(Y_{01}\\), and a treated unit with a treated cluster neighbor as \\(Y_{11}\\). When we conduct a cluster-randomized experiment, we typically assume that a unit’s outcome is not a function of the treatment status of the units with whom it shares a cluster, or formally \\(Y_{01}=Y_{11}=Y_1\\) and \\(Y_{10}=Y_{00}=Y_0\\). Yet, for all sorts of reasons this may not be the case: depending on whom someone finds themselves in the same cluster with, and whether or not that cluster is assigned to treatment, their outcomes may be very different.\nConsider an experiment in which five pairs of students living in dorms are randomly assigned to either receive or not receive a food subsidy, and their stated well-being is the outcome of interest. Let us assume that four students are vegetarian (V) and six are meat-eaters (M). When a VV, MM, or VM pair is assigned to control, they do not receive the subsidy and their well-being is unaffected. However, when assigned to treatment, VM pairs quarrel and this reduces their well-being, whereas VV and MM pairs do not fight and are affected only by the treatment. Let us denote \\(x_k \\in \\{0,1\\}\\) an indicator for whether the pair is mismatched, where the unit’s outcome is denoted \\(Y_{z_j,z_j,x_k}\\). This implies that \\(Y_{110} = Y_1\\) and \\(Y_{000} = Y_{001} = Y_0\\), whereas \\(Y_{111} \\neq Y_1\\). To understand how this matters, let us simulate such an experiment.\n\n\n\n[Click to show code]\n\n\n\nCode\n# Create experimental data\nN &lt;- 10\ntypes &lt;- c(rep(\"V\",.4*N),rep(\"M\",.6*N))\nID &lt;- 1:length(types)\nbaseline &lt;- rnorm(length(ID))\n# The true treatment effect is 5\ntrue_ATE &lt;- 5\n# If a pair is mismatched (VM, MV), they get a spillover of -10\nspillover_or_not &lt;- function(type_i,type_j){\n  ifelse(type_i==type_j,yes = 0,no = -10)\n}\n# A function for forming pairs\nform_pairs &lt;- function(ID,types){\n  N &lt;- length(ID)\n  k &lt;- rep(1:(N/2),2)\n  pair_place &lt;- rep(c(1,2),c(N/2,N/2))\n  ID_draw &lt;- sample(ID)\n  type_i &lt;- types[ID_draw]\n  pair_1 &lt;- type_i[pair_place==1]\n  pair_2 &lt;- type_i[pair_place==2]\n  ID_j_1 &lt;- ID_draw[pair_place==1]\n  ID_j_2 &lt;- ID_draw[pair_place==2]\n  type_j &lt;- c(pair_2,pair_1)\n  j &lt;- c(ID_j_2,ID_j_1)\n  return(data.frame(i = ID_draw,j = j,k = k, type_i = type_i, type_j = type_j))\n}\n# A function for assigning treatment and revealing the outcome\nassign_reveal_est &lt;- function(k,i,effect,spillover){\n  Z &lt;- (k %in% sample(k,N/2))*1\n  Y &lt;- baseline[i] + Z*effect + Z*spillover\n  mean(Y[Z==1])-mean(Y[Z==0])\n}\n# A function for simulating the experiment\nsimulate_exp &lt;- function(){\n  data &lt;- form_pairs(ID,types)\n  spillover &lt;- spillover_or_not(data$type_i,data$type_j)\n  estimate &lt;- assign_reveal_est(k = data$k,effect = true_ATE,spillover = spillover,i = data$i)\n  return(estimate)\n}\n# Estimate the effects one thousand times\nest_effects &lt;- replicate(n = 1000,expr = simulate_exp())\n# Plot the estimates as bars, the expected ATE in blue, and the true ATE in red\nhist(est_effects,breaks = 100,xlim = c(-7,7))\nabline(v = true_ATE,col = \"red\")\nabline(v = mean(est_effects,na.rm = T),col = \"blue\")\n\n\n\n\n\nAs the plot above shows, this is a biased estimator of the true individual level treatment effect, \\(Y_{01} - Y_{00}\\). In expectation, we estimate an effect close to 0, obtaining very negative effects in almost half of the simulations of this experiment. The key point here is that the estimand is changed: rather than the ATE, we obtain a combination of the true treatment effect among those who are matched (do not experience spillovers) \\(E[Y_{110}-Y_{00x_k}]\\), and the combined treatment and spillover effect for those who are unmatched \\(E[Y_{111}-Y_{00x_k}]\\). Crucially, however, we cannot identify the impact of the spillover, \\(E[Y_{101}-Y_{00x_k}]\\), independently of the direct effect. This is because the randomization is clustered: it is not possible to observe \\(Y_{101}\\) in a cluster-randomized scheme, because all units within a cluster are always treated. Generally speaking, this issue is true of any cluster-randomized study: in order to make the claim that we identify the individual-level effect of the treatment, we must assume that \\(Y_{11}=Y_{1}\\) and \\(Y_{00}=Y_{0}\\).\n\n\n7. What to do about within-cluster spillovers\nIf there are strong reasons to believe that intra-cluster spillovers occur, then researchers can take different approaches depending on the manner in which clusters are formed. In some studies researchers must themselves sort units into groups for the purposes of experimentation: for example, in a study involving a vocational programme, the researcher may be able to decide who is recruited into which class. In such cases, if the researcher can make plausible assumptions about spillovers, then the individual-level treatment effect may be recoverable.\nConsider a researcher conducting the previous study above who correctly assumed that spillovers would occur between mismatched pairs. In this case, the researcher can recover the true individual treatment effect by forming clusters that are not susceptible to spillover.\n\n\n\n[Click to show code]\n\n\n\nCode\nform_matched_pairs &lt;- function(ID,types){\n  pair_list &lt;- lapply(unique(types),function(type){\n    ID &lt;- ID[types == type]\n    types &lt;- types[types == type]\n    draw_ID &lt;- 1:length(types)\n    matched_pairs &lt;- form_pairs(ID = draw_ID,types = types)\n    matched_pairs$i &lt;- ID[matched_pairs$i]\n    matched_pairs$j &lt;- ID[matched_pairs$j]\n    matched_pairs$k &lt;- paste0(type,\"_\",matched_pairs$k)\n    return(matched_pairs)\n  })\n  data &lt;- rbind(pair_list[[1]],pair_list[[2]])\n  return(data)\n}\nsimulate_matched_exp &lt;- function(){\n  data &lt;- form_matched_pairs(ID,types)\n  spillover &lt;- spillover_or_not(data$type_i,data$type_j)\n  estimate &lt;- assign_reveal_est(k = data$k,effect = true_ATE,spillover = spillover,i = data$i)\n  return(estimate)\n}\n# Estimate the effects one thousand times\nest_matched_effects &lt;- replicate(n = 1000,expr = simulate_matched_exp())\nhist(est_matched_effects,breaks = 100,xlim = c(-7,7))\nabline(v = true_ATE,col = \"red\")\nabline(v = mean(est_matched_effects,na.rm = T),col = \"blue\")\n\n\n\n\n\nIn the case that researchers are not able to control how clusters are formed, they can still investigate cluster-level heterogeneity in treatment effects as a way of understanding possible spillovers. However, in both cases, assumptions must be made about the nature of spillovers. Strictly speaking, these cannot be causally identified due to the unobservability of the outcomes \\(Y_{01}\\) and \\(Y_{10}\\). Ultimately, one would need to combine clustered and non-clustered randomization schemes in order to estimate the effects of intra-cluster spillover, \\(Y_{11} - Y_{01}\\) and \\(Y_{01} - Y_{00}\\). Therefore, in the interests of interpreting results correctly, researchers should be careful when defining their estimand to take account of the potential for intra-cluster spillover.\n\n\n8. Performance of design- vs. model-based analyses of clustered studies\nIn our discussion of information loss, we assessed approaches that require (1) that treatment was randomized as planned and (2) that the treatment assigned to one unit did not change the potential outcomes for any other unit. In cases where these assumptions may be violated, it is sometimes simpler to specify statistical models that attempt to describe the features of complex designs. Even if we do not believe the models as scientific descriptions of a known process, this can be a more informative and flexible way of analyzing an experiment than to derive complex new expressions for design-based estimators.\nIn model-based approaches, the sampling distribution of an estimator is approximated using probability distributions to characterize our uncertainty about unknown quantities, such as the true treatment effect or the true mean of the outcome at the cluster level. Such approaches are referred to as ‘model-based’, because they depict causal relationships as arising from interrelated probability distributions. Often, such approaches use ‘multilevel models’, in which unknown parameters — such as differences between clusters — are themselves understood as arising from probability distributions. Thus, for example, there might be a model for individual-level outcomes, whose intercept and/or coefficients vary from one cluster to another. In this manner, it is possible to model the ‘effect of being a unit in cluster A’, separately from the estimation of the treatment effect. The advantage of such approaches is that they allow for ‘partial pooling’ of the variance in the population and the variance among clusters. When a given cluster is poorly estimated, it contributes less weight to the estimation, and vice versa. Such models therefore often work well in situations where there is very little data in some clusters: through the specification of a Bayesian posterior distribution, they are able to leverage information from all parts of the study. The trade-off is that such assumption-heavy models are only correct to the extent that the assumptions underlying them are correct.\nHere we show that the estimated effect is the same whether we use a simple difference of means (via OLS) or a multilevel model in our very simplified cluster randomized trial setup.\n\n\n\n[Click to show code]\n\n\n\nCode\nlibrary(lme4)\nsimple_OLS &lt;- lm(Y ~ Z,data = J_30_clusters)\nmultilevel &lt;- lmer(Y ~ Z + (1 | j),\n                   data = J_30_clusters,\n  control = lmerControl(optimizer = \"bobyqa\"),\n  REML = TRUE\n)\nkable(round(data.frame(\n  OLS=coef(simple_OLS)[\"Z\"],Multilevel=fixef(multilevel)[\"Z\"]\n  ),3))\n\n\n\n\n\n\nOLS\nMultilevel\n\n\n\n\nZ\n-0.13\n-0.13\n\n\n\n\n\nThe confidence intervals differ even though the estimates are the same — and there is more than one way to calculate confidence intervals and hypothesis tests for multilevel models. The software in R (Bates et al. 2015) includes three methods by default and Gelman and Hill (2006) recommend MCMC sampling from the implied posterior. Here we focus on the Wald method only because it is the fastest to compute.\n\n\n\n[Click to show code]\n\n\n\nCode\n# This function calculates confidence intervals for linear models\n# with custom variance-covariance matrices\nconfint_HC&lt;-function (coefficients, df, level = 0.95, vcov_mat, ...) {\n  a &lt;- (1 - level)/2\n  a &lt;- c(a, 1 - a)\n  fac &lt;- qt(a, df)\n  ses &lt;- sqrt(diag(vcov_mat))\n  coefficients + ses %o% fac\n}\nsimple_OLS_CI &lt;-\n  confint_HC(\n    coefficients = coef(simple_OLS),\n    vcov_mat = RCSE(\n      model = simple_OLS,\n                             cluster = J_30_clusters$j,\n      return_cov = TRUE\n    ),\n    df = simple_OLS$df.residual\n  )[\"Z\", ]\nmulti_wald_CI &lt;- lme4::confint.merMod(\n  multilevel,\n  parm = \"Z\", method = \"Wald\"\n)[\"Z\", ]\nmulti_profile_CI &lt;- lme4::confint.merMod(\n  multilevel,\n  parm = 4, method = \"profile\"\n)[\"Z\", ]\nknitr::kable(round(rbind(\n  Design_Based_CI = simple_OLS_CI,\n  Model_Based_Wald_CI = multi_wald_CI,\n  Model_Based_Profile_CI = multi_profile_CI\n),3))\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\nDesign_Based_CI\n-0.416\n0.156\n\n\nModel_Based_Wald_CI\n-0.421\n0.161\n\n\nModel_Based_Profile_CI\n-0.420\n0.161\n\n\n\n\n\nWe can calculate an estimate of the ICC directly from the model quantities (the variance of the Normal prior that represents the cluster-to-cluster differences in the intercept over the total variance of the Normal posterior).\n\n\n\n[Click to show code]\n\n\n\nCode\nVC &lt;- as.data.frame(lme4:::VarCorr.merMod(multilevel))\nround(c(ICC = VC$vcov[1] / sum(VC$vcov)),2)\n\n\n ICC \n0.09 \n\n\nIn order to assess the performance of this model-based approach, as opposed to the robust-clustered standard-error (RCSE) and cluster-aggregated approaches outlined above, we can check how often the different approaches falsely reject the sharp null hypothesis of no effects for any unit, when we know that this null is true.\nTo do so, we write a function that firstly breaks the relationship between the treatment assignment and the outcome by randomly shuffling the assignment, and then tests whether 0 is in the 95% confidence interval for each of the three approaches, as it should be. Recall that, valid tests would have error rates within 2 simulation standard errors of .95 - this would mean that a correct null hypothesis would be rejected no more than 5% of the time.\n\n\n\n[Click to show code]\n\n\n\nCode\n# Make a function for checking whether 0 is in the confidence interval of\n# the RCSE, cluster-aggregated, and multilevel estimation approaches\nsim_0_ate &lt;- function(J,Y) {\n  #   Make the true relationship between treatment and outcomes equal zero by\n  #   shuffling Z but not revealing new potential outcomes\n  z.sim &lt;- sample(1:max(J), max(J) / 2)\n  Z_new &lt;- ifelse(J %in% z.sim == TRUE, 1, 0)\n  # Estimate using the linear model for RCSE\n  linear_fit &lt;- lm(Y ~ Z_new)\n  linear_RCSE &lt;- RCSE(\n    model = linear_fit,\n                  cluster = J,\n    return_cov = TRUE\n  )\n  linear_CI &lt;- confint_HC(\n    coefficients = coef(linear_fit),\n                      vcov_mat = linear_RCSE,\n    df = linear_fit$df.residual\n  )[\"Z_new\", ]\n  # Check if the confidence interval bounds 0\n  zero_in_CI_RCSE &lt;- (0 &gt;= linear_CI[1]) & (0 &lt;= linear_CI[2])\n  # Estimate using cluster-aggregated approach (Hansen and Bowers 2008)\n  Yj &lt;- tapply(Y, J, sum)\n  Zj &lt;- tapply(Z_new, J, mean)\n  m0 &lt;- unique(table(J))\n  n  &lt;- length(Zj)\n  nt &lt;- sum(Zj)\n  # Do Hansen and Bowers 2008 based test for difference of means\n  # with cluster-level assignment (assuming same size clusters)\n  ones &lt;- rep(1, length(Yj))\n  dp   &lt;- crossprod(Zj,Yj) * (n / (m0 * nt * (n - nt))) -\n    crossprod(ones,Yj) / (m0 * (n - nt))\n  obs_ATE &lt;- dp[1,1]\n  # Two tailed p-value for the test of the null of no effects\n  Vdp &lt;- (n / (m0 * nt * (n - nt))) * (var(Yj) / m0)\n  HB_pval &lt;- 2 * (1 - pnorm(abs(obs_ATE) / sqrt(Vdp)))\n  # Check if the p-value is greater than .05\n  zero_not_rej_HB &lt;- HB_pval &gt;= .05\n  # Estimate using a multilevel model\n  multilevel_fit &lt;- lmer(Y ~ Z_new + (1 | J),\n    control = lmerControl(optimizer = \"bobyqa\"),\n    REML = FALSE\n  )\n  multilevel_CI &lt;- lme4:::confint.merMod(\n    multilevel_fit,\n    parm = \"Z_new\", method = \"Wald\"\n  )\n  # Check if the confidence interval bounds 0\n  zero_in_CI_multilevel &lt;- (0 &gt;= multilevel_CI[1]) & (0 &lt;= multilevel_CI[2])\n  return(\n    c(\n      ATE = fixef(multilevel_fit)[\"Z_new\"],\n      zero_in_CI_RCSE = zero_in_CI_RCSE,\n      zero_not_rej_HB = zero_not_rej_HB,\n      zero_in_CI_multilevel = zero_in_CI_multilevel\n    )\n  )\n}\n# Now simulate each of the estimates 1000 times\nJ_4_comparison &lt;- replicate(1000, sim_0_ate(J = J_4_clusters$j, Y = J_4_clusters$Y))\nJ_4_error_rates &lt;- apply(J_4_comparison,1,mean)\nJ_4_error_rates[-1] &lt;- 1-J_4_error_rates[-1]\nJ_10_comparison &lt;- replicate(1000, sim_0_ate(J = J_10_clusters$j, Y = J_10_clusters$Y))\nJ_10_error_rates &lt;- apply(J_10_comparison,1,mean)\nJ_10_error_rates[-1] &lt;- 1-J_10_error_rates[-1]\nJ_30_comparison &lt;- replicate(1000, sim_0_ate(J = J_30_clusters$j, Y = J_30_clusters$Y))\nJ_30_error_rates &lt;- apply(J_30_comparison,1,mean)\nJ_30_error_rates[-1] &lt;- 1-J_30_error_rates[-1]\nJ_100_comparison &lt;- replicate(1000, sim_0_ate(J = J_100_clusters$j, Y = J_100_clusters$Y))\nJ_100_error_rates &lt;- apply(J_100_comparison,1,mean)\nJ_100_error_rates[-1] &lt;- 1-J_100_error_rates[-1]\nerror_comparison &lt;- data.frame(round(rbind(\n  J_4_error_rates,\n  J_10_error_rates,\n  J_30_error_rates,\n  J_100_error_rates\n),3))\ncolnames(error_comparison) &lt;- c(\n  \"Estimated ATE\",\n                                \"OLS + RCSE\",\n                                \"Cluster-Level\",\n  \"Multi-Level\"\n)\nkable(error_comparison,align = \"c\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimated ATE\nOLS + RCSE\nCluster-Level\nMulti-Level\n\n\n\n\nJ_4_error_rates\n0.002\n0.000\n0.000\n0.334\n\n\nJ_10_error_rates\n0.005\n0.101\n0.042\n0.101\n\n\nJ_30_error_rates\n0.003\n0.061\n0.040\n0.063\n\n\nJ_100_error_rates\n-0.001\n0.058\n0.052\n0.059\n\n\n\n\n\nIn our simple setup, the individual-level approaches behave about the same way: neither the design-based nor the model-based approach produces valid statistical inferences until the number of clusters is at least 30. This makes sense: both approaches rely on central limit theorems so that a Normal law can describe the distribution of the test statistic under the null hypothesis. The cluster-level approach is always valid, but sometimes produces overly large confidence intervals (when the number of clusters is small). When the number of clusters is large (say, 100), then all approaches are equivalent in terms of their error rates. Designs with few clusters should consider either the cluster-level approach using the normal approximation shown here or even direct permutation based approaches to statistical inference.\n\n\n9. Power analysis for clustered designs\nWe want designs that are likely to reject hypotheses inconsistent with the data, and unlikely to reject hypotheses consistent with the data. We’ve seen that the assumptions required for the validity of common tests (typically, large numbers of observations, or large quantities of information in general) are challenged by clustered designs, and the tests which account for clustering can be invalid if the number of clusters is small (or information is low at the cluster level in general). We’ve also seen that we can produce valid statistical tests for hypotheses about the average treatment effect using either Robust Clustered Standard Errors (RCSE), multilevel models or using the cluster-level approach described by Hansen and Bowers (2008), and that pair-matching can drastically minimize bias in designs with unequal cluster sizes.\nThe most important rule regarding the statistical power of clustered designs is that more small clusters are better than fewer larger ones. This can be demonstrated through simulated experiments. Generally speaking, the most flexible way to evaluate the power of a design is through simulation, as it allows for complex clustering and blocking schemes, and can incorporate covariates. In the following we use the OLS estimator with Robust Clustered Standard Errors, in order to save on computation time, but the same analysis can be achieved using any estimator and test statistic.\n\n\n\n[Click to show code]\n\n\n\nCode\n# A function to test the null hypothesis and the true hypothesis\ntest_H0_and_Htrue &lt;- function(J = J,n = n,treatment_effect = treatment_effect,ICC = ICC) {\n  # Make data:\n  data &lt;- make_clustered_data(\n    J = J,\n                      n = n,\n                      treatment_effect = treatment_effect,\n    ICC = ICC\n  )\n  linear_fit &lt;- lm(Y ~ Z,data = data)\n  RCSE_CI &lt;- confint_HC(\n    coefficients = coef(linear_fit),\n    vcov_mat = RCSE(\n      model = linear_fit,\n                             cluster = data$j,\n      return_cov = TRUE\n    ),\n    df = linear_fit$df.residual\n  )[\"Z\", ]\n  # Zero should not be in this CI very often as the null of 0 is false here\n  correct_reject &lt;- !((0 &gt;= RCSE_CI[1]) & (0 &lt;= RCSE_CI[2]))\n  # Test null of true taus (first attempt is use true, second is to make 0 true)\n  # Reassign village level treatment so that Y is indep of Z --- so true effect is 0\n  data$Z_new &lt;- ifelse(data$j %in% sample(1:J, max(J)/2), 1, 0)\n  linear_fit_true &lt;-lm(Y ~ Z_new,data = data)\n  RCSE_CI_true &lt;- confint_HC(\n    coefficients = coef(linear_fit_true),\n    vcov_mat = RCSE(\n      model = linear_fit_true,\n                                             cluster = data$j,\n      return_cov = TRUE\n    ),\n    df = linear_fit$df.residual\n  )[\"Z_new\", ]\n  # Zero should be in this CI very often as the null of 0 is true here\n  false_positive &lt;-  !((0 &gt;= RCSE_CI_true[1]) & (0 &lt;= RCSE_CI_true[2]))\n  return(c(\n    correct_reject = correct_reject,\n    false_positive = false_positive\n  ))\n}\n\n\nWe can now analyze how power is affected when the number of clusters and the size of clusters vary, holding the ICC constant at .01 and the treatment effect constant at .2. We look both at the power — how often we correctly reject the null of no effect when there really is one - as well as at the error — how often we incorrectly reject the null of no effect when there really isn’t an effect. Typically we want the power to be around .8 and the error rate to be around .5 (when using a 95% confidence level).\n\n\n\n[Click to show code]\n\n\n\nCode\n# The numbers of clusters we will consider\nJs &lt;- c(8,20,40,80,160,320)\n# The cluster sizes we will consider\nn_js &lt;- c(8,20,40,80,160,320)\n# Create an empty matrix to store the results\n# The first stores the power and the second stores the error\npower_J_n &lt;- error_J_n &lt;- matrix(\n  data = NA,\n  nrow = length(Js),\n  ncol = length(n_js),\n  dimnames = list(\n    paste(\"J =\",Js),\n    paste(\"n_j =\",n_js)\n  )\n)\n# Set the number of simulations\nsims &lt;- 100\n# Loop through the different cluster numbers\nfor( j in 1:length(Js)){\n  # Loop through the different cluster sizes\n  for(n in 1:length(n_js)){\n    # Estimate the power and error rate for each combination\n    test_sims &lt;- replicate(\n      n = sims,\n      expr = test_H0_and_Htrue(\n        J = Js[j],\n                                       n = n_js[n],\n                                       treatment_effect = .25,\n        ICC = .01\n      )\n    )\n    power_error &lt;- rowMeans(test_sims)\n    # Store them in the matrices\n    power_J_n[j,n] &lt;- power_error[1]\n    error_J_n[j,n] &lt;- power_error[2]\n  }\n}\n# Plot the power under the different scenarios\nmatplot(power_J_n, type = c(\"b\"),pch=1,axes = F,ylim = c(0,1.5),ylab = \"power\")\naxis(side = 1,labels = rownames(power_J_n),at = 1:6)\naxis(side = 2,at = seq(0,1,.25))\nabline(h=.8)\nlegend(\"top\", legend = colnames(power_J_n),col = 1:6 ,pch=1,horiz = TRUE)\n\n\n\n\n\n\n\n\n[Click to show code]\n\n\n\nCode\n# Plot the error rate under the different scenarios\nmatplot(error_J_n, type = c(\"b\"),pch=1,axes = F,ylim = c(0,.5),ylab = \"error rate\")\naxis(side = 1,labels = rownames(error_J_n),at = 1:6)\naxis(side = 2,at = seq(0,1,.25))\nabline(h=.05)\nlegend(\"top\", legend = colnames(error_J_n),col = 1:6 ,pch=1,horiz = TRUE)\n\n\n\n\n\nWe see that power is always low when the number of clusters is low, regardless of how large the clusters are. Even with huge clusters (with 320 units each), the statistical power of the study is still relatively low when the number of clusters is 8. Similarly, it takes a large number of clusters to power a study with small clusters: although it is sufficient to have many clusters in order to power an experiment, irrespective of cluster size, power increases much more rapidly when the clusters are larger. Note also that while error rates appear systematically related to the number of clusters, the same is not true for cluster sizes.\nNext, we can evaluate how the intra-cluster correlation affects power. We will hold the structure of the sample size constant at \\(J=80,n_j=80\\) and \\(J=160,n_j=160\\), and compare across a range of ICC values, from low (.01) to high (.6).\n\n\n\n[Click to show code]\n\n\n\nCode\nJ_njs &lt;- c(80,160)\nICCs &lt;- seq(0,.6,.1)+c(.01,0,0,0,0,0,0)\npower_ICC &lt;- error_ICC &lt;- matrix(\n  data = NA,\n  nrow = length(ICCs),\n  ncol = length(J_njs),\n  dimnames = list(\n    paste(\"ICC =\",ICCs),\n    paste(\"J =\",J_njs,\"n_j = \",J_njs)\n  )\n)\n# Set the number of simulations\nsims &lt;- 100\n# Loop through the different ICCs\nfor( i in 1:length(ICCs)){\n  # Loop through the different cluster sizes\n  for(j in 1:length(J_njs)){\n    # Estimate the power and error rate for each combination\n    test_sims &lt;- replicate(\n      n = sims,\n      expr = test_H0_and_Htrue(\n        J = J_njs[j],\n                                       n = J_njs[j],\n                                       treatment_effect = .25,\n        ICC = ICCs[i]\n      )\n    )\n    power_error &lt;- rowMeans(test_sims)\n    # Store them in the matrices\n    power_ICC[i,j] &lt;- power_error[1]\n    error_ICC[i,j] &lt;- power_error[2]\n  }\n}\n# Plot the power under the different scenarios\nmatplot(power_ICC, type = c(\"b\"),pch=1,axes = F,ylim = c(0,1.5),ylab = \"power (high ICC)\")\naxis(side = 1,labels = rownames(power_ICC),at = 1:7)\naxis(side = 2,at = seq(0,1,.25))\nabline(h=.8)\nlegend(\"top\", legend = colnames(power_ICC),col = 1:2 ,pch=1,horiz = TRUE)\n\n\n\n\n\n\n\n\n[Click to show code]\n\n\n\nCode\n# Plot the error rate under the different scenarios\nmatplot(error_ICC, type = c(\"b\"),pch=1,axes = F,ylim = c(0,.5),ylab = \"error rate\")\naxis(side = 1,labels = rownames(error_ICC),at = 1:7)\naxis(side = 2,at = seq(0,1,.25))\nabline(h=.05)\nlegend(\"top\", legend = colnames(error_ICC),col = 1:2 ,pch=1,horiz = TRUE)\n\n\n\n\n\nAs this example illustrates, high ICC can severely diminish the statistical power of the study, even with many large clusters.\n\n\n10. How to check balance in clustered designs\nRandomization checks in clustered designs follow the same form as the preceding discussion. A valid test for a treatment effect is a valid test for placebo or covariate balance. The only difference from our preceding discussion is that one uses a background covariate or baseline outcome — some variable putatively uninfluenced by the treatment — in place of the outcome itself. So, randomization tests with small numbers of clusters may be too quick to declare an experiment ill-randomized if the analyst is not aware of the methods of error-rate analysis that we described above.\nOne new problem does arise in the context of randomization tests. Often one has many covariates which could be used to detect unlucky imbalances or field problems with the randomization itself. And, if one uses hypothesis tests, then, of course, a valid test which encourages us to declare “imbalance” when \\(p&lt;.05\\) would do so falsely for one in every twenty variables tested. For this reason, we recommend using one-by-one testing as an exploratory tool and using omnibus tests (like the Hotelling T-test or an F-test or the Hansen and Bowers (2008) \\(d^2\\) test), which can combine information across many dependent tests into one test statistic to make balance tests directly. However, these tests must account for the clustered nature of the design: a simple F-test without accounting for the clustered-design will likely mislead an analyst into declaring a design unbalanced and perhaps charging the field staff with a randomization failure.\nSince cluster randomized experiments tend to have cluster-level covariates (say, village size, etc..) balance checks at the cluster level make sense and do not require explicit changes to account for clustered-assignment. Hansen and Bowers (2008) develop such a test and provide software to implement it. So, for example, if we had 10 covariates measured at the village level, and we had a large number of villages we could assess an omnibus balance hypothesis using this design-based but large-sample tool.\nHere we show only the omnibus test results. The one-by-one assessments that make up the omnibus test are also available in the balance_test object. Here, the omnibus test tell us that we have little information against the null that these observations arose from a randomized study.\n\n\n\n[Click to show code]\n\n\n\nCode\nlibrary(RItools)\n\n\nLoading required package: SparseM\n\n\n\nAttaching package: 'SparseM'\n\n\nThe following object is masked from 'package:base':\n\n    backsolve\n\n\nLoading required package: dgof\n\n\n\nAttaching package: 'dgof'\n\n\nThe following object is masked from 'package:stats':\n\n    ks.test\n\n\nWarning: replacing previous import 'stats::ks.test' by 'dgof::ks.test' when\nloading 'RItools'\n\n\nCode\noptions(digits=3)\n# Make a village level dataset\nvillages &lt;- aggregate(pretend_data,by = list(village = pretend_data$j), mean)\n# Generate 10 fake covariates\nset.seed(12345)\nvillages[paste(\"x\",1:10,sep=\"\")] &lt;- matrix(rnorm(nrow(villages)*10), ncol=10)\nbalance_formula &lt;- reformulate(paste(\"x\",1:10,sep=\"\"), response=\"Z\")\n# Do a design-based, large sample balance test\nbalance_test &lt;-xBalance(balance_formula,\n                        strata=list(noblocks=NULL),\n                        data=villages,\n  report = c(\n    \"std.diffs\", \"z.scores\", \"adj.means\",\n    \"adj.mean.diffs\", \"chisquare.test\", \"p.values\"\n  )\n)\n# The results of the 1-by-1 balance tests\nkable(round(balance_test$results,2),align = \"c\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nControl.noblocks\nTreatment.noblocks\nadj.diff.noblocks\nstd.diff.noblocks\nz.noblocks\np.noblocks\n\n\n\n\nx1\n-0.24\n-0.02\n0.22\n0.26\n0.43\n0.67\n\n\nx2\n0.68\n-0.11\n-0.78\n-1.01\n-1.47\n0.14\n\n\nx3\n0.37\n-0.20\n-0.57\n-0.47\n-0.77\n0.44\n\n\nx4\n1.15\n0.30\n-0.86\n-0.71\n-1.11\n0.27\n\n\nx5\n-0.16\n0.04\n0.20\n0.14\n0.24\n0.81\n\n\nx6\n1.08\n-0.54\n-1.62\n-1.55\n-1.97\n0.05\n\n\nx7\n-0.04\n0.08\n0.12\n0.09\n0.15\n0.88\n\n\nx8\n0.76\n0.12\n-0.63\n-0.58\n-0.92\n0.36\n\n\nx9\n1.32\n0.37\n-0.95\n-1.30\n-1.76\n0.08\n\n\nx10\n-0.33\n0.28\n0.61\n0.51\n0.82\n0.41\n\n\n\n\n\n\n\n\n[Click to show code]\n\n\n\nCode\n# The overall omnibus p-value\nkable(round(balance_test$overall,2),align = \"c\")\n\n\n\n\n\n\nchisquare\ndf\np.value\n\n\n\n\nnoblocks\n9\n9\n0.44\n\n\n\n\n\nIn this case, we cannot reject the omnibus hypotheses of balance even though, as we expected, we have a few covariates with falsely low \\(p\\)-values. One way to interpret this omnibus result is to say that such imbalances on a few covariates would not appreciably change any statistical inferences we make about treatment effects as long as these covariates did not strongly predict outcomes in the control group. Alternatively, we could say that any large experiment can tolerant chance imbalance on a few covariates (no more than roughly 5% if we are using \\(\\alpha=.05\\) as our threshold to reject hypotheses).\n\n\n\n\n\n\n\n\nReferences\n\nArceneaux, Kevin, and David W Nickerson. 2009. “Modeling Certainty with Clustered Data: A Comparison of Methods.” Political Analysis 17 (2): 177–90.\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical Software 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge university press.\n\n\nHansen, Ben B, and Jake Bowers. 2008. “Covariate Balance in Simple, Stratified and Clustered Comparative Studies.” Statistical Science, 219–36.\n\n\nImai, Kosuke, Gary King, and Clayton Nall. 2009. “The Essential Role of Pair Matching in Cluster-Randomized Experiments, with Application to the Mexican Universal Health Insurance Evaluation.” Statistical Science 24 (1): 29–53.\n\n\nKish, Leslie. 1965. Survey Sampling. New York: John Wiley & Sons."
  },
  {
    "objectID": "guides/planning/pap_en.html",
    "href": "guides/planning/pap_en.html",
    "title": "10 Things to Know About Pre-Analysis Plans",
    "section": "",
    "text": "This guide introduces the idea of a pre-analysis plan (PAP), offers a model and guiding questions for writing pre-analysis plans for your studies, and explains the uses of a pre-analysis plan. Links to PAP registries with example plans are provided at the end of this document."
  },
  {
    "objectID": "guides/planning/pap_en.html#footnotes",
    "href": "guides/planning/pap_en.html#footnotes",
    "title": "10 Things to Know About Pre-Analysis Plans",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPAPs are encouraged as part of the Transparency and Openness Promotion (TOP) Guidelines (Nosek et al. 2015), published in Science, with leading social science journals committing to implementing TOP Guidelines.↩︎\nResources like the DeclareDesign project can assist you with simulating and analyzing fake data that mimics the real data your project will gather. Power analysis, an important component of a PAP, requires simulated data.↩︎\nAs R.A. Fisher said and has often been requoted, “Make your theories elaborate…when constructing a causal hypothesis one should envisage as many different consequences of the truth as possible,” (Cochran (1965); cited in Rosenbaum (2010), pp. 327). Though this was said about determining causation in observational studies, the logic also applies to experimental studies.↩︎\nFor an example of an omnibus test, see Caughey, Dafoe, and Seawright (2017).↩︎\nYou may also be interested in estimating other types of effects. See this guide on types of treatment effects for more information about effect types.↩︎\nFor example, you could calculate standard errors and \\(p\\)-values using permutation-based randomization inference. Or you could closely approximate standard errors and \\(p\\)-values using analytic methods as in Samii and Aronow (2012).↩︎\nNote that some organizations do not use third party sites. For example, the U.S. General Services Administration Office of Evaluation Sciences process uses Github, which has timestamps that verify the PAP was created before the analysis was conducted.↩︎"
  },
  {
    "objectID": "guides/planning/survey-design_en.html",
    "href": "guides/planning/survey-design_en.html",
    "title": "10 Things to Know About Survey Design",
    "section": "",
    "text": "Surveys are the most frequently-used tool for collecting experimental data in social science research, and the design of these surveys can have a profound effect on the conclusions we draw about the treatments we study. Therefore, the stakes are high when designing surveys around your experimental projects.\nAt a minimum, all that is needed to estimate a treatment effect after an experiment has been conducted is a measure of the outcome, collected after the treatment has been delivered, with a sufficient number of observations across the treatment and control groups. If time and budget allow, baseline surveys, conducted before the implementation of an experiment, serve important functions as well and can improve analysis greatly.\n\n\nBaseline surveys should produce data that describe the experimental subject population as they were before the treatment is delivered. This is accomplished through the measurement of covariates, which are observed pre-treatment characteristics of experimental subjects. Using covariate data you can: 1) describe the subject population, 2) improve the precision with which you estimate treatment effects, 3) report balance, and 4) estimate heterogeneous treatment effects.\n\n\n\nCovariates improve the precision with which you can estimate treatment effects by reducing variance in three ways; covariates can be used to rescale your dependent variable, as controls when using regression to estimate treatment effects, and to construct blocks in order to conduct blocked random assignment.1 In order for covariate data to be used to reduce variance in our estimates of treatment effects, they need to be unaffected by treatment assignment, i.e. collected sometime before treatment is delivered. See the guide on covariate adjustment for more about how to use covariate data.\nThe greater the predictive power of included covariates, the greater increase in the power of your design and the precision with which you can estimate effects. If you believe covariates will likely predict outcomes in your experiment, then that is grounds to include them in your survey. For example, if the intervention involves providing a service at a cost to treated users, income will likely explain some variation in outcomes and is therefore a useful covariate to measure at the baseline stage.\nBecause pre-treatment covariates can improve precision, conducting a baseline becomes more important when the sample size is limited.\nCovariates also allow you to conduct sub-group analyses. Heterogeneous effects are not causal, and so interpretation is limited. Still, understanding how treatment effects vary by subejcts’ attributes can provide you with important clues about mechanisms. The implication for design is to include covariates for which you would like to report heterogeneous effects. Covariate data will also be used to show “balance”, or the extent to which the treatment and control populations resemble each other. Although random assignment alone ensures that outcomes across the treatment and control group are in expectation the same, it is standard practice to show that random assignment resulted in two groups that are “balanced” on covariates of interest. If, for example, the treatment group included 25% more men than the control group, we might worry that random assignment failed in some way. Collecting pre-treatment covariate data allows us to evaluate and report balance.\n\n\n\nThe baseline provides an opportunity to measure the outcome before the experiment was conducted, later allowing you to use change scores as your outcome and the difference-in-differences estimator. The difference-in-differences estimator will improve precision only when a covariate strongly predicts outcomes.2\n\n\n\nEndline surveys, conducted after the treatment is delivered, are primarily used to measure outcomes. Including questions about implementation can improve analysis and interpretation greatly.\nSurveys conducted after treatments are delivered are one way to understand if there were compliance issues or other implementation issues that may have consequences for analysis. Survey data can help to determine the scope of non-compliance with the assigned treatment, and the underlying causes. This is an opportunity to ask subjects directly about reasons for noncompliance. Subjects may have understood the treatment differently than the researchers, and survey data can be used to both show this and speculate why this may have happened.\nIn the endline, you can learn about spillover by asking subjects in the control condition about their knowledge and access to the treatment. Interviews with treated subjects are useful for understanding spillover as well, because survey data can be used to understand the networks through which the treatment could have “spilled-over” into the control group.\nDescribing the population is important here as well, but covariate data collected after implementation are less useful for improving precision. Ordinarily, covariates collected after treatment assignment are considered suspect, as they could conceivably be affected by treatment.\n\n\n\n\n\n\n\nBaseline checklist\nEndline checklist\n\n\n\n\nWill your data allow you to:\nWill your data allow you to:\n\n\n• Describe the population\n• Estimate effects\n\n\n• Adjust treatment effect estimates (are the covariates included likely to be prognostic of outcomes?)\n• Assess if spillover occurred, or if there was interference\n\n\n• Estimate heterogeneous treatment effects\n• Measure non-compliance (and the reasons for non-compliance if it occurred)\n\n\n• Design a blocked randomization procedure\n• Look for causal mechanisms (how are effects transmitted?)\n\n\n• Describe balance across treatment and control conditions"
  },
  {
    "objectID": "guides/planning/survey-design_en.html#baseline-surveys",
    "href": "guides/planning/survey-design_en.html#baseline-surveys",
    "title": "10 Things to Know About Survey Design",
    "section": "",
    "text": "Baseline surveys should produce data that describe the experimental subject population as they were before the treatment is delivered. This is accomplished through the measurement of covariates, which are observed pre-treatment characteristics of experimental subjects. Using covariate data you can: 1) describe the subject population, 2) improve the precision with which you estimate treatment effects, 3) report balance, and 4) estimate heterogeneous treatment effects."
  },
  {
    "objectID": "guides/planning/survey-design_en.html#covariates",
    "href": "guides/planning/survey-design_en.html#covariates",
    "title": "10 Things to Know About Survey Design",
    "section": "",
    "text": "Covariates improve the precision with which you can estimate treatment effects by reducing variance in three ways; covariates can be used to rescale your dependent variable, as controls when using regression to estimate treatment effects, and to construct blocks in order to conduct blocked random assignment.1 In order for covariate data to be used to reduce variance in our estimates of treatment effects, they need to be unaffected by treatment assignment, i.e. collected sometime before treatment is delivered. See the guide on covariate adjustment for more about how to use covariate data.\nThe greater the predictive power of included covariates, the greater increase in the power of your design and the precision with which you can estimate effects. If you believe covariates will likely predict outcomes in your experiment, then that is grounds to include them in your survey. For example, if the intervention involves providing a service at a cost to treated users, income will likely explain some variation in outcomes and is therefore a useful covariate to measure at the baseline stage.\nBecause pre-treatment covariates can improve precision, conducting a baseline becomes more important when the sample size is limited.\nCovariates also allow you to conduct sub-group analyses. Heterogeneous effects are not causal, and so interpretation is limited. Still, understanding how treatment effects vary by subejcts’ attributes can provide you with important clues about mechanisms. The implication for design is to include covariates for which you would like to report heterogeneous effects. Covariate data will also be used to show “balance”, or the extent to which the treatment and control populations resemble each other. Although random assignment alone ensures that outcomes across the treatment and control group are in expectation the same, it is standard practice to show that random assignment resulted in two groups that are “balanced” on covariates of interest. If, for example, the treatment group included 25% more men than the control group, we might worry that random assignment failed in some way. Collecting pre-treatment covariate data allows us to evaluate and report balance."
  },
  {
    "objectID": "guides/planning/survey-design_en.html#pre-treatment-measurement-of-outcomes",
    "href": "guides/planning/survey-design_en.html#pre-treatment-measurement-of-outcomes",
    "title": "10 Things to Know About Survey Design",
    "section": "",
    "text": "The baseline provides an opportunity to measure the outcome before the experiment was conducted, later allowing you to use change scores as your outcome and the difference-in-differences estimator. The difference-in-differences estimator will improve precision only when a covariate strongly predicts outcomes.2"
  },
  {
    "objectID": "guides/planning/survey-design_en.html#endline-surveys",
    "href": "guides/planning/survey-design_en.html#endline-surveys",
    "title": "10 Things to Know About Survey Design",
    "section": "",
    "text": "Endline surveys, conducted after the treatment is delivered, are primarily used to measure outcomes. Including questions about implementation can improve analysis and interpretation greatly.\nSurveys conducted after treatments are delivered are one way to understand if there were compliance issues or other implementation issues that may have consequences for analysis. Survey data can help to determine the scope of non-compliance with the assigned treatment, and the underlying causes. This is an opportunity to ask subjects directly about reasons for noncompliance. Subjects may have understood the treatment differently than the researchers, and survey data can be used to both show this and speculate why this may have happened.\nIn the endline, you can learn about spillover by asking subjects in the control condition about their knowledge and access to the treatment. Interviews with treated subjects are useful for understanding spillover as well, because survey data can be used to understand the networks through which the treatment could have “spilled-over” into the control group.\nDescribing the population is important here as well, but covariate data collected after implementation are less useful for improving precision. Ordinarily, covariates collected after treatment assignment are considered suspect, as they could conceivably be affected by treatment.\n\n\n\n\n\n\n\nBaseline checklist\nEndline checklist\n\n\n\n\nWill your data allow you to:\nWill your data allow you to:\n\n\n• Describe the population\n• Estimate effects\n\n\n• Adjust treatment effect estimates (are the covariates included likely to be prognostic of outcomes?)\n• Assess if spillover occurred, or if there was interference\n\n\n• Estimate heterogeneous treatment effects\n• Measure non-compliance (and the reasons for non-compliance if it occurred)\n\n\n• Design a blocked randomization procedure\n• Look for causal mechanisms (how are effects transmitted?)\n\n\n• Describe balance across treatment and control conditions"
  },
  {
    "objectID": "guides/planning/survey-design_en.html#gathering-behavioral-data-doesnt-have-to-be-expensive.-here-is-how-to-develop-low-cost-measures",
    "href": "guides/planning/survey-design_en.html#gathering-behavioral-data-doesnt-have-to-be-expensive.-here-is-how-to-develop-low-cost-measures",
    "title": "10 Things to Know About Survey Design",
    "section": "Gathering behavioral data doesn’t have to be expensive. Here is how to develop low-cost measures:",
    "text": "Gathering behavioral data doesn’t have to be expensive. Here is how to develop low-cost measures:\n\nBrainstorm a set of actions that subjects would do if the treatment had had an effect or would not do in the case that the treatment did not have an effect. It also works to think about behavior on a continuum—i.e., what would people be more likely to do if affected, and less likely to do if not? Local context matters a lot here; rely on your enumerators, local survey staff, or implementation partners to help you think through a set of possibilities. One nice way to think about this is to challenge yourself to think about “hints.” In the above example, we might think about a group of opposition activists wearing wristbands publicly as a “hint” that people are more likely to take risks. Keep in mind your eventual audience: what behaviors are frequently tracked in the literature you hope to speak to?\nIsolate the set of behaviors that are feasible to measure. This will most likely be the set of behaviors that can be immediately observed by the enumerator and involve minimal materials. What is the least expensive or costly action that would be associated with the behavioral change you want to detect?\nIdeally, pre-test the measures either with the rest of your survey, or in smaller focus groups. Learning why respondents did or did not behave a certain way will increase confidence in your results."
  },
  {
    "objectID": "guides/planning/survey-design_en.html#how-do-you-construct-questions-that-accomplish-these-goals",
    "href": "guides/planning/survey-design_en.html#how-do-you-construct-questions-that-accomplish-these-goals",
    "title": "10 Things to Know About Survey Design",
    "section": "How do you construct questions that accomplish these goals?",
    "text": "How do you construct questions that accomplish these goals?\n\nUse the simplest possible form of each question, using the most widely-understood words. Avoid jargon or technical terms, and be straightforward and brief.\nBe specific, such that if the question were to be lifted from the section and asked without context you would get the same response.\nIt helps to begin the question by providing a context. For example, you can prime a time period (“Thinking of the last year: has your income been better, the same, or worse?”), or a place (“Thinking of people in this village: have people earned more or less this year as compared to last year?”)\nAvoid measuring multiple things at once. For example, the following question measures attitudes about both the president and government concurrently, making it difficult to draw a clear conclusion from the data: “Do you think the president and the government are doing a good job in terms of protecting basic freedoms?”\nWhen constructing response categories, be as comprehensive as possible. Include all possible responses. You don’t want to record a lot of “don’t know” responses and miss important information. See below for a discussion of scales.\nKeep in mind the concerns with social desirability discussed above; the wording of the question shouldn’t lead the respondent towards a certain response."
  },
  {
    "objectID": "guides/planning/survey-design_en.html#footnotes",
    "href": "guides/planning/survey-design_en.html#footnotes",
    "title": "10 Things to Know About Survey Design",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Gerber and Green (2012).↩︎\nSee Gerber and Green (2012), equation 4.2.↩︎\nLink to fishing article: http://www.columbia.edu/~mh2245/papers1/PA_2012b.pdf. Link to Casey et al: http://qje.oxfordjournals.org/content/127/4/1755.full.pdf+html↩︎\nIf employing multiple survey measures for the same outcome; pre-commit to how you will analyze them e.g. in an index/with accounting for multiple comparisons etc. to avoid cherry-picking measures post-hoc.↩︎\nSee Young (2018).↩︎\nThis approach involves estimating the upper and lower “bounds,” which are the largest and smallest ATEs we would obtain if the missing information were filled in with the highest and lowest outcomes that appear in the data we have.↩︎"
  },
  {
    "objectID": "guides/planning/pilots_en.html",
    "href": "guides/planning/pilots_en.html",
    "title": "10 Things to Know About Pilot Studies",
    "section": "",
    "text": "1. What is a pilot, and what is it good for?\nDuring the process of planning an experiment, researchers often face questions regarding their study’s theoretical and conceptual underpinnings, its measurement approach, and associated logistics. Pilot studies can help you to consider and improve these elements of your research whether you are running a survey, lab, or field experiment. In particular, a pilot study is a smaller scale preliminary test or trial run, used to assist with the preparation of a more comprehensive investigation. Pilots are typically administered before a research design is finalized in order to evaluate and improve the feasibility, reliability, and validity of the proposed study.\nWhile it may be tempting to think about a pilot as simply a miniature version of one’s final study, helpful for doing an initial test of one’s hypotheses, pilot studies are neither especially appropriate for hypothesis testing, nor are they limited to it. Given smaller sample sizes, pilots are typically underpowered for evaluating hypotheses. Besides, deciding whether to continue a study based on initial results contributes to the “file drawer” problem where important studies and results—including null results—are never published, leading to misrepresentative bodies of published research (Franco, Malhotra, and Simonovits 2014).\nFortunately, as depicted in the table below, pilots can be useful for a wide range of research purposes including theory development, research design, improving measurement, sampling considerations, evaluating logistics, pre-planning analysis, weighing ethical considerations, and communicating one’s research (Teijilngen et al. (2001); Thabane et al. (2010)). Each of these activities can help improve the quality of one’s main study and render it more compelling and ultimately successful.\nIn the sections below, we review these many benefits. First, we consider how pilots can assist with a study’s theory and measurement approach, evaluate its logistical feasibility, and provide information about the sample size needed to test hypotheses. In addition, we discuss how piloting may help you secure research funding and institutional support, gather feedback, and incorporate best practices in research ethics. Finally, we offer recommendations on how to use pilots to inform your main study design, reveal important unknowns, and contribute to your broader research agenda.\n\n\n\n\n\n\n\nTheory Development\nResearch Design\n\n\n\n\nRefining your research questions and hypotheses\nEvaluating suitability of study context for answering research questions\n\n\nDetermining initial plausibility of theorized mechanisms\nDetermining construction and strength of treatments\n\n\nIdentifying additional research questions of interest\nDetermining number, levels, and timing of treatments\n\n\nExploring unknowns related to your research questions and hypotheses\nConsidering alternative and/or multiple measures of participant response to treatment\n\n\n\n\n\n\n\n\n\n\nMeasurement\nSample\n\n\n\n\nTesting alternative treatments and outcomes to improve clarity, validity, and reliability\nDetermining power and sample size needed for main study\n\n\nUncovering issues related to asymmetry in delivery and measurement of treatments and/or outcomes\nEvaluating recruitment procedures and eligibility criteria\n\n\nConsidering covariate measurement approach\nTesting participant understanding of research tools and procedures\n\n\nEnsuring successful data collection and data entry\nAssessing participant compliance and treatment uptake\n\n\nIdentifying additional information that may need to be collected\nAssessing participant response rate, quality, and attrition\n\n\nConsidering the use of combined measures such as scales and indices\n\n\n\n\n\n\n\n\n\n\n\nLogistics\nAnalysis\n\n\n\n\nDetermining time and resources needed to conduct main study\nConsidering feasibility of software, statistical techniques, or other analytic tools given preliminary data\n\n\nEvaluating randomization procedure\nPreparing analysis procedures for main study\n\n\nAssessing research team understanding of protocols and procedures\nConsidering feasibility of incorporating data from partners or other sources\n\n\nDetermining partner capacity and willingness\nConsidering responses to missingness in data\n\n\nTraining research team members and partners\n\n\n\nAssessing inter-researcher reliability in data collection, procedures, and analysis\n\n\n\n\n\n\n\n\n\n\n\nEthics\nCommunicating Research\n\n\n\n\nEnsuring participant safety and well-being\nCollecting preliminary data to share externally\n\n\nEvaluating consent procedures\nSoliciting feedback on research design\n\n\nAssessing participant time and burdens\nLeveraging pilot results transparently to explain design choices for main study\n\n\nEnsuring normative acceptability of study elements in given context\nDemonstrating research team capacity and competence\n\n\nAssessing privacy and confidentiality procedures\nPersuading funders, ethics committees, and other stakeholders of study feasibility and value\n\n\nAssessing security of data and other study materials\nInforming interested scholars and other stakeholders\n\n\n\nEducating students about research practices\n\n\n\n\n\n2. Pilots are useful for improving your study’s measurement approach in relation to your theory\nFewer things are more frustrating for a researcher than investing significant resources and time into a study only to find that one’s outcome measures lack reliability to accurately assess the concepts of interest, or that participants did not receive treatments in the way that was anticipated. Unexpected results are common, even when one approaches measurement carefully, as operationalizing concepts into an effective measurement strategy in the social sciences is a complex endeavor.\nAs a remedy, pilots are great for testing different versions of treatments and outcomes to see which of them “work” or whether any changes may be needed to increase validity, reliability, and clarity. For example, you can include a larger set of outcome measures in a pilot than is feasible in the main study, and perform simple factor analysis to identify a preferred subset of measures. Pilot results can also inform whether it makes sense to create indices or scales in order to improve reliability and decrease variance. Subtle variation in the strength, nature, timing, or number of treatments can also significantly alter study findings. Pilots offer researchers the opportunity to evaluate multiple possibilities for one’s treatment design, and to assess how these options influence participant compliance, uptake of treatment, attrition, and more.\nWhen refining one’s approach to measuring treatments, outcomes, and covariates, it is especially important to keep in mind how these elements of one’s research design speak to the broader concepts and theory under study. Will the data you receive provide the necessary information regarding the theoretical elements and causal mechanisms under study? Are there other causal channels that may be in play, or heterogeneous effects within subgroups that you hadn’t thought about previously? These kinds of considerations can inform changes to your research design, such as alterations to your randomization strategy, the introduction of new dimensions in your treatments, and decisions about which aspects of your study are core and which can be saved for a later date.\nWhen you begin a pilot study, you may have an initial conception of your research questions, hypotheses, and measurement approach; but with a careful pilot, you have the opportunity to refine all of these aspects in a way that can increase your (and others’) confidence in the overall quality of the study.\n\n\n3. Pilots can also help you to prepare for the logistics of running your experiment\nPerhaps the most often emphasized purpose of pilots is to work out any logistical kinks that might impede the main study (Thabane et al. 2010). Logistical considerations include those implicating a project’s overall resources, the study team, the participants, and the administration of study instruments.\nIn terms of participant logistics, is important to establish whether your study participants understand the research tools and procedures, are able to receive treatment, and feel comfortable answering questions or performing tasks. Based on how successful the pilot is, you may find ways to improve your recruitment strategy, adjust your eligibility criteria, and improve the clarity of your research instruments. It is also important to ensure that basic elements like randomization and data collection are working as anticipated. Data simulations and pilots with very small samples can also be used to test certain study elements.\nSimilar considerations apply to your study team and partners. Do they understand the protocols or might they require additional training, for example, to promote reliability in procedures such as data collection? Are there any asymmetries in delivery or measurement of treatment or outcomes of which you were not aware? A pilot can also be very helpful for determining the resources needed to conduct the main study. For example, how much time does it take—for both members of the study team and participants—and what might this imply for the size of the sample and complexity of the research design that is ultimately feasible for the main study?\nWhile the specific logistical considerations will vary depending on whether your research design is centered around a field experiment, survey experiment, lab experiment, or something else, pilots will help you ensure that your experiment goes as planned. Thus, when constructing a pilot study, consider making a list of the logistical elements you want to evaluate and designing the pilot to facilitate answering associated questions. Asking your participants whether they were able to “hear the video” or “understood the instructions” can make your research easier down the road.\n\n\n4. You can use pilots for power analysis or for calculating minimum detectable effects\nAnother common purpose of pilot studies, in light of limited resources, is assessing statistical power, which helps to avoid the risk of false negatives (or false positives) from underpowered studies. As described in the EGAP methods guide about power analysis, a researcher’s goal is to answer the following: “supposing there truly is a treatment effect and you were to run your experiment a huge number of times, how often will you get a statistically significant result?” To improve the likelihood that your main study will achieve a typical target power value of 80%, you can use a pilot study combined with careful simulations. These results are helpful for determining appropriate sample sizes, the extent to which you can subset your sample for various analyses, and whether adjustments may be necessary for increasing power.\nTraditionally, a pilot study is used to obtain an estimate of the effect size, which becomes the presumed estimate for simulations used to determine power and sample size in one’s main study. However, the DeclareDesign cautions that effect size estimates are very noisy in small pilots, especially when true effect sizes are small (for example, under 0.2 sd). As an alternative, they recommend using pilot studies to estimate the standard deviation of the outcome variable. Using this estimate, one can easily obtain an unbiased estimate of a main study’s minimum detectable effect (MDE) for a given outcome and with 80% power as 2.8 times the estimated standard error of the associated outcome variable (Gelman and Hill 2006). The goal is to ensure that the MDE is small enough that the study would capture any substantively meaningful effect.\nUsing the recommendations from DeclareDesign, we provide sample code based on a hypothetical pilot study. In the code below, we assume we have already conducted a pilot study and have calculated the standard deviation of a key outcome measure for both the control and treatment groups. Next, we use these estimates to calculate MDEs for different possible sample sizes, in order to inform our target sample size for a future main study.\n\nrequire(tidyverse)\n#Standard deviations of outcome measure for treatment & control groups calculated from pilot study results\n#The ones provided are for purposes of demonstration\n#We assume one treatment and control group--you will need to adjust to study specifics\nsd_control &lt;- 1.5\nsd_treat &lt;- 1.2\n#MDE calculation for various possible sample sizes\nN &lt;- seq(from=500, to=3000, by=100)\nMDE &lt;- vector()\nfor (i in 1:length(N)){\n  #We assume an equal number of participants in treatment and control--you will need to adjust to study specifics\n  MDE[i] &lt;- 2.8 * sqrt((sd_control^2/(N[i]/2)) + (sd_treat^2/(N[i]/2))) #The true effect size must be at least 2.8 standard errors from zero to detect it with 80% probability using 95% confidence intervals (Gelman and Hill 2006). To estimate the standard error of the ATE, we use equation 3.6 in @gerber_green_2012. Thus, simply multiply 2.8 by the standard error of the ATE to calculate the MDE. \n}\n#Visualizing results to identify target sample size for main study\nmde_data &lt;- as.data.frame(cbind(N, MDE))\nmde_plot &lt;- ggplot(mde_data, aes(x=N, y=MDE)) + geom_point() +\n  xlab(\"Sample Size\") + ylab(\"MDE\") + ggtitle(\"Minimum Detectable Effect (MDE) for Main Study by Sample Size\") + \n  theme_bw() +\n  geom_hline(yintercept = .2, lty = \"dashed\")\nmde_plot\n\n\n\n\nBased on the hypothetical standard deviations in the control and treatment groups from the pilot study, the main study would need a sample size of approximately 1,500 to ensure that effect sizes as small as 0.2 sd are detectable. While calculating the MDE based on pilot results is straightforward, determining how small an MDE should be is more subjective, and should be informed by theory and prior work.\nMDE calculations are based on the design of the pilot and the specific outcome measures. Thus, you may need to perform MDE calculations for each estimand of interest to determine what sample sizes are needed and which hypotheses can be addressed with sufficient power given your experimental design. Keep in mind that as you use pilot results to refine your study design, including treatments and outcomes, the relevant standard deviations of the outcome measures and the resultant MDE calculations may change. Another quick pilot is an option, though it is important to take resource constraints into account.\n\n\n5. Piloting may help you secure funding and support for your research\nPilots are not only helpful for improving the quality of your main study, they may also help to ensure you have the support and funding to enable your study to go forward. Given a general trend of tightening of per-researcher funding, particularly for smaller projects (Bloch and Sørensen 2015), and a movement towards evidence-based decision-making, you may wish to draw on pilots to provide initial evidence that your study is worthwhile. For example, the National Science Foundation (NSF) recognizes the need for more early-stage exploratory studies that can provide a basis for future larger-scale studies, and Time-sharing Experiments for the Social Sciences (TESS) notes that “Proposals that report trial runs of novel and focal ideas will be viewed as more credible.”\nThis does not entail that one needs to show that effect sizes are large enough nor that hypotheses are likely to be confirmed. Instead, a pilot can demonstrate that your research project is feasible in terms of time and resources, that your study design is adequate for answering the research questions proposed, and that your research team has the expertise and capacity to administer the study, perform analyses, and even present results in a compelling fashion (Teijilngen et al. 2001). In a similar fashion, piloting can help you to recruit study team members and organizational partners, or solicit institutional support.\n\n\n6. You can use a pilot to get feedback on your study design\nSharing initial findings, successes, and challenges is a great way to help you prepare for your main study. In light of the growth of the open science movement (Christensen et al. 2020), conferences and workshops are increasingly open to accepting submissions based on pre-analysis plans and pilot results. Whether through these more formal venues, or by reaching out to colleagues or experts, you can use pilot results to receive feedback about your study design, such as strategies to address possible challenges and unexplored theoretical or empirical directions that you can incorporate in your main study.\nFurther, it can take a long time to complete an experimental study and publish results. Sharing intermediate findings allows you to coordinate with other researchers in the field, helping you to align your work and incorporate recent theoretical and empirical innovations relevant to your study.\nUltimately, the design and piloting stage is the best time to receive feedback, as you still have time to make improvements. In contrast, most key research design decisions will already have been finalized by the time your study undergoes formal peer review for publication.\n\n\n7. Keep an eye out for ethical considerations when piloting\nWhen you design your study initially, all of the relevant ethical considerations and risks may not be immediately apparent. The piloting stage is thus a good opportunity to review whether any risks or harms you anticipated may come into play and whether still other ethical considerations should be incorporated into your main study.\nYou can use your pilot to evaluate whether your procedures around informed consent are adequate and to assess the extent of burdens such as time required of participants. You may find, for example, that certain topics—such as mental health and personal identity—are more sensitive than anticipated, or that survey questions (even those based on validated and popular measures) use outdated and offensive language. The best way to find out is to ask. Consider including open-ended survey questions or talking to participants directly to determine what participants think of the study in terms of its normative and cultural acceptability in a given context.\nIn addition, the piloting process is a good time to practice your procedures around privacy, confidentiality, and security of data and other materials. You may find that other procedures for obtaining consent, ensuring participant safety and well-being, and promoting privacy and anonymity are necessary to improve the ethical dimensions of your main study. This can include decreasing risks as well as increasing benefits to participants, such as by providing helpful resources and information that may help to mitigate possible harms.\nNote that this doesn’t imply any less ethical consideration should be given to your pilot itself. All appropriate safeguards, including IRB review, apply to human subjects research for a pilot study as well.\n\n\n8. Be transparent about how your pilot informs the design of your main study\nAs noted, it’s helpful to pre-identify a set of questions about logistics, measurement, or other features of your study that you believe a pilot can help to answer. Putting these questions into writing, designing your pilot to facilitate answering them, and reporting on how the answers shape any modifications to your main study is a way in which you can promote transparency in research. This includes transparency within one’s study team about the purposes of a pilot, as well as for external audiences such as funders or peer reviewers. Transparency helps to alleviate concerns such as the file drawer problem, for example by demonstrating that a pilot study does not function as a method for cherry-picking statistically significant results. It also facilitates understanding and receipt of feedback.\nThe table below is a simple illustrative example of how one could transparently present lessons learned from a hypothetical pilot, for example, in a pre-analysis plan or research proposal. The first column indicates the question the pilot is intended to help answer, in this case questions related to logistical adequacy, manipulation checks, delivery of treatments, and measurement of outcomes. The second column presents the associated findings from the pilot, and the third column discusses how the lessons learned will inform design choices for the main study.\n\n\n\n\n\n\n\n\nQuestions\nPilot Results\nDesign Decision\n\n\n\n\nCan participants hear the video treatments?\n10% reported difficulty.\nWe will add subtitles to the video treatments.\n\n\nCan participants identify the politician party, indicating successful treatment?\n75% correctly identified the politician party.\nWe will mention the politician party additional times to make this treatment component more memorable to participants.\n\n\nWhich video treatments from the available options are best to use?\nParticipants were more familiar with some politicians in the videos than others.\nWe will randomize subjects to a more familiar or less familiar politician and may explore heterogeneous effects by politician familiarity.\n\n\nWhat is the best way to measure uncertainty as an outcome?\nRespondents appear to have been confused by bidirectional scales and questions with reversed scales.\nWe will use unidirectional scales and also consider the distribution (variance) of other key outcome measures.\n\n\n\n\n\n9. Explore unknowns\nWhile you may begin your study with a set of preestablished questions—or “known unknowns”— that your pilot can help to address, keep in mind that there is a whole universe of “unknown unknowns” left to be explored. Of course, not all of these will be relevant to your study, but some are likely to be. For example, you may find that the participants have vastly different interpretations of an informational treatment, understand survey scales differently, or are reluctant to share responses given a perceived political bias.\nThere are a few ways to explore these unknowns in your pilot study. You might wish to include additional exploratory outcome questions, collect extra covariate data, or use a variety of treatments drawing on innovative or untested ideas. Another great way to identify unknowns is through open-ended questions of study participants. Regardless of your study design, you might consider surveying or interviewing participants to ask “what they think about topic X,” or “what comes to mind when they hear the term Y.” The world is often more complex than researchers model in their study designs, and study participants often have more diverse perspectives and relationships with the issues at hand than researchers expect.\nThrough this process, you may identify novel research questions, potential theoretical dimensions or causal mechanisms, and hypotheses that you had not originally formulated when you began studying the topic. Exploring unknowns can therefore lead to refining the ideas you had originally conceived of, as you are unlikely to have determined the best version of your study from the beginning. It can also lead to entirely novel considerations and ideas altogether, some of which you may be able to incorporate into the current study or future studies.\nOverall, pilots afford a wonderful opportunity for open-ended exploration and idea generation.\n\n\n10. A pilot is part of a broader sequence of research activities\nWhile we’ve talked about research in terms of “pilots” and “main studies,” this is an oversimplification of the research process. Indeed one can (and often should) employ multiple pilot studies, perhaps with different sample sizes, to evaluate different questions and to continue refining one’s research design as new questions and possibilities emerge. Pilot studies certainly do set the stage for main studies as well as subsequent “follow-up” studies, but there may not be a sharp demarcation between these kinds of studies. As depicted in the graphic, pilot studies do not merely lead to main studies. They can also help one develop new ideas and they may serve as a venue through which one can contribute to or coordinate with the broader research community around topics of shared interest.\n\n\n\n\nThe Role of Pilots in Experimental Research\n\n\n\n\nAs pilots are part of this broader sequence of research activities, an important consideration is what portion of one’s funding and resources should be allocated to a pilot study versus one’s main study. Researchers may be discouraged from devoting research resources to pilots due to a perception that pilots only provide preliminary data that cannot be used for final research products. While there is no simple “rule of thumb” about the share of the budget that researchers should apply to pilots, we believe that pilots can pay off when they empower researchers to improve their study designs and make more grounded decisions about their research.\nAdministering a second pilot study is especially prudent when pilot results or procedures deviate significantly from your expectations, or if you make substantial alterations to your design. Piloting one or more times is particularly beneficial for field experiments, as researchers often have just one opportunity to successfully implement their full study. However, when resources are constrained, you may opt instead to use simulations to evaluate alternative research designs based on results from your single pilot study. The DeclareDesign package in R is a helpful resource.\nIn short, pilots are neither pre-tests of hypotheses nor merely checks on basic study logistics that deplete one’s research funds. Instead, pilots are living and breathing elements of the broader research process that provide value in their own right.\n\n\n\n\n\n\n\n\nReferences\n\nBloch, Carter, and Mads P. Sørensen. 2015. “The Size of Research Funding: Trends and Implications.” Science and Public Policy 42 (1): 30–43.\n\n\nChristensen, Garret, Zenan Wang, Elizabeth Levy Paluck, Nicholas Swanson, David Birke, Edward Miguel, and Rebecca Littman. 2020. “Open Science Practices Are on the Rise: The State of Social Science (3S) Survey.”\n\n\nFranco, Annie, Neil Malhotra, and Gabor Simonovits. 2014. “Publication Bias in the Social Sciences: Unlocking the File Drawer.” Science 345 (6203): 1502–5.\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge, United Kingdom: Cambridge University Press.\n\n\nTeijilngen, Edwin R. van, Anne-Marie Rennie, Vanora Hundley, and Wendy Graham. 2001. “The Importance of Conducting and Reporting Pilot Studies: The Example of the Scottish Births Survey.” Journal of Advanced Nursing 34 (3): 289–95.\n\n\nThabane, Lehana, Jinhui Ma, Rong Chu, Ji Cheng, Afisi Ismaila, Lorena P. Rios, Robson Reid, Marroon Thabane, Lora Giangregorio, and Charles H. Goldsmith. 2010. “A Tutorial on Pilot Studies: The What, Why and How.” BMC Medical Research Methodology 10 (1): 1–10."
  },
  {
    "objectID": "guides/analysis-procedures/covariates_en.html",
    "href": "guides/analysis-procedures/covariates_en.html",
    "title": "10 Things to Know About Covariate Adjustment",
    "section": "",
    "text": "This guide will help you think through when it makes sense to try to “control for other things” when estimating treatment effects using experimental data. We focus on the big ideas and provide examples in R."
  },
  {
    "objectID": "guides/analysis-procedures/covariates_en.html#footnotes",
    "href": "guides/analysis-procedures/covariates_en.html#footnotes",
    "title": "10 Things to Know About Covariate Adjustment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee, e.g., pages 217–219 of Bruhn and McKenzie (2013).↩︎\nA brief review of bias and precision: Imagine replicating the experiment many times (without changing the experimental sample and conditions, but re-doing random assignment each time). An unbiased estimator may overestimate or underestimate the ATE on any given replication, but its expected value (the average over all possible replications) will equal the true ATE. We usually prefer unbiased or approximately unbiased estimators, but we also value precision (which is formally defined as the inverse of the variance). Imagine you’re throwing a dart at a dartboard. If you hit the center of the dartboard on average but your shots are often far from the mark, you have an unbiased but imprecise estimator. If you hit close to the center every time, your estimator is more precise. A researcher may choose to accept a small bias in return for a large improvement in precision. One possible criterion for evaluating estimators is the mean squared error, which equals the variance plus the square of the bias. See, e.g., Lohr (2010), pp. 31-32↩︎\n“Sampling variability” refers to the spread of estimates that will be produced just because of the different random assignments that could have been drawn. When the luck of the draw of random assignment produces a treatment group with more As and a control group with more Bs, it is more difficult to separate background characteristics (A and B) from treatment assignment as the predictor of the observed outcomes.↩︎\nThe estimated bias is 0.0003 with a margin of error (at the 95% confidence level) of 0.0018.↩︎\nSee, e.g.: Cox and Reid (2000), (pp. 29-32), Holt and Smith (1979), and Royall (1976). For an introduction to philosophical disagreements about statistical inference, see Efron (1978).↩︎\nLin, Green, and Coppock (2016). Italics in the original.↩︎\nThe estimated bias is \\(-\\) 0.459 with a margin of error (at the 95% confidence level) of 0.002.↩︎\nSee Freedman (2008). See also Winston Lin’s blog posts (part I and part II) discussing his response to Freedman.↩︎\nFor more discussion of pre-analysis plans, see, e.g., Olken (2015).↩︎"
  },
  {
    "objectID": "guides/analysis-procedures/hypothesis-testing_en.html",
    "href": "guides/analysis-procedures/hypothesis-testing_en.html",
    "title": "10 Things to Know About Hypothesis Testing",
    "section": "",
    "text": "When researchers report that “The estimated average treatment effect is 5 (\\(p=.02\\)),” they are using a shorthand to say, “Dear reader, in case you were wondering whether we could distinguish signal from noise in the this experiment using averages, in fact, we can. The experimental results are not consistent with the idea that the treatment had no effects.” People do hypothesis tests in observational studies as well as in randomized experiments. This guide focuses on their use in randomized experiments or research designs that try to organize data so that they look “as-if-randomized” (such as regression discontinuity designs or other natural or quasi-experimental designs).\nThe \\(p\\)-value summarizes the ability of a given test to distinguish signal from noise. As we saw in 10 Things You Need to Know about Statistical Power, whether an experiment can detect a treatment effect depends not only on the size of the experimental pool, but also the distribution of the outcome1, the distribution of the treatment, and the substantive strength of the intervention itself. When a researcher calculates a \\(p\\)-value as the result of a hypothesis test, she is summarizing all of these aspects of a research design as they bear on a particular claim—usually a claim that the treatment had no causal effect.\nThe rest of this guide explains the pieces of a hypothesis test piece by piece: from null hypothesis (the claim that the treatment had no causal effect), to test-statistic summarizing the observed data (like a difference of means), to the creation of a probability distribution that allows calculation of a \\(p\\)-value. It also discusses the idea of rejecting (but not accepting) a hypothesis and touches on the question of what makes for a good hypothesis test (hint: an ideal hypothesis test should cast doubt on the truth rarely and distinguish even faint signals from noise). See also 10 Things You Need to Know about Randomization Inference for more discussion of these ideas."
  },
  {
    "objectID": "guides/analysis-procedures/hypothesis-testing_en.html#a-quick-overview-of-the-fundamental-problem-of-causal-inference-and-an-introduction-to-some-notation",
    "href": "guides/analysis-procedures/hypothesis-testing_en.html#a-quick-overview-of-the-fundamental-problem-of-causal-inference-and-an-introduction-to-some-notation",
    "title": "10 Things to Know About Hypothesis Testing",
    "section": "A quick overview of the fundamental problem of causal inference and an introduction to some notation",
    "text": "A quick overview of the fundamental problem of causal inference and an introduction to some notation\nRecall from 10 Things to Know about Causal Inference that the counterfactual conceptualization of causality uses the idea of potential outcomes to define cause and to formalize what we mean when we say “X causes Y” or “Smoking causes cancer” or “Information increases tax compliance.” Although there are other ways to think about causality (Brady 2008), the counterfactual idea suggests that we imagine that each person, \\(i\\), would pay their taxes, \\(y_{i}\\), if given information about the use to which those taxes are put. Write \\(Z_i=1\\) to mean that information was giving to the person and \\(Z_i=0\\) if no information was given so that we can write \\(y_{i,Z_i=1}\\) to refer to the amount of taxes paid by someone given information and \\(y_{i,Z_i=0}\\) to refer to the amount of taxes paid by someone not given any information in particular. In an actual experiment, we might randomize the provision of information to citizens, so some people will have the information and others will not. We observe the taxes paid by people in both conditions but, for any one person, we can only observe the taxes that they pay in one of the two conditions. What does it mean when we say “causal effect”? It often means that the outcome in one condition (\\(y_{i,Z_i=1}\\) written slightly more simply as \\(y_{i,1}\\)) and the outcome in the other condition (\\(y_{i,Z_i=0}\\) or \\(y_{i,0}\\)) differ for a given person, such that we would write \\(y_{i,Z_i=1} \\ne y_{i,Z_i=0}\\).\nWe cannot observe both \\(y_{i,1}\\) and \\(y_{i,0}\\) for each person — if we gave information about taxes to a person we observe \\(y_{i,1}\\) and so we cannot observe how they would have acted if they had not been given this information (\\(y_{i,0}\\)). So, we cannot use direct observation to learn about this counterfactual causal effect and we can only infer about it. Holland (1986) calls this inability to use direct observation to learn about counterfactual causality the “fundamental problem of causal inference.”"
  },
  {
    "objectID": "guides/analysis-procedures/hypothesis-testing_en.html#an-overview-of-estimation-based-approaches-to-causal-inference-in-randomized-experiments.",
    "href": "guides/analysis-procedures/hypothesis-testing_en.html#an-overview-of-estimation-based-approaches-to-causal-inference-in-randomized-experiments.",
    "title": "10 Things to Know About Hypothesis Testing",
    "section": "An overview of estimation based approaches to causal inference in randomized experiments.",
    "text": "An overview of estimation based approaches to causal inference in randomized experiments.\nThere are three main ways that the statistical sciences have engaged with the fundamental problem of causal inference. All of these ways involve changing the target of inference. That is, when asked, “Does information cause people to pay their taxes?” we tend to say, “We cannot answer that question directly. However, we can answer a related question.”\nThe first approach changes the question from whether information causes a particular person to pay her taxes to whether information causes people to pay their taxes on average. 10 Types of Treatment Effect You Should Know About describes how a scientist can estimate average causal effects in a randomized experiment even though individual causal effects are unobservable. This insight is credited to Jerzy Neyman. Judea Pearl’s work on estimating the conditional probability distribution of an outcome based on a causal model of that outcome is similar to this idea. Both approaches address the fundamental problem of causal inference by changing the question to focus on averages or conditional probabilities rather than individuals.\nA related approach that is due to Don Rubin involves predicting the individual level potential outcomes. The predictions are based on a probability model of treatment assignment \\(Z_i\\) (for example, \\(Z \\sim \\text{Bernoulli}(\\pi)\\)) and a probability model of the two potential outcomes (for example, \\((y_{i,1},y_{i,0}) \\sim \\text{Multivariate Normal}(\\boldsymbol{\\beta}\\mathbf{X}, \\boldsymbol{\\Sigma})\\) with a vector of coefficients \\(\\boldsymbol{\\beta}\\), an \\(n \\times p\\) matrix of variables \\(\\mathbf{X}\\) (containing both treatment assignment and other background information) and a \\(p \\times p\\) variance-covariance matrix \\(\\Sigma\\) describing how all of the columns in \\(\\mathbf{X}\\) relate to one another). The probability models relate treatment, background information, and outcomes to one another. The approach combines these models with data using Bayes’ Rule to produce posterior distributions for quantities like the individual level treatment effects or average treatment effects (see Imbens and Rubin (2007) for more on what they call the Bayesian Predictive approach to causal inference). So, this predictive approach focuses not on averages but on differences in predicted potential outcomes for each person (although mostly these individual predicted differences are summarized using characteristics of the posterior distributions, like the average of the predictions.)"
  },
  {
    "objectID": "guides/analysis-procedures/hypothesis-testing_en.html#hypothesis-testing-is-a-statistical-approach-to-the-fundamental-problem-of-causal-inference-using-claims-about-the-unobserved.",
    "href": "guides/analysis-procedures/hypothesis-testing_en.html#hypothesis-testing-is-a-statistical-approach-to-the-fundamental-problem-of-causal-inference-using-claims-about-the-unobserved.",
    "title": "10 Things to Know About Hypothesis Testing",
    "section": "Hypothesis testing is a statistical approach to the fundamental problem of causal inference using claims about the unobserved.",
    "text": "Hypothesis testing is a statistical approach to the fundamental problem of causal inference using claims about the unobserved.\nThe third approach to this problem changes the question again. Fisher (1935, chap. 2) taught us that we can ask the fundamental question about whether there is a causal effect for a single person, but the answer can only be in terms of how much information the research design and data provide about the question. That is, one can hypothesize that, for person \\(i\\), the information made no difference to their outcomes, such that \\(y_{i,1}=y_{i,0}\\) or \\(y_{i,1}=y_{i,0}+\\tau_i\\) where \\(\\tau_i=0\\) for everyone. However, the answer to this question has to be something like “This research design and dataset provide a lot of information about this model or idea or hypothesis.” or, as above, “This research design is not consistent with that claim.” (See Rosenbaum (2002, chap. 2), Rosenbaum (2010b, chap. 2), and Rosenbaum (2017), for more details about this approach.)"
  },
  {
    "objectID": "guides/analysis-procedures/hypothesis-testing_en.html#an-example-testing-the-sharp-null-hypothesis-of-no-effects",
    "href": "guides/analysis-procedures/hypothesis-testing_en.html#an-example-testing-the-sharp-null-hypothesis-of-no-effects",
    "title": "10 Things to Know About Hypothesis Testing",
    "section": "An example: Testing the Sharp Null Hypothesis of No Effects",
    "text": "An example: Testing the Sharp Null Hypothesis of No Effects\nLet us test the sharp null hypothesis of no effects. In the case of the example experiment, the treatment was assigned to exactly 5 observations out of 10 completely at random. To repeat that operation, we need only permute or shuffle the given \\(Z\\) vector.\n\nrepeatExperiment &lt;- function(Z){\n    sample(Z)\n}\n\nWe already know that \\(H_0: y_{i,1} = y_{i,0}\\) implies that \\(Y_i=y_{i,0}\\). So, we can describe all of the ways that the experiment would work out under this null by simply repeating the experiment (i.e. re-assigning treatment) and recalculating a test statistic each time. The following code repeatedly re-assigns treatment following the design and calculates the test statistic each time.\n\nset.seed(123457)\npossibleMeanDiffsH0 &lt;- replicate(10000,meanTZ(ys=Y,z=repeatExperiment(Z=Z)))\nset.seed(123457)\npossibleMeanRankDiffsH0 &lt;- replicate(10000,meanrankTZ(ys=Y,z=repeatExperiment(Z=Z)))\n\nAnd these plots show the distributions of the two different test statistics that would emerge from the world of the null hypothesis (the curves and short ticks at the bottom of the plots). The plots also show the observed values for the test statistics that we can use to compare what we observe (the long thick lines) with what we hypothesize (the distributions).\n\npMeanTZ &lt;- min( mean( possibleMeanDiffsH0 &gt;= observedMeanTZ ),\n           mean( possibleMeanDiffsH0 &gt;= observedMeanTZ ))\npMeanRankTZ &lt;- min( mean( possibleMeanRankDiffsH0 &gt;= observedMeanRankTZ ),\n            mean( possibleMeanRankDiffsH0 &lt;= observedMeanRankTZ ))\npMeanTZ\npMeanRankTZ\n\n\npar(mfrow=c(1,2),mgp=c(1.5,.5,0),mar=c(3,3,0,0),oma=c(0,0,3,0))\nplot(density(possibleMeanDiffsH0),\n     ylim=c(0,.04),\n     xlim=range(possibleMeanDiffsH0),\n     lwd=2,\n     main=\"\",#Mean Difference Test Statistic\",\n     xlab=\"Mean Differences Consistent with H0\",cex.lab=0.75)\nrug(possibleMeanDiffsH0)\nrug(observedMeanTZ,lwd=3,ticksize = .51)\ntext(observedMeanTZ+8,.022,\"Observed Test Statistic\")\nplot(density(possibleMeanRankDiffsH0),lwd=2,\n     ylim=c(0,.45),\n     xlim=c(-10,10), #range(possibleMeanDiffsH0),\n     main=\"\", #Mean Difference of Ranks Test Statistic\",\n     xlab=\"Mean Difference of Ranks Consistent with H0\",cex.lab=0.75)\nrug(possibleMeanRankDiffsH0)\nrug(observedMeanRankTZ,lwd=3,ticksize = .9)\ntext(observedMeanRankTZ,.45,\"Observed Test Statistic\")\nmtext(side=3,outer=TRUE,text=expression(paste(\"Distributions of Test Statistics Consistent with the Design and \",H0: y[i1]==y[i0])))\n\n\n\n\nAn example of using the design of the experiment to test a hypothesis.\n\n\n\n\nTo formalize the comparison between observed and hypothesized, we can calculate a \\(p\\)-value, i.e., the proportion of the hypothetical experiments that yield test statistics greater than or equal to the observed experiment. In the left panel of the figure we see that a wide range of differences of means between treated and control groups are compatible with the treatment having no effects (with the overall range between -425.6 and 425.6). The right panel shows that transforming the outcomes to ranks before taking the difference of means reduces the range of the test statistics — after all the ranks themselves go from 1 to 10 rather than from 1 to 280.\n\nOne-sided \\(p\\)-values\nOne-sided \\(p\\)-values capture the probability that a test statistic is at least as big or bigger (upper \\(p\\)-value) or at least as smaller or smaller (lower \\(p\\)-value) than the observed test statistic. Here, the one-sided \\(p\\)-values are 0.2034 for the simple mean difference and 0.15 for the mean difference of the rank-transformed outcomes. Each test statistic casts a different amount of doubt, or quantifies a different amount of surprise, about the same null hypothesis of no effects. The outcome itself is so noisy that the mean difference of the rank-transformed outcomes does a better job of picking up the signal than the simple mean difference. These data were generated with treatment effects built in, so the null hypothesis of no effects is false, but the information about the effects is noisy — the sample size is small, and the distribution of the outcomes involves some strange outlying points and treatment effects themselves vary greatly.\n\n\nTwo-sided \\(p\\)-values\nSay we did not know in advance whether our experiment would show a negative effect or a positive effect. Then we might make two hypothesis tests — one calculating the one-sided upper \\(p\\)-value and the other calculating the one-sided lower \\(p\\)-value. Now, if we did this we would be calculating two \\(p\\)-values and, if we made a standard practice of this, we would run the risk of misleading ourselves. After all, recall from 10 Things You Need to Know About Multiple Comparisons that even if there really is no effect, 100 independent and well operating tests of the null of no effects will yield no more than 5 \\(p\\)-values less than .05. One easy solution to the challenge of summarizing extremity of a experiment in either direction rather than just focusing on greater-than or less-than is to calculate a two-sided \\(p\\)-value. This, by the way, is the standard \\(p\\)-value produced by most canned software such as lm() and t.test() and wilcox.test() in R. The basic idea is to calculate both \\(p\\)-values and then multiply the smaller \\(p\\)-value by 2. (The idea here is that you are penalizing yourself for making two tests – see Rosenbaum (2010a, chap. 2) and Cox et al. (1977) for more on the idea of multiplying the smaller p-value by two.)\n\n## Here I use &lt;= and &gt;= rather than &lt; and &gt; because of the discreteness of the\n## randomization distribution with only 10 observations. See discussions of\n## the \"mid-p-value\"\np2SidedMeanTZ &lt;-     2*min( mean( possibleMeanDiffsH0 &gt;= observedMeanTZ ),\n               mean( possibleMeanDiffsH0 &lt;= observedMeanTZ ))\np2SidedMeanRankTZ &lt;- 2*min( mean( possibleMeanRankDiffsH0 &gt;= observedMeanRankTZ ),\nmean( possibleMeanRankDiffsH0 &lt;= observedMeanRankTZ ))\n\nIn this case the two-sided \\(p\\)-values are 0.4068 and 0.296 for the simple mean differences and means differences of ranks respectively. We interpret them in terms of “extremity” — we would only see an observed mean difference as far away from zero as the one manifest in our results roughly 18% of the time, for example.\nAs a side note: The test of the sharp null shown here can be done without writing the code yourself. The code that you’ll see here (by clicking the code buttons) shows how to use different R packages to test hypotheses using randomization-based inference.\n\n## using the coin package\nlibrary(coin)\n\nLoading required package: survival\n\nset.seed(12345)\npMean2 &lt;- pvalue(oneway_test(Y~factor(Z),data=dat,distribution=approximate(nresample=1000)))\ndat$rankY &lt;- rank(dat$Y)\npMeanRank2 &lt;- pvalue(oneway_test(rankY~factor(Z),data=dat,distribution=approximate(nresample=1000)))\npMean2\n\n[1] 0.405\n99 percent confidence interval:\n 0.3650502 0.4458670 \n\npMeanRank2\n\n[1] 0.28\n99 percent confidence interval:\n 0.2440542 0.3180731 \n\n# renv::install(\"dgof\")\n# renv::install(\"hexbin\")\n# renv::install(\"kSamples\")\n# renv::install(\"ddst\")\n# remotes::install_github(\"markmfredrickson/RItools@randomization-distribution\",dependencies=TRUE)\n\nlibrary(RItools)\n\nLoading required package: SparseM\n\n\n\nAttaching package: 'SparseM'\n\n\nThe following object is masked from 'package:base':\n\n    backsolve\n\n\nLoading required package: dgof\n\n\n\nAttaching package: 'dgof'\n\n\nThe following object is masked from 'package:stats':\n\n    ks.test\n\n\nWarning: replacing previous import 'stats::ks.test' by 'dgof::ks.test' when\nloading 'RItools'\n\nthedesignA &lt;- simpleRandomSampler(total=N,z=dat$Z,b=rep(1,N))\npMean4 &lt;- RItest(y=dat$Y,z=dat$Z,samples=1000, test.stat= meanTZ ,\n         sampler = thedesignA)\npMeanRank4 &lt;- RItest(y=dat$Y,z=dat$Z,samples=1000, test.stat= meanrankTZ ,\n             sampler = thedesignA)\npMean4\n\nCall:  RItest(y = dat$Y, z = dat$Z, test.stat = meanTZ, sampler = thedesignA,  \n          samples = 1000)\n\n                        Value Pr(&gt;x)\nObserved Test Statistic   102 0.2024\n\npMeanRank4\n\nCall:  RItest(y = dat$Y, z = dat$Z, test.stat = meanrankTZ, sampler = thedesignA,  \n          samples = 1000)\n\n                        Value Pr(&gt;x)\nObserved Test Statistic   2.2 0.1508\n\n# dev_mode(on=FALSE,path=here(\"R-dev\"))"
  },
  {
    "objectID": "guides/analysis-procedures/hypothesis-testing_en.html#using-the-ri2-package",
    "href": "guides/analysis-procedures/hypothesis-testing_en.html#using-the-ri2-package",
    "title": "10 Things to Know About Hypothesis Testing",
    "section": "using the ri2 package",
    "text": "using the ri2 package\n\nlibrary(ri2)\n\nLoading required package: estimatr\n\nthedesign &lt;- declare_ra(N=N)\npMean4 &lt;- conduct_ri( Y ~ Z, declaration = thedesign,\n             sharp_hypothesis = 0, data = dat, sims = 1000)\nsummary(pMean4)\n\n  term estimate two_tailed_p_value\n1    Z      102          0.4047619\n\npMeanRank4 &lt;- conduct_ri( rankY ~ Z, declaration = thedesign,\n             sharp_hypothesis = 0, data = dat, sims = 1000)\nsummary(pMeanRank4)\n\n  term estimate two_tailed_p_value\n1    Z      2.2          0.3015873"
  },
  {
    "objectID": "guides/analysis-procedures/hypothesis-testing_en.html#an-example-testing-the-weak-null-of-no-average-effects",
    "href": "guides/analysis-procedures/hypothesis-testing_en.html#an-example-testing-the-weak-null-of-no-average-effects",
    "title": "10 Things to Know About Hypothesis Testing",
    "section": "An example: Testing the weak null of no average effects",
    "text": "An example: Testing the weak null of no average effects\nThe weak null hypothesis is a claim about aggregates, and is nearly always stated in terms of averages: \\(H_0: \\bar{y}_{1} = \\bar{y}_{0}\\) The test statistic for this hypothesis nearly always is the difference of means (i.e. meanTZ() above. The below code shows the use of least squares (lm() in R) for the purpose of calculating differences of means as a test statistic for hypotheses about average effects. Notice that the OLS-based \\(p\\)-values differ from those calculated by t.test() and difference_of_means(). Recall that the OLS statistical inference is justified by the assumption of independent and identically distributed observations yet, in most experiments, the treatment itself changes the variation in the treatment group (thereby violating the identical-distribution/homoskedasticity assumption of OLS). This is one of a few reasons why best practice in testing the weak null hypothesis of no average treatment effects uses tools other than those provided by simple canned OLS procedures.\n\nlm1 &lt;- lm(Y~Z,data=dat)\nlm1P &lt;- summary(lm1)$coef[\"Z\",\"Pr(&gt;|t|)\"]\nttestP1 &lt;- t.test(Y~Z,data=dat)$p.value\nlibrary(estimatr)\nttestP2 &lt;- difference_in_means(Y~Z,data=dat)\nc(lmPvalue=lm1P,\nttestPvalue=ttestP1,\ndiffOfMeansPvalue=ttestP2$p.value)\n\n           lmPvalue         ttestPvalue diffOfMeansPvalue.Z \n          0.5106994           0.5110166           0.5110166 \n\n\nThis code produces the same results without using least squares — after all, we are just calculating differences of means and the variances of those means as they might vary across repeated experiments in the same pool of experimental units.\n\nvarEstATE &lt;- function(Y,Z){\n    var(Y[Z==1])/sum(Z) + var(Y[Z==0])/sum(1-Z)\n}\nseEstATE &lt;- sqrt(varEstATE(dat$Y,dat$Z))\nobsTStat &lt;- observedMeanTZ/seEstATE\nc(observedTestStat=observedMeanTZ,stderror=seEstATE,tstat=obsTStat,\n  pval=2*min(pt(obsTStat,df=8,lower.tail = TRUE),\n         pt(obsTStat,df=8,lower.tail = FALSE))\n  )\n\nobservedTestStat         stderror            tstat             pval \n     102.0000000      148.1816453        0.6883444        0.5106994 \n\n\nNotice that these tests all assume that the distribution of the test statistic across repeated experiments would be well characterized by a \\(t\\)-distribution. The left-hand panel in the figure above shows the realized distribution of one way for the weak null to be true (i.e. if the sharp null is true): there are many ways for the weak null to be true — some of which are compatible with large positive effects on some units and large negative effects on other units, others are compatible with other patterns of individual level effects. In this particular small data set, engineered to have a very skewed outcome distribution, however, none of those patterns will produce a reference distribution that look like a Normal or \\(t\\)-curve if the mean difference is used as a test statistic. We will return to this point later when we discuss the characteristics of a good test — one of which is a controlled false positive rate."
  },
  {
    "objectID": "guides/analysis-procedures/hypothesis-testing_en.html#what-does-it-mean-to-reject-a-null-hypothesis",
    "href": "guides/analysis-procedures/hypothesis-testing_en.html#what-does-it-mean-to-reject-a-null-hypothesis",
    "title": "10 Things to Know About Hypothesis Testing",
    "section": "What does it mean to reject a null hypothesis?",
    "text": "What does it mean to reject a null hypothesis?\nNotice that a \\(p=.01\\) only reflects extremity of the observed data compared with the hypothesis — it means that the observed test statistic looks extreme when considered from the perspective of the distribution of tests statistics that are generated from the null hypothesis and research design. So, we think of \\(p=.01\\) (and other small \\(p\\)-values) as casting doubt on whether the specific hypothesis is a good model of the observed data. Often the only model of potential outcomes that is tested is the model of no effects, so a small \\(p\\)-value should make us doubt the model of no effects. The makers of canned regression software tend to print out a \\(p\\)-value that refers to this hypothesis automatically, so that it is difficult to not see the results of this test even if you just want to describe differences of means in the data but you are using least squares as your difference of means calculator."
  },
  {
    "objectID": "guides/analysis-procedures/hypothesis-testing_en.html#what-does-it-mean-to-not-reject-a-null-hypothesis",
    "href": "guides/analysis-procedures/hypothesis-testing_en.html#what-does-it-mean-to-not-reject-a-null-hypothesis",
    "title": "10 Things to Know About Hypothesis Testing",
    "section": "What does it mean to not reject a null hypothesis?",
    "text": "What does it mean to not reject a null hypothesis?\nNotice that a \\(p=.50\\) only reflects extremity of the observed data compared with the hypothesis—but the observed data, in this case, do not look extreme but common from the perspective of the null hypothesis. So, \\(p=.5\\) (and other large p-values) do not encourage us to doubt the model of the null hypothesis. It does not encourage us to accept that model—it is only a model after all. We do not know how reasonable the model was a priori, for example. So, a single large \\(p\\)-value is some argument in favor of the null, but not a very clear argument."
  },
  {
    "objectID": "guides/analysis-procedures/hypothesis-testing_en.html#how-to-learn-about-errors-of-missing-the-signal-in-the-noise",
    "href": "guides/analysis-procedures/hypothesis-testing_en.html#how-to-learn-about-errors-of-missing-the-signal-in-the-noise",
    "title": "10 Things to Know About Hypothesis Testing",
    "section": "How to learn about errors of missing the signal in the noise?",
    "text": "How to learn about errors of missing the signal in the noise?\nThe 10 Things You Need to Know About Statistical Power guide explains how we want hypotheses to reject false nulls (i.e. detect signal from noise). When we think about the power of statistical tests, we need to consider the alternative hypothesis. However, as we have shown above, we can test null hypotheses without the idea of rejecting or accepting them although then the “power” of a test is harder to define and work with."
  },
  {
    "objectID": "guides/analysis-procedures/hypothesis-testing_en.html#how-to-learn-about-false-positive-errors",
    "href": "guides/analysis-procedures/hypothesis-testing_en.html#how-to-learn-about-false-positive-errors",
    "title": "10 Things to Know About Hypothesis Testing",
    "section": "How to learn about false positive errors?",
    "text": "How to learn about false positive errors?\nThe easiest way to learn about false positive errors is by simulation. First, we create the situation where the null is true and known, and then we test that null under the many ways that it is possible for that null to be true. For example, in the example experiment used here, we have 5 units assigned to treatment out of 10. This means that there are \\(\\binom{10}{5}=252\\) different ways to assign treatment — and 252 ways for the experiment to have had no effects on the individuals.\nWe demonstrate here setting the sharp or strict null hypothesis to be zero, but one could also assess the false positive rate for different hypotheses. We compare error rates for a few of the approaches used so far, including the test of the weak null of no effects. The following plot shows the proportion of \\(p\\)-values less than any given level of significance (i.e. rejection threshold) for each of four tests. That is, this is a plot of false positive rates for any given significance threshold. A test that has a controlled or known false positive rate would have symbols on or below the line across the whole x-axis or range of the plot. As we can see here, the two tests using permutations of treatment to assess the sharp null of no effects have this feature. The tests of the weak null using the mean difference test statistic and appealing to the large sample theory to justify the use of a \\(t\\)-distribution do not have a controlled false positive rate: the proportion of \\(p\\)-values below any given rejection threshold can be too high or too low.\n\ncollectPValues &lt;- function(y,z){\n    ## Make Y and Z have no relationship by re-randomizing Z\n    newz &lt;- repeatExperiment(z)\n        thelm &lt;- lm(y~newz,data=dat)\n    ttestP2 &lt;- difference_in_means(y~newz,data=dat)\n    owP &lt;- pvalue(oneway_test(y~factor(newz),distribution=exact()))\n    ranky &lt;- rank(y)\n    owRankP &lt;- pvalue(oneway_test(ranky~factor(newz),distribution=exact()))\n    return(c(lmp=summary(thelm)$coef[\"newz\",\"Pr(&gt;|t|)\"]\n,\n         neyp=ttestP2$p.value[[1]],\n         rtp=owP,\n         rtpRank=owRankP))\n}\n\n\nset.seed(12345)\npDist &lt;- replicate(5000,collectPValues(y=dat$Y,z=dat$Z))\n\n\npar(mfrow=c(1,1),mgp=c(1.25,.5,0),oma=rep(0,4),mar=c(3,3,0,0))\nplot(c(0,1),c(0,1),type=\"n\",\n     xlab=\"p-value=p\",ylab=\"Proportion p-values &lt; p\")\nfor(i in 1:nrow(pDist)){\n    lines(ecdf(pDist[i,]),pch=i,col=i)\n}\nabline(0,1,col=\"gray\")\nlegend(\"topleft\",legend=c(\"OLS\",\"Neyman\",\"Rand Inf Mean Diff\",\"Rand Inf Mean \\n Diff Ranks\"),\n              pch=1:5,col=1:5,lty=1,bty=\"n\")\n\n\n\n\nIn this particular case, at the threshold of \\(\\alpha=.05\\), all of the tests except for the rank based test report less than a 5% false positive rate – this is good, it should be 5% or less. However, this is no guarantee of good performance by the large-sample based tests in other small experiments, or experiments with highly skewed outcomes, etc… When in doubt it is easy to assess the false positive rate of a test by using the code in this guide to make your own simulation.\n\napply(pDist,1,function(x){ mean(x&lt;.05)})\n\n    lmp    neyp     rtp rtpRank \n 0.0378  0.0378  0.0456  0.0450"
  },
  {
    "objectID": "guides/analysis-procedures/hypothesis-testing_en.html#footnotes",
    "href": "guides/analysis-procedures/hypothesis-testing_en.html#footnotes",
    "title": "10 Things to Know About Hypothesis Testing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOutcomes with big outliers add noise, outcomes that are mostly 0 have little signal, blocking or pre-stratification or covariance adjustment can reduce noise.↩︎"
  },
  {
    "objectID": "guides/analysis-procedures/meta-analysis_en.html",
    "href": "guides/analysis-procedures/meta-analysis_en.html",
    "title": "10 Things to Know About Conducting a Meta-Analysis",
    "section": "",
    "text": "Meta-analysis is a method for summarizing the statistical findings from a research literature. For example, if five experiments have been conducted using the same intervention and outcome measure on the same population of people with five separate estimates of an average treatment effect, one might imagine pooling these five studies together into a single dataset and analyzing them jointly. In broad strokes, in such a case, we could act as though the studies came from five blocks within a single experiment rather than five separate experiments. The benefit of such an approach would be more statistical power in the estimation of one overall average treatment effect. In essence, a meta-analysis produces a weighted average of the five studies’ results. As explained below, this method is also used to summarize research literatures that comprise a diverse array of interventions and outcomes measured in diverse settings, under the assumption that the interventions are theoretically similar and the outcome measures tap into a shared underlying trait."
  },
  {
    "objectID": "guides/analysis-procedures/meta-analysis_en.html#footnotes",
    "href": "guides/analysis-procedures/meta-analysis_en.html#footnotes",
    "title": "10 Things to Know About Conducting a Meta-Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://twitter.com/uri_sohn/status/471318552470126592↩︎\nFor a list of R packages useful in conducting meta-analysis, see here.↩︎\nSee for example https://www.stata.com/support/faqs/statistics/meta-analysis/ and https://cran.r-project.org/web/views/MetaAnalysis.html.↩︎"
  },
  {
    "objectID": "guides/analysis-procedures/ri2_en.html",
    "href": "guides/analysis-procedures/ri2_en.html",
    "title": "10 Randomization Inference Procedures with ri2",
    "section": "",
    "text": "Randomization inference is a procedure for conducting hypothesis tests that takes explicit account of a study’s randomization procedure. See 10 things about Randomization Inference for more about the theory behind randomization inference. In this guide, we’ll see how to use the ri2 package for r to conduct 10 different analyses. This package was developed with funding from EGAP’s innaugural round of standards grants, which are aimed at projects designed to improve the quality of experimental research.\nTo illustrate what you can do with ri2, we’ll use some data from a hypothetical experiment involving 200 students in 20 schools. We’ll consider how to do randomization inference using a variety of different designs, including complete random assignment, block random assignment, cluster random assignment, and a multi-arm trial. You can check the kinds of random assignment methods guide for more on the varieties of random assignment.\nFollow the links below to download the four datasets we’ll use in the examples:\n\ncomplete randomization assignment dataset\nblocked randomization assignment dataset\nclustered randomization assignment dataset- three-arm randomization assignment datase\n\n\n1. Randomization inference for the Average Treatment Effect\nWe’ll start with the most common randomization inference task: testing an observed average treatment effect estimate against the sharp null hypothesis of no effect for any unit.\nIn ri2, you always “declare” the random assignment procedure so the computer knows how treatments were assigned. In the first design we’ll consider, exactly half of the 200 students were assigned to treatment using complete random assignment.\n\nlibrary(ri2)\ncomplete_dat &lt;- read.csv(\"ri2_complete_dat.csv\")\ncomplete_dec &lt;- declare_ra(N = 200)\n\nNow all that remains is a call to conduct_ri. The sharp_hypothesis argument is set to 0 by default corresponding to the sharp null hypothesis of no effect for any unit. We can see the output using the summary and plot commands.\n\nsims &lt;- 10000\nri_out &lt;-\n  conduct_ri(\n    Y ~ Z,\n    declaration = complete_dec,\n    sharp_hypothesis = 0,\n    data = complete_dat,\n    sims = sims\n  )\nsummary(ri_out)\n\n  term estimate two_tailed_p_value\n1    Z    41.98             0.1144\n\nplot(ri_out)\n\n\n\n\nYou can obtain one-sided p-values with a call to summary:\n\nsummary(ri_out, p = \"upper\")\n\n  term estimate upper_p_value\n1    Z    41.98        0.0564\n\nsummary(ri_out, p = \"lower\")\n\n  term estimate lower_p_value\n1    Z    41.98        0.9436\n\n\n\n\n2. Randomization inference for alternative designs\nThe answer that ri2 produces depends deeply on the randomization procedure. The next example imagines that the treatment was blocked at the school level.\n\nblocked_dat &lt;- read.csv(\"ri2_blocked_dat.csv\")\nblocked_dec &lt;- declare_ra(blocks = blocked_dat$schools)\nri_out &lt;-\n  conduct_ri(\n    Y ~ Z,\n    declaration = blocked_dec,\n    data = blocked_dat,\n    sims = sims\n  )\nsummary(ri_out)\n\n  term estimate two_tailed_p_value\n1    Z    91.98              2e-04\n\nplot(ri_out)\n\n\n\n\nA very similar syntax accommodates a cluster randomized trial.\n\nclustered_dat &lt;- read.csv(\"ri2_clustered_dat.csv\")\nclustered_dec &lt;- declare_ra(clusters =  clustered_dat$schools)\nri_out &lt;-\n  conduct_ri(\n    Y ~ Z,\n    declaration = clustered_dec,\n    data = clustered_dat,\n    sims = sims\n  )\nsummary(ri_out)\n\n  term estimate two_tailed_p_value\n1    Z    79.32             0.0111\n\nplot(ri_out)\n\n\n\n\n\n\n3. Randomization inference with covariate adjustment\nCovariate adjustment can often produce large gains in precision. To analyze an experiment with covariate adjustment, simply include the covariates in the formula argument of conduct_ri:\n\ncomplete_dec &lt;- declare_ra(N = 200)\nri_out &lt;-\n  conduct_ri(\n    Y ~ Z + PSAT,\n    declaration = complete_dec,\n    data = complete_dat,\n    sims = sims\n  )\nsummary(ri_out)\n\n  term estimate two_tailed_p_value\n1    Z 59.27132                  0\n\nplot(ri_out)\n\n\n\n\n\n\n4. Randomization inference for a balance test\nYou can use randomization inference to conduct a balance test (or randomization check). In this case, we write a function of data that return some balance statistic (the F-statistic from a regression of the treatment assignment on two covariates).\n\nbalance_fun &lt;- function(data) {\n  summary(lm(Z ~ professionalism + PSAT, data = data))$f[1]\n}\nri_out &lt;-\n  conduct_ri(\n    test_function = balance_fun,\n    declaration = complete_dec,\n    data = complete_dat,\n    sims = sims\n  )\n\nWarning in data.frame(est_sim = test_stat_sim, est_obs = test_stat_obs, : row\nnames were found from a short variable and have been discarded\n\nsummary(ri_out)\n\n                   term  estimate two_tailed_p_value\n1 Custom Test Statistic 0.2924994             0.7489\n\nplot(ri_out)\n\n\n\n\n\n\n5. Randomization inference for treatment effect heterogeneity by subgroups\nYou can assess whether the treatment engenders different treatment effects among distinct subgroups by comparing the model fit (using an F-statistic) of two nested models:\n\nA regression of the outcome on the treatment assignment and the subgroup indicator\nA regression of the outcome on the treatment assignment, subgroup indicator, and their interaction\n\nThe null hypothesis we’re testing against in this example is the sharp hypothesis that the true treatment effect for each unit is the observed average treatment effect, i.e., that effects are constant.\n\nate_obs &lt;- with(complete_dat, mean(Y[Z == 1]) - mean(Y[Z == 0]))\nri_out &lt;-\n    conduct_ri(\n      model_1 = Y ~ Z + high_quality,\n      model_2 = Y ~ Z + high_quality + Z * high_quality,\n      declaration = complete_dec,\n      sharp_hypothesis = ate_obs,\n      data = complete_dat, \n      sims = sims\n    )\nsummary(ri_out)\n\n         term estimate two_tailed_p_value\n1 F-statistic 1.095294             0.7015\n\nplot(ri_out)\n\n\n\n\n\n\n6. Randomization inference for unmodeled treatment effect heterogeneity\nAnother way to investigate treatment effect heterogeneity is to consider whether the variance in the treatment and control groups are different. We can therefore test whether the difference-in-variances is larger in magnitude than what we would expect under the sharp null hypothesis of no effect for any unit.\n\nd_i_v &lt;- function(dat) {\n    with(dat, var(Y[Z == 1]) - var(Y[Z == 0]))\n  }\nri_out &lt;-\n    conduct_ri(\n      test_function = d_i_v,\n      declaration = complete_dec,\n      data = complete_dat, \n      sims = sims\n    )\nsummary(ri_out)\n\n                   term  estimate two_tailed_p_value\n1 Custom Test Statistic -8408.684             0.2824\n\nplot(ri_out)\n\n\n\n\n\n\n7. Randomization inference for multi-arm trials\nIn a three-arm trial, the research might wish to compare each treatment to control separately. To do this, we must change the null hypothesis in a subtle way: we are going to assume the sharp null for each pairwise comparison. For example, when comparing treatment 1 to control, we exclude the subjects assigned to treatment 2 and pretend we simply have a two arm trial conducted among the subjects assigned to control and treatment 1.\n\nthree_arm_dat &lt;- read.csv(\"ri2_three_arm_dat.csv\")\nthree_arm_dec &lt;- declare_ra(N = 200, \n                            conditions = c(\"Control\", \"Treatment 1\", \"Treatment 2\"))\nri_out &lt;-\n    conduct_ri(\n      formula = Y ~ Z,\n      declaration = three_arm_dec,\n      data = three_arm_dat,\n      sims = sims\n    )\nsummary(ri_out)\n\n          term  estimate two_tailed_p_value\n1 ZTreatment 1  26.72546                 NA\n2 ZTreatment 2 -48.52827                 NA\n\n## plot(ri_out)\n\n\n\n8. Randomization inference for joint significance\nIn that same three-arm trial that compares two treatments to a control, we might be interested in testing whether, jointly, the treatments appear to change outcomes relative to the control. This is analogous to a joint F-test, conducted via randomization inference. We assume the sharp null that a unit would express their observed outcome in any of the three conditions.\n\nF_statistic &lt;- function(data) {\n  summary(lm(Y ~ Z, data = data))$f[1]\n}\nri_out &lt;-\n   conduct_ri(\n     test_function = F_statistic,\n     declaration = three_arm_dec,\n     data = three_arm_dat,\n     sims = sims\n   )\n\nWarning in data.frame(est_sim = test_stat_sim, est_obs = test_stat_obs, : row\nnames were found from a short variable and have been discarded\n\nsummary(ri_out)\n\n                   term estimate two_tailed_p_value\n1 Custom Test Statistic 2.802927             0.0664\n\nplot(ri_out)\n\n\n\n\n\n\n9. Randomization inference under noncompliance\nSome experiments encounter noncompliance, the slippage between treatment as assigned and treatment as delivered. The Complier Average Causal Effect (\\(CACE\\)) can be shown (under standard assumptions plus monotonicity) to be the ratio of the effect of assignment on the outcome – the “Intention-to-Treat” (\\(ITT_y\\)) and the effect of assignment on treatment receipt the (\\(ITT_D\\)). (The \\(CACE\\) is also called Local Average Treatment Effect. See our guide 10 Things to Know About the Local Average Treatment Effect for more details.) Because the \\(CACE\\) is just a rescaled, \\(ITT_y\\), a hypothesis test with respect to the \\(ITT_y\\) is a valid test for the \\(CACE\\). In practice, researchers can simply conduct a randomization inference test exactly as they would for the ATE, ignoring noncompliance altogether.\n\nITT_y = with(complete_dat, mean(Y[Z == 1]) - mean(Y[Z == 0])) \nITT_d = with(complete_dat, mean(D[Z == 1]) - mean(D[Z == 0])) \nCACE &lt;- ITT_y / ITT_d\n           \nri_out &lt;-\n  conduct_ri(\n    Y ~ Z, # notice we do inference on the ITT_y\n    declaration = complete_dec,\n    data = complete_dat,\n    sims = sims\n  )\nsummary(ri_out)\n\n  term estimate two_tailed_p_value\n1    Z    41.98             0.1156\n\nplot(ri_out)\n\n\n\n\n\n\n10. Randomization inference for arbitrary test statistics\nri2 can accommodate any scalar test statistic. A favorite among some analysts is the Wilcox rank-sum statistic, which can be extracted from the wilcox.test() function:\n\nwilcox_fun &lt;- function(data){\n  wilcox_out &lt;- with(data, wilcox.test(Y ~ Z))\n  wilcox_out$statistic\n}\nri_out &lt;-\n  conduct_ri(\n    test_function = wilcox_fun,\n    declaration = complete_dec,\n    data = complete_dat,\n    sims = sims\n  )\n\nWarning in data.frame(est_sim = test_stat_sim, est_obs = test_stat_obs, : row\nnames were found from a short variable and have been discarded\n\nsummary(ri_out)\n\n                   term estimate two_tailed_p_value\n1 Custom Test Statistic     4320             0.9565\n\nplot(ri_out)"
  },
  {
    "objectID": "guides/analysis-procedures/multiple-comparisons_en.html",
    "href": "guides/analysis-procedures/multiple-comparisons_en.html",
    "title": "10 Things You Need to Know About Multiple Comparisons",
    "section": "",
    "text": "The “Multiple Comparisons Problem” is the problem that standard statistical procedures can be misleading when researchers conduct a large group of hypothesis tests. When a researcher does more than one test of a hypothesis (or set of closely related hypotheses), the chances are that some finding will appear “significant” even when there’s nothing going on.\nClassical hypothesis tests assess statistical significance by calculating the probability under a null hypothesis of obtaining estimates as large or larger as the observed estimate. When multiple tests are conducted, however, classical p-values can mislead — they no longer reflect the true probability under the null.\nThis guide will help you guard against drawing false conclusions from your experiments. We focus on the big ideas and provide examples and tools that you can use in R."
  },
  {
    "objectID": "guides/analysis-procedures/multiple-comparisons_en.html#footnotes",
    "href": "guides/analysis-procedures/multiple-comparisons_en.html#footnotes",
    "title": "10 Things You Need to Know About Multiple Comparisons",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFormally \\(E[FP/R|R&gt;0]P(R&gt;0)\\) to avoid dividing by 0.↩︎\nSection 9 updated by Tara Lyn Slough, 4 February 2016↩︎"
  },
  {
    "objectID": "guides/analysis-procedures/randomization-inference_en.html",
    "href": "guides/analysis-procedures/randomization-inference_en.html",
    "title": "10 Things to Know About Randomization Inference",
    "section": "",
    "text": "One of the advantages of conducting a randomized trial is that the researcher knows the precise procedure by which the units were allocated to treatment and control. Randomization inference considers what would have happened under all possible random assignments, not just the one that happened to be selected for the experiment at hand. Against the backdrop of all possible random assignments, is the actual experimental result unusual? How unusual is it?"
  },
  {
    "objectID": "guides/analysis-procedures/randomization-inference_en.html#footnotes",
    "href": "guides/analysis-procedures/randomization-inference_en.html#footnotes",
    "title": "10 Things to Know About Randomization Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAcknowledgements: I am grateful to Winston Lin and Gareth Nellis, who commented on an earlier draft.↩︎\nWe focus here on randomization inference as applied to hypothesis testing. Randomization inference may also be used for construction of confidence intervals, but this application requires stronger assumptions. See Gerber and Green (2012), chapter 3.↩︎\nAs explained in other guides, the fundamental problem of causal inference is that we cannot observe what would have happened to those in control group had they be treated, nor can we observe what would have happened to those in the treatment group had they not been treated.↩︎\nOne-tailed tests consider the null hypothesis of no effect against an alternative hypothesis that the average treatment effect is positive (negative). Two-tailed tests evaluate a null hypothesis against the alternative that the ATE is nonzero, whether positive or negative. In that case, the p-value may be assessed by calculating the proportion of simulated random assignments that are at least as large as the observed test statistic in absolute value.↩︎\nSee Chung and Romano (2013). This “studentized” approach makes sense when there is reason to believe that the treatment changes the variance in outcomes in an experiment with different numbers of subjects in treatment and control.↩︎\nSee 10 Things to Know about Heterogeneous Treatment Effects, especially section 7 on multiple comparisons.↩︎\nThe alternative is to make stronger assumptions. See Small, Ten Have, and Rosenbaum (2008).↩︎"
  },
  {
    "objectID": "guides/research-questions/late_en.html",
    "href": "guides/research-questions/late_en.html",
    "title": "10 Things You Need to Know About the Local Average Treatment Effect",
    "section": "",
    "text": "Sometimes a treatment or a program is delivered but for some reason or another only some individuals or groups actually take the treatment. In this case it can be hard to estimate treatment effects for the whole population. For example maybe people for whom the treatment would have had a big effect decided not to take up the treatment. In these cases it is still possible to estimate what’s called the “Local Average Treatment Effect,” or LATE. This guide discusses the LATE: what it is, how to estimate it, and how to interpret it.1"
  },
  {
    "objectID": "guides/research-questions/late_en.html#footnotes",
    "href": "guides/research-questions/late_en.html#footnotes",
    "title": "10 Things You Need to Know About the Local Average Treatment Effect",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor more extensive overviews, see: Angrist, Imbens, and Rubin (1996); Angrist (2006); Angrist and Pischke (2009), sections 4.4-4.6 and 6.2; Dunning (2012); Baiocchi, Cheng, and Small (2014) (with correction in Statistics in Medicine 33: 4859-4860).↩︎\nSee Angrist and Evans (1998).↩︎\nSee: Imbens (2010); Imbens and Angrist (1994); Imbens and Wooldridge (2007).↩︎\nSee Hirano et al. (2000).↩︎\nSee Green and Gerber (2002).↩︎\nSee Gerber et al. (2010).↩︎\nSee Qi Long, Little, and Lin (2010); Jin and Rubin (2009); Jin and Rubin (2008).↩︎"
  },
  {
    "objectID": "guides/research-questions/effect-types_en.html",
    "href": "guides/research-questions/effect-types_en.html",
    "title": "10 Types of Treatment Effect You Should Know About",
    "section": "",
    "text": "This guide1 describes ten distinct types of causal effects that researchers may want to estimate. As discussed in our guide 10 Things to Know About Causal Inference, simple randomization allows one to produce estimates of the average of the unit level causal effects in a sample. This average causal effect or average treatment effect (ATE) is a powerful concept because it is one solution to the problem of not observing all relevant counterfactuals. Yet, it is not the only productive engagement with this problem. In fact, there are many different types of quantities of causal interest. The goal of this guide is to help you choose estimands (a parameter of interest) and estimators (procedures for calculating estimates of those parameters) that are appropriate and meaningful for your data."
  },
  {
    "objectID": "guides/research-questions/effect-types_en.html#footnotes",
    "href": "guides/research-questions/effect-types_en.html#footnotes",
    "title": "10 Types of Treatment Effect You Should Know About",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor a more formal discussion of independence and the assumptions necessary to estimate causal effects, see Holland (1986) and Angrist and Pischke (2008).↩︎\nSee Holland (1986) and Angrist and Pischke (2008) again for more formal discussion of independence and the assumptions necessary to estimate causal effects.↩︎\nEstimates are often written with a hat ( \\(\\widehat{ATE}\\) ) to reflect the difference between the estimate from our particular sample and the estimand, target of our estimation that is unobserved. Unless otherwise stated, in this guide we focus on generating sample estimates and subsequently omit this explicit notation for brevity. See Gerber and Green (2012) for concise introduction to this distinction and Imbens and Wooldridge (2007) for a thorough treatment of these concepts.↩︎\nThe covariance of \\(Y_{i}(1),Y_{i}(0)\\) is impossible to observe but the “Neyman” estimator of the variance omitting the covariance term provides a conservative (too large) estimate of the true sample variance because we tend to assume that the covariance is positive. Since we are generally worried about minimizing type I error rate (incorrectly rejecting true null hypothesis), we prefer using conservative estimates of the variance. See also Dunning (2010) and Gerber and Green (2012) for justification of the conservative variance estimator.↩︎\nSee Lin (2013).↩︎\nSee Brambor, Clark, and Golder (2006).↩︎\nWe typically assume monotonicity, meaning there are no defiers or people who only take the treatment when assigned to control (\\(D_{i}=1\\) when \\(Z_i=0\\)) and refuse the treatment when assigned to treatment (\\(D_{i}=0\\) when \\(Z_{i}=1\\)).↩︎\nSee Angrist and Pischke (2008); Bound, Jaeger, and Baker (1995).↩︎\nSee Imai, King, and Stuart (2008) for a more detailed review of the issues discussed in this section.↩︎\nSee Imbens and Wooldridge (2007).↩︎\nAngrist and Pischke (2008) provide a brief introduction of topics covered in more detail by Hirano, Imbens, and Ridder (2003), Aronow and Middleton (2013), Glynn and Quinn (2010), and Hartman et al. (2015).↩︎\nSee Glynn and Quinn (2010).↩︎\nSee Hartman et al. (2015) for an example of efforts to combine experimental and observational data to move from a sample ATE to an estimate of a population ATT.↩︎\nSee Rosenbaum and Rubin (1983).↩︎\nSee Abadie, Angrist, and Imbens (2002).↩︎\n\nThat is, treatment can have heterogeneous effects but the ordering of potential outcomes is preserved. See Angrist and Pischke (2008). See Frölich and Melly (2010) for fairly concise discussions of these issues and Abbring and Heckman (2007) for a thorough overview.\n\n↩︎\nSee Koenker and Hallock (2001) for a concise overview of quantile regression.↩︎\nFormally, Imai, Keele, and Yamamoto (2010) define the necessary conditions of sequential ignorability as: \\({Y_i(d',m),M_i(d)}⊥D_i|X_i=x, Y_i(d',m)⊥M_i(d)|D_i=d,X_i=x\\). That is, first, given pre-treatment covariates, the potential outcomes of Y and M are independent of treatment D, and, second, that conditional on pre-treatment covariates and treatment status, potential outcomes are also independent of the mediator.↩︎\nSee for example Imai, Keele, and Yamamoto (2010), Imai et al. (2011), Imai, Tingley, and Yamamoto (2013), and Imai and Yamamoto (2013). Also see the discussion of Imai, Tingley, and Yamamoto (2013) for different perspectives on the desirability of addressing mediation-type claims with sensitivity or bounds-style analyses.↩︎"
  },
  {
    "objectID": "guides/research-questions/mechanisms_en.html",
    "href": "guides/research-questions/mechanisms_en.html",
    "title": "10 Things to Know About Mechanisms",
    "section": "",
    "text": "As social scientists, we are fascinated by causal questions. As soon as we learn that X causes Y, we want to better understand why X causes Y. This guide explores the role of “mechanisms” in causal analysis and will help you to understand what kinds of conclusions you may draw about them."
  },
  {
    "objectID": "guides/research-questions/mechanisms_en.html#footnotes",
    "href": "guides/research-questions/mechanisms_en.html#footnotes",
    "title": "10 Things to Know About Mechanisms",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Hsiang, Meng, and Cane (2011).↩︎\nSee Gurr (1970).↩︎\nNote that Chong et al. (2015) test their argument using data at the precinct level, not the individual level, but we’ve adapted their argument to the individual level for ease of exposition.↩︎\nFor a more rigorous discussion of these fallacies, see Glynn (2012).↩︎\nExplanation adapted from Gerber and Green (2012), chapter 10.↩︎\nSee Levy (1966).↩︎\nSee Fanon (1964); Lott Jr. (1999).↩︎\nSee Almond and Verba (1963); Mattes and Bratton (2007).↩︎\nIn the actual study, the authors were surprised to uncover evidence that education also increased individuals’ acceptance of political violence. While they still argue that individual empowerment is responsible for the relationship between education and democracy, they caution that education does not always lead to democratization (that is, M3→Y but it is also possible that M3→NOT Y). Nonetheless, their approach is a useful demonstration of how multiple outcomes may shed light on mechanisms.↩︎"
  },
  {
    "objectID": "guides/research-questions/heterogeneous-effects_en.html",
    "href": "guides/research-questions/heterogeneous-effects_en.html",
    "title": "10 Things to Know About Heterogeneous Treatment Effects",
    "section": "",
    "text": "This guide discusses methods for analyzing heterogeneous treatment effects: testing for heterogeneity, estimating subgroup treatment effects and their differences, and addressing the pitfalls of multiple comparisons and ad hoc specification search.1"
  },
  {
    "objectID": "guides/research-questions/heterogeneous-effects_en.html#footnotes",
    "href": "guides/research-questions/heterogeneous-effects_en.html#footnotes",
    "title": "10 Things to Know About Heterogeneous Treatment Effects",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis guide draws heavily from Gerber and Green (2012) and from Don Green’s course notes for Experimental Methods at Columbia University.↩︎\nThis is known as the Fundamental Problem of Causal Inference. For more background, see 10 Things You Need to Know About Causal Inference.↩︎\nFor further reading, see Gerber and Green (2012) and Ding, Feller, and Miratrix (2016).↩︎\nFor more background and a range of views on the multiple comparisons problem, see, e.g.: 10 Things You Need to Know About Multiple Comparisons; Cook and Farewell (1996); Schulz and Grimes (2005a); Schulz and Grimes (2005b); Anderson (2008); Westfall, Tobias, and Wolfinger (2011); Gelman, Hill, and Yajima (2012).↩︎\nThe Westfall–Young method’s ability to control the FWER depends on a “subset pivotality” assumption that may be violated when outcomes are heteroskedastic or when there are multiple treatment arms. Westfall, Tobias, and Wolfinger (2011) (p. 421) write: “However, this theoretical shortcoming is only rarely a practical one for continuously distributed data. Experience shows that this issue is most likely to arise in cases with extreme heteroscedasticity and unbalanced sample sizes. … These issues can become even more problematic when testing binary data.” See also Bretz, Hothorn, and Westfall (2011), pp. 133–137. Bootstrap methods that relax the subset pivotality assumption are discussed in: Romano and Wolf (2005a); Romano and Wolf (2005b); Romano and Wolf (2016); List, Shaikh, and Xu (2016).↩︎\nThis description of the algorithm is adapted from Anderson (2008) and Gubits et al. (2014).↩︎\nSee Pollard, Dudoit, and Laan (n.d.); Dudoit and Laan (2008).↩︎\nStrictly speaking, to guarantee that the Benjamini–Hochberg procedure controls the FDR, we need to assume either that the \\(p\\)-values corresponding to the true null hypotheses are independent or that they obey a positive dependence condition. For a brief overview of work addressing dependence, see section 3.2 of Benjamini (2010).↩︎\nSee, for example, Imai and Ratkovic (2013).↩︎\nSee Chipman, George, and McCulloch (2010); Hill (2011); Green and Kern (2012).↩︎\nSee Hainmueller and Hazlett (2013).↩︎\nSee Laan, Polley, and Hubbard (2007); Grimmer, Messing, and Westwood (2014).↩︎\nFor further reading (at an advanced technical level), see Vansteelandt and Goetghebeur (2003); Vansteelandt and Goetghebeur (2004); Vansteelandt (2010); Stephens, Keele, and Joffe (2016).↩︎"
  },
  {
    "objectID": "guides/interpretation/regression-table_en.html",
    "href": "guides/interpretation/regression-table_en.html",
    "title": "10 Things to Know About Reading a Regression Table",
    "section": "",
    "text": "This guide gives basic information to help you understand how to interpret the results of ordinary least squares (OLS) regression in social science research. The guide focuses on regression but also discusses general concepts such as confidence intervals.\nThe table below that will be used throughout this methods guide is adapted from a study done by EGAP members Miriam Golden, Eric Kramon and their colleagues (Asunka et al. 2013). The authors performed a field experiment in Ghana in 2012 to test the effectiveness of domestic election observers on combating two common electoral fraud problems: ballot stuffing and overvoting. Ballot stuffing occurs when more ballots are found in a ballot box than are known to have been distributed to voters. Overvoting occurs when more votes are cast at a polling station than the number of voters registered. This table reports a multiple regression (this is a concept that will be further explained below) from their experiment that explores the effects of domestic election observers on ballot stuffing. The sample consists of 2,004 polling stations."
  },
  {
    "objectID": "guides/interpretation/regression-table_en.html#standard-error",
    "href": "guides/interpretation/regression-table_en.html#standard-error",
    "title": "10 Things to Know About Reading a Regression Table",
    "section": "Standard Error",
    "text": "Standard Error\nThe standard error (SE) is an estimate of the standard deviation of an estimated coefficient.3 It is often shown in parentheses next to or below the coefficient in the regression table. It can be thought of as a measure of the precision with which the regression coefficient is estimated. The smaller the SE, the more precise is our estimate of the coefficient. SEs are of interest not so much for their own sake as for enabling the construction of confidence intervals (CIs) and significance tests. An often-used rule of thumb is that when the sample is reasonably large, the margin of error for a 95% CI is approximately twice the SE. However, explicit CI calculations are preferable. We discuss CIs in more detail in the next section.\nThe table above from Asunka et al. (2013) shows “robust” standard errors, which have attractive properties in large samples because they remain valid even when some of the regression model assumptions are violated. The key assumptions that “conventional” or “classical” SEs make and robust SEs relax are that (1) the expected value of Y, given X, is a linear function of X, and (2) the variance of Y does not depend on X (conditional homoskedasticity). Robust SEs do assume (unless they are “clustered”) either that the observations are statistically independent or that the treatment was randomly assigned to the units of observation (the polling stations in this example).4"
  },
  {
    "objectID": "guides/interpretation/regression-table_en.html#t-statistic",
    "href": "guides/interpretation/regression-table_en.html#t-statistic",
    "title": "10 Things to Know About Reading a Regression Table",
    "section": "t-Statistic",
    "text": "t-Statistic\nThe t-statistic (in square brackets in the example table) is the ratio of the estimated coefficient to its standard error. T-statistics usually appear in the output of regression procedures but are often omitted from published regression tables, as they’re just a tool for constructing confidence intervals and significance tests."
  },
  {
    "objectID": "guides/interpretation/regression-table_en.html#p-values-and-significance-tests",
    "href": "guides/interpretation/regression-table_en.html#p-values-and-significance-tests",
    "title": "10 Things to Know About Reading a Regression Table",
    "section": "p-Values and Significance Tests",
    "text": "p-Values and Significance Tests\nIn the table above, if an estimated coefficient (in bold) is marked with one or more asterisks,5 that means the estimate is “statistically significant” at the 1%, 5%, or 10% level—in other words, the p-value (from a two-sided test6 of the null hypothesis that the true coefficient is zero) is below 0.01, 0.05, or 0.1.\nTo calculate a p-value, we typically assume that the data on which you run your regression are a random sample from some larger population. We then imagine that you draw a new random sample many times and run your regression for every new sample. (Alternatively, we may imagine randomly assigning some treatment many times. See our guide on hypothesis testing for more details.) This procedure would create a distribution of estimates and t-statistics. Given this distribution, the p-value captures the probability that the absolute value of the t-statistic would have been at least as large as the value that you actually observed if the true coefficient were zero. If the p-value is greater than or equal to some conventional threshold (such as 0.05 or 0.1), the estimate is “not statistically significant” (at the 5% or 10% level). According to convention, estimates that are not statistically significant are not considered evidence that the true coefficient is nonzero.\nIn the table, the only estimated coefficient that is statistically significant at any of the conventional levels is the intercept (which is labeled “Constant/Intercept” because in the algebra of regression, the intercept is the coefficient on the constant 1). The intercept is the predicted value of the outcome when the values of the explanatory variables are all zero. In this example, the question of whether the true intercept is zero is of no particular interest, but the table reports the significance test for completeness. The research question is about observer effects on ballot stuffing (as shown in the heading of the table). The estimated coefficient on “Observer Present (OP)” is of main interest, and it is not statistically significant.\nIt is easy to misinterpret p-values and significance tests. Many scholars believe that although significance tests can be useful as a restraining device, they are often overemphasized. Helpful discussions include the American Statistical Association’s 2016 statement on p-values; the invited comments on the statement, especially Greenland et al. (2016); and short posts by David Aldous and Andrew Gelman."
  },
  {
    "objectID": "guides/interpretation/regression-table_en.html#f-test-and-degrees-of-freedom",
    "href": "guides/interpretation/regression-table_en.html#f-test-and-degrees-of-freedom",
    "title": "10 Things to Know About Reading a Regression Table",
    "section": "F-Test and Degrees of Freedom",
    "text": "F-Test and Degrees of Freedom\nThe bottom section of the table includes a row with the heading “F(5, 59)”, the value 1.43 (the F-statistic), and the p-value .223. This F-test is a test of the null hypothesis that the true values of the regression coefficients, excluding the intercept, are all zero. In other words, the null hypothesis is that none of the explanatory variables actually help predict the outcome. In this example, the p-value associated with the F-statistic is 0.223, so the null hypothesis is not rejected at any of the conventional significance levels. However, since our main interest is in the effects of observers, the F-test isn’t of much interest in this application. (We already knew that the estimated coefficient on “Observer Present” is not statistically significant, as noted above.)\nThe numbers 5 and 59 in parentheses are the degrees of freedom (df) associated with the numerator and denominator in the F-statistic formula. The numerator df (5) is the number of parameters that the null hypothesis claims are zero. In this example, those parameters are the coefficients on the 5 explanatory variables shown in the table. The denominator df (59) equals the sample size minus the total number of parameters estimated. (In this example, the sample size is 2,004 and there are only 6 estimated parameters shown in the table, but the regression also included many dummy variables for constituencies that were used in blocking.)"
  },
  {
    "objectID": "guides/interpretation/regression-table_en.html#footnotes",
    "href": "guides/interpretation/regression-table_en.html#footnotes",
    "title": "10 Things to Know About Reading a Regression Table",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor in-depth discussions, see: Angrist and Pischke (2009), chapters 3 and 8; Berk et al. (2014); Buja et al. (2015); Hansen (2022)↩︎\nOn regression in observational studies, see: 10 Strategies for Figuring out if X Caused Y; Freedman (1991); Angrist and Pischke (2015); Angrist and Pischke (2009); Imbens and Wooldridge (2009); Imbens (2015). On regression adjustment in randomized experiments, see 10 Things to Know About Covariate Adjustment and Winston Lin’s Development Impact blog posts (here and here).↩︎\nStrictly speaking, the true SE is the standard deviation of the estimated coefficient, while what we see in the regression table is the estimated SE. However, in common parlance, people often say “standard error” when they mean the estimated SE, and we’ll do the same.↩︎\nRobust SEs are also known as Huber–White or sandwich SEs. On the properties of robust SEs, see: Angrist and Pischke (2009), section 3.1.3 and chapter 8; Imbens and Kolesár (2016); Reichardt and Gollog (1999); Samii and Aronow (2012); Lin (2013); Abadie et al. (2014).↩︎\nThe use of asterisks to flag statistically significant results is common but not universal. Our intention here is merely to explain what the asterisks mean, not to recommend that they should or should not be used.↩︎\nTwo-sided tests are the default in most software packages and in some research fields, so when tables do not explicitly note whether the p-values associated with regression coefficients are one- or two-sided, they are usually two-sided. Olken (2015) (pp. 67, 70) notes that since “convention typically dictates two-sided hypothesis tests,” researchers who prefer one-sided tests should commit to that choice in a pre-analysis plan so “they cannot be justly accused of cherry-picking the test after the fact.” Greenland et al. (2016) (p. 342) argue against the view that one should always use two-sided p-values, but write, “Nonetheless, because two-sided P values are the usual default, it will be important to note when and why a one-sided P value is being used instead.”↩︎\nThe framework where “the only thing that varies from one replication to another is which units are randomly assigned to treatment” is known as randomization-based inference. This isn’t the only framework for frequentist inference. In the classical regression framework, the only thing that varies is that on each replication, different values of ε are randomly drawn. And in the random sampling framework, on each replication a different random sample is drawn from the population. On randomization-based inference, see Reichardt and Gollog (1999), Samii and Aronow (2012), Lin (2013), and Abadie et al. (2014); on the random sampling framework, see the references in note 2.↩︎\nSee, e.g., Humphreys, Sanchez de la Sierra, and Windt (2013).↩︎\nSimmons, Nelson, and Simonsohn (2011).↩︎\nGelman and Loken (2013).↩︎\nSee Humphreys, Sanchez de la Sierra, and Windt (2013), Monogan (2013), Anderson (2013), and Gelman (2013).↩︎\nSee Olken (2015) and Coffman and Niederle (2015).↩︎\nFor more discussion of attrition bias in randomized experiments, see, e.g., chapter 7 of Gerber and Green (2012).↩︎\nSee also: Angrist and Pischke (2009), section 3.2.3; Rosenbaum (1984).↩︎\nRosenthal (1979). On reforms to counter publication bias, see: Nyhan (2015); Findley et al. (2016).↩︎\nWeighting by the inverse of the variance of ε is a form of generalized least squares (GLS). The classical argument is that GLS is more efficient (i.e., has lower variance) than OLS under heteroskedasticity. However, when the goal is to estimate an average treatment effect, some researchers question the relevance of the classical theory, because if treatment effects are heterogeneous, GLS and OLS are not just more efficient and less efficient ways to estimate the same treatment effect. Instead, they estimate different weighted average treatment effects. In other words, they answer different questions, and choosing GLS for efficiency is arguably like looking for your keys where the light’s better. Angrist and Pischke (2010) write: “Today’s applied economists have the benefit of a less dogmatic understanding of regression analysis. Specifically, an emerging grasp of the sense in which regression and two-stage least squares produce average effects even when the underlying relationship is heterogeneous and/or nonlinear has made functional form concerns less central. The linear models that constitute the workhorse of contemporary empirical practice usually turn out to be remarkably robust, a feature many applied researchers have long sensed and that econometric theory now does a better job of explaining. Robust standard errors, automated clustering, and larger samples have also taken the steam out of issues like heteroskedasticity and serial correlation. A legacy of White (1980)’s paper on robust standard errors, one of the most highly cited from the period, is the near death of generalized least squares in cross-sectional applied work. In the interests of replicability, and to reduce the scope for errors, modern applied researchers often prefer simpler estimators though they might be giving up asymptotic efficiency.” Similarly, Stock (2010) comments: “The 1970s procedure for handling potential heteroskedasticity was either to ignore it or to test for it, to model the variance as a function of the regressors, and then to use weighted least squares. While in theory weighted least squares can yield more statistically efficient estimators, modeling heteroskedasticity in a multiple regression context is difficult, and statistical inference about the effect of interest becomes hostage to the required subsidiary modeling assumptions. White (1980)’s important paper showed how to get valid standard errors whether there is heteroskedasticity or not, without modeling the heteroskedasticity. This paper had a tremendous impact on econometric practice: today, the use of heteroskedasticity-robust standard errors is standard, and one rarely sees weighted least squares used to correct for heteroskedasticity.” (Emphasis added in both quotations.)↩︎\nLogit, probit, and other limited dependent variable (LDV) models do not immediately yield estimates of the average treatment effect (ATE). To estimate ATE, one needs to compute an average marginal effect (or average predictive comparison) after estimating the LDV model (see, e.g., Gelman and Pardoe (2007)). Some researchers argue that the complexity of marginal effect calculations for LDV models is unnecessary because OLS tends to yield similar ATE estimates (see Angrist and Pischke (2009), section 3.4.2, and the debate between Angrist and his discussants in Angrist (2001)). In randomized experiments, the robustness of OLS is supported by both asymptotic theory and simulation evidence. For theory, see Lin (2013). For simulations, see Humphreys, Sanchez de la Sierra, and Windt (2013) and Judkins and Porter (2016). See also Lin’s comments on this MHE blog post.↩︎"
  },
  {
    "objectID": "guides/interpretation/null-results_en.html",
    "href": "guides/interpretation/null-results_en.html",
    "title": "10 Things Your Null Result Might Mean",
    "section": "",
    "text": "After the excitement and hard work of running a field experiment is over, it’s not uncommon to hear policymakers and researchers express disappointment when they end up hearing that the intervention did not have a detectable impact. This guide explains that a null result rarely means “the intervention didn’t work,” even though that tends to be the shorthand many people use. Instead, a null result can reflect the myriad design choices that policy implementers and researchers make in the course of developing and testing an intervention. After all, people tend to label hypothesis tests with high p-values as “null results”, and hypothesis tests (as summaries of information about design and data) can produce large p-values for many reasons. Policymakers can make better decisions about what to do with a null result when they understand how and why they got that result.\nImagine you lead the department of education for a government and are wondering about how to boost student attendance. You decide to consider a text message intervention that offers individual students counseling. Counselors at each school can help students address challenges specifically related to school attendance. Your team runs a randomized trial of the intervention, and tells you there is a null result.\nHow should you understand the null result, and what should you do about it? It could be a result of unmet challenges at several stages of your work – in the way the intervention is designed, the way the intervention is implemented, or the way study is designed Below are 10 things to consider when interpreting your null result."
  },
  {
    "objectID": "guides/interpretation/null-results_en.html#your-intervention-theory-and-approach-are-mismatched-to-the-problem.",
    "href": "guides/interpretation/null-results_en.html#your-intervention-theory-and-approach-are-mismatched-to-the-problem.",
    "title": "10 Things Your Null Result Might Mean",
    "section": "1. Your intervention theory and approach are mismatched to the problem.",
    "text": "1. Your intervention theory and approach are mismatched to the problem.\nYou delivered a counseling intervention because you thought that students needed support to address challenges in their home life. However, students who had the greatest needs never actually met with a counselor, in part because they did not trust adults at the school. The theory of change assumed that absenteeism was a function primarily of a student’s personal decisions or family circumstances and that the offer of counseling without changes to school climate would be sufficient; it did not account appropriately for low levels of trust in teacher-student relationships. Therefore, this null effect does not suggest that counseling per se cannot boost attendance, but that counseling in the absence of other structural or policy changes or in the context of low-trust schools may not be sufficient.\nHow can you tell if…you have a mismatch between your theory of change and the problem that needs to be solved? List all potential barriers and consider how they connect. Does the intervention as designed address only one of those barriers, and, if so, can it succeed without addressing others? Are there assumptions made about one source or one cause that may undermine the success of the intervention?"
  },
  {
    "objectID": "guides/interpretation/null-results_en.html#your-intervention-strength-or-dosage-is-too-low-for-the-problem-or-outcome-of-interest.",
    "href": "guides/interpretation/null-results_en.html#your-intervention-strength-or-dosage-is-too-low-for-the-problem-or-outcome-of-interest.",
    "title": "10 Things Your Null Result Might Mean",
    "section": "2. Your intervention strength or dosage is too low for the problem or outcome of interest.",
    "text": "2. Your intervention strength or dosage is too low for the problem or outcome of interest.\nAfter talking to experts, you learn that counseling interventions can build trust, but usually require meetings that are more frequent and regular than your intervention offered to have the potential for an effect. Maybe your “dose” of services is too small.\nHow can you tell if…you did not have a sufficient “dose”? Even if no existing services tackle your problem of interest, consider what is a minimum level, strength, or dose that is both feasible to implement and could yield an effect. When asking sites what they are willing to take on, beware of defaulting to the lowest dose. The more complex the problem or outcome is to move, the stronger or more comprehensive the intervention may need to be."
  },
  {
    "objectID": "guides/interpretation/null-results_en.html#your-intervention-does-not-represent-a-large-enough-enhancement-over-usual-services.",
    "href": "guides/interpretation/null-results_en.html#your-intervention-does-not-represent-a-large-enough-enhancement-over-usual-services.",
    "title": "10 Things Your Null Result Might Mean",
    "section": "3. Your intervention does not represent a large enough enhancement over usual services.",
    "text": "3. Your intervention does not represent a large enough enhancement over usual services.\nIn your position at the state department of education, you learn that students at the target schools were already receiving some counseling and support services. Even though the existing services were not sufficient to boost attendance to the targeted levels, the new intervention did not add enough content or frequency of the counseling services to reach those levels either—the intervention yielded show-up rates that were about the same as existing services. So this null effect does not reflect that counseling has no effect, but rather that the version of counseling your intervention offered was not effective over and above existing counseling services.\nHow can you tell if…the relative strength of your intervention was not sufficient to yield an effect? Take stock of the structure and content of existing services, and consider if the extent or form in which clients respond to existing services indicates that the theory of change or approach needs to be revised. If the theory holds, use existing services as a benchmark and consider whether your proposed intervention needs to include something supplementary and/or something complementary."
  },
  {
    "objectID": "guides/interpretation/null-results_en.html#your-implementation-format-was-not-reliable.",
    "href": "guides/interpretation/null-results_en.html#your-implementation-format-was-not-reliable.",
    "title": "10 Things Your Null Result Might Mean",
    "section": "4. Your implementation format was not reliable.",
    "text": "4. Your implementation format was not reliable.\nIn the schools in your study, counseling interventions sometimes occurred in person, sometimes happened by text message, sometimes by phone. Anticipating and allowing for some variation and adaptation is important. Intervention dosage and strength is often not delivered as designed nor to as many people as expected.\nBut unplanned variations in format can reflect a host of selection bias issues, such that you cannot disentangle whether counseling as a concept does not work or whether certain formats of outreach did not work. This is especially important to guard against if you intend to test specific channels or mechanisms critical to your theory of change.\nHow can you tell if…an unreliable format is the reason for your null? Were you able to specify or standardize formats in a checklist? Could you leave enough discretion but still incentivize fidelity? Pre-specifying what the intervention should look like can help staff and researchers monitor along the way and correct inconsistencies or deviations that may affect the results. This could include a training protocol for those implementing the intervention. If nothing was specified or no one was trained, then the lack of consistency may be part of the explanation."
  },
  {
    "objectID": "guides/interpretation/null-results_en.html#your-intervention-and-outcome-measure-are-mismatched-to-your-randomization-design.",
    "href": "guides/interpretation/null-results_en.html#your-intervention-and-outcome-measure-are-mismatched-to-your-randomization-design.",
    "title": "10 Things Your Null Result Might Mean",
    "section": "5. Your intervention and outcome measure are mismatched to your randomization design.",
    "text": "5. Your intervention and outcome measure are mismatched to your randomization design.\nYou expected counseling to be more effective in schools with higher student-to-teacher ratios, but did not block randomize by class size (for more on block randomization, see our guide on 10 Things to Know About Randomization). then it may no longer have the potential to be more effective for students in high class size schools.\nHow can you tell if…you have a mismatch between your intervention and randomization design? Consider whether treatment effects could vary, or service delivery might occur in a cluster, or intervention concepts could spill over, and to what extent your randomization design accounted for that."
  },
  {
    "objectID": "guides/interpretation/null-results_en.html#your-study-sample-includes-people-whose-behavior-could-not-be-moved-by-the-intervention.",
    "href": "guides/interpretation/null-results_en.html#your-study-sample-includes-people-whose-behavior-could-not-be-moved-by-the-intervention.",
    "title": "10 Things Your Null Result Might Mean",
    "section": "6. Your study sample includes people whose behavior could not be moved by the intervention.",
    "text": "6. Your study sample includes people whose behavior could not be moved by the intervention.\nYou ask schools to randomize students into an intervention or control (business-as-usual) group. Some students in both your intervention and control groups will always attend school, while some students will rarely attend school, regardless of what interventions are or are not offered to them. Your intervention’s success depends on not just whether students actually receive the message and/or believe it, but also on whether it can shift behavior among such potential responders.\nIf the proportion of potential responders is too small, then it may be difficult to detect an effect. In addition, your intervention may need to be targeted and modified in some way to address the needs of potential responders.\nHow can you tell if…the proportion of potential responders may be too small? Take a look at the pre-intervention attendance rate. If it is extremely low, does that rate reflect low demand or structural barriers that may limit the potential for response? Is it so high that it tells us that most people who could respond have already done so (say, 85% or higher)? Even if there is a large proportion of hypothetical potential responders, is it lower when you consider existing barriers preventing students from using counseling services that your intervention is not addressing?"
  },
  {
    "objectID": "guides/interpretation/null-results_en.html#your-measure-is-not-validated-or-reliable-it-varies-too-much-and-systematically-across-sites.",
    "href": "guides/interpretation/null-results_en.html#your-measure-is-not-validated-or-reliable-it-varies-too-much-and-systematically-across-sites.",
    "title": "10 Things Your Null Result Might Mean",
    "section": "7. Your measure is not validated or reliable: It varies too much and systematically across sites.",
    "text": "7. Your measure is not validated or reliable: It varies too much and systematically across sites.\nAs a leader of the state’s department of education, you want to measure the effectiveness of your intervention using survey data on student attitudes related to attendance. You learn that only some schools administer a new survey measuring student attitudes, and those with surveys changed the survey items so that there is not the same wording across surveys or schools. If you observe no statistically significant difference on a survey measure that is newly developed or used by only select schools, it may be difficult to know whether the intervention “has no effect” or whether the outcome is measuring something different in each school because of different wording.\nHow can you tell if… outcome measurement is the problem? Check to see whether the outcome is (1) collected in the same way across your sites and (2) if it means the same thing to the participants as it means to you. In addition, check on any reporting bias and if your study participants or sites face any pressure from inside or outside of their organizations to report or answer in a particular way."
  },
  {
    "objectID": "guides/interpretation/null-results_en.html#your-outcome-is-not-validated-or-reliable-it-varies-too-little.",
    "href": "guides/interpretation/null-results_en.html#your-outcome-is-not-validated-or-reliable-it-varies-too-little.",
    "title": "10 Things Your Null Result Might Mean",
    "section": "8. Your outcome is not validated or reliable: It varies too little.",
    "text": "8. Your outcome is not validated or reliable: It varies too little.\nGiven the problems with the survey measure, you then decide to use administrative data from student records to measure whether students show up on time to school. But it turns out that schools used a generic definition of “on time” such that almost every student looks like they arrive on time. An outcome that does not have enough variation in it to detect an effect between intervention and control groups can be especially limiting if your intervention potentially could have had different effects on different types of students, but the outcome measure used in the study lacks the precision to capture the effects on different subgroups.\nHow can you tell if…your null result arises from measures that are too coarse or subject to response biases? Pressures to report a certain kind of outcome faced by people at your sites could again yield this kind of problem with outcome measurement. So, it is again worth investigating the meaning of the outcomes as reported by the sites from the perspective of those doing the reporting. This problem differs from the kind of ceiling and floor effects discussed elsewhere in this guide; it arises more from the strategic calculations of those producing administrative data and less from the natural behavior of those students whose behavior you are trying to change."
  },
  {
    "objectID": "guides/interpretation/null-results_en.html#your-statistical-power-is-insufficient-to-detect-an-effect-for-the-intervention-as-implemented.",
    "href": "guides/interpretation/null-results_en.html#your-statistical-power-is-insufficient-to-detect-an-effect-for-the-intervention-as-implemented.",
    "title": "10 Things Your Null Result Might Mean",
    "section": "9. Your statistical power is insufficient to detect an effect for the intervention as implemented.",
    "text": "9. Your statistical power is insufficient to detect an effect for the intervention as implemented.\nThis may sound obvious to people with experience testing interventions at scale. But researchers and policymakers can fall into two traps:\n\nThinking about statistical significance rather than what represents a meaningful and feasible effect. Although a study with an incredibly large sample size can detect small effects with precision, one does not want to trade precision for meaning. Moreover, an intervention known to be weak during the intervention design is likely to be weaker when implemented, especially across multiple sites or months. So it may not be sufficient to simply enroll more subjects to study an intervention known to be weak (even though strong research design cannot compensate for a weak intervention in any easy or direct way);\nThinking that the only relevant test statistic for an experiment effect is a difference of means (even though we have long known that differences of means are valid but low-powered test statistics when outcomes do not neatly fit into a normal distribution).\n\nHow can you tell if…your null result arises mostly from low statistical power? Recall that statistical power depends on (a) effect size or intervention strength, (b) variability in outcomes, (c) the number of independent observations (often well measured with sample size), and (d) the test statistic you use. The previous discussions pointed out ways to learn whether an intervention you thought might be strong was weak, or whether an outcome that you thought might be clear could turn out to be very noisy.\nA formal power analysis could also tell you that, given the variability in your outcome and the size of your effect, you would have needed a larger sample size to detect this effect reliably. For example, if you had known about the variability in administration of the treatment or the variability in the outcome (let alone surprises with missing data) in advance, your pre-field power analysis would have told you to use a different sample size.\nA different test statistic can also change a null result into a positive result if, say, the effect is large but it is not an effect that shifts means as much as moves people who are extreme, or has the effect of making moderate students extreme. A classic example of this problem occurs with outcomes that have very long tails – such as those involving money, like annual earnings or auction spending. A t-test might produce a p-value of .20 but a rank-based test might produce a p-value of &lt; .01. The t-test is using evidence of a shift in averages (means) to reflect on the null hypothesis of no effects. The rank-based test is merely asking whether the treatment group outcomes tend to be bigger than (or smaller than) the control group outcomes (whether or not they differ in means)."
  },
  {
    "objectID": "guides/interpretation/null-results_en.html#your-null-needs-to-be-published.",
    "href": "guides/interpretation/null-results_en.html#your-null-needs-to-be-published.",
    "title": "10 Things Your Null Result Might Mean",
    "section": "10. Your null needs to be published.",
    "text": "10. Your null needs to be published.\nIf you addressed all the issues above related to intervention design, sample size and research design, and have a precisely estimated, statistically significant null result, it is time to publish. Your colleagues and other researchers need to learn from this finding, so do not keep it to yourself.\nWhen you have a precise null, you do not have a gap in evidence–you are generating evidence.\nWhat can you do to convince editors and reviewers they should publish your null results? This guide should help you reason about your null results and thus explain their importance. If other studies on your topic exist, you can also contextualize your results; for example, follow some of the ideas from Abadie (2020).\nFor an example, see how Bhatti et al. (2018)–in their study of a Danish governmental voter turnout intervention–used previous work on face-to-face voter turnout (reported on as a meta-analysis in Bhatti et al. (2016)) to contextualize their own small effects.\nIf you are unable to find a publication willing to include a study with null results in their journal, you can still contribute to the evidence base on the policy area under examination by making your working paper, data, and/or analysis code publicly available. Many researchers choose to do so via their personal websites; in addition, there are repositories (such as the Open Science Framework) that provide a platform for researchers to share their in-progress and unpublished work."
  },
  {
    "objectID": "guides/assessing-designs/power_en.html",
    "href": "guides/assessing-designs/power_en.html",
    "title": "10 Things You Need to Know About Statistical Power",
    "section": "",
    "text": "This guide will help you assess and improve the power of your experiments. We focus on the big ideas and provide examples and tools that you can use in R and Google Spreadsheets."
  },
  {
    "objectID": "guides/assessing-designs/power_en.html#footnotes",
    "href": "guides/assessing-designs/power_en.html#footnotes",
    "title": "10 Things You Need to Know About Statistical Power",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nReproduced from Gerber and Green (2012), page 93.↩︎\nFor an additional online power visualization tool, see Kristoffer Magnusson’s R Psychologist blog.↩︎"
  },
  {
    "objectID": "guides/assessing-designs/external-validity_en.html",
    "href": "guides/assessing-designs/external-validity_en.html",
    "title": "10 Things You Need to Know About External Validity",
    "section": "",
    "text": "After months or years under development and implementation, navigating the practical, theoretical and inferential pitfalls of experimental social science research, your experiment has finally been completed. Comparing the treatment and control groups, you find a substantively and statistically significant result on an outcome of theoretical interest. Before you can pop the champagne in celebration of an intervention well evaluated, a friendly colleague asks: “But what does this tell us about the world?”"
  },
  {
    "objectID": "guides/assessing-designs/external-validity_en.html#footnotes",
    "href": "guides/assessing-designs/external-validity_en.html#footnotes",
    "title": "10 Things You Need to Know About External Validity",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Shadish, Cook, and Campbell (2002).↩︎\nSee Campbell (1957).↩︎\nMore details can be found in the causal inference methods guide.↩︎\nSee Gerber and Green (2012).↩︎\nSee Bisbee et al. (2016).↩︎\nSee Bisbee et al. (2016)↩︎\nSee Kern et al. (2016)↩︎"
  },
  {
    "objectID": "guides/implementation/evaluation-conversations_en.html",
    "href": "guides/implementation/evaluation-conversations_en.html",
    "title": "10 Conversations that Implementers and Evaluators Need to Have",
    "section": "",
    "text": "If you are a practitioner or official from a governmental or non-governmental organization (an \"implementer\"), this guide is intended to help you to take advantage of opportunities to collaborate with external researchers (\"evaluators\") to evaluate your organization's policies or programs. Researchers working on evaluations can also benefit from this guide by understanding how to take implementers' needs into account.\nAs an implementer, what should you expect or request when collaborating with an external evaluator? How can that evaluation be designed to help your organization and the broader field to learn and innovate? What conversations should you have with an evaluator upfront so that the evaluation runs smoothly? How can you work with the evaluator to decide on and communicate actionable and convincing analyses about the impact of a program? How will you and the evaluator share what you learned to improve the practice of your organization and others? How can you ensure that what you learn helps the whole community of practice so that overall social welfare improves? This short guide offers questions to guide conversations with prospective evaluators, points to some practices that have worked well elsewhere, and explains why they might be important or useful in future evaluations.1"
  },
  {
    "objectID": "guides/implementation/evaluation-conversations_en.html#footnotes",
    "href": "guides/implementation/evaluation-conversations_en.html#footnotes",
    "title": "10 Conversations that Implementers and Evaluators Need to Have",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMuch of this document is inspired by the Project Process of the OES as well as discussions hosted by the Causal Inference for Social Impact project at CASBS and the Evidence in Governance and Politics network. See the MIT Gov Lab Guide to Difficult Conversations for more guidance about academic-practitioner collaborations as well as the Research4Impact findings about cross-sector collaborations. We anticipate that this document will be open source and revised over time based on your comments and suggestions. Thanks much to Carrie Cihak, Matt Lisiecki, Ruth Ann Moss, Betsy Rajala, Cyrus Samii, Rebecca Thornton,, and folks at the organizations listed above for helpful comments.↩︎\nWe do recognize that some donors may not allow this (though many are becoming proponents of it), and the competitive nature of fundraising may make sharing data seem risky, especially as a first mover.↩︎"
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html",
    "href": "guides/implementation/survey-implementation_en.html",
    "title": "10 Things to Know About Survey Implementation",
    "section": "",
    "text": "There are two discrete skill sets needed on the ground when implementing a survey. The first skill set is focused on administration/logistics, and the second skill set is focused on research design. The first set of skills is needed for administration: budgeting, creating route plans, recruitment, and management of staff. Administration requires a level of familiarity with local conditions; for example, the ability to quickly estimate costs and troubleshoot logistical issues are important here. The second skill set requires knowledge of research methods to ensure that survey implementation is consistent with the research design. Researchers must be able to recognize any deviations from the protocol and address them in a way that leads to as little bias as possible. Important here is a deep understanding of the survey protocols and possible alternatives, in the case that changes need to be made on the ground.\nIf you will not be present during survey administration, you will need to either hire a firm or individuals who can work together to cover both sets of needs. There are clear advantages to hiring a firm if you have the budget, the biggest being that firms coordinate internally and balance both sets of needs, ensuring that logistics accommodate the design and vice versa. A possible drawback is that firms frequently have their own protocols, and these default procedures are usually at a lower standard than the latest protocol being used in academia. Upgrading protocols is a costly process and firms may push back against the use of stricter, or simply different, practices.\nIf you will be present during enumeration but you have a small budget, or do not feel you could manage the entire implementation (administration and design) yourself, a good alternative is to hire a field coordinator from a survey or research firm on a consultant basis. This person can help with administration while you take on the design-related work. Additionally, hiring someone for administration locally can do a lot to help with cross-cultural management. The types of management procedures that might work to motivate or sanction employees in the US may not work in another context, so someone who knows what is acceptable and effective can add a lot of value.\n\n\nWhen setting up contracts with local firms it is important to get the incentives right—thorough and good work should also be the most profitable for the firm. You can do a lot to set expectations and incentives in the contract. For example, pay on delivery where possible (although it is customary to pay some costs upfront to cover fixed expenses like transport and early salaries). You can also choose to impose financial penalties for late or low-quality data, but be sure to make these requirements clear up front and provide specific rules for what constitutes low-quality work and how late penalties will be assigned.\nIn addition to direct costs, it is reasonable for a local firm to charge overhead. This can vary from context to context, and it is best to check against the budgets of other similar projects to make sure the rate is reasonable."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#contracts-with-local-firms",
    "href": "guides/implementation/survey-implementation_en.html#contracts-with-local-firms",
    "title": "10 Things to Know About Survey Implementation",
    "section": "",
    "text": "When setting up contracts with local firms it is important to get the incentives right—thorough and good work should also be the most profitable for the firm. You can do a lot to set expectations and incentives in the contract. For example, pay on delivery where possible (although it is customary to pay some costs upfront to cover fixed expenses like transport and early salaries). You can also choose to impose financial penalties for late or low-quality data, but be sure to make these requirements clear up front and provide specific rules for what constitutes low-quality work and how late penalties will be assigned.\nIn addition to direct costs, it is reasonable for a local firm to charge overhead. This can vary from context to context, and it is best to check against the budgets of other similar projects to make sure the rate is reasonable."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#salaries",
    "href": "guides/implementation/survey-implementation_en.html#salaries",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Salaries",
    "text": "Salaries\nEstimating total salary costs before drawing the sample (needed in order to determine the teams and route plans) requires a bit of guesswork. One approach is to estimate the work-hours needed to conduct the survey (survey length x sample size) and divide by some estimated number of enumerators to come up with the number of enumerator days you will need to pay. The per diem may need to cover food and lodging, and make this clear to enumerators so they can plan accordingly. For surveys that will require long fieldwork, it is good practice to pay salary on a rest day each week although some enumerators prefer to work continuously in order to finish sooner and return home. This choice is context-specific."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#per-diems",
    "href": "guides/implementation/survey-implementation_en.html#per-diems",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Per Diems",
    "text": "Per Diems\nPer diems cover enumerator’s expenses associated with doing fieldwork. This means lodging for overnight stays, all meals, and sometimes also transportation. Per diems should also be paid on rest days that fall in between work days. In the case that the variation in lodging and food costs is low, it is not important to change the per diem rate according to location. Teams will know when to save and when to spend.\n\nQuick calculator:\n((survey time to complete * sample size)/workable hours in a day)/# of enumerators = number of days\nnumber of days * (daily rate + per diem) + supervisors = approx. total salary cost"
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#transportation",
    "href": "guides/implementation/survey-implementation_en.html#transportation",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Transportation",
    "text": "Transportation\nIt’s important to, ex ante, be as accurate as possible in estimating the full cost of transportation as this is frequently both least flexible and most variable cost. Typically, it is good practice to build in contingency on the cost of fuel, as the price can change over the several months it takes to go from the grant application stage to the implementation stage. If you are budgeting before drawing your sample, pay particular attention to hard-to-reach areas in your population (islands, places without road access) and pad your transport line for the possibility you randomly sample enumeration areas that carry these higher costs."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#equipment",
    "href": "guides/implementation/survey-implementation_en.html#equipment",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Equipment",
    "text": "Equipment\nLater on in this guide we present the benefits of using personal digital assistants (PDAs) or tablets for data collection (see section 3). PDAs/tablets can be either purchased using survey funds or leased from a research firm, university, or other researchers."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#why-personal-digital-assistants-pdas-or-tablets-are-better-if-you-have-the-budget",
    "href": "guides/implementation/survey-implementation_en.html#why-personal-digital-assistants-pdas-or-tablets-are-better-if-you-have-the-budget",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Why Personal Digital Assistants [PDAs] or Tablets are better (if you have the budget)",
    "text": "Why Personal Digital Assistants [PDAs] or Tablets are better (if you have the budget)\nUse of a PDA/tablet allows the collection of more accurate and detailed data (Goldstein, 2012) because of:\n\nAutomated skip patterns\nMore detail, e.g. the ability to program a multi-stage code list\nThe ability to program some randomization algorithms (permuted block randomization, for example)\nSensitive questions can be recorded by the respondent themselves on the tablet (instead of the enumerator). There are even ways, using sound and video playback, to do this with illiterate respondents\nThe ability to audio record responses for later transcription\n\nPDAs/tablets have lower error rates than paper-based surveys (Caeyers, 2010) and have superior quality control options including:\n\nReal-time data upload\nReal-time survey modification in the case of error or oversight in terms of questions included\nAudio recording of portions of surveys to verify enumerator delivery\nTimers that measure how long respondents spend on the entire survey and each individual question\nReal-time validation checks to make sure numerical questions don’t have answers that are nonsensical\nThe ability to generate several orthogonal treatments within a single survey (either for multiple experiments or for conjoint experiments)"
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#tips-for-pda-or-tablet-use",
    "href": "guides/implementation/survey-implementation_en.html#tips-for-pda-or-tablet-use",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Tips for PDA or tablet use",
    "text": "Tips for PDA or tablet use\n\nAlways buy extra equipment – chargers, battery packs, power strips, and tablets can go missing, be stolen, or get broken. In many countries you can’t buy extra equipment, even in capital cities, and it’s often more expensive and lower quality than what you can get at your home base. Buying 10-20% more equipment than you need can be expensive, but it is usually far cheaper than the salaries that you will have to pay for enumerators with equipment problems who do not have backups.\nPay close attention to battery life when you buy your equipment. If you want full days of enumeration, some of the cheaper tablets will not work.\nBudget for extra battery packs for your enumerators to carry in the field, particularly if they will travel to rural areas where they may not always be able to charge the tablets every night.\nForecast how frequently teams will be able to upload recorded data. The PDAs/tablets need to be able to store data from completed interviews until uploading is possible. In rural environments this can mean quite a lot of memory is needed, particularly if the survey is long and/or complex.\nBudget extra costs for charging of the PDAs or tablets in the field (i.e. paying for extra generator time from hotels) and for potential delays because of lack of power.\nBudget extra time for exhaustive testing of the PDA/tablet once the survey is fully coded. Code failures can be disastrous the more complex the code/randomization becomes. Run through as many different responses to the survey as you can yourself, and do a “fake” pre-test during training in which you collect data and inspect it to make sure there are no errors.\nHave someone available to make on-the-spot changes to the code in case problems are discovered in the field that stall enumeration until the change is implemented. In addition, give your enumerators enough paper versions of the survey to last for one or more days to serve as a holdover until the code is remedied.\nIf possible, name your variables in the survey software to avoid a really laborious process of manually re-naming later. This also makes it easier to inspect the data in real-time.\nCode answer values (i.e. the values that will be outputted into the dataset) in advance so that you can standardize scales and easily clean the data (for example, use different negative numbers for “don’t know” and “refuse” options so a 10-character command in R can clean the whole dataset).\nTake advantage of having more space for text by giving enumerators directions for complicated items in the displayed question text itself."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#surveying",
    "href": "guides/implementation/survey-implementation_en.html#surveying",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Surveying",
    "text": "Surveying\nField teams are made up of enumerators and a team leader. Team leaders report to a field manager, or in a case of a large survey, a regional supervisor.\n\nThe enumerator’s role is sampling and selecting households and respondents within enumeration areas, gain consent, and conduct the interview. The enumerator’s tasks include:\n\nSelecting the household. For an enumerator this is the first stage in the random selection process and is done according to a clearly specified procedure, which should be easily referenced in both the manual and the survey instrument itself.\nSelecting respondents. Once the household has been selected, the enumerator should follow a similarly-specified random (or systematic) selection process in order to select the subject.\nConsent. Enumerators need to know the definition of informed consent and how to make sure the respondent understands his/her rights during the interview.\nConducting the interview. This will involve asking questions and closely following the instructions communicated in training and on the questionnaires. A hard copy of question-by-question instructions should be provided to enumerators for use as a reference.\nControlling the interview situation. The enumerator must work towards reducing or eliminating suspicion and prejudice within the interview environment. This may involve asking bystanders not to congregate or dealing with sensitive situations with respect to other family members within the home.\nAvoiding bias. The enumerator’s personal views must not be reflected in the data collected. This means, among other things, that the enumerator must remain neutral and respectful during the data collection by not expressing his or her opinion and ensuring that the respondent trusts that the enumerator will respect his or her privacy. Emphasize during training that there really is no right answer and that the goal of the survey is to find out what people really think – because without knowing that, we can’t find solutions to problems.\nPresentation. Interpersonal skills such as manners, dressing, body language and ability to persuade are all important for data quality and will help in obtaining the target respondents for each day.\n\nThe team leader manages a group of enumerators and can conduct interviews him/herself. The team leader is responsible for:\n\nLogistics. The team leader is responsible for organizing the transport of teams, gathering materials, transporting paper instruments, and managing the technology.\nPermissions. Team leaders make contact with local authorities to introduce and explain the survey and get permission to work.\nSupervising. Team Leaders ensure the team arrives on time to the enumeration area and proceeds to oversee within-enumeration area selection of households.\nCorrecting. Once interviewing has begun, team leaders should move between enumerators and check they are following protocols. Supervision should not, however, make subjects feel uncomfortable.\nData Quality. Team Leaders check all questionnaires in the field and at the end of the day. If PDAs/tablets are used either the team leader or an RA will check data. The team leader ensures that data errors are fixed by revisiting respondents."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#oversight",
    "href": "guides/implementation/survey-implementation_en.html#oversight",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Oversight",
    "text": "Oversight\nEnumeration teams and field management can quite easily deviate from important protocols—these deviations can range from replacing sampled households based on the ease of getting respondents to creating fake data. In many cases, cutting corners is not easy to detect and can save money and time for the enumerators, field managers, and even the survey research firm. PDAs/tablets can reduce the number of total possible types of fraud, but some level of field supervision is always necessary. A parallel reporting structure, with independent oversight, can help guard against these deviations.\nOn the oversight side, there are two types of checks that should be conducted– audits and backchecks:\n\nAuditors arrive at randomly selected villages on the days they are slated for enumeration, without advance warning to the team. When an auditor visits a team, they make sure the team is in the correct enumeration area, that they have sought consent from local leaders, complied with the household selection procedure, and that all team members are working. The auditor then tracks the performance of the team throughout the day— as they seek consent, build rapport, conduct within-enumeration area sampling, and survey respondents.\nA backcheck consists of a revisit to a respondent who completed a survey not more than a few days prior. Using the data collected, they locate respondents and verify responses on a few key questions, for which the response was not likely to have changed (for example, age or household size) in the period since they were initially contacted. Backchecking can both help to identify enumerators who are not performing and establish an error rate. If phone numbers are being collected in the survey, this can be done more cheaply by telephone.\n\nConducting both audits and backchecks means that for each individual survey there is some non-zero probability that the work will be checked in some way. In the case that there are only backchecks, teams will never be monitored in terms of their adherence to protocols as they sample and conduct interviews. In the case that there are only audits, if a team is not visited on a particular day of work there is no chance to check that they actually interviewed subjects and recorded their responses accurately.\nAuditors and backcheckers must report directly to survey management. Imagine an example: Say a village is difficult to find and the team of enumerators chooses a replacement (rather than resampling by the PIs), and the auditors visit the sampled village and uncover it was not surveyed. If this error is communicated to someone also managing the enumerators, their best response is to cover this up or try to fix it without the PI knowing. This prevents having to admit a management mistake, and having to add a day of work to revisit the original or resampled village. If the survey team is notified directly, there is an opportunity to fix the mistake and make personnel changes as needed. Unmonitored communication between auditing teams and enumeration teams can result in a lot of unauthorized fixes and unexplainable data patterns."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#sources",
    "href": "guides/implementation/survey-implementation_en.html#sources",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Sources",
    "text": "Sources\nIf you are working alone, recruit experienced enumerators through contacts at survey firms, NGOs, or universities. It is important that enumerators are experienced, literate, educated, and able to build rapport with subjects. Hiring enumerators who are connected, in some way, with the survey leaders or local coordinator, e.g. through a youth organization or other social tie, can help immensely with oversight as the enumerators have bigger reputational costs if they shirk their duties."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#languages",
    "href": "guides/implementation/survey-implementation_en.html#languages",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Languages",
    "text": "Languages\nThe foremost requirement is that the enumerators speak the required local languages. We know that coethnicity between enumerators and subjects can reduce bias, so recruitment of coethnic interviewers, and balancing across the sample if using treatment and control groups, is important."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#gender-parity",
    "href": "guides/implementation/survey-implementation_en.html#gender-parity",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Gender parity",
    "text": "Gender parity\nHaving a team of mostly male enumerators interview a sample with equal numbers of men and women there can introduce response bias. For sensitive questions, such as questions on sexual behavior or violence, it is strongly recommended that women interview other women. If it is difficult to recruit experienced women enumerators, it usually makes sense to hold a special training for women candidates with less experience in order to ensure teams are balanced in the end."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#standards",
    "href": "guides/implementation/survey-implementation_en.html#standards",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Standards",
    "text": "Standards\nTrainings establish consistent standards for data collection. If you are contracting a survey firm and are not on the ground yourself, training is the most important part of the process to personally attend. It’s a key moment to communicate quality standards, expectations, the intended meaning of each question, and teach important procedures that may be more technical than what the firm is used to, such as a list experiment. It is also a key moment to motivate the team, by communicating the project’s goals and importance. In order to ensure that the each member of the team is prepared to a certain standard, it is a good idea to test each team member at the end of the training period. There should be an expectation that some team members will be asked not to proceed any further with the project as a result of the test, which will emphasize the importance of taking training to heart and taking the test seriously."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#assessment",
    "href": "guides/implementation/survey-implementation_en.html#assessment",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Assessment",
    "text": "Assessment\nTrainings are a key moment for assessment as well. If you train teams together it is easy to spot management issues and leadership capabilities. A good practice is to train teams together, and select team leaders at the end of training—this gives you a few days to gauge skills and also incentivizes trainees to perform during the training."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#participation",
    "href": "guides/implementation/survey-implementation_en.html#participation",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Participation",
    "text": "Participation\nTrainings set the tone for the rest of fieldwork. Beyond communicating standards and expectations, this is also a key moment to create a culture of participation. Encouraging trainees to speak out about issues with the survey can show that you are open to feedback and increase the chances that they will report adverse events or challenges during the actual data collection.\nTraining sections:\n\nProtocol review, e.g. how to get permission to work in village/household, how to select respondent, etc.\nDiscussion of unforeseen contingencies, e.g. what happens when you get a refusal, when enumerators are targeted or threatened, when enumerators observe perverse reactions to or consequences of the survey\nQuestion by question review\nInterviewing techniques (rapport building, discussions)\nReview translated instrument\nUsing a PDA/tablet and conducting the interview on a PDA/tablet\nUsing GPS\nGroup practice (enumerators interview each other and get feedback)\nField practice– at least one day of training (or more for complicated or long surveys) should be spent interviewing real people who are similar to the survey subjects\nCertification exam\n\nTraining usually takes several more days than you expect. See below for a rough guide to realistic training schedules.\n\nList of documents needed for training:\n\nInstrument + translated instrument\nQuestion-by-question guide\nManual with expectations, instructions for filling responses, tips on interviewing etc.\nProtocol for sampling, consent, reporting, and unforeseen contingencies\n\nBefore being deployed to the field, each enumerator must:\n\nBe able to correctly list, sample and interview individuals in the enumeration area\nUnderstand their role\nUnderstand and correctly follow interviewing protocols\nBe informed about oversight procedures\nComplete an IRB-approved module on human subjects protection\n\nData from mock surveys must be individually assessed and feedback given to each enumerator. You can check whether certain enumerators are entering data differently than their peers, for example by entering lots of “Don’t know” or “Refuse” answers, finding low prevalence of sensitive behaviors, or entering data that is logically inconsistent. However, there is a lot that you can’t tell from the data alone. Spending a lot of time observing enumerators while they run surveys can greatly increase the quality of the data by improving their training, allowing you to select the best enumerators more accurately, and allowing you to understand how the questions are being implemented in the field."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#expectations-and-quality-control",
    "href": "guides/implementation/survey-implementation_en.html#expectations-and-quality-control",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Expectations and Quality Control",
    "text": "Expectations and Quality Control\nExpectations should be laid out clearly during training, in a manual, and reiterated in clearly worded contracts (signed after training).\nBasic expectations of enumerators:\n\nBeing on time\nAdhering to within-enumeration area sampling and replacement scheme\nGetting informed consent\nBuilding rapport with subjects\nAccurately recording responses\nCommunication with supervisors"
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#contracts-and-payment",
    "href": "guides/implementation/survey-implementation_en.html#contracts-and-payment",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Contracts and Payment",
    "text": "Contracts and Payment\nAs much as possible, make payment dependent on delivery. Enumerators have less and less incentive to stick with the project towards the end of fieldwork. The marginal returns are lower and they may be concerned about finding new work. In order to offset this, it is good practice to withhold a portion of their total salary (+/- 30%) until the end of fieldwork, and sometimes until data has been thoroughly reviewed if using paper instruments that need to be entered manually. At the same time, enumerators are often living paycheck to paycheck and may have expenses to cover during their long absence in the field. It is important to pay an advance up front to allow enumerators to take care of personal expenses that may otherwise make them anxious and unhappy during fieldwork. Having a strong local manager who understands the enumerators financial situations can help you create incentives while still making sure that they perceive the compensation structure as fair and adequate."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#soft-incentives",
    "href": "guides/implementation/survey-implementation_en.html#soft-incentives",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Soft Incentives",
    "text": "Soft Incentives\nSoft incentives help to keep teams happy and motivated throughout work. Some examples are:\n\nPerformance-based bonuses: Allowing managers to give performance-based bonuses for exceptional performance on a daily or weekly basis.\nCertificates: Survey trainings often involve learning portable skills, like the use of tablets or PDAs. Certificates can help enumerators prove to new employers that that they have these skills.\nLetters of Recommendation\nRecommendations to other survey firms, NGOs, etc.\nWrap party"
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#checking-data-entered-from-paper-instruments",
    "href": "guides/implementation/survey-implementation_en.html#checking-data-entered-from-paper-instruments",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Checking data entered from paper instruments",
    "text": "Checking data entered from paper instruments\nAfter interviewing the team leader needs to review all instruments for completeness and accuracy. If there are missing data or other inconsistencies, the team leader should send the enumerator back to revisit the respondent to correct all problems before leaving the area.\nOnce instruments are collected, data entry should commence as soon as possible. All data should be entered twice, and any discrepancies should be checked by a supervisor against the paper instrument."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#checking-data-gathered-using-pdas-or-tablets",
    "href": "guides/implementation/survey-implementation_en.html#checking-data-gathered-using-pdas-or-tablets",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Checking data gathered using PDAs or tablets",
    "text": "Checking data gathered using PDAs or tablets\nWhen using tablets or PDAs, checking the data is the responsibility of the RA and PIs. In addition to using a script that checks for patterns and outliers, it is also best practice to record selected portions of the interview and listen to a subsample of responses, both for errors and quality."
  },
  {
    "objectID": "guides/implementation/survey-implementation_en.html#footnotes",
    "href": "guides/implementation/survey-implementation_en.html#footnotes",
    "title": "10 Things to Know About Survey Implementation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThanks to Brandon de la Cuesta for help with this section↩︎"
  },
  {
    "objectID": "guides/implementation/workflow_en.html",
    "href": "guides/implementation/workflow_en.html",
    "title": "10 Things You Need to Know About Project Workflow",
    "section": "",
    "text": "There are limits to human memory. Since most experimental research requires at least months if not years of design, monitoring, analysis, and reporting, few researchers can maintain mental oversight of all of a project’s moving pieces over time. Introduce additional investigators into the mix, and the questions of who did what, when, and why (if not how) multiply and become harder to answer. As replication becomes more important (by the original project team or outside researchers), maintaining a written record of decisions, actions, and questions becomes essential. Bowers and Voors (2016) provide a framework and steps for improving project’s workflow; this guide draws upon their paper and upon additional tools aimed at documenting the important choices made by researchers and effectively communicating those choices to the project team."
  },
  {
    "objectID": "guides/implementation/workflow_en.html#footnotes",
    "href": "guides/implementation/workflow_en.html#footnotes",
    "title": "10 Things You Need to Know About Project Workflow",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCalled, anachronistically, Sweave↩︎\nNote that all of the EGAP methods guides that produce figures or tables are produced this way (see https://github.com/egap/methods-guides)↩︎\nThere are many ways to type text that includes code such that parenthesis are automatically closed, spelling is checked, or even help is provided about the code itself. RStudio is an integrated development environment (IDE) that includes a nice editor. Some of us use older tools that are more widespread such as vim, or eMacs. Others any of the contemporary improvements on those old tools such as Atom.↩︎\nhttps://docs.google.com↩︎\nhttps://www.dropbox.com↩︎\nSee King (1995).↩︎"
  },
  {
    "objectID": "guides.html",
    "href": "guides.html",
    "title": "Methods guides",
    "section": "",
    "text": "EGAP seeks to improve methods for rigorous impact evaluation throughout the social and behavioral science research community, as well as to make existing methods accessible to a wider audience. The methods guides are primers on a range of methodological and research topics. Each guide takes one issue and outlines the ten most important points to understand about it or the ten essential steps necessary to addressing that issue in your work."
  },
  {
    "objectID": "guides.html#causal-inference",
    "href": "guides.html#causal-inference",
    "title": "Methods guides",
    "section": "Causal Inference",
    "text": "Causal Inference\n\n\n\n\n\n\n\n\n\n\n10 Strategies for Figuring out if X Causes Y\n\n\n\nMacartan Humphreys\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About Causal Inference\n\n\n\nMacartan Humphreys\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides.html#research-questions",
    "href": "guides.html#research-questions",
    "title": "Methods guides",
    "section": "Research Questions",
    "text": "Research Questions\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About the Local Average Treatment Effect\n\n\n\nPeter van der Windt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Heterogeneous Treatment Effects\n\n\n\nAlbert Fang\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Mechanisms\n\n\n\nLindsay Dolan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Types of Treatment Effect You Should Know About\n\n\n\nPaul Testa\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides.html#data-strategies",
    "href": "guides.html#data-strategies",
    "title": "Methods guides",
    "section": "Data Strategies",
    "text": "Data Strategies\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About Randomization\n\n\n\nLindsay Dolan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Adaptive Experimental Design\n\n\n\nDonald Green, Molly Offer-Westort\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Cluster Randomization\n\n\n\nJake Bowers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Measurement in Experiments\n\n\n\nTara Slough\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Missing Data\n\n\n\nTara Slough\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Multisite or Block-Randomized Trials\n\n\n\nKristen Hunter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Sampling\n\n\n\nAnna M. Wilke\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Spillovers\n\n\n\nAlexander Coppock\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Survey Experiments\n\n\n\nChristopher Grady\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides.html#analysis-procedures",
    "href": "guides.html#analysis-procedures",
    "title": "Methods guides",
    "section": "Analysis Procedures",
    "text": "Analysis Procedures\n\n\n\n\n\n\n\n\n10 Randomization Inference Procedures with ri2\n\n\n\nAlexander Coppock\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About Multiple Comparisons\n\n\n\nAlexander Coppock\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Conducting a Meta-Analysis\n\n\n\nDonald Green\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Covariate Adjustment\n\n\n\nLindsay Dolan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Hypothesis Testing\n\n\n\nJake Bowers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Randomization Inference\n\n\n\nDonald Green\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides.html#assessing-designs",
    "href": "guides.html#assessing-designs",
    "title": "Methods guides",
    "section": "Assessing Designs",
    "text": "Assessing Designs\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About External Validity\n\n\n\nRenard Sexton\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About Statistical Power\n\n\n\nAlexander Coppock\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides.html#planning",
    "href": "guides.html#planning",
    "title": "Methods guides",
    "section": "Planning",
    "text": "Planning\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Pilot Studies\n\n\n\nKaylyn Jackson Schiff, Daniel S. Schiff, Natália S. Bueno\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Pre-Analysis Plans\n\n\n\nNuole (Lula) Chen, Christopher Grady\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Survey Design\n\n\n\nGabriella Sacramone-Lutz\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides.html#implementation",
    "href": "guides.html#implementation",
    "title": "Methods guides",
    "section": "Implementation",
    "text": "Implementation\n\n\n\n\n\n\n\n\n\n\n10 Conversations that Implementers and Evaluators Need to Have\n\n\n\nJake Bowers, Rebecca Wolfe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About Project Workflow\n\n\n\nMatthew Lisiecki\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Survey Implementation\n\n\n\nGabriella Sacramone-Lutz\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides.html#interpretation",
    "href": "guides.html#interpretation",
    "title": "Methods guides",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n\n\n\n\n\n\n\n\n10 Things Your Null Result Might Mean\n\n\n\nRekha Balu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Reading a Regression Table\n\n\n\nAbby Long\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Methods guides",
    "section": "",
    "text": "Evidence in Governance and Politics (EGAP) is a global research, evaluation, and learning network that promotes rigorous knowledge accumulation, innovation, and evidence-based policy in various governance and accountability domains.\nOur methods resources are aimed at learners and teachers of all levels. Guides are for self-learning, and our coursebook is for teachers of impact evaluation methods.\n\nMethods guides\nLearn methods for impact evaluations just-in-time to use them with guides for impact evaluation methods – for researchers, evidence users, and students\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About Causal Inference\n\n\n\nMacartan Humphreys\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things You Need to Know About External Validity\n\n\n\nRenard Sexton\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Things to Know About Covariate Adjustment\n\n\n\nLindsay Dolan\n\n\n\n\n\n\n\n\nNo matching items\n\n\nMore…\n\n\nSoftware\nSoftware tools in R and Stata can be useful for designing, implementing, analysis, and reporting on impact evaluations. We collected relevant software and provide a search interface for it.\nMore…\n\n\nTeaching coursebook\n\n\n\nLearning days host cities (red) and participant home countries (blue).\n\n\nOver the past decade, Evidence in Governance and Politics (EGAP) has organized Learning Days workshops with the aim of building experimental social-science research capacity among principal investigators (PIs) – both researchers and practitioners – in Africa and Latin America. By sharing the practical and statistical methods of randomized field experiments with workshop participants, the Learning Days effort hopes to identify and nurture researcher networks around the world and to create strong, productive connections between these researchers and EGAP members. The teaching coursebook, which grew out of a desire to share the materials we developed for the Learning Days, is a comprehensive overview of causal inference methods for researchers developing an experimental research design."
  },
  {
    "objectID": "coursebook/researchdesignform.html",
    "href": "coursebook/researchdesignform.html",
    "title": "The Research Design Process",
    "section": "",
    "text": "This book aims to help you understand and design randomized field experiments. But before we dive into the details of research design, we need a good research question – a question that will advance knowledge or help make a policy decision or both. There is no simple recipe for finding or developing a good scientific or policy question, but our theories are important for articulating good questions that underlie impactful research. After formulating our question, we develop the best design possible within our resource constraints, using our knowledge of causal inference and statistics from the modules that follow.\nThis module introduces the EGAP Research Design Form, a checklist to guide you through the many stages of the research process. The Learning Days workshops are organized around the Research Design Form. We also point you towards the DeclareDesign software package to explore the implications of different choices we could make for our research designs. Finally, this module discusses pre-analysis plans and pre-registration. When plan our analyses and make these plans public, we improve our chances of persuading others with our results.\n\n\n\nA good research question advances science and/or is a question the answer to which will inform a policy decision.\nCertain research designs are better able to address certain questions. We want to choose the design that best answers our key questions within our constraints.\nThe questions we ask arise — often implicitly — from our values and from our understanding about how the world works. These theories make our questions relevant. And the experiments that we execute teach us about the theory. That is, we hope that the evidence and data arising from these research designs improve our understanding.\nCore components of a research design\nIntroduce core components of the EGAP Research Design Form.\nIntroduce a research design software package, DeclareDesign.\nThe move in social science towards the review of designs, rather than outcomes.\nPre-registration – what it is, and why and how we should do it.\n\n\n\n\nBelow are slides with the core content that we cover in our lecture on research design. You can directly use these slides or make your own local copy and edit.\n\nR Markdown Source\nPDF Version\nHTML Version\n\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe DeclareDesign presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nThe DeclareDesign presentation from EGAP Learning Days in Salima, Malawi, February 2017\nThe DeclareDesign presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\n\nYou can also see slides for Design Talks in previous EGAP Learning Days, where presenters focus on issues that come up in designing the research, rather than the results:\n\nDesign Talk from EGAP Learning Days at the African School of Economics, Benin, March 2018\nDesign Talk 1 from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nDesign Talk 2 from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nDesign Talk 3 from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nDesign Talk 1 from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\nDesign Talk 2 from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\nDesign Talk 3 from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\nDesign Talk from EGAP Learning Days in Guatemala City, Guatemala, August 2017\n\n\n\n\n\nEGAP Research Design Form. A checklist we created for the Learning Days to guide you through the stages of the research process.\n\nDocx Version\nPDF Version\nHTML Version\n\nLinks to repositories for pre-registration/pre-analysis plans:\n\nEGAP registry, hosted by OSF (https://egap.org/registry/)\nAEA RCT registry (https://www.socialscienceregistry.org/)\nOSF (https://osf.io/registries)\n\nExamples of other pre-registrations/pre-analysis plans:\n\nSMS Messages in Mozambique from the US Federal Government\nPolice Body-Cameras from the Lab @ DC\n\n\n\n\n\n\n\n\nEGAP Methods Guide 10 Things to Know about Pre-Analysis Plans\nEGAP Methods Guide 10 Things to Know about Measurement in Experiments\n\n\n\n\n\nPreregistration as a Tool for Strengthening Federal Evaluation. A white paper from the US Government’s Office of Evaluation Sciences. You can also see examples of their pre-analysis plans on all of their field experiment pages.\n[@christensen_transparent_2019]. The book summarizes new approaches in social science research on transparency and reproducibility.\n[@gerber_field_2012]. Chapter 12 includes some examples of experimental research designs.\n\n\n\n\n\nDeclareDesign, an exciting and comprehensive set of software tools for describing, assessing, and conducting empirical research."
  },
  {
    "objectID": "coursebook/researchdesignform.html#core-content",
    "href": "coursebook/researchdesignform.html#core-content",
    "title": "The Research Design Process",
    "section": "",
    "text": "A good research question advances science and/or is a question the answer to which will inform a policy decision.\nCertain research designs are better able to address certain questions. We want to choose the design that best answers our key questions within our constraints.\nThe questions we ask arise — often implicitly — from our values and from our understanding about how the world works. These theories make our questions relevant. And the experiments that we execute teach us about the theory. That is, we hope that the evidence and data arising from these research designs improve our understanding.\nCore components of a research design\nIntroduce core components of the EGAP Research Design Form.\nIntroduce a research design software package, DeclareDesign.\nThe move in social science towards the review of designs, rather than outcomes.\nPre-registration – what it is, and why and how we should do it."
  },
  {
    "objectID": "coursebook/researchdesignform.html#slides",
    "href": "coursebook/researchdesignform.html#slides",
    "title": "The Research Design Process",
    "section": "",
    "text": "Below are slides with the core content that we cover in our lecture on research design. You can directly use these slides or make your own local copy and edit.\n\nR Markdown Source\nPDF Version\nHTML Version\n\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe DeclareDesign presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nThe DeclareDesign presentation from EGAP Learning Days in Salima, Malawi, February 2017\nThe DeclareDesign presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\n\nYou can also see slides for Design Talks in previous EGAP Learning Days, where presenters focus on issues that come up in designing the research, rather than the results:\n\nDesign Talk from EGAP Learning Days at the African School of Economics, Benin, March 2018\nDesign Talk 1 from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nDesign Talk 2 from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nDesign Talk 3 from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nDesign Talk 1 from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\nDesign Talk 2 from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\nDesign Talk 3 from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\nDesign Talk from EGAP Learning Days in Guatemala City, Guatemala, August 2017"
  },
  {
    "objectID": "coursebook/researchdesignform.html#design-form-and-pre-registration",
    "href": "coursebook/researchdesignform.html#design-form-and-pre-registration",
    "title": "The Research Design Process",
    "section": "",
    "text": "EGAP Research Design Form. A checklist we created for the Learning Days to guide you through the stages of the research process.\n\nDocx Version\nPDF Version\nHTML Version\n\nLinks to repositories for pre-registration/pre-analysis plans:\n\nEGAP registry, hosted by OSF (https://egap.org/registry/)\nAEA RCT registry (https://www.socialscienceregistry.org/)\nOSF (https://osf.io/registries)\n\nExamples of other pre-registrations/pre-analysis plans:\n\nSMS Messages in Mozambique from the US Federal Government\nPolice Body-Cameras from the Lab @ DC"
  },
  {
    "objectID": "coursebook/researchdesignform.html#resources",
    "href": "coursebook/researchdesignform.html#resources",
    "title": "The Research Design Process",
    "section": "",
    "text": "EGAP Methods Guide 10 Things to Know about Pre-Analysis Plans\nEGAP Methods Guide 10 Things to Know about Measurement in Experiments\n\n\n\n\n\nPreregistration as a Tool for Strengthening Federal Evaluation. A white paper from the US Government’s Office of Evaluation Sciences. You can also see examples of their pre-analysis plans on all of their field experiment pages.\n[@christensen_transparent_2019]. The book summarizes new approaches in social science research on transparency and reproducibility.\n[@gerber_field_2012]. Chapter 12 includes some examples of experimental research designs.\n\n\n\n\n\nDeclareDesign, an exciting and comprehensive set of software tools for describing, assessing, and conducting empirical research."
  },
  {
    "objectID": "coursebook/measurement.html",
    "href": "coursebook/measurement.html",
    "title": "Measurement",
    "section": "",
    "text": "To estimate effects and test hypotheses, we often use an outcome of interest measured with quantitative data from surveys, behavioral games, or administrative records. For causal questions, we typically use data on immediate and final outcomes and core mechanisms. We use baseline data to identify relevant subgroups, adjust our estimates, or help block-randomize our treatment. Measurements should be valid and reliable. Be aware that data can be noisy (random error) and/or biased (systematic error).\nThis module discusses what to measure and how to measure. It shows how good measurement is closely linked to your research design and statistical power.\n\n\n\nWhen we represent some attribute of a unit by some number, letter, word, or symbol in some systematic way (perhaps in a cell in a simple dataset), we are measuring.\nA valid measure of a concept or phenomenon of interest should clearly represent that underlying and often abstract entity.\nA reliable measure of a concept would provide the same score for the unit of measurement (for example, a person or a village) if conditions were not changed.\nWe can assess our theories of measurement using multiple approaches to measuring outcomes, covariates, or differences between units implied by different accounts of causal mechanisms.\nInvalid measurement can make it hard for your research design to effectively distinguish between alternative explanations for the relationship between treatment and outcome.\nUnreliable measurement can diminish statistical power.\nDifficult measurement may call for a pilot study focused on measurement itself.\n\n\n\n\nBelow are slides with the core content that we cover in our lecture on measurement. You can directly use these slides or make your own local copy and edit.\n\nR Markdown Source\nPDF Version\nHTML Version\n\n\n\n\n\n\n\nEGAP Methods Guide 10 Things to Know about Measurement in Experiments\nEGAP Methods Guide 10 Things to Know about Survey Design\nEGAP Methods Guide 10 Things to Know about Survey Implementation\n\n\n\n\n\n[@adcocoll:2001]\n[@scacco_can_2018]\n[@shadish2002experimental]\n[@vicente_is_2014]\n\n\n\n\nUsing survey data at multiple levels\n\nEGAP Policy Brief 58: Does Bottom-Up Accountability Work?\n\nUsing text messages\n\nEGAP Policy Brief 27: ICT and Politicians in Uganda\nEGAP Policy Brief 56: Reporting Corruption in Nigeria\n\nUsing administrative data\n\nEGAP Policy Brief 16: Spillover Effects of Observers in Ghana\nEGAP Policy Brief 67: Electoral Administration in Kenya"
  },
  {
    "objectID": "coursebook/measurement.html#core-content",
    "href": "coursebook/measurement.html#core-content",
    "title": "Measurement",
    "section": "",
    "text": "When we represent some attribute of a unit by some number, letter, word, or symbol in some systematic way (perhaps in a cell in a simple dataset), we are measuring.\nA valid measure of a concept or phenomenon of interest should clearly represent that underlying and often abstract entity.\nA reliable measure of a concept would provide the same score for the unit of measurement (for example, a person or a village) if conditions were not changed.\nWe can assess our theories of measurement using multiple approaches to measuring outcomes, covariates, or differences between units implied by different accounts of causal mechanisms.\nInvalid measurement can make it hard for your research design to effectively distinguish between alternative explanations for the relationship between treatment and outcome.\nUnreliable measurement can diminish statistical power.\nDifficult measurement may call for a pilot study focused on measurement itself."
  },
  {
    "objectID": "coursebook/measurement.html#slides",
    "href": "coursebook/measurement.html#slides",
    "title": "Measurement",
    "section": "",
    "text": "Below are slides with the core content that we cover in our lecture on measurement. You can directly use these slides or make your own local copy and edit.\n\nR Markdown Source\nPDF Version\nHTML Version"
  },
  {
    "objectID": "coursebook/measurement.html#resources",
    "href": "coursebook/measurement.html#resources",
    "title": "Measurement",
    "section": "",
    "text": "EGAP Methods Guide 10 Things to Know about Measurement in Experiments\nEGAP Methods Guide 10 Things to Know about Survey Design\nEGAP Methods Guide 10 Things to Know about Survey Implementation\n\n\n\n\n\n[@adcocoll:2001]\n[@scacco_can_2018]\n[@shadish2002experimental]\n[@vicente_is_2014]\n\n\n\n\nUsing survey data at multiple levels\n\nEGAP Policy Brief 58: Does Bottom-Up Accountability Work?\n\nUsing text messages\n\nEGAP Policy Brief 27: ICT and Politicians in Uganda\nEGAP Policy Brief 56: Reporting Corruption in Nigeria\n\nUsing administrative data\n\nEGAP Policy Brief 16: Spillover Effects of Observers in Ghana\nEGAP Policy Brief 67: Electoral Administration in Kenya"
  },
  {
    "objectID": "coursebook/threats.html",
    "href": "coursebook/threats.html",
    "title": "Threats to the Internal Validity of Randomized Experiments",
    "section": "",
    "text": "Randomized experiments can run into issues that undermine their ability to demonstrate causal effects – that is, threaten the internal validity of randomized experiments. Some units might be missing outcome data and that missingness may be due to the treatment. They may not take the treatment status assigned to them or be subject to spillover effects from a treated neighbor.\nIn this module, we cover some common threats and some best practices to avoid or work around them.\n\n\n\nReview the three core assumptions discussed in the causal inference module.\nWe have said “Analyze as you randomize” in the module on estimands and estimators. Remember that you randomized treatment assignment, not whether the treatment is received or whether a unit participates in data collection.\nMissing data on the outcome (attrition) is especially a problem if the patterns of missingness are caused by the treatment itself. This is a very common problem.\n\nDo not drop observations that are missing outcome data from your analysis.\nYou may be able to bound estimates of treatment effects.\n\nNon-compliance. The effect of treatment assignment is not the same as the effect of receiving the treatment. Sometimes units will not comply with their assigned treatment status.\n\nOne-sided compliance occurs when some units assigned to treatment fail to take the treatment, but all units assigned to control do not take the treatment.\nThe local average treatment effect (LATE, also known as the complier average causal effect, CACE) is the average effect for the units that take the treatment when assigned, but not otherwise. If the monotonicity assumption and the exclusion restriction hold, we may be able to estimate LATE when we have non-compliance.\n\n“Spillover effects” or interference between units is a violation of one of the core assumptions for causal inference (causal inference).\n\nHowever, this may not be a problem if you are interested in spillover effects and/or have designed your research to account for it.\n\nHawthorne effects are when subjects behave differently because they are being observed.\nNon-excludability. Treating treatment and control units differently, such as with different data collection processes or extra attention to the treated units, can confuse interpretation of experimental results.\n\nIf Hawthorne effects are present for treated units but not control units, then we have a violation of the excludability assumption.\n\n\n\n\n\nBelow are slides with the core content that we cover in our lecture on threats to the internal validity of randomized experiments. You can directly use these slides or make your local copy and edit.\n\nR Markdown Source\nPDF version\nHTML version\n\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe threats presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019 (first section reviews randomization designs)\nThe attrition and missing data presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\n\n\n\n\n\n\n\nEGAP Methods Guide 10 Things to Know about Missing Data\nEGAP Methods Guide 10 Types of Treatment Effect You Should Know About\nEGAP Methods Guide 10 Things to Know about the Local Average Treatment Effect\n\n\n\n\n\nStandard operating procedures for Don Green’s lab at Columbia University. A comprehensive set of procedures and rules of thumb for conducting experimental studies.\n[@gerber_field_2012]. Chapters 5–8 address non-compliance, attrition, and interference.\n\n\n\n\n\nEGAP Policy Brief 11: Election Observers and Fraud in Ghana\nEGAP Policy Brief 16: Spillover Effects of Observers in Ghana"
  },
  {
    "objectID": "coursebook/threats.html#core-content",
    "href": "coursebook/threats.html#core-content",
    "title": "Threats to the Internal Validity of Randomized Experiments",
    "section": "",
    "text": "Review the three core assumptions discussed in the causal inference module.\nWe have said “Analyze as you randomize” in the module on estimands and estimators. Remember that you randomized treatment assignment, not whether the treatment is received or whether a unit participates in data collection.\nMissing data on the outcome (attrition) is especially a problem if the patterns of missingness are caused by the treatment itself. This is a very common problem.\n\nDo not drop observations that are missing outcome data from your analysis.\nYou may be able to bound estimates of treatment effects.\n\nNon-compliance. The effect of treatment assignment is not the same as the effect of receiving the treatment. Sometimes units will not comply with their assigned treatment status.\n\nOne-sided compliance occurs when some units assigned to treatment fail to take the treatment, but all units assigned to control do not take the treatment.\nThe local average treatment effect (LATE, also known as the complier average causal effect, CACE) is the average effect for the units that take the treatment when assigned, but not otherwise. If the monotonicity assumption and the exclusion restriction hold, we may be able to estimate LATE when we have non-compliance.\n\n“Spillover effects” or interference between units is a violation of one of the core assumptions for causal inference (causal inference).\n\nHowever, this may not be a problem if you are interested in spillover effects and/or have designed your research to account for it.\n\nHawthorne effects are when subjects behave differently because they are being observed.\nNon-excludability. Treating treatment and control units differently, such as with different data collection processes or extra attention to the treated units, can confuse interpretation of experimental results.\n\nIf Hawthorne effects are present for treated units but not control units, then we have a violation of the excludability assumption."
  },
  {
    "objectID": "coursebook/threats.html#slides",
    "href": "coursebook/threats.html#slides",
    "title": "Threats to the Internal Validity of Randomized Experiments",
    "section": "",
    "text": "Below are slides with the core content that we cover in our lecture on threats to the internal validity of randomized experiments. You can directly use these slides or make your local copy and edit.\n\nR Markdown Source\nPDF version\nHTML version\n\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe threats presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019 (first section reviews randomization designs)\nThe attrition and missing data presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016"
  },
  {
    "objectID": "coursebook/threats.html#resources",
    "href": "coursebook/threats.html#resources",
    "title": "Threats to the Internal Validity of Randomized Experiments",
    "section": "",
    "text": "EGAP Methods Guide 10 Things to Know about Missing Data\nEGAP Methods Guide 10 Types of Treatment Effect You Should Know About\nEGAP Methods Guide 10 Things to Know about the Local Average Treatment Effect\n\n\n\n\n\nStandard operating procedures for Don Green’s lab at Columbia University. A comprehensive set of procedures and rules of thumb for conducting experimental studies.\n[@gerber_field_2012]. Chapters 5–8 address non-compliance, attrition, and interference.\n\n\n\n\n\nEGAP Policy Brief 11: Election Observers and Fraud in Ghana\nEGAP Policy Brief 16: Spillover Effects of Observers in Ghana"
  },
  {
    "objectID": "coursebook/statisticalpower.html",
    "href": "coursebook/statisticalpower.html",
    "title": "Statistical Power and Design Diagnosands",
    "section": "",
    "text": "Before we run a study, we would like to know whether a particular design has the statistical power to detect an effect if it exists. It is difficult to learn from an under-powered study, since it would be unclear whether a null result indicates that there was no effect or just that we failed to detect a non-zero effect that exists. A power analysis can help you improve your design and allocate your resources better; it may even help you decide against conducting the study.\nIn this module, we introduce statistical power, core approaches to calculating power through analytical calculations and through simulation, and how design features such as blocking, covariate adjustment, and clustering impact power.\n\n\n\nStatistical power is the ability of a study to detect an effect given that it exists.\nPower analysis is something we do before a study. It helps you figure out the sample you need or what effects you can detect. It is an essential step in research design and helps you communicate your design.\nCommon approaches to power calculation:\n\nAnalytical power calculations (using a formula)\nSimulations (for example, using DeclareDesign)\n\nCovariate adjustment and blocking can increase power.\nFor clustered designs you need to take account of the intra-cluster correlation (the within-cluster variance relative to the overall variance).\nPower is closely liked to study design, hypothesis testing and estimation.\n\n\n\n\nBelow are slides with the core content that we cover in our lecture on power. You can directly use these slides or make your local copy and edit.\n\nR Markdown Source\nPDF version\nHTML version\n\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe power presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019\nThe power presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019\nThe power presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nThe power presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe power presentation from EGAP Learning Days in Salima, Malawi, February 2017\nThe power presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\n\n\n\n\n\n\n\nEGAP Methods Guide 10 Things to Know about Statistical Power\nEGAP Methods Guide 10 Things to Know about Covariate Adjustment\nEGAP Methods Guide 10 Things Your Null Results Might Mean\n\n\n\n\nSome examples of power analysis in designs:\n\nPre-Analysis Plan. Accountability Can Transform (ACT) Health: A Replication and Extension of Bjorkman and Svensson (2009)\nEGAP Policy Brief 58: Can bottom-up accountability generate improvements in health outcomes?\n\n\n\n\n\nInteractive power analysis\n\nEGAP Power Calculator\nrpsychologist\n\nR packages for power analysis\n\npwr\nDeclareDesign, see also https://declaredesign.org/"
  },
  {
    "objectID": "coursebook/statisticalpower.html#core-content",
    "href": "coursebook/statisticalpower.html#core-content",
    "title": "Statistical Power and Design Diagnosands",
    "section": "",
    "text": "Statistical power is the ability of a study to detect an effect given that it exists.\nPower analysis is something we do before a study. It helps you figure out the sample you need or what effects you can detect. It is an essential step in research design and helps you communicate your design.\nCommon approaches to power calculation:\n\nAnalytical power calculations (using a formula)\nSimulations (for example, using DeclareDesign)\n\nCovariate adjustment and blocking can increase power.\nFor clustered designs you need to take account of the intra-cluster correlation (the within-cluster variance relative to the overall variance).\nPower is closely liked to study design, hypothesis testing and estimation."
  },
  {
    "objectID": "coursebook/statisticalpower.html#slides",
    "href": "coursebook/statisticalpower.html#slides",
    "title": "Statistical Power and Design Diagnosands",
    "section": "",
    "text": "Below are slides with the core content that we cover in our lecture on power. You can directly use these slides or make your local copy and edit.\n\nR Markdown Source\nPDF version\nHTML version\n\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe power presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019\nThe power presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019\nThe power presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nThe power presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe power presentation from EGAP Learning Days in Salima, Malawi, February 2017\nThe power presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016"
  },
  {
    "objectID": "coursebook/statisticalpower.html#resources",
    "href": "coursebook/statisticalpower.html#resources",
    "title": "Statistical Power and Design Diagnosands",
    "section": "",
    "text": "EGAP Methods Guide 10 Things to Know about Statistical Power\nEGAP Methods Guide 10 Things to Know about Covariate Adjustment\nEGAP Methods Guide 10 Things Your Null Results Might Mean\n\n\n\n\nSome examples of power analysis in designs:\n\nPre-Analysis Plan. Accountability Can Transform (ACT) Health: A Replication and Extension of Bjorkman and Svensson (2009)\nEGAP Policy Brief 58: Can bottom-up accountability generate improvements in health outcomes?\n\n\n\n\n\nInteractive power analysis\n\nEGAP Power Calculator\nrpsychologist\n\nR packages for power analysis\n\npwr\nDeclareDesign, see also https://declaredesign.org/"
  },
  {
    "objectID": "coursebook/randomization.html",
    "href": "coursebook/randomization.html",
    "title": "Randomization",
    "section": "",
    "text": "The module on causal inference discussed the crucial role of randomization for drawing valid inferences from a comparison of treated and untreated groups. In this module, we move from theory to the first of many concrete choices for your research design.\nWe introduce four common ways to randomize treatment – simple, complete, block, and clustered – and when these different types of randomization may be available and appropriate. We also cover several popular designs including factorial designs and encouragement designs. The module provides some guidance on implementation, including best practices for checking for balance and ensuring replicability.\n\n\n\nWhat is randomization? Random assignment is not the same as random sampling.\nFour common ways to randomize treatment:\n\nSimple: randomly assign units to treatment (like a coin flip).\nComplete: within a list of eligible units, a assign a fixed number to receive a treatment (like drawing from a urn).\nBlock (or stratified): assign treatment within specific strata or blocks, as if you are running an experiment within each block.\nCluster: assign groups (clusters) of observations to the same treatment condition.\n\nSome popular designs:\n\nRandomized access: randomization to availability of a treatment.\nRandomized delayed access: randomize the timing of access.\nFactorial: randomize units to combinations of treatment arms.\nEncouragement: randomize the invitation to receive treatment.\n\nHow do you check whether your randomization produced balance on observables? Typically we conduct randomization tests also known as balance tests using the \\(d^2\\) omnibus test from xBalance in the RItools package (because it is randomization inference) or approximate this result with an \\(F\\)-test.\nThere are, of course, limits to randomization. We discuss some here and direct you to the module on threats for more.\n\n\n\n\nBelow are slides with the core content that we cover in our lecture on randomization. You can directly use these slides or make your own local copy and edit.\n\nR Markdown Source\nPDF version\nHTML version\n\nThe linked files shows how to do replicable randomization in R. You can also see more examples of randomization in R at 10 Things You Need to Know About Randomization.\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe design issues presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019 (first section reviews randomization designs)\nThe randomization presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019\nThe randomization presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nThe randomization presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe randomization presentation from EGAP Learning Days in Salima, Malawi, February 2017\nThe randomization presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\n\n\n\n\n\n\n\nEGAP Methods Guide 10 Things You Need to Know About Randomization\nEGAP Methods Guide 10 Things You Need to Know About Cluster Randomization\n\n\n\n\n\nStandard operating procedures for Don Green’s lab at Columbia University. A comprehensive set of procedures and rules of thumb for conducting experimental studies.\n[@glennerster_running_2013]. Chapter 2 on randomization.\n[@gerber_field_2012]. Chapter 2: Causal Inference and Experimentation\n\n\n\n\nFactorial designs\n\nEGAP Policy Brief 57: How Media Influence Social Norms: Evidence from Mexico\nEGAP Policy Brief 58: Does Bottom-Up Accountability Work?\n\nRandomizing access\n\nEGAP Policy Brief 24: Reducing Elite Capture in the Solomon Islands\n\nRandomizing delayed access\n\nEGAP Policy Brief 35: Reducing Reconvictions Among Released Prisoners\nEGAP Policy Brief 60: Reducing Youth Support for Violence through Training and Cash Transfers in Afghanistan\n\nCluster randomization\n\nEGAP Policy Brief 22: Getting Out the Vote\n\nBlocked cluster randomization\n\nEGAP Policy Brief 54: Incumbent Malfeasance Revelations\nEGAP Policy Brief 56: Reporting Corruption\n\n\n\n\n\nRItools, a set of tools for randomization-based inference including balance testing.\n\n\n\n\n\nRandomization vs. Random Sampling\nCluster vs. Block Randomization"
  },
  {
    "objectID": "coursebook/randomization.html#core-content",
    "href": "coursebook/randomization.html#core-content",
    "title": "Randomization",
    "section": "",
    "text": "What is randomization? Random assignment is not the same as random sampling.\nFour common ways to randomize treatment:\n\nSimple: randomly assign units to treatment (like a coin flip).\nComplete: within a list of eligible units, a assign a fixed number to receive a treatment (like drawing from a urn).\nBlock (or stratified): assign treatment within specific strata or blocks, as if you are running an experiment within each block.\nCluster: assign groups (clusters) of observations to the same treatment condition.\n\nSome popular designs:\n\nRandomized access: randomization to availability of a treatment.\nRandomized delayed access: randomize the timing of access.\nFactorial: randomize units to combinations of treatment arms.\nEncouragement: randomize the invitation to receive treatment.\n\nHow do you check whether your randomization produced balance on observables? Typically we conduct randomization tests also known as balance tests using the \\(d^2\\) omnibus test from xBalance in the RItools package (because it is randomization inference) or approximate this result with an \\(F\\)-test.\nThere are, of course, limits to randomization. We discuss some here and direct you to the module on threats for more."
  },
  {
    "objectID": "coursebook/randomization.html#slides",
    "href": "coursebook/randomization.html#slides",
    "title": "Randomization",
    "section": "",
    "text": "Below are slides with the core content that we cover in our lecture on randomization. You can directly use these slides or make your own local copy and edit.\n\nR Markdown Source\nPDF version\nHTML version\n\nThe linked files shows how to do replicable randomization in R. You can also see more examples of randomization in R at 10 Things You Need to Know About Randomization.\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe design issues presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019 (first section reviews randomization designs)\nThe randomization presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019\nThe randomization presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nThe randomization presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe randomization presentation from EGAP Learning Days in Salima, Malawi, February 2017\nThe randomization presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016"
  },
  {
    "objectID": "coursebook/randomization.html#resources",
    "href": "coursebook/randomization.html#resources",
    "title": "Randomization",
    "section": "",
    "text": "EGAP Methods Guide 10 Things You Need to Know About Randomization\nEGAP Methods Guide 10 Things You Need to Know About Cluster Randomization\n\n\n\n\n\nStandard operating procedures for Don Green’s lab at Columbia University. A comprehensive set of procedures and rules of thumb for conducting experimental studies.\n[@glennerster_running_2013]. Chapter 2 on randomization.\n[@gerber_field_2012]. Chapter 2: Causal Inference and Experimentation\n\n\n\n\nFactorial designs\n\nEGAP Policy Brief 57: How Media Influence Social Norms: Evidence from Mexico\nEGAP Policy Brief 58: Does Bottom-Up Accountability Work?\n\nRandomizing access\n\nEGAP Policy Brief 24: Reducing Elite Capture in the Solomon Islands\n\nRandomizing delayed access\n\nEGAP Policy Brief 35: Reducing Reconvictions Among Released Prisoners\nEGAP Policy Brief 60: Reducing Youth Support for Violence through Training and Cash Transfers in Afghanistan\n\nCluster randomization\n\nEGAP Policy Brief 22: Getting Out the Vote\n\nBlocked cluster randomization\n\nEGAP Policy Brief 54: Incumbent Malfeasance Revelations\nEGAP Policy Brief 56: Reporting Corruption\n\n\n\n\n\nRItools, a set of tools for randomization-based inference including balance testing.\n\n\n\n\n\nRandomization vs. Random Sampling\nCluster vs. Block Randomization"
  },
  {
    "objectID": "coursebook/ethics.html",
    "href": "coursebook/ethics.html",
    "title": "Ethical Considerations",
    "section": "",
    "text": "A randomized experiment involves one group of humans changing the lives of another group humans. Those working in government do this as a matter of course — their very job is to provide food, shelter, safety, justice, etc., to their people. Academics, whose work does not usually have immediate impacts on the public, must remember to also carefully consider how their research might change the lives of those exposed to the intervention, as well as those not exposed. When one person influences the life of another, the influencer has responsibilities not to harm the person being influenced.\nThis module discusses the core topics on research ethics, such as privacy and autonomy; the basic principles relating to respect for persons, beneficence, and justice; and how informed consent helps communicate these principles to study participants.\n\n\n\nResearch must weigh the potential benefits of the knowledge to be gained from the research against the potential harms it may do to human subjects.\nHow would you feel if you were a research subject in your study? In the control group? In the treatment group? A relatively high-status member of the community? A relatively low-status member of the community?\nKey tenets: privacy and autonomy.\nBasic principles in the Belmont Report: respect for persons, beneficence, justice.\nInformed consent: Can you ensure that research subjects have the freedom to refuse to participate and/or drop out of the study if they want to? Can you ensure that research subjects can report problems that might arise?\nChallenges for social science experimental research in general:\n\nMany more people may benefit (or suffer from) your intervention than directly participate in your study.\nChanging election results or corruption can produce large societal changes. Is this beyond the remit of research?\n\n\n\n\n\nBelow are slides with the core content that we cover in this session.\n\nR Markdown Source\nPDF Version\nHTML Version\n\n\n\n\n\nEGAP Research Principles and Work on Ethics\nAPSA Principles and Guidance on Human Subjects Research\nBelmont Report\nInstitutional Review Boards in the US\nExample: Research Ethics at Oxford University in the UK\nExample: Ethics for Researchers in the EU\nExample: Research Ethics at the Universidad Catolica de Chile\n\n\n\n\n[@Asieduetal2021ethics]\n[@Evans2021ethics] \n\n\n\n\nExamples of PAPs and papers that discuss ethical issues:\n\nPre Analysis Plan: The Effects of Non-Food Item Vouchers in a Humanitarian Context The Case of the Rapid Response to Movements of Population Program in Congo\nPaper: Appendix E.1 in Countering violence against women by encouraging disclosure: A mass media experiment in rural Uganda"
  },
  {
    "objectID": "coursebook/ethics.html#core-content",
    "href": "coursebook/ethics.html#core-content",
    "title": "Ethical Considerations",
    "section": "",
    "text": "Research must weigh the potential benefits of the knowledge to be gained from the research against the potential harms it may do to human subjects.\nHow would you feel if you were a research subject in your study? In the control group? In the treatment group? A relatively high-status member of the community? A relatively low-status member of the community?\nKey tenets: privacy and autonomy.\nBasic principles in the Belmont Report: respect for persons, beneficence, justice.\nInformed consent: Can you ensure that research subjects have the freedom to refuse to participate and/or drop out of the study if they want to? Can you ensure that research subjects can report problems that might arise?\nChallenges for social science experimental research in general:\n\nMany more people may benefit (or suffer from) your intervention than directly participate in your study.\nChanging election results or corruption can produce large societal changes. Is this beyond the remit of research?"
  },
  {
    "objectID": "coursebook/ethics.html#slides",
    "href": "coursebook/ethics.html#slides",
    "title": "Ethical Considerations",
    "section": "",
    "text": "Below are slides with the core content that we cover in this session.\n\nR Markdown Source\nPDF Version\nHTML Version"
  },
  {
    "objectID": "coursebook/ethics.html#resources",
    "href": "coursebook/ethics.html#resources",
    "title": "Ethical Considerations",
    "section": "",
    "text": "EGAP Research Principles and Work on Ethics\nAPSA Principles and Guidance on Human Subjects Research\nBelmont Report\nInstitutional Review Boards in the US\nExample: Research Ethics at Oxford University in the UK\nExample: Ethics for Researchers in the EU\nExample: Research Ethics at the Universidad Catolica de Chile\n\n\n\n\n[@Asieduetal2021ethics]\n[@Evans2021ethics] \n\n\n\n\nExamples of PAPs and papers that discuss ethical issues:\n\nPre Analysis Plan: The Effects of Non-Food Item Vouchers in a Humanitarian Context The Case of the Rapid Response to Movements of Population Program in Congo\nPaper: Appendix E.1 in Countering violence against women by encouraging disclosure: A mass media experiment in rural Uganda"
  },
  {
    "objectID": "coursebook/causalinference.html",
    "href": "coursebook/causalinference.html",
    "title": "Causal Inference",
    "section": "",
    "text": "Much of social science is about causality. We might ask questions like whether voter registration increases political participation, whether bottom-up accountability can improve health outcomes, or whether personal narratives of immigrants help reduce prejudicial attitudes towards them.\nOver the past decade, social science has become much more serious about how causal claims are made, building on a long history of work on causality dating back to the classic writings of Fisher and Rubin. We make greater use of experiments, and randomization has become the gold standard for addressing causal questions.\nIn this module, we introduce the counterfactual approach to causal inference and how statements with causal claims can be interpreted. We introduce the potential outcome framework and how random assignment helps us make claims about what would have happened in the absence of the policy, action or program we study. We discuss the three core assumptions for causal inference: random assignment of subjects to treatment, non-interference, and excludability.\n\n\n\nWhat do we mean when we say “cause”? And why does it matter to be clear about the meaning of causal claims?\nAn introduction to potential outcomes as a way to think about alternative states of the world.\nRandomization helps us learn about counterfactual causal claims in a particularly useful way.\nThe three key core assumptions for causal inference: random assignment of subjects to treatment, non-interference, and excludability.\nComparison of randomized studies with observational studies.\nRandomization brings high internal validity, but it can’t promise external validity.\nYour causal question is closely linked so your research design.\n\n\n\n\nBelow are slides with the core content that we cover in our lecture on causality. You can directly use these slides or make your own local copy and edit.\n\nR Markdown Source\nPDF Version\nHTML Version\n\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe causal inference presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019\nThe causal inference presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019\nThe causal inference presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nThe causal inference presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe introduction to experiments presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe causal inference presentation from EGAP Learning Days in Salima, Malawi, February 2017\nThe causal inference presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\n\n\n\n\n\n\n\nEGAP Methods Guide 10 Things You Need to Know about Causal Inference\nEGAP Methods Guide 10 Strategies for Figuring Out If X Caused Y\nEGAP Methods Guide 10 Things You Need to Know about Mechanisms\nEGAP Methods Guide 10 Things to Know About External Validity\n\n\n\n\n\n\n\n[@fisher_design_1935]. Fisher introduces the idea of randomization and hypothesis testing as a way to learn about causal inference.\n[@rubin:1974]. Rubin introduces the idea of potential outcomes and links counterfactual conceptualizations of causality to statistical inference.\n\n\n\n\n\n[@brady2008causation].\n[@gerber_field_2012, Chapter 1]. This book is a great resource for many topics in experimental design.\n[@morgan_counterfactuals_2007, Chapter 1]. This book includes nice examples of thinking through making causal claims from observational data.\n[@glennerster_running_2013]. This is a great introduction to running field experiments and discusses many examples.\n\n\n\n\n\nSome examples of causal questions:\n\nEGAP Policy Brief 38: Are radio voter education campaigns effective in discouraging voters from voting for parties/candidates that engage in vote-buying?\nEGAP Policy Brief 51: Can free and anonymous information communication technology strengthen local accountability and improve the delivery of public services?\nEGAP Policy Brief 58: Can bottom-up accountability generate improvements in health outcomes?\nEGAP Policy Brief 69: Does bottom-up, citizen-based monitoring improve public service delivery?"
  },
  {
    "objectID": "coursebook/causalinference.html#core-content",
    "href": "coursebook/causalinference.html#core-content",
    "title": "Causal Inference",
    "section": "",
    "text": "What do we mean when we say “cause”? And why does it matter to be clear about the meaning of causal claims?\nAn introduction to potential outcomes as a way to think about alternative states of the world.\nRandomization helps us learn about counterfactual causal claims in a particularly useful way.\nThe three key core assumptions for causal inference: random assignment of subjects to treatment, non-interference, and excludability.\nComparison of randomized studies with observational studies.\nRandomization brings high internal validity, but it can’t promise external validity.\nYour causal question is closely linked so your research design."
  },
  {
    "objectID": "coursebook/causalinference.html#slides",
    "href": "coursebook/causalinference.html#slides",
    "title": "Causal Inference",
    "section": "",
    "text": "Below are slides with the core content that we cover in our lecture on causality. You can directly use these slides or make your own local copy and edit.\n\nR Markdown Source\nPDF Version\nHTML Version\n\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe causal inference presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019\nThe causal inference presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019\nThe causal inference presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nThe causal inference presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe introduction to experiments presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe causal inference presentation from EGAP Learning Days in Salima, Malawi, February 2017\nThe causal inference presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016"
  },
  {
    "objectID": "coursebook/causalinference.html#resources",
    "href": "coursebook/causalinference.html#resources",
    "title": "Causal Inference",
    "section": "",
    "text": "EGAP Methods Guide 10 Things You Need to Know about Causal Inference\nEGAP Methods Guide 10 Strategies for Figuring Out If X Caused Y\nEGAP Methods Guide 10 Things You Need to Know about Mechanisms\nEGAP Methods Guide 10 Things to Know About External Validity\n\n\n\n\n\n\n\n[@fisher_design_1935]. Fisher introduces the idea of randomization and hypothesis testing as a way to learn about causal inference.\n[@rubin:1974]. Rubin introduces the idea of potential outcomes and links counterfactual conceptualizations of causality to statistical inference.\n\n\n\n\n\n[@brady2008causation].\n[@gerber_field_2012, Chapter 1]. This book is a great resource for many topics in experimental design.\n[@morgan_counterfactuals_2007, Chapter 1]. This book includes nice examples of thinking through making causal claims from observational data.\n[@glennerster_running_2013]. This is a great introduction to running field experiments and discusses many examples.\n\n\n\n\n\nSome examples of causal questions:\n\nEGAP Policy Brief 38: Are radio voter education campaigns effective in discouraging voters from voting for parties/candidates that engage in vote-buying?\nEGAP Policy Brief 51: Can free and anonymous information communication technology strengthen local accountability and improve the delivery of public services?\nEGAP Policy Brief 58: Can bottom-up accountability generate improvements in health outcomes?\nEGAP Policy Brief 69: Does bottom-up, citizen-based monitoring improve public service delivery?"
  },
  {
    "objectID": "coursebook/estimation.html",
    "href": "coursebook/estimation.html",
    "title": "Estimands and Estimators",
    "section": "",
    "text": "Randomized experiments generate good guesses about the average outcome under treatment and the average outcome under control. This allows us to write down unbiased estimators of average treatment effects. We can also use the randomization to describe how estimates generated by an estimator can vary from experiment to experiment in the form of standard errors and confidence intervals.\nIn this module, we introduce several types of estimands, the target quantity to be estimated. The choice of estimand is a scientific and policy-informed decision – what quantity is useful for us to learn about? In addition, we want to select an appropriate estimator for this estimand as part of the research design. We discuss how estimators are applied to data to generate an estimate of our estimand and how to characterize the variability of this estimate.\n\n\n\nA causal effect, \\(\\tau_i\\), is a comparison of unobserved potential outcomes for each unit \\(i\\). For example, this can be a difference or a ratio of unobserved potential outcomes.\nTo learn about \\(\\tau_{i}\\), we can treat \\(\\tau_{i}\\) as an estimand or target quantity to be estimated (this module) or as a target quantity to be hypothesized about (hypothesis testing module).\nMany focus on the average treatment effect (ATE), \\(\\bar{\\tau}=\\sum_{i=1}^n  \\tau_{i}\\), in part, because it allows for easy estimation.\nAn estimator is a recipe for calculating a guess about the value of an estimand. For example, the difference between the mean of observed outcomes for \\(m\\) treated units and the mean of observed outcomes for \\(N-m\\) untreated units is one estimator of \\(\\bar{\\tau}\\).\nDifferent randomizations will produce different values of the same estimator targeting the same estimand. A standard error summarizes this variability in an estimator.\nA \\(100(1-\\alpha)\\)% confidence interval is a collection of hypotheses that cannot be rejected at the \\(\\alpha\\) level. We tend to report confidence intervals containing hypotheses about values of our estimand and use our estimator as a test statistic.\nEstimators should (1) avoid systematic error in their guessing of the estimand (be unbiased); (2) vary little in their guesses from experiment to experiment (be precise or efficient); and perhaps ideally (3) converge to the estimand as they use more and more information (be consistent).\nAnalyze as you randomize in the context of estimation means that (1) our standard errors should measure the variability from randomization and (2) our estimators should target estimands defined in terms of potential outcomes.\nWe do not control for background covariates when we analyze data from randomized experiments. But covariates can make our estimation more precise. This is called covariance adjustment. Covariance adjustment in randomized experiments differs from controlling for variables in observational studies.\nA policy intervention (like a letter that encourages exercise) may intend to change behavior via an active dose (actual exercise). We can learn about the causal effect of the intention by randomly assigning letters; this is the intent to treat effect, ITT.\nWe can learn about the causal effect of actual exercise by using the random assignment of letters as an instrument for the active dose (exercise itself) in order to learn about the causal effect of exercise among those who would change their behavior after receiving the letter. The average causal effect versions of these effects are often known as the complier average causal effect or the local average treatment effect.\n\n\n\n\nBelow are slides with the core content that we cover in this session.\n\nR Markdown Source\nPDF Version\nHTML Version\n\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe estimation presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019\nThe estimation presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019\nThe estimation presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nThe estimation presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\n\nYou can also see discussion of the problems of estimating the effect of the active dose of a treatment in these slides (as well as discussion of the problems that missing data on outcomes cause for estimation of average causal effects):\n\nThe design issues presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019 (first section reviews randomization designs)\nThe spillovers and attrition presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe threats presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe complications presentation from EGAP Learning Days in Salima, Malawi, February 2017\nThe threats presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016 (the middle section reviews ITT and non-compliance )\n\n\n\n\n\n\n\nEGAP Methods Guide 10 Types of Treatment Effect You Should Know About\nEGAP Methods Guide 10 Things to Know about Covariate Adjustment\nEGAP Methods Guide 10 Things to Know about Missing Data\nEGAP Methods Guide 10 Things to Know about the Local Average Treatment Effect\nEGAP Methods Guide 10 Things to Know about Spillovers\n\n\n\n\n\n[@gerber_field_2012]. Chapter 2.7 on excludability and non-interference, Chapter 3, Chapter 5 on one-sided noncompliance, Chapter 6 on two-sided noncompliance, Chapter 7 on attrition, Chapter 8 on interference between experimental units.\n[@bowers2020causality].\n\n\n\n\n\nDeclareDesign\nestimatr package for R"
  },
  {
    "objectID": "coursebook/estimation.html#core-content",
    "href": "coursebook/estimation.html#core-content",
    "title": "Estimands and Estimators",
    "section": "",
    "text": "A causal effect, \\(\\tau_i\\), is a comparison of unobserved potential outcomes for each unit \\(i\\). For example, this can be a difference or a ratio of unobserved potential outcomes.\nTo learn about \\(\\tau_{i}\\), we can treat \\(\\tau_{i}\\) as an estimand or target quantity to be estimated (this module) or as a target quantity to be hypothesized about (hypothesis testing module).\nMany focus on the average treatment effect (ATE), \\(\\bar{\\tau}=\\sum_{i=1}^n  \\tau_{i}\\), in part, because it allows for easy estimation.\nAn estimator is a recipe for calculating a guess about the value of an estimand. For example, the difference between the mean of observed outcomes for \\(m\\) treated units and the mean of observed outcomes for \\(N-m\\) untreated units is one estimator of \\(\\bar{\\tau}\\).\nDifferent randomizations will produce different values of the same estimator targeting the same estimand. A standard error summarizes this variability in an estimator.\nA \\(100(1-\\alpha)\\)% confidence interval is a collection of hypotheses that cannot be rejected at the \\(\\alpha\\) level. We tend to report confidence intervals containing hypotheses about values of our estimand and use our estimator as a test statistic.\nEstimators should (1) avoid systematic error in their guessing of the estimand (be unbiased); (2) vary little in their guesses from experiment to experiment (be precise or efficient); and perhaps ideally (3) converge to the estimand as they use more and more information (be consistent).\nAnalyze as you randomize in the context of estimation means that (1) our standard errors should measure the variability from randomization and (2) our estimators should target estimands defined in terms of potential outcomes.\nWe do not control for background covariates when we analyze data from randomized experiments. But covariates can make our estimation more precise. This is called covariance adjustment. Covariance adjustment in randomized experiments differs from controlling for variables in observational studies.\nA policy intervention (like a letter that encourages exercise) may intend to change behavior via an active dose (actual exercise). We can learn about the causal effect of the intention by randomly assigning letters; this is the intent to treat effect, ITT.\nWe can learn about the causal effect of actual exercise by using the random assignment of letters as an instrument for the active dose (exercise itself) in order to learn about the causal effect of exercise among those who would change their behavior after receiving the letter. The average causal effect versions of these effects are often known as the complier average causal effect or the local average treatment effect."
  },
  {
    "objectID": "coursebook/estimation.html#slides",
    "href": "coursebook/estimation.html#slides",
    "title": "Estimands and Estimators",
    "section": "",
    "text": "Below are slides with the core content that we cover in this session.\n\nR Markdown Source\nPDF Version\nHTML Version\n\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe estimation presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019\nThe estimation presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019\nThe estimation presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nThe estimation presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\n\nYou can also see discussion of the problems of estimating the effect of the active dose of a treatment in these slides (as well as discussion of the problems that missing data on outcomes cause for estimation of average causal effects):\n\nThe design issues presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019 (first section reviews randomization designs)\nThe spillovers and attrition presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe threats presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe complications presentation from EGAP Learning Days in Salima, Malawi, February 2017\nThe threats presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016 (the middle section reviews ITT and non-compliance )"
  },
  {
    "objectID": "coursebook/estimation.html#resources",
    "href": "coursebook/estimation.html#resources",
    "title": "Estimands and Estimators",
    "section": "",
    "text": "EGAP Methods Guide 10 Types of Treatment Effect You Should Know About\nEGAP Methods Guide 10 Things to Know about Covariate Adjustment\nEGAP Methods Guide 10 Things to Know about Missing Data\nEGAP Methods Guide 10 Things to Know about the Local Average Treatment Effect\nEGAP Methods Guide 10 Things to Know about Spillovers\n\n\n\n\n\n[@gerber_field_2012]. Chapter 2.7 on excludability and non-interference, Chapter 3, Chapter 5 on one-sided noncompliance, Chapter 6 on two-sided noncompliance, Chapter 7 on attrition, Chapter 8 on interference between experimental units.\n[@bowers2020causality].\n\n\n\n\n\nDeclareDesign\nestimatr package for R"
  },
  {
    "objectID": "coursebook/glossary.html#key-concepts",
    "href": "coursebook/glossary.html#key-concepts",
    "title": "(APPENDIX) Appendix",
    "section": "Key Concepts",
    "text": "Key Concepts\nSee the module on causal inference, estimands and estimators.\n\nPotential outcome \\(Y_i(T)\\) The outcome \\(Y\\) that unit \\(i\\) would have under treatment condition \\(T\\). We think of these as fixed quantities for a specific point in time. \\(T\\) can be 0 for control or 1 for treatment if there is only one type of treatment. See the module on causal inference.\nTreatment effect \\(\\tau_i\\) for unit \\(i\\) The contrast between potential outcomes under two treatment conditions for unit \\(i\\). We typically define the treatment effect as the difference in potential outcomes under treatment and control, \\(Y_i(1)-Y_i(0)\\). See the module on causal inference.\nFundamental problem of causal inference in the counterfactual framework. We can’t observe both \\(Y_i(1)\\) and \\(Y_i(0)\\) for a given unit, so we can’t get \\(\\tau_i\\) directly. See the module on causal inference.\nEstimand The thing you want to estimate. An example of an estimand is the average treatment effect. In counterfactual causal inference, this is a function of potential outcomes, not fully observed outcomes. See the module on estimands and estimators.\nEstimator How you make a guess about the value of your estimand from the data you have (i.e., observed). An example of an estimator is the difference-in-means. See the module on estimands and estimators.\n\nAverage treatment effect, ATE The average of the treatment effect for all individuals in your subject pool. This is a type of estimand. If we define \\(\\tau_i\\) to be \\(Y_i(1)-Y_i(0)\\), then the ATE is \\(\\overline{Y_i(1)-Y_i(0)}\\), which is also equivalent to \\(\\overline{{Y}_i(1)}-\\overline{{Y}_i(0)}\\). Notice that we do not use the \\(E[Y_i (1)]\\) style of notation here because \\(E[]\\) means “average over repeated operations,” but \\(\\overline{Y}\\) means “average over a set of observations”. See the module on causal inference and the module on estimands and estimators.\n\nRandom sampling Selecting subjects from a population with known probabilities strictly between 0 and 1.\n\\(k\\)-arm experiment An experiment that has \\(k\\) treatment conditions (including control). See the module on randomization.\nRandom assignment Assigning subjects to experimental conditions with known probabilities strictly between 0 and 1. This is equivalent to random sampling without replacement from the potential outcomes. There are several strategies for random assignment: simple, complete, cluster, block, blocked-cluster. See the module on randomization.\nExternal validity Findings from your study teach you about contexts outside of your sample — in other locations or for other interventions."
  },
  {
    "objectID": "coursebook/glossary.html#statistical-inference",
    "href": "coursebook/glossary.html#statistical-inference",
    "title": "(APPENDIX) Appendix",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nSee modules on hypothesis testing and statistical power.\n\nHypothesis A simple, clear, falsifiable claim about the world. In counterfactual causal inference, this is a statement about a relationship among potential outcomes, like \\(H_0: Y_i(T_i=0) = Y_i(T_i=1) + \\tau_i\\) for the hypothesis that the potential outcome under treatment is the potential outcome under control plus some effect for each unit \\(i\\). See the module on hypothesis testing.\nNull hypothesis A conjecture about the world that you may reject after seeing the data. See the module on hypothesis testing.\nSharp null hypothesis of no effect The null hypothesis that there is no treatment effect for any subject. This means \\(Y_i(1)=Y_i(0)\\) for all \\(i\\). We might write this as \\(H_0: Y_i(T_i=0) = Y_i(T_i=1)\\). See the module on hypothesis testing.\n\\(p\\)-value The probability of seeing a test statistic as large (in absolute value) as or larger than the test statistic calculated from observed data. See the module on hypothesis testing.\nOne-sided vs. two-sided test When you have a strong expectation that the effect is either positive or negative, you can conduct a one-sided test. When you do not have such a strong expectation, conduct a two-sided test. A one-sided test has more power than a two-sided test for the same experiment. See the module on hypothesis testing.\nStandard deviation Square root of the mean-square deviation from the average of a variable. It is a measure of the dispersion or spread of a statistic. \\(SD_x=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_i-\\bar{x})^2}\\)\nFalse Positive Rate/Type I Error of a Test A well-operating hypothesis test rejects a hypothesis about a true causal effect no more than \\(\\alpha\\) % of the time. The false positive rate is the rate at which a test will cast doubt on a true hypothesis. It is the rate at which the test will encourage the analyst to say “statistically significant” when, in fact, there is no causal relationship. See the module on hypothesis testing.\nSampling distribution The distribution of estimates (e.g., estimates of the ATE) for all possible treatment assignments. In design-based statistical inference for randomized experiments, the distribution of estimates from an estimator is generated from randomizations. Many call this a “sampling distribution” because textbooks often use the idea of repeated samples from a population rather than repeated randomizations to describe this kind of variation.\nStandard error The standard deviation of the sampling distribution. A bigger standard error means that our estimates are more susceptible to sampling variation. See the module on estimands and estimators.\nCoverage of a confidence interval A well-operating confidence interval contains the true causal effect \\(100 ( 1 - \\alpha)\\) % of the time. A confidence interval has incorrect coverage when it excludes the true parameter less than \\(100 (1 - \\alpha)\\)% of the time. For example, a 95% confidence interval is supposed to only exclude the true parameter less than 5% of the time.\nStatistical power of a test Probability that a test of causal effects will detect a statistically significant treatment effect if the effect exists. See the module on statistical power. This depends on:\n\nThe number of observations in each arm of the experiment\nEffect size (usually measured in standardized units)\nNoisiness of the outcome variable\nSignificance level (\\(\\alpha\\), which is fixed by convention)\nOther factors including what proportion of your units are assigned to different treatment conditions.\n\nIntra-cluster correlation How correlated the potential outcomes of units are within clusters compared to across clusters. Higher intra-cluster correlation hurts power.\nUnbiased An estimator is unbiased if you expect that it will return the right outcome. That means that if you were to run the experiment many times, the estimate might be too high or to low sometimes but it will be right on average. See the module on estimands and estimators.\nBias Bias is the difference between the average value of the estimator across its sampling distribution and the single, fixed value of the estimand. See the module on estimands and estimators.\nConsistency of an estimator An estimator that produces answers that become ever nearer to the true value of the estimand as the sample size increases is a consistent estimator of that estimand. A consistent estimator may or may not be unbiased. See the module on estimands and estimators.\nPrecision/Efficiency of an estimator The variation in or width of the sampling distribution of an estimator. See the module on estimands and estimators."
  },
  {
    "objectID": "coursebook/glossary.html#randomization-strategies",
    "href": "coursebook/glossary.html#randomization-strategies",
    "title": "(APPENDIX) Appendix",
    "section": "Randomization Strategies",
    "text": "Randomization Strategies\nSee the module on randomization.\n\nSimple An independent coin flip for each unit. You are not guaranteed that your experiment will have a specific number of treated units.\nComplete Assign \\(m\\) out of \\(N\\) units to treatment. You know how many units will be treated in your experiment and each unit has a \\(m/N\\) probability of being treated. The number of ways treatment can be assigned (number of permutations of treatment assignment) is \\(\\frac{N!}{m!(N-m)!}\\).\nBlock First divide the sample into blocks, then do complete randomization separately in each block. A block is a set of units within which you conduct random assignment.\nCluster Clusters of units are randomly assigned to treatment conditions. A cluster is a set of units that will always be assigned to the same treatment status.\nBlocked-Cluster First form blocks of clusters. Then in each block, randomly assign the clusters to treatment conditions using complete randomization."
  },
  {
    "objectID": "coursebook/glossary.html#factorial-designs",
    "href": "coursebook/glossary.html#factorial-designs",
    "title": "(APPENDIX) Appendix",
    "section": "Factorial Designs",
    "text": "Factorial Designs\nSee the module on randomization.\n\nFactorial design A design with more than one treatment, with each treatment assigned independently. The simplest factorial design is a 2 by 2.\nConditional marginal effect The effect of one treatment, conditional on the other being held at a fixed value. For example: \\(Y_i(T_1=1|T_2=0)-Y_i(T_1=0|T_2=0)\\) is the marginal effect of \\(T_1\\) conditional on \\(T_2=0\\).\nAverage marginal effect Main effect of each treatment in a factorial design. It is the average of the conditional marginal effects for all the conditions of the other treatment, weighted by the proportion of the sample that was assigned to each condition.\nInteraction effect In a factorial design, we may also estimate interaction effects.\n\nNo interaction effect: one treatment does not amplify or reduce the effect of the other treatment.\nMultiplicative interaction effect: the effect of one treatment depends on which condition a unit was assigned for the other treatment. This means one treatment does amplify or reduce the effect of the other. The effect of two treatments together is not the sum of the effect of each treatment."
  },
  {
    "objectID": "coursebook/glossary.html#threats",
    "href": "coursebook/glossary.html#threats",
    "title": "(APPENDIX) Appendix",
    "section": "Threats",
    "text": "Threats\nSee the module on threats.\n\nHawthorne effect When a subject responds to being observed.\nSpillover When a subject responds to another subject’s treatment status. Example: my health depends on whether my neighbor is vaccinated, as well as whether I am vaccinated.\nAttrition When outcomes for some subjects are not measured. This might be caused, for example, by people migrating, refusing to respond to endline surveys, or dying. This is especially problematic for inference when it is correlated with treatment status.\nCompliance A unit’s treatment status matches its assigned treatment condition. Example of non-compliance: a unit assigned to treatment doesn’t take it. Example of compliance: a unit assigned to control does not take treatment.\nCompliance types There are four types of units in terms of compliance:\n\nCompliers Units that would take treatment if assigned to treatment and would be untreated if assigned to control.\nAlways-takers Units that would take treatment if assigned to treatment and if assigned to control.\nNever-takers Units that would be untreated if assigned to treatment and if assigned to control.\nDefiers Units that would be untreated if assigned to treatment and would take treatment if assigned to control.\n\nOne-sided non-compliance The experiment has only compliers and either always-takers or never-takers. Usually, we think of one-sided non-compliance as having only never-takers and compliers, meaning that that local average treatment effect is the effect of treatment on the treated.\nTwo-sided non-compliance The experiment may have all four latent groups.\nEncouragement design An experiment that randomizes \\(T\\) (treatment assignment), and we measure \\(D\\) (whether the unit takes treatment) and \\(Y\\) (outcome). We can estimate the ITT and the LATE (local average treatment effect, aka CACE—complier average causal effect). It requires three assumptions.\n\nMonotonicity Assumption of either no defiers or no compliers. Usually we assume no defiers which means that the effect of assignment on take up of treatment is either positive or zero but not negative.\nFirst stage Assumption that there is an effect of \\(T\\) on \\(D\\).\nExclusion restriction Assumption that \\(T\\) affects \\(Y\\) only through \\(D\\). This is usually the most problematic assumption.\n\nIntention-to-treat effect (ITT) The effect of \\(T\\) (treatment assignment) on \\(Y\\).\nLocal average treatment effect (LATE) The effect of \\(D\\) (taking treatment) on \\(Y\\) for compliers. Also known as the complier average causal effect (CACE). Under the exclusion restriction and monotonicity, the LATE is equal to ITT divided by the proportion of your sample who are compliers.\nDownstream experiment An encouragement design study that takes advantage of the randomization of \\(T\\) by a previous study. The outcome from that previous study is the \\(D\\) in the downstream experiment."
  },
  {
    "objectID": "coursebook/index.html",
    "href": "coursebook/index.html",
    "title": "Theory and Practice of Field Experiments",
    "section": "",
    "text": "Over the past decade, Evidence in Governance and Politics (EGAP) has organized Learning Days workshops with the aim of building experimental social-science research capacity among principal investigators (PIs) – both researchers and practitioners – in Africa and Latin America. By sharing the practical and statistical methods of randomized field experiments with workshop participants, the Learning Days effort hopes to identify and nurture researcher networks around the world and to create strong, productive connections between these researchers and EGAP members.\nThe Learning Days workshops are a combination of design clinics, research presentations, guided work with statistical software, and topical lectures by a small group of instructors, largely professors and PhD students from the EGAP network. The workshops focus on methods for the design and analysis of randomized experiments in the field rather than on randomized experiments in the lab or non-randomized studies.\nThis book grew out of a desire to share the materials we developed for the Learning Days. The current version is written primarily for instructors and organizers of similar workshops and courses aimed at PIs — i.e., professors, post-doctoral fellows, PhD students, and NGO/government agency evaluators — who will implement randomized studies of programs related to institutions, governance, and development. Much of the material will also be useful as a refresher for past participants of the Learning Days workshops.\nThis book is a comprehensive overview of causal inference methods for researchers developing an experimental research design. It is organized in modules and covers topics such as causal inference, randomization, hypothesis testing, estimands, estimators, statistical power, measurement, threats to internal validity, and the ethics of experimentation. The modules appear in the order the Learning Days instructors have found most useful. However, the modules are linked to one another and can be reordered to suit your needs as an instructor. In the appendix, we include some course preliminaries including a glossary of terms and an introduction to R and RStudio.\nThe book includes slides on the core content, the EGAP Research Design Form, and references to research examples and slides used in previous Learning Days. This material builds significantly on and links to EGAP’s work on methodology, summarized in the EGAP Methods Guides. We have made significant extensions to the past Learning Days’ materials on hypothesis testing, estimation, and statistical power and added new modules on the research design process, measurement, and ethical considerations. The slides and modules presented here contain too much information to be covered in a single week, the usual length of a Learning Days workshop. We have chosen to present more rather than less information, however, to help instructors tailor their courses to their specific audiences.\n\n\nTo gain the most benefit from this book, please have R and RStudio installed on your machine. In fact, the slides assume that you will use Rmarkdown to adapt them for your own purposes.\nTo get going with R, see the module Introduction to R and RStudio.\nYou can copy this book or parts thereof (e.g., slides, etc.) either by using the Download button on the front page of http://github.com/egap/theory_and_practice_of_field_experiments or by using github directly (by forking this repository).\nWe are happy for anyone to use the materials as long as EGAP is attributed. See Creative Commons Attribution-ShareAlike 4.0 International License for the precise terms.\n\n\n\nIf you have any questions, feedback or have organized your own event, please get in touch! Simply post an issue on Github or make comments using hypothes.is in your browser and let us know via email, admin@egap.org. We’ll periodically go through the comments.\n\n\n\nThe materials included in this book have been developed over the past years by various Learning Days instructors. These include (in alphabetical order) Jake Bowers, Jasper Cooper, Ana De la O, Lindsay Dolan, Natalia Garbiras Díaz, Macartan Humphreys, Nahomi Ichino, Salif Jaiteh, Gareth Nellis, Dan Nielson, Rafael Piñeiro, Fernando Rosenblatt, Tara Slough, Peter van der Windt and Maarten Voors. We thank Natalia Garbiras Díaz, Macartan Humphreys, Anghella Brigeth Rosero Rodriguez, and Tara Slough in particular for comments on an early draft of the book. We also thank Brice Bado, Simon Chauchard, Jasper Cooper, Simone Dietrich, Thad Dunning, Jessica Gottlieb, Macartan Humphreys, Julien Labonne, Ambaliou Olounlade, Daniel Rubenson, and Saloua Zerhouni for their help in reviewing the French translation and Rosario Aguilar, Ana De la O, Pablo Egaña del Sol, Omar Garcia Ponce, Paul Lagunes, Luis Maldonado, Fernando Martel Garcia, Paula Muñoz, Raul Pacheco-Vega, Rafael Piñiero, Pablo Querubin, Mauricio Romero, Fernando Rosenblatt, Santiago Saavedra, Lucía Tiscornia, Santiago Tobón, and Juan Vargas for their help in reviewing the Spanish translation of these materials.\nAt EGAP, Matt Lisiecki, Ingrid Lee, Goldie Negelev, Max Mendez-Back and others have provided wonderful support. Learning Days have been generously funded by the Hewlett Foundation and supported by institutions around the world including the African School of Economics (Benin), Universidad Diego Portales (Chile), Universidad de los Andes (Colombia), Ghana Center for Democratic Development (Ghana), Mercy Corps (Guatemala), Invest in Knowledge (Malawi), NYU Abu Dhabi (UAE), Universidad Católica del Uruguay (Uruguay)."
  },
  {
    "objectID": "coursebook/index.html#how-to-use-the-book",
    "href": "coursebook/index.html#how-to-use-the-book",
    "title": "Theory and Practice of Field Experiments",
    "section": "",
    "text": "To gain the most benefit from this book, please have R and RStudio installed on your machine. In fact, the slides assume that you will use Rmarkdown to adapt them for your own purposes.\nTo get going with R, see the module Introduction to R and RStudio.\nYou can copy this book or parts thereof (e.g., slides, etc.) either by using the Download button on the front page of http://github.com/egap/theory_and_practice_of_field_experiments or by using github directly (by forking this repository).\nWe are happy for anyone to use the materials as long as EGAP is attributed. See Creative Commons Attribution-ShareAlike 4.0 International License for the precise terms."
  },
  {
    "objectID": "coursebook/index.html#we-would-love-to-hear-from-you",
    "href": "coursebook/index.html#we-would-love-to-hear-from-you",
    "title": "Theory and Practice of Field Experiments",
    "section": "",
    "text": "If you have any questions, feedback or have organized your own event, please get in touch! Simply post an issue on Github or make comments using hypothes.is in your browser and let us know via email, admin@egap.org. We’ll periodically go through the comments."
  },
  {
    "objectID": "coursebook/index.html#acknowledgments",
    "href": "coursebook/index.html#acknowledgments",
    "title": "Theory and Practice of Field Experiments",
    "section": "",
    "text": "The materials included in this book have been developed over the past years by various Learning Days instructors. These include (in alphabetical order) Jake Bowers, Jasper Cooper, Ana De la O, Lindsay Dolan, Natalia Garbiras Díaz, Macartan Humphreys, Nahomi Ichino, Salif Jaiteh, Gareth Nellis, Dan Nielson, Rafael Piñeiro, Fernando Rosenblatt, Tara Slough, Peter van der Windt and Maarten Voors. We thank Natalia Garbiras Díaz, Macartan Humphreys, Anghella Brigeth Rosero Rodriguez, and Tara Slough in particular for comments on an early draft of the book. We also thank Brice Bado, Simon Chauchard, Jasper Cooper, Simone Dietrich, Thad Dunning, Jessica Gottlieb, Macartan Humphreys, Julien Labonne, Ambaliou Olounlade, Daniel Rubenson, and Saloua Zerhouni for their help in reviewing the French translation and Rosario Aguilar, Ana De la O, Pablo Egaña del Sol, Omar Garcia Ponce, Paul Lagunes, Luis Maldonado, Fernando Martel Garcia, Paula Muñoz, Raul Pacheco-Vega, Rafael Piñiero, Pablo Querubin, Mauricio Romero, Fernando Rosenblatt, Santiago Saavedra, Lucía Tiscornia, Santiago Tobón, and Juan Vargas for their help in reviewing the Spanish translation of these materials.\nAt EGAP, Matt Lisiecki, Ingrid Lee, Goldie Negelev, Max Mendez-Back and others have provided wonderful support. Learning Days have been generously funded by the Hewlett Foundation and supported by institutions around the world including the African School of Economics (Benin), Universidad Diego Portales (Chile), Universidad de los Andes (Colombia), Ghana Center for Democratic Development (Ghana), Mercy Corps (Guatemala), Invest in Knowledge (Malawi), NYU Abu Dhabi (UAE), Universidad Católica del Uruguay (Uruguay)."
  },
  {
    "objectID": "coursebook/hypothesistesting.html",
    "href": "coursebook/hypothesistesting.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "We cannot directly observe causal effects because of the fundamental problem of counterfactual causal inference (causal inference module). So how can we learn about these unobserved causal effects using what we do observe? In a randomized experiment, we can assess guesses or hypotheses about the unobserved causal effects by comparing what we observe in a given experiment to what we would observe if we were able to repeat the experimental manipulation and the guess or hypothesis were true.\nIn this module we introduce hypothesis testing, how it relates to causal inference, \\(p\\)-values, and what to do when we have multiple hypotheses to test.\n\n\n\nWhat is a good hypothesis?\nThe relationship between hypothesis testing and causal inference.\nHypothesis tests.\n\nNull hypotheses.\nEstimators versus test statistics.\nIn an experiment, a reference distribution for a hypothesis test comes from the experimental design and the randomization.\n\\(p\\)-values and how to interpret the results of hypothesis tests.\n\nA good hypothesis test should (1) cast doubt on the truth rarely (i.e., have a controlled and low false positive rate), and (2) easily distinguish signal from noise (i.e., cast doubt on falsehoods often; have high statistical power).\nHow would we know when our hypothesis test is doing a good job? (Power analysis is its own module).\n\nFalse positive rates.\nCorrect coverage of a confidence interval.\nAssessing the false positive rate of a hypothesis test for a given design and choice of test statistic; the case of cluster-randomized trials and robust cluster standard errors.\n\nBe careful when testing many hypotheses, such as when you have more than two treatment arms or you are assessing the effects of a treatment on multiple outcomes. We should be careful to adjust the \\(p\\)-values or confidence intervals to reflect the number of tests/intervals produced.\n\n\n\n\nBelow are slides with the core content that we cover in our lecture on hypothesis testing. You can directly use these slides or make your own local copy and edit.\n\nR Markdown Source\nPDF Version\nHTML Version\n\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe hypothesis testing presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019\nThe hypothesis testing presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019\nThe hypothesis testing presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nThe hypothesis testing presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe hypothesis testing presentation from EGAP Learning Days in Salima, Malawi, February 2017\nThe hypothesis testing presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016\n\n\n\n\n\n\n\nEGAP Methods Guide 10 Things to Know about Hypothesis Testing\nEGAP Methods Guide 10 Things You Need to Know about Multiple Comparisons\n\n\n\n\n\n[@gerber_field_2012]. Chapter 3: Sampling Distributions, Statistical Inference, and Hypothesis Testing.\n[@rosenbaum2010design]. Chapter 2: Causal Inference in Randomized Experiments.\n[@rosenbaum2017observation]. Part I: Randomized Experiments."
  },
  {
    "objectID": "coursebook/hypothesistesting.html#core-content",
    "href": "coursebook/hypothesistesting.html#core-content",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "What is a good hypothesis?\nThe relationship between hypothesis testing and causal inference.\nHypothesis tests.\n\nNull hypotheses.\nEstimators versus test statistics.\nIn an experiment, a reference distribution for a hypothesis test comes from the experimental design and the randomization.\n\\(p\\)-values and how to interpret the results of hypothesis tests.\n\nA good hypothesis test should (1) cast doubt on the truth rarely (i.e., have a controlled and low false positive rate), and (2) easily distinguish signal from noise (i.e., cast doubt on falsehoods often; have high statistical power).\nHow would we know when our hypothesis test is doing a good job? (Power analysis is its own module).\n\nFalse positive rates.\nCorrect coverage of a confidence interval.\nAssessing the false positive rate of a hypothesis test for a given design and choice of test statistic; the case of cluster-randomized trials and robust cluster standard errors.\n\nBe careful when testing many hypotheses, such as when you have more than two treatment arms or you are assessing the effects of a treatment on multiple outcomes. We should be careful to adjust the \\(p\\)-values or confidence intervals to reflect the number of tests/intervals produced."
  },
  {
    "objectID": "coursebook/hypothesistesting.html#slides",
    "href": "coursebook/hypothesistesting.html#slides",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Below are slides with the core content that we cover in our lecture on hypothesis testing. You can directly use these slides or make your own local copy and edit.\n\nR Markdown Source\nPDF Version\nHTML Version\n\nYou can also see the slides used in previous EGAP Learning Days:\n\nThe hypothesis testing presentation from EGAP Learning Days at the African School of Economics, Abomey-Calavi, June 2019\nThe hypothesis testing presentation from EGAP Learning Days at Universidad de Los Andes, Bogotá, April 2019\nThe hypothesis testing presentation from EGAP Learning Days at Universidad Católica del Uruguay, Montevideo, March 2018\nThe hypothesis testing presentation from EGAP Learning Days in Guatemala City, Guatemala, August 2017\nThe hypothesis testing presentation from EGAP Learning Days in Salima, Malawi, February 2017\nThe hypothesis testing presentation from EGAP Learning Days at Universidad Diego Portales in Santiago, Chile, May 2016"
  },
  {
    "objectID": "coursebook/hypothesistesting.html#resources",
    "href": "coursebook/hypothesistesting.html#resources",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "EGAP Methods Guide 10 Things to Know about Hypothesis Testing\nEGAP Methods Guide 10 Things You Need to Know about Multiple Comparisons\n\n\n\n\n\n[@gerber_field_2012]. Chapter 3: Sampling Distributions, Statistical Inference, and Hypothesis Testing.\n[@rosenbaum2010design]. Chapter 2: Causal Inference in Randomized Experiments.\n[@rosenbaum2017observation]. Part I: Randomized Experiments."
  },
  {
    "objectID": "coursebook/references.html",
    "href": "coursebook/references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "We collected statistical software tools useful for designing, implementing, and analyzing results from impact evaluations. You can search for functions and for software packages in R and Stata below.\n\n\n\n\n\npackage\nfn\nMember(s)\nFunctionality\nPlatform\nURL\n\n\n\n\nri2\nconduct_ri\nAlex Coppock\nRandomization Inference for Randomized Experiments\nR\nhttps://rdrr.io/cran/ri2/\n\n\nri2\ntidy\nAlex Coppock\nRandomization Inference for Randomized Experiments\nR\nhttps://rdrr.io/cran/ri2/\n\n\nhettx\ndetect_idiosyncratic\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nestimate_systematic\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nget.p.value\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nKS.stat\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nmake.linear.data\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nmake.quadradic.data\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nmake.randomized.compliance.dat\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nmake.randomized.dat\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nmake.skew.data\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nPenn46_ascii\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nR2\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nrq.stat\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nrq.stat.cond.cov\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nrq.stat.uncond.cov\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nSE\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nSKS.pool.t\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nSKS.stat\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nSKS.stat.cov\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nSKS.stat.cov.pool\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nSKS.stat.cov.rq\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nSKS.stat.int.cov\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nSKS.stat.int.cov.pool\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\ntest.stat.info\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nToyData\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nvariance.ratio.test\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nhettx\nWSKS.t\nAvi Feller\nFisherian and Neymanian Methods for Detecting and Measuring Treatment Effect Variation\nR\nhttps://rdrr.io/cran/hettx/\n\n\nsensemakr\nadd_bound_to_contour\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nadjusted_estimate\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nadjusted_partial_r2\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nadjusted_se\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nadjusted_t\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nbias\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\ncolombia\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\ndarfur\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\ngroup_partial_r2\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nmodel_helper\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\novb_bounds\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\novb_contour_plot\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\novb_extreme_plot\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\novb_minimal_reporting\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\novb_partial_r2_bound\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\npartial_f\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\npartial_f2\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\npartial_r2\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nrel_bias\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nrelative_bias\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nresid_maker\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nrobustness_value\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nsensemakr\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nsensemakr\nsensitivity_stats\nChad Hazlett\nsensitivity analysis for linear regression\nR, Stata\nhttps://rdrr.io/cran/sensemakr/\n\n\nCBPS\nAsyVar\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nCBPS\nbalance\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nCBPS\nBlackwell\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nCBPS\nCBIV\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nCBPS\nCBMSM\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nCBPS\nCBPS\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nCBPS\nCBPS.fit\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nCBPS\nhdCBPS\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nCBPS\nLaLonde\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nCBPS\nnpCBPS\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nCBPS\nvcov_outcome\nChad Hazlett, Kosuke Imai\nCovariate Balancing Propensity Score\nR\nhttps://rdrr.io/cran/CBPS/\n\n\nDeclareDesign\ncite_design\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ncompare_design_code\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ncompare_design_data\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ncompare_design_estimates\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ncompare_design_inquiries\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ncompare_design_summaries\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ncompare_designs\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ncompare_diagnoses\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_assignment\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_diagnosands\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_estimand\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_estimands\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_estimator\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_estimators\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_inquiries\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_inquiry\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_measurement\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_model\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_population\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_potential_outcomes\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_reveal\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_sampling\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_step\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndeclare_test\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndelete_step\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndiagnose_design\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndiagnose_designs\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndraw_data\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndraw_estimand\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndraw_estimands\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ndraw_estimates\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nexpand_design\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nget_diagnosands\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nget_estimates\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nget_simulations\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ninsert_step\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nlabel_estimator\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nlabel_test\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nmodel_handler\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\npop.var\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nprint_code\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nredesign\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nreplace_step\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nreshape_diagnosis\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nrun_design\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nset_citation\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nset_diagnosands\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nsimulate_design\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nsimulate_designs\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ntidy\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ntidy_estimator\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\ntidy_try\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/\n\n\nDeclareDesign\nvars\nGraeme Blair, Jasper Cooper, Alex Coppock, Macartan Humphreys\ndiagnosing and improving research designs\nR\nhttps://rdrr.io/cran/DeclareDesign/"
  }
]