<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-0.9.600">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Macartan Humphreys">

<title>EGAP Learning - 10 Things You Need to Know About Causal Inference</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">EGAP Learning</span>
  </a>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">Home</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../guides.html">Methods guides</a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#a-causal-claim-is-a-statement-about-what-didnt-happen." id="toc-a-causal-claim-is-a-statement-about-what-didnt-happen." class="nav-link active" data-scroll-target="#a-causal-claim-is-a-statement-about-what-didnt-happen.">1. A causal claim is a statement about what didn’t happen.</a></li>
  <li><a href="#there-is-no-causation-without-manipulation." id="toc-there-is-no-causation-without-manipulation." class="nav-link" data-scroll-target="#there-is-no-causation-without-manipulation.">2. There is no causation without manipulation.</a></li>
  <li><a href="#causes-are-non-rival." id="toc-causes-are-non-rival." class="nav-link" data-scroll-target="#causes-are-non-rival.">3. Causes are non-rival.</a></li>
  <li><a href="#x-can-cause-y-even-if-x-is-not-a-necessary-condition-or-a-sufficient-condition-for-y." id="toc-x-can-cause-y-even-if-x-is-not-a-necessary-condition-or-a-sufficient-condition-for-y." class="nav-link" data-scroll-target="#x-can-cause-y-even-if-x-is-not-a-necessary-condition-or-a-sufficient-condition-for-y.">4. <span class="math inline">\(X\)</span> can cause <span class="math inline">\(Y\)</span> even if <span class="math inline">\(X\)</span> is not a necessary condition or a sufficient condition for <span class="math inline">\(Y\)</span>.</a></li>
  <li><a href="#there-is-a-fundamental-problem-of-causal-inference." id="toc-there-is-a-fundamental-problem-of-causal-inference." class="nav-link" data-scroll-target="#there-is-a-fundamental-problem-of-causal-inference.">5. There is a fundamental problem of causal inference.</a></li>
  <li><a href="#you-can-estimate-average-causal-effects-even-though-you-cannot-observe-any-individual-causal-effects." id="toc-you-can-estimate-average-causal-effects-even-though-you-cannot-observe-any-individual-causal-effects." class="nav-link" data-scroll-target="#you-can-estimate-average-causal-effects-even-though-you-cannot-observe-any-individual-causal-effects.">6. You can estimate average causal effects even though you cannot observe any individual causal effects.</a></li>
  <li><a href="#estimating-average-causal-effects-does-not-require-that-treatment-and-control-groups-are-identical." id="toc-estimating-average-causal-effects-does-not-require-that-treatment-and-control-groups-are-identical." class="nav-link" data-scroll-target="#estimating-average-causal-effects-does-not-require-that-treatment-and-control-groups-are-identical.">7. Estimating average causal effects does not require that treatment and control groups are identical.</a></li>
  <li><a href="#correlation-is-not-causation." id="toc-correlation-is-not-causation." class="nav-link" data-scroll-target="#correlation-is-not-causation.">8. Correlation is not causation.</a></li>
  <li><a href="#if-you-know-that-on-average-a-causes-b-and-b-causes-c-this-does-not-mean-that-on-average-a-causes-c." id="toc-if-you-know-that-on-average-a-causes-b-and-b-causes-c-this-does-not-mean-that-on-average-a-causes-c." class="nav-link" data-scroll-target="#if-you-know-that-on-average-a-causes-b-and-b-causes-c-this-does-not-mean-that-on-average-a-causes-c.">9. If you know that, on average, <span class="math inline">\(A\)</span> causes <span class="math inline">\(B\)</span> and <span class="math inline">\(B\)</span> causes <span class="math inline">\(C\)</span>, this does not mean that, on average, <span class="math inline">\(A\)</span> causes <span class="math inline">\(C\)</span>.</a></li>
  <li><a href="#its-easier-to-learn-about-the-effects-of-causes-than-to-learn-about-the-causes-of-effects." id="toc-its-easier-to-learn-about-the-effects-of-causes-than-to-learn-about-the-causes-of-effects." class="nav-link" data-scroll-target="#its-easier-to-learn-about-the-effects-of-causes-than-to-learn-about-the-causes-of-effects.">10. It’s easier to learn about the “effects of causes” than to learn about the “causes of effects.”</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">10 Things You Need to Know About Causal Inference</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Macartan Humphreys </p>
          </div>
  </div>
    
    
  </div>
  
<div>
  <div class="abstract">
    <div class="abstract-title">Abstract</div>
    <p>The philosopher David Lewis described causation as “something that makes a difference, and the difference it makes must be a difference from what would have happened without it.”[^1] This is the interpretation given to causality by most experimentalists. Even though the definition seems simple, it has many subtle implications. Here are ten ideas implied by this notion of causality that matter for research design.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
  </div>
</div>

</header>

<section id="a-causal-claim-is-a-statement-about-what-didnt-happen." class="level1">
<h1>1. A causal claim is a statement about what didn’t happen.</h1>
<p>For most experimentalists, the statement “<span class="math inline">\(X\)</span> caused <span class="math inline">\(Y\)</span>” means that <span class="math inline">\(Y\)</span> is present <em>and</em> <span class="math inline">\(Y\)</span> would not have been present if <span class="math inline">\(X\)</span> were not present. This definition requires a notion of what could have happened, but did not happen.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> Similarly, the “effect” of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> is thought of as the difference between the value that <span class="math inline">\(Y\)</span> would have taken given one value of <span class="math inline">\(X\)</span> and the value that <span class="math inline">\(Y\)</span> would have taken given another value of <span class="math inline">\(X\)</span>. Because of the focus on differences in outcomes, this approach is sometimes called the “difference making” or “counterfactual” approach to causation.</p>
<p><strong>Technical Note:</strong> Statisticians employ the “potential outcomes” framework to describe counterfactual relations. In this framework, we let <span class="math inline">\(Y_i(1)\)</span> denote the outcome for unit <span class="math inline">\(i\)</span> that would be observed under one condition (e.g., if unit <span class="math inline">\(i\)</span> received a treatment) and <span class="math inline">\(Y_i(0)\)</span> the outcome that would be observed in another condition (e.g., if unit <span class="math inline">\(i\)</span> did not receive the treatment). One causal effect of the treatment for unit <span class="math inline">\(i\)</span> might be a simple difference of the potential outcomes <span class="math inline">\(τ_i=Y_i(1)−Y_i(0)\)</span>. A treatment has a (positive or negative) causal effect on <span class="math inline">\(Y\)</span> for unit <span class="math inline">\(i\)</span> if <span class="math inline">\(Y_i(1)≠Y_i(0)\)</span>.</p>
</section>
<section id="there-is-no-causation-without-manipulation." class="level1">
<h1>2. There is no causation without manipulation.</h1>
<p>The “counterfactual” definition of causality requires one to be able to think through what outcomes may result in different conditions. How would things look if one party as opposed to another was elected? Everyday causal statements often fall short of this requirement in one of two ways.</p>
<ul>
<li>First, some statements do not specify clear counterfactual conditions. For example the claim that “the recession was caused by Wall Street” does not point to an obvious counterfactual— are we to consider whether there would have been a recession if Wall Street did not exist? Or is the statement really a statement about particular actions that Wall Street could have taken but did not. If so, which actions? The validity of such statements is hard to assess, and can depend on which counterfactual conditions are implied by a statement.</li>
<li>Second, some statements involve counterfactual conditions that cannot be imagined. For example, the claim that Peter got the job because he is Peter implies a consideration of what would have happened if Peter was not Peter. Alternatively, the claim that Peter got the job because he is a man requires considering Peter as other than a man. The problem is that the counterfactuals in these cases imply a change not just in the condition facing an individual but in the individual themselves.</li>
</ul>
<p>To avoid such problems, some statisticians urge a restriction of causal claims to treatments that can conceivably (not necessarily practically) be manipulated.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> For example, while we might have difficulties with the claim that Peter got the job because he was a man, we have no such difficulties with the claim that Peter got the job because the hiring agency thought he was a man.</p>
</section>
<section id="causes-are-non-rival." class="level1">
<h1>3. Causes are non-rival.</h1>
<p>Even though we may focus on the effect of a single cause <span class="math inline">\(X\)</span> on an outcome <span class="math inline">\(Y\)</span>, we generally do not expect that there is ever only a single cause of <span class="math inline">\(Y\)</span>.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> Moreover, if you add up the causal effects of different causes, there is no reason to expect them to add up to 100%. Hence, there is not much point trying to “apportion” outcomes to different causal factors. In other words, causes are not rival. The National Rifle Association argues, for example, that guns don’t kill people, people kill people. That statement does not make much sense in the counterfactual framework. Take away guns and you have no deaths from gunshot wounds. So guns are a cause. Take away people and you also have no deaths from gunshot wounds, so people are also a cause. Put differently, these two factors are simultaneously causes of the same outcomes.</p>
</section>
<section id="x-can-cause-y-even-if-x-is-not-a-necessary-condition-or-a-sufficient-condition-for-y." class="level1">
<h1>4. <span class="math inline">\(X\)</span> can cause <span class="math inline">\(Y\)</span> even if <span class="math inline">\(X\)</span> is not a necessary condition or a sufficient condition for <span class="math inline">\(Y\)</span>.</h1>
<p>We often talk about causal relations in deterministic terms. Even the Lewis quote at the top of this page seems to suggest a deterministic relation between causes and effects. Sometimes causal relations are thought to entail necessary conditions (for <span class="math inline">\(Y\)</span> to occur, <span class="math inline">\(X\)</span> has to happen); sometimes such relations are thought to entail sufficient conditions (if <span class="math inline">\(X\)</span> occurs, then <span class="math inline">\(Y\)</span> occurs). But once we are talking about multiple units, there are at least two ways in which we can think of <span class="math inline">\(X\)</span> causing <span class="math inline">\(Y\)</span> even if <span class="math inline">\(X\)</span> is neither a necessary nor a sufficient condition for <span class="math inline">\(Y\)</span>. The first is to reinterpret everything in probabilistic terms: by <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span>, we simply mean that the probability of <span class="math inline">\(Y\)</span> is higher when <span class="math inline">\(X\)</span> is present. Another is to allow for contingencies — for example, <span class="math inline">\(X\)</span> may cause <span class="math inline">\(Y\)</span> if condition <span class="math inline">\(Z\)</span> is present, but not otherwise.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
</section>
<section id="there-is-a-fundamental-problem-of-causal-inference." class="level1">
<h1>5. There is a fundamental problem of causal inference.</h1>
<p>If causal effects are statements about the difference between what happened and what could have happened, then causal effects cannot be measured. That’s bad news. Prospectively, you can arrange things so that you can observe either what happens if someone gets a treatment or what happens if they do not get the treatment. Yet, for the same person, you will never be able to observe both of these outcomes and hence also not the difference between them. This inability to observe unit-level causal effects is often called the “fundamental problem of causal inference.”</p>
</section>
<section id="you-can-estimate-average-causal-effects-even-though-you-cannot-observe-any-individual-causal-effects." class="level1">
<h1>6. You can estimate average causal effects even though you cannot observe any individual causal effects.</h1>
<p>Even though you cannot observe whether <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span> for any given unit, it can still be possible to figure out whether <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span> on average. The key insight here is that the average causal effect equals the difference between the average outcome across all units if all units were in the control condition and the average outcome across all units if all units were in the treatment condition. Many strategies for causal identification (see <a href="https://egap.org/resource/10-strategies-for-figuring-out-if-x-caused-y">10 Strategies for Figuring Out If X Caused Y</a>) focus on ways to learn about these average potential outcomes.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p><a href="https://egap.org/resource/10-things-to-know-about-hypothesis-testing/">10 Things to Know About Hypothesis Testing</a> describes how one can learn about individual causal effects rather than average causal effects given the fundamental problem of causal inference.</p>
</section>
<section id="estimating-average-causal-effects-does-not-require-that-treatment-and-control-groups-are-identical." class="level1">
<h1>7. Estimating average causal effects does not require that treatment and control groups are identical.</h1>
<p>One strategy that people use to learn about average causal effects is to create treatment and control groups through randomization (see <a href="https://egap.org/resource/10-strategies-for-figuring-out-if-x-caused-y">10 Strategies for Figuring Out If X Caused Y</a>). When doing so, researchers sometimes worry if they find that the resulting treatment and control groups do not look the same along relevant dimensions.</p>
<p>The good news is that the argument for why differences in average outcomes across randomly assigned treatment and control groups capture average treatment effects (in expectation across repeated randomizations within the same pool of units) does <em>not</em> rely on treatment and control groups being similar in their observed characteristics. It relies only on the idea that, on average, the outcomes in the treated and control groups will capture the average outcomes for all units in the experimental pool if they were, respectively, in treatment or in control. In practice actual treatment and control groups will not be identical.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
</section>
<section id="correlation-is-not-causation." class="level1">
<h1>8. Correlation is not causation.</h1>
<p>A correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is a statement about relations between actual outcomes in the world, not about the relation between actual outcomes and counterfactual outcomes. So statements about causes and correlations don’t have much to do with each other. Positive correlations can be consistent with positive causal effects, no causal effects, or even negative causal effects. For example taking cough medication is positively correlated with coughing but hopefully has a negative causal effect on coughing.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
</section>
<section id="if-you-know-that-on-average-a-causes-b-and-b-causes-c-this-does-not-mean-that-on-average-a-causes-c." class="level1">
<h1>9. If you know that, on average, <span class="math inline">\(A\)</span> causes <span class="math inline">\(B\)</span> and <span class="math inline">\(B\)</span> causes <span class="math inline">\(C\)</span>, this does not mean that, on average, <span class="math inline">\(A\)</span> causes <span class="math inline">\(C\)</span>.</h1>
<p>You might expect that if <span class="math inline">\(A\)</span> causes <span class="math inline">\(B\)</span> and <span class="math inline">\(B\)</span> causes <span class="math inline">\(C\)</span> that therefore <span class="math inline">\(A\)</span> causes <span class="math inline">\(C\)</span>.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> But there is no reason to believe that average causal relations are transitive in this way. To see why, imagine <span class="math inline">\(A\)</span> caused <span class="math inline">\(B\)</span> for men but not women and <span class="math inline">\(B\)</span> caused <span class="math inline">\(C\)</span> for women but not men. Then on average <span class="math inline">\(A\)</span> causes <span class="math inline">\(B\)</span> and <span class="math inline">\(B\)</span> causes <span class="math inline">\(C\)</span> but there may still be no one for whom <span class="math inline">\(A\)</span> causes <span class="math inline">\(C\)</span> through <span class="math inline">\(B\)</span>.</p>
</section>
<section id="its-easier-to-learn-about-the-effects-of-causes-than-to-learn-about-the-causes-of-effects." class="level1">
<h1>10. It’s easier to learn about the “effects of causes” than to learn about the “causes of effects.”</h1>
<p>Though it might sound like two ways of saying the same thing, there is a difference between understanding what the effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> is (the “effects of a cause”) and whether an outcome <span class="math inline">\(Y\)</span> was <em>due</em> to cause <span class="math inline">\(X\)</span> (the “cause of an effect”).<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> Consider the following example. Suppose we run an experiment with a sample that contains an equal number of men and women. The experiment randomly assigns men and women to a binary treatment <span class="math inline">\(X\)</span> and measures a binary outcome <span class="math inline">\(Y\)</span>. Further, suppose that <span class="math inline">\(X\)</span> has a positive effect of 1 for all men, i.e.&nbsp;men’s control potential outcome is zero (<span class="math inline">\(Y_i(0) = 0\)</span>) and their treated potential outcome is one (<span class="math inline">\(Y_i(1) = 1\)</span>). For all women, <span class="math inline">\(X\)</span> has a negative effect of <span class="math inline">\(-1\)</span>, i.e., women’s control potential outcome is one (<span class="math inline">\(Y_i(0) = 1\)</span>) and their treated potential outcome is zero (<span class="math inline">\(Y_i(1) = 0\)</span>). In this example, the average effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> is zero. But for all participants in the treatment group with <span class="math inline">\(Y=1\)</span>, it is the case that <span class="math inline">\(Y=1\)</span> <em>because</em> <span class="math inline">\(X=1\)</span>. Similarly, for all participants in the treatment group with <span class="math inline">\(Y=0\)</span>, it is the case that <span class="math inline">\(Y=0\)</span> <em>because</em> <span class="math inline">\(X=1\)</span>. Experimentation can get an exact answer to the question about the “effects of a cause”, but generally it is not possible to get an exact answer to the question about the “cause of an effect”.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>


</section>


<div id="quarto-appendix" class="default"><section class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1" role="doc-endnote"><p>Originating author: Macartan Humphreys. Minor revisions: Winston Lin and Donald P. Green, 24 Jun 2016. Revisions MH 6 Jan 2020. Revisions Anna Wilke May 2021. The guide is a live document and subject to updating by EGAP members at any time; contributors listed are not responsible for subsequent edits.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Holland, Paul W. “Statistics and causal inference.” Journal of the American Statistical Association 81.396 (1986): 945-960.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Holland, Paul W. “Statistics and causal inference.” Journal of the American Statistical Association 81.396 (1986): 945-960.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>In some accounts this has been called the “Problem of Profligate Causes”.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>Following Mackie, sometimes the idea of “INUS” conditions is invoked to capture the dependency of causes on other causes. Under this account, a cause may be an Insufficient but Necessary part of a condition which is itself Unnecessary but Sufficient. For example dialing a phone number is a cause of contacting someone since having a connection and dialing a number is sufficient (S) for making a phone call, whereas dialing alone without a connection alone would not be enough (I), nor would having a connection (N). There are of course other ways to contact someone without making phone calls (U). Mackie, John L. “The cement of the universe.” London: Oxford Uni (1974).<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p><strong>Technical Note:</strong> The key technical insight is that the difference of averages is the same as the average of differences. That is, using the “expectations operator,” <span class="math inline">\(𝔼(τ_i)=𝔼(Y_i(1)−Y_i(0))=𝔼(Y_i(1))−𝔼(Y_i(0))\)</span>. The terms inside the expectations operator in the second quantity cannot be estimated, but the terms inside the expectations operators in the third quantity can be.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> See illustration <a href="https://raw.githubusercontent.com/egap/methods-guides/master/causal-inference/PO.jpg">here</a>.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>For this reason <span class="math inline">\(t\)</span>-tests to check whether “randomization worked” do not make much sense, at least if you know that a randomized procedure was followed — just by chance 1 in 20 such tests will show statistically detectable differences between treated and control groups. If there are doubts about whether a randomized procedure was correctly implemented these tests can be used to test the hypothesis that the data was indeed generated by a randomized procedure. This later reason for randomization tests can be especially important in field experiments where chains of communication from the person creating random numbers and the person implementing treatment assignment may be long and complex.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p><strong>Technical Note:</strong> Let <span class="math inline">\(D_i\)</span> be an indicator for whether unit <span class="math inline">\(i\)</span> has received a treatment or not. Then the difference in average outcomes between those that receive the treatment and those that do not can be written as <span class="math inline">\(\frac{∑_i D_i×Y_i(1)}{∑_iD_i}−\frac{∑_i (1−D_i)×Y_i(0)}{∑_i (1−D_i)}\)</span>. In the absence of information about how treatment was assigned, we can say little about whether this difference is a good estimator of the average treatment effect, i.e., of the difference in average treated and control potential outcomes across all units. What matters is whether <span class="math inline">\(\frac{∑_i D_i×Y_i(1)}{∑_iD_i}\)</span> is a good estimate of <span class="math inline">\(\frac{∑_i 1×Y_i(1)}{∑_i1}\)</span> and whether <span class="math inline">\(\frac{∑_i (1−D_i)×Y_i(0)}{∑_i (1−D_i)}\)</span> is a good estimate of <span class="math inline">\(\frac{∑_i 1×Y_i(0)}{∑_i1}\)</span>. This might be the case if those who received treatment are a representative sample of all units, but otherwise there is no reason to expect that it would be.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>Interpret “<span class="math inline">\(A\)</span> causes <span class="math inline">\(B\)</span>, on average” as “the average effect of <span class="math inline">\(A\)</span> on <span class="math inline">\(B\)</span> is positive.”<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p>Some reinterpret the “causes of effects” question to mean: what are the causes that have effects on outcomes. See Andrew Gelman and Guido Imbens, “Why ask why? Forward causal inference and reverse causal questions”, NBER Working Paper No.&nbsp;19614 (Nov.&nbsp;2013).<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><p>See, for example, Tian, J., Pearl, J. 2000. “Probabilities of Causation: Bounds and Identification.” Annals of Mathematics and Artificial Intelligence 28:287–313.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>