<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Christopher Grady">

<title>Methods for Impact Evaluations - 10 Things to Know About Survey Experiments</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../img/egap-logo.svg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Methods for Impact Evaluations</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../guides.html">
 <span class="menu-text">Guides</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#what-is-a-survey-experiment" id="toc-what-is-a-survey-experiment" class="nav-link" data-scroll-target="#what-is-a-survey-experiment">1 What is a Survey Experiment</a></li>
  <li><a href="#survey-experiments-to-measure-sensitive-topics-list-experiment" id="toc-survey-experiments-to-measure-sensitive-topics-list-experiment" class="nav-link" data-scroll-target="#survey-experiments-to-measure-sensitive-topics-list-experiment">2 Survey Experiments to Measure Sensitive Topics: List Experiment</a>
  <ul class="collapse">
  <li><a href="#pitfalls-of-list-experiments" id="toc-pitfalls-of-list-experiments" class="nav-link" data-scroll-target="#pitfalls-of-list-experiments">Pitfalls of list experiments</a></li>
  <li><a href="#variantsmodifications" id="toc-variantsmodifications" class="nav-link" data-scroll-target="#variantsmodifications">Variants/Modifications</a></li>
  </ul></li>
  <li><a href="#survey-experiments-to-measure-sensitive-topics-randomized-response" id="toc-survey-experiments-to-measure-sensitive-topics-randomized-response" class="nav-link" data-scroll-target="#survey-experiments-to-measure-sensitive-topics-randomized-response">3 Survey Experiments to Measure Sensitive Topics: Randomized Response</a>
  <ul class="collapse">
  <li><a href="#pitfalls-of-the-randomized-response-technique" id="toc-pitfalls-of-the-randomized-response-technique" class="nav-link" data-scroll-target="#pitfalls-of-the-randomized-response-technique">Pitfalls of the randomized response technique</a></li>
  <li><a href="#variantsmodifications-1" id="toc-variantsmodifications-1" class="nav-link" data-scroll-target="#variantsmodifications-1">Variants/modifications</a></li>
  </ul></li>
  <li><a href="#survey-experiments-to-measure-sensitive-topics-priming-experiment" id="toc-survey-experiments-to-measure-sensitive-topics-priming-experiment" class="nav-link" data-scroll-target="#survey-experiments-to-measure-sensitive-topics-priming-experiment">4 Survey Experiments to Measure Sensitive Topics: Priming Experiment</a>
  <ul class="collapse">
  <li><a href="#pitfalls-of-priming-experiments" id="toc-pitfalls-of-priming-experiments" class="nav-link" data-scroll-target="#pitfalls-of-priming-experiments">Pitfalls of priming experiments</a></li>
  <li><a href="#variantsmodifications-2" id="toc-variantsmodifications-2" class="nav-link" data-scroll-target="#variantsmodifications-2">Variants/modifications</a></li>
  </ul></li>
  <li><a href="#survey-experiments-to-measure-sensitive-topics-endorsement-experiments" id="toc-survey-experiments-to-measure-sensitive-topics-endorsement-experiments" class="nav-link" data-scroll-target="#survey-experiments-to-measure-sensitive-topics-endorsement-experiments">5 Survey Experiments to Measure Sensitive Topics: Endorsement Experiments</a>
  <ul class="collapse">
  <li><a href="#pitfalls-of-endorsement-experiments" id="toc-pitfalls-of-endorsement-experiments" class="nav-link" data-scroll-target="#pitfalls-of-endorsement-experiments">Pitfalls of endorsement experiments</a></li>
  <li><a href="#variantsmodifications-3" id="toc-variantsmodifications-3" class="nav-link" data-scroll-target="#variantsmodifications-3">Variants/modifications</a></li>
  </ul></li>
  <li><a href="#limitations-of-survey-experiments-as-a-measurement-technique" id="toc-limitations-of-survey-experiments-as-a-measurement-technique" class="nav-link" data-scroll-target="#limitations-of-survey-experiments-as-a-measurement-technique">6 Limitations of Survey Experiments as a Measurement Technique</a></li>
  <li><a href="#survey-experiments-to-determine-a-causal-relationship-vignette-and-factorial-designs" id="toc-survey-experiments-to-determine-a-causal-relationship-vignette-and-factorial-designs" class="nav-link" data-scroll-target="#survey-experiments-to-determine-a-causal-relationship-vignette-and-factorial-designs">7 Survey Experiments to Determine a Causal Relationship: Vignette and Factorial Designs</a>
  <ul class="collapse">
  <li><a href="#pitfalls-of-vignettefactorial-experiments" id="toc-pitfalls-of-vignettefactorial-experiments" class="nav-link" data-scroll-target="#pitfalls-of-vignettefactorial-experiments">Pitfalls of vignette/factorial experiments</a></li>
  <li><a href="#variantsmodifications-4" id="toc-variantsmodifications-4" class="nav-link" data-scroll-target="#variantsmodifications-4">Variants/modifications</a></li>
  </ul></li>
  <li><a href="#survey-experiments-to-determine-a-causal-relationship-conjoint-experiments" id="toc-survey-experiments-to-determine-a-causal-relationship-conjoint-experiments" class="nav-link" data-scroll-target="#survey-experiments-to-determine-a-causal-relationship-conjoint-experiments">8 Survey Experiments to Determine a Causal Relationship: Conjoint Experiments</a>
  <ul class="collapse">
  <li><a href="#pitfalls-of-conjoint-experiments" id="toc-pitfalls-of-conjoint-experiments" class="nav-link" data-scroll-target="#pitfalls-of-conjoint-experiments">Pitfalls of conjoint experiments</a></li>
  </ul></li>
  <li><a href="#limitations-of-survey-experiments-for-causal-identification" id="toc-limitations-of-survey-experiments-for-causal-identification" class="nav-link" data-scroll-target="#limitations-of-survey-experiments-for-causal-identification">9 Limitations of Survey Experiments for Causal Identification</a></li>
  <li><a href="#considerations-when-using-survey-experiments" id="toc-considerations-when-using-survey-experiments" class="nav-link" data-scroll-target="#considerations-when-using-survey-experiments">10 Considerations when using Survey Experiments</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">10 Things to Know About Survey Experiments</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Christopher Grady </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<style>
p.comment{
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 25px;
border-radius: 5px;
color: black;
}

</style>
<section id="abstract" class="level1">
<h1>Abstract</h1>
<p>This guide discusses techniques for using randomization to create experiments within the text of a survey (i.e.&nbsp;survey experiments). These survey experiments are distinct from studies that use surveys to gather information related to an experiment that occurs outside of the survey. The guide distinguishes between survey experiments that are used mainly for measuring sensitive attitudes, like list experiments, and those that are mainly used to learn about causal effects, like conjoint experiments. Survey experiments for measurement attempt to ensure honest responses to sensitive questions by providing anonymity to respondents. Survey experiments for causal identification randomize images and text to learn how the image or text influences respondents. Both types of survey experiments face challenges, such as respondents not perceiving anonymity or not interpreting images and text as the researcher intended. New experimental techniques seek to address these challenges.</p>
</section>
<section id="what-is-a-survey-experiment" class="level1">
<h1>1 What is a Survey Experiment</h1>
<p>A survey experiment is an experiment conducted within a survey. In an experiment, a researcher randomly assigns participants to at least two experimental conditions. The researcher then treats each condition differently. Due to random assignment, the researcher can assume that the only difference between conditions is the difference in treatment. For example, a medical experiment may learn about the effect of a pill by creating two experimental conditions and giving the pill to participants in only one condition. In a survey experiment, the randomization and treatment occur within a survey questionnaire.</p>
<p>There are two types of survey experiments. One type is used to measure sensitive attitudes or behaviors and the other is used to learn about causal relationships. By sensitive attitudes and behaviors, we mean any attitude or behavior that the respondent does not want to be publicly associated with. Many respondents, for example, do not want to be associated with racism or illegal behaviors.</p>
<p>Survey experiments for measurement attempt to provide respondents with anonymity so that they can express potentially sensitive attitudes without being identified as holding the sensitive attitude. These measurement survey experiments are alternatives to asking direct questions when direct questions are likely subject to response biases (i.e.&nbsp;when the respondents are likely to lie). These indirect measures are especially useful in contexts where direct questions may be dangerous for survey respondents and enumerators <span class="citation" data-cites="bullock2011statistical">(<a href="#ref-bullock2011statistical" role="doc-biblioref">Bullock, Imai, and Shapiro 2011</a>)</span>.</p>
<p>Survey experiments to measure causal relationships are just like any other experiment, except the experimental intervention and outcome measurement occur within the context of a survey. Researchers randomly assign respondents to one or more experimental conditions or a control group, administer treatments, and measure outcomes. As their experimental manipulation, survey experiments tend to present a randomized stimulus (such as text, short videos, audio clips, or other stimuli that can easily fit into a survey format) in the context of a survey question or use variation in question wording and question ordering. They often administer these randomized manipulations by using survey questionnaire software or by providing different physical survey forms to enumerators.</p>
</section>
<section id="survey-experiments-to-measure-sensitive-topics-list-experiment" class="level1">
<h1>2 Survey Experiments to Measure Sensitive Topics: List Experiment</h1>
<p>List experiments (also known as the item-count technique) measure a sensitive attitude or behavior when the researcher expects the respondent to lie if asked a direct question about the attitude or behavior. For example, respondents may be reluctant to admit racial animus <span class="citation" data-cites="kuklinski1997list">(<a href="#ref-kuklinski1997list" role="doc-biblioref">Kuklinski et al. 1997</a>)</span> or illegal behavior <span class="citation" data-cites="garcia2020weed">(<a href="#ref-garcia2020weed" role="doc-biblioref">Garcia-Sanchez and Quierolo, n.d.</a>)</span>, even after being assured of the survey’s anonymity.</p>
<p>In a list experiment the researcher randomly assigns respondents to a control or treatment condition. The control condition presents respondents with a list of items; the treatment condition presents respondents with the same list plus a treatment item measuring the attitude or behavior of interest. Respondents are then asked how many of those items apply to them. The average difference between the treatment and control condition represents the percentage of respondents for whom the treatment item applies. A list experiment does not tell the researcher about the attitude of any individual respondent, but it tells her about the prevalence of the sensitive attitude in her survey population. Answers to this question are anonymous because the respondent’s attitude towards each item cannot be determined unless the respondent answers that all or none of the items apply to them. Respondents for whom the treatment item applies can increase their response by one without revealing their attitude.</p>
<p>For example, <span class="citation" data-cites="kuklinski1997list">Kuklinski et al. (<a href="#ref-kuklinski1997list" role="doc-biblioref">1997</a>)</span> studied racial animus with a survey list experiment. They told respondents:</p>
<!--<p class="comment">-->
<blockquote class="blockquote">
<p>Now I am going to read you three things that sometimes make people angry or upset. After I read all three, just tell me HOW MANY of them upset you. I don’t want to know which ones, just HOW MANY. <br><br> (1) the federal government increasing the tax on gasoline <br> (2) professional athletes getting million-dollar contracts<br> (3) large corporations polluting the environment<br> <em>(4) a black family moving in next door</em> <!--</p>--> The 4th item was withheld from the control condition.</p>
</blockquote>
<p>For that paper’s population of interest, the mean number of items chosen in the treatment group was 2.37, compared to 1.95 in the control. The difference of 0.42 between treatment and control indicates that 42% of respondents would be upset by a black family moving in next door.</p>
<section id="pitfalls-of-list-experiments" class="level3">
<h3 class="anchored" data-anchor-id="pitfalls-of-list-experiments">Pitfalls of list experiments</h3>
<p>List experiments are <em>vulnerable to satisficing</em>. Satisficing occurs when respondents put in minimal effort to understand and answer a survey question <span class="citation" data-cites="krosnick1991response simon2006administrative">(<a href="#ref-krosnick1991response" role="doc-biblioref">Krosnick 1991</a>; <a href="#ref-simon2006administrative" role="doc-biblioref">Simon and March 2006</a>)</span>. In a list experiment, satisficing manifests when respondents do not count the number of items that apply to them, instead answering with a number of items that seems reasonable <span class="citation" data-cites="eso2012list schwarz1999self">(<a href="#ref-eso2012list" role="doc-biblioref">Kramon and Weghorst 2012</a>; <a href="#ref-schwarz1999self" role="doc-biblioref">Schwarz 1999</a>)</span>.</p>
<p>Respondents may <em>perceive a lack of anonymity</em>. Despite the anonymity provided by a list experiment, respondents may still worry that their response reflects their attitudes about the sensitive item. When respondents worry about a lack of anonymity, they may increase or decrease their response to portray themselves in the best light possible, rather than answer honestly <span class="citation" data-cites="leary1990impression">(<a href="#ref-leary1990impression" role="doc-biblioref">Leary and Kowalski 1990</a>)</span>. For example, the addition of a treatment item about race can <em>decrease</em> the number of items that respondents report because being associated with “three of the four [list items] may be interpreted as a 75% chance that they are racist” <span class="citation" data-cites="zigerell2011you">(<a href="#ref-zigerell2011you" role="doc-biblioref">Zigerell 2011, 544</a>)</span>.</p>
<p>The lack of anonymity is most obvious when all or none of the list items apply to the respondent. Researchers can reduce this possibility by using <em>uncorrelated or negatively correlated control items</em> that are unlikely to apply to one respondent. In the <span class="citation" data-cites="kuklinski1997list">Kuklinski et al. (<a href="#ref-kuklinski1997list" role="doc-biblioref">1997</a>)</span> example above, the type of person who is upset by pollution is unlikely to also be upset by a gasoline tax. Negatively correlated items also reduce likelihood that respondent will satisfice because negatively correlated items are unlikely to be interpreted as a scale measuring one concept. The control items should also fit with the treatment item in some way so that the treatment item does not jump out to respondents as the real item of interest to researchers. <!--More attention to piloting to find control items that not only fit with the treatment item, but that are negatively correlated with other control items . Negatively correlated control items minimize the number of people who will score very high or very low on the control list, a problem that can compromise anonymity.--></p>
</section>
<section id="variantsmodifications" class="level3">
<h3 class="anchored" data-anchor-id="variantsmodifications">Variants/Modifications</h3>
<p><em>Double list experiments</em> help overcome some pitfalls of single list experiments <span class="citation" data-cites="glynn2013double droitcour2004item">(<a href="#ref-glynn2013double" role="doc-biblioref">Glynn 2013</a>; <a href="#ref-droitcour2004item" role="doc-biblioref">Droitcour et al. 2004</a>)</span>. In a double list experiment, the treatment item is randomly selected to appear on either the first or the second control list, so that some respondents see it on the first list and some respondents see it on the second. If researchers observe the same treatment effect on both lists, there is less risk that the effect depends on a particular control list or on how respondents interpret the list. The double list experiment is also more statistically efficient than a single list experiment <span class="citation" data-cites="glynn2013double">(<a href="#ref-glynn2013double" role="doc-biblioref">Glynn 2013</a>)</span>.</p>
<p><em>Placebo-controlled list experiments</em> ensure that the difference in responses to the treatment and control lists is due to the treatment item and not due to the treatment list having more items than the control list. A placebo-controlled list experiment uses an additional item as a placebo on the control list; unlike the additional item on the treatment list, the additional item on the control list is something innocuous that would not apply to any respondent. The placebo item ensures that the difference between the two lists is due to the treatment item, not the presence of an additional item <span class="citation" data-cites="riambau2019placebo">(<a href="#ref-riambau2019placebo" role="doc-biblioref">Riambau and Ostwald 2019</a>)</span>.</p>
<p><em>Visual aids</em> also help reduce satisficing and ensure that respondents follow the instruction to count list items instead of satisfice. If enumerators can carry a laminated copy of the list and a dry erase marker, respondents can check off items on the list to get an exact count and erase it before handing it back to the enumerator <span class="citation" data-cites="eso2012list kramon2019mis">(<a href="#ref-eso2012list" role="doc-biblioref">Kramon and Weghorst 2012</a>, <a href="#ref-kramon2019mis" role="doc-biblioref">2019</a>)</span>.</p>
<!--**Conclusion**: Can be effective; needs attention to design; needs attention to psychological biases that prevent lists from working.-->
</section>
</section>
<section id="survey-experiments-to-measure-sensitive-topics-randomized-response" class="level1">
<h1>3 Survey Experiments to Measure Sensitive Topics: Randomized Response</h1>
<p>The randomized response technique is also used to measure a sensitive attitude or behavior when the researcher expects the respondent to lie if asked a direct question <span class="citation" data-cites="warner1965randomized boruch1971assuring cpo2015rr gingerich2010understanding">(<a href="#ref-warner1965randomized" role="doc-biblioref">Warner 1965</a>; <a href="#ref-boruch1971assuring" role="doc-biblioref">Boruch 1971</a>; <a href="#ref-cpo2015rr" role="doc-biblioref">D. Gingerich 2015</a>; <a href="#ref-gingerich2010understanding" role="doc-biblioref">D. W. Gingerich 2010</a>)</span>.</p>
<p>In the most common version of the randomized response technique, respondents are directly asked a yes or no question about a sensitive topic. The respondent is also given some randomization device, like a coin or die. The respondent is told to answer the direct question when the randomization device takes on a certain value (tails) or to say “yes” when the randomization device takes a different value (heads). Researchers assume that respondents will believe their anonymity is protected because the researcher cannot know whether a “yes” resulted from agreement with the sensitive item or the randomization device.</p>
<p>For example, <span class="citation" data-cites="blair2015design">Blair, Imai, and Zhou (<a href="#ref-blair2015design" role="doc-biblioref">2015</a>)</span> studied support for militants in Nigeria with the randomized response technique. They gave respondents a die and had the respondent practice throwing it. They then told respondents:</p>
<!--<p class="comment">-->
<blockquote class="blockquote">
<p>For this question, I want you to answer yes or no. But I want you to consider the number of your dice throw. If 1 shows on the dice, tell me no. If 6 shows, tell me yes. But if another number, like 2 or 3 or 4 or 5 shows, tell me your opinion about the question that I will ask you after you throw the dice. <br><br> [ENUMERATOR TURN AWAY FROM THE RESPONDENT]<br><br> Now throw the dice so that I cannot see what comes out. Please do not forget the number that comes out.<br><br> [ENUMERATOR WAIT TO TURN AROUND UNTIL RESPONDENT SAYS YES TO]: Have you thrown the dice? Have you picked it up?<br><br> Now, during the height of the conflict in 2007 and 2008, did you know any militants, like a family member, a friend, or someone you talked to on a regular basis? Please, before you answer, take note of the number you rolled on the dice. <!--</p>--> In expectation, 1/6th of respondents answer “yes” due to the die throw. The researcher can thus determine what percentage of respondents engaged in the sensitive behavior. <!--by subtracting 1/6 from the survey mean of the randomized response question and multiplying by 1/6.--></p>
</blockquote>
<section id="pitfalls-of-the-randomized-response-technique" class="level3">
<h3 class="anchored" data-anchor-id="pitfalls-of-the-randomized-response-technique">Pitfalls of the randomized response technique</h3>
<p><em>Some versions are complicated</em>. Even the common version described above, valued in part for its simplicity, requires respondents to use some randomization device and remember the outcome of the randomization device. Other versions use more complicated techniques to ensure anonymity; these versions may be difficult both for the respondent and the enumerator <span class="citation" data-cites="blair2015design gingerich2010understanding">(<a href="#ref-blair2015design" role="doc-biblioref">Blair, Imai, and Zhou 2015</a>; <a href="#ref-gingerich2010understanding" role="doc-biblioref">D. W. Gingerich 2010</a>)</span>. It is possible that some respondents do not understand the instructions and some enumerators do not implement the randomized response technique properly.</p>
<p>Respondents may <em>perceive a lack of anonymity</em>. As was true for list experiments, respondents may not feel that their answers to randomized response questions are truly anonymous. If a respondent answers “yes”, the answer could have been dictated by the randomization device, but it could also signal agreement with the sensitive item <span class="citation" data-cites="edgell1982validity yu2008two">(<a href="#ref-edgell1982validity" role="doc-biblioref">Edgell, Himmelfarb, and Duchan 1982</a>; <a href="#ref-yu2008two" role="doc-biblioref">Yu, Tian, and Tang 2008</a>)</span>. Thus, answering “yes” is not unequivocally protected by the design. <span class="citation" data-cites="edgell1982validity">Edgell, Himmelfarb, and Duchan (<a href="#ref-edgell1982validity" role="doc-biblioref">1982</a>)</span> surreptitiously set the randomization device to always dictate “yes” or “no” for specific questions and observed as high as 26% of respondents say “no” even when the randomization device dictated they say “yes”.</p>
</section>
<section id="variantsmodifications-1" class="level3">
<h3 class="anchored" data-anchor-id="variantsmodifications-1">Variants/modifications</h3>
<p>The <em>repeated randomized response technique</em> helps researchers identify respondents who lie on randomized response questions <span class="citation" data-cites="azfar2009identifying">(<a href="#ref-azfar2009identifying" role="doc-biblioref">Azfar and Murrell 2009</a>)</span>. The repeated technique asks a series of randomized response questions with sensitive and non-sensitive items. The probability of the randomization device dictating that the respondent should answer “no” for all of the <em>sensitive</em> items is very low. The technique thus allows researchers to identify and remove from analysis the respondents who are likely saying “no” even when their coin flip dictates they say “yes”. Researchers can also determine if certain questions induce widespread lying if the “yes” rate for that question is lower than the randomization device would dictate. The repeated randomized response technique, however, may be impractical to include on a large survey.</p>
<p>The <em>Crosswise model</em> modifies the randomized response technique so that respondents have no incentive to answer “yes” or “no” <span class="citation" data-cites="yu2008two jann2011asking">(<a href="#ref-yu2008two" role="doc-biblioref">Yu, Tian, and Tang 2008</a>; <a href="#ref-jann2011asking" role="doc-biblioref">Jann, Jerke, and Krumpal 2011</a>)</span>. In the Crosswise model, respondents are presented with two statements, one sensitive statement and one non-sensitive statement for which the population mean is known. The respondent is asked to say if (a) neither or both statements are true, or (b) one statement is true. Unlike a typical randomized response question, where individuals who agree with the sensitive statement only occupy the “yes” group, people who agree with the sensitive statement could occupy either group using the Crosswise model. Since being in category (a) and (b) are equally uninformative about the respondent’s agreement with the sensitive statement, the Crosswise model removes a respondent’s incentive to lie. The Crosswise model can be used any time researchers know the population mean of a non-sensitive statement, such as “My mother was born in April.”</p>
<!--**Conclusion**: Crosswise model should ensure anonymity and is minimally confusing.  But it, and all RR techniques, require some randomization device, either a physical item like a coin or dice or a non-sensitive question for which the population mean is known.-->
</section>
</section>
<section id="survey-experiments-to-measure-sensitive-topics-priming-experiment" class="level1">
<h1>4 Survey Experiments to Measure Sensitive Topics: Priming Experiment</h1>
<p>List experiments and randomized response techniques do not uncover implicit attitudes, but many sensitive topics appear so sensitive that an individual’s conscious, explicit attitudes may differ from their implicit attitudes <span class="citation" data-cites="greenwald1995implicit">(<a href="#ref-greenwald1995implicit" role="doc-biblioref">Greenwald and Banaji 1995</a>)</span>. Even many nonsensitive attitudes seem to be beyond an individual’s conscious awareness <span class="citation" data-cites="nisbett1977telling">(<a href="#ref-nisbett1977telling" role="doc-biblioref">Nisbett and Wilson 1977</a>)</span>. Whereas techniques to measure explicit attitudes seek to provide respondents with anonymity, techniques to measure implicit attitudes seek to keep the respondent consciously unaware of the implicit attitude being measured. To do so, researchers often use priming experiments.</p>
<p>In a priming experiment, researchers expose respondents to a stimulus representing topic <em>X</em> in order to influence their response to a survey question about topic <em>Y</em>, without the respondent realizing that the researchers are interested in topic <em>X</em>. A control group is not exposed to the stimuli representing topic <em>X</em>, so the difference between the treatment group and control group is due to exposure to the treatment stimuli. Priming experiments work by directing respondents’ consciousness away from topic <em>X</em> and towards topic <em>Y</em> so that respondents do not consciously censor their feelings about topic <em>X</em> <span class="citation" data-cites="macrae1994out schwarz1983mood">(<a href="#ref-macrae1994out" role="doc-biblioref">Macrae et al. 1994</a>; <a href="#ref-schwarz1983mood" role="doc-biblioref">Schwarz and Clore 1983</a>)</span>.</p>
<p>Priming experiments are a broad class and include any experiment that makes a sensitive topic salient in the mind of the respondent. One common method of priming is the use of images. For example, <span class="citation" data-cites="brader2008triggers">Brader, Valentino, and Suhay (<a href="#ref-brader2008triggers" role="doc-biblioref">2008</a>)</span> use images in a priming experiment to estimate the effect that race plays in opposition to immigration. The researchers show subjects a positive or negative news article about immigration paired with a picture of a European immigrant or an Hispanic immigrant. Subjects expressed negative attitudes about immigration when the negative news article is paired with the Hispanic immigrant picture but not in other conditions. The picture primes people to think about Hispanic immigrants, and thinking about Hispanic immigrants reduces support for immigration compared to thinking about European immigrants even though subjects do not consciously admit to bias.</p>
<p>Survey experiments for measurement and for causal identification overlap in priming experiments. Researchers can use them to measure implicit attitudes or to assess how the activation of implicit attitudes affects another outcome, like attitudes towards immigration.</p>
<section id="pitfalls-of-priming-experiments" class="level3">
<h3 class="anchored" data-anchor-id="pitfalls-of-priming-experiments">Pitfalls of priming experiments</h3>
<p><em>Priming experiments are difficult</em>. Priming attitudes experimentally is difficult because the researcher cannot be certain that the prime affects subjects as the researcher intended. A prime intended to induce fear, for example, may induce fear in some subjects and excitement in others. Priming sensitive attitudes is especially difficult because the researcher must prime a sensitive attitude without the respondent becoming aware that the researcher is interested in the sensitive attitude. If respondents realize what the priming experiment is about, the experiment fails because respondents will consciously censor their attitude, rather than passively allow their implicit attitude to influence their response <span class="citation" data-cites="macrae1994out schwarz1983mood">(<a href="#ref-macrae1994out" role="doc-biblioref">Macrae et al. 1994</a>; <a href="#ref-schwarz1983mood" role="doc-biblioref">Schwarz and Clore 1983</a>)</span>. To prevent subjects from ascertaining the goal of the study, researchers try to hide the prime amid other, ostensibly more important, information.</p>
<p><em>Priming experiments can suffer from confounding and lack of “information equivalence” between treatment groups</em> <span class="citation" data-cites="dafoe2018information">(<a href="#ref-dafoe2018information" role="doc-biblioref">Dafoe, Zhang, and Caughey 2018</a>)</span>. The researchers may prime topic <span class="math inline">\(X\)</span> with the intent of learning about respondents’ implicit attitudes towards topic <span class="math inline">\(X\)</span>, but if topic <span class="math inline">\(X\)</span> is strongly linked with topic <span class="math inline">\(Y\)</span> then the researcher will estimate the effect of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, not just <span class="math inline">\(X\)</span>. For example, priming a partisan group may also prime ideological and policy views associated with the partisan group <span class="citation" data-cites="nicholson2011dominating">(<a href="#ref-nicholson2011dominating" role="doc-biblioref">Nicholson 2011</a>)</span>. A basic priming experiment cannot differentiate the effect of priming the partisan group from the effect of priming the ideological and policy views associated with the partisan group.</p>
<p><em>Respondents may be pretreated before the experiment</em>. Individuals are exposed to stimuli that prime attitudes during their daily lives. News broadcasts prime people to think about issues covered on the news, and anti-racism protests prime people to think about racial issues. Even words seen immediately before answering survey questions influences responses to those survey questions <span class="citation" data-cites="norenzayan1999telling">(<a href="#ref-norenzayan1999telling" role="doc-biblioref">Norenzayan and Schwarz 1999</a>)</span>. If subjects, before participating in the experiment, encounter the stimuli that the researcher wants to prime, there may be no difference between treatment and control groups because all subjects were “pretreated” with the prime, even subjects in the control group <span class="citation" data-cites="gaines2007logic druckman2012learning">(<a href="#ref-gaines2007logic" role="doc-biblioref">Gaines, Kuklinski, and Quirk 2007</a>; <a href="#ref-druckman2012learning" role="doc-biblioref">Druckman and Leeper 2012</a>)</span>. If the issue being primed is already salient in the mind of the respondent, priming experiments fail.</p>
</section>
<section id="variantsmodifications-2" class="level3">
<h3 class="anchored" data-anchor-id="variantsmodifications-2">Variants/modifications</h3>
<p>To ensure information equivalence and to reduce confounding the prime with an associated factor, researchers utilize priming experiments as part of <em>factorial experiments</em>. Factorial experiments vary multiple factors that may be linked in the minds of respondents. <span class="citation" data-cites="nicholson2011dominating">Nicholson (<a href="#ref-nicholson2011dominating" role="doc-biblioref">2011</a>)</span>, for example, asked respondents about support for a policy. He varied both partisan endorsement and policy details to learn how partisan bias influenced respondents’ attitudes beyond any assumptions about the party’s policy positions. Factorial experiments are mainly used to determine causal relationships and are discussed in section 7.</p>
<!--**Conclusion**: Priming experiments can help determine respondents' attitudes in cases where respondents do not have conscious access to the attitude that the researcher is interested in.
-->
</section>
</section>
<section id="survey-experiments-to-measure-sensitive-topics-endorsement-experiments" class="level1">
<h1>5 Survey Experiments to Measure Sensitive Topics: Endorsement Experiments</h1>
<p>Endorsement experiments measure sensitive attitudes towards an attitude object, like a political actor or a policy. They were first developed to study partisan bias <span class="citation" data-cites="cohen2003party kam2005toes">(<a href="#ref-cohen2003party" role="doc-biblioref">Cohen 2003</a>; <a href="#ref-kam2005toes" role="doc-biblioref">Kam 2005</a>)</span> but have since been used to measure support for militant groups <span class="citation" data-cites="bullock2011statistical lyall2013explaining">(<a href="#ref-bullock2011statistical" role="doc-biblioref">Bullock, Imai, and Shapiro 2011</a>; <a href="#ref-lyall2013explaining" role="doc-biblioref">Lyall, Blair, and Imai 2013</a>)</span>. They have also been inverted to measure support for a policy rather than a political actor <span class="citation" data-cites="rosenfeld2016empirical">(<a href="#ref-rosenfeld2016empirical" role="doc-biblioref">Rosenfeld, Imai, and Shapiro 2016</a>)</span>.<!--They resulted from literature in psychology about source cues and persuasion [@chaiken1980heuristic; @wegener1998attitude] --></p>
<p>In a typical endorsement experiment, respondents are asked how much they support a policy. In the treatment condition, the policy is “endorsed” by a group that respondents would not consciously admit to influencing their opinion. In the control condition, the policy is not endorsed by any group. The average difference in support between the endorsed and unendorsed policy represents the change in support for the policy because of the endorsement.</p>
<p>Endorsement experiments can measure implicit attitudes or explicit attitudes. They measure implicit attitudes like a priming experiment if respondents do not realize the group’s endorsement is what the researcher is interested in. They measure explicit attitudes like a list experiment if respondents realize the group’s endorsement is what the researcher is interested in. Whereas list experiments hide the respondent’s opinion by pairing the sensitive item with non-sensitive control items, endorsement experiments hide the respondent’s opinion by pairing the sensitive item with a policy that could feasibly be responsible for the respondent’s attitude.<br>
<!--where individuals can freely express their support for the group through supporting the policy because the researcher cannot differentiate policy support from group support at an individual level.--></p>
<!--chris: example here -->
<p>For example, <span class="citation" data-cites="nicholson2012polarizing">Nicholson (<a href="#ref-nicholson2012polarizing" role="doc-biblioref">2012</a>)</span> used an endorsement experiment to study partisan bias in the United States during the 2008 Presidential campaign. The researchers asked respondents about policies, varying whether the policy was unendorsed or endorsed by the Presidential candidates of the two main political parties, Barack Obama (Democrat) and John McCain (Republican). Respondents were told:</p>
<!--<p class="comment">-->
<blockquote class="blockquote">
<p>As you know, there has been a lot of talk about immigration reform policy in the news. One proposal [<strong>backed by Barack Obama</strong>/<strong>backed by John McCain</strong>] provided legal status and a path to legal citizenship for the approximately 12 million illegal immigrants currently residing in the United States. What is your view of this immigration reform policy? <!--</p>--> The difference between the control condition and the Obama (McCain) condition for Democrats (Republicans) shows in-party bias. The difference between the control condition and the Obama (McCain) condition for Republicans (Democrats) shows out-party bias.</p>
</blockquote>
<section id="pitfalls-of-endorsement-experiments" class="level3">
<h3 class="anchored" data-anchor-id="pitfalls-of-endorsement-experiments">Pitfalls of endorsement experiments</h3>
<p>As with priming experiments, endorsement experiments suffer from <em>confounding and a lack of information equivalence</em>. Researchers cannot be certain if differential support for the policy is due to the endorsement or due to different substantive assumptions about the policy that respondents make as a result of the endorsement.</p>
<p><em>Choosing a policy is difficult</em>. The value of the endorsement experiments depends largely on the characteristics of the policy being (or not being) endorsed. The chosen policy must not possess too much or too little support in the survey population, otherwise attitudes towards the policy will wipe out the effect of the group’s endorsement. Too much or too little support could also reduce perceived anonymity if respondents think that no one would support/oppose the policy unless they liked/disliked the endorsing group.</p>
<p>Endorsement experiments can have <em>low power to detect effects</em>, even relative to other survey experiments <span class="citation" data-cites="bullock2011statistical">(<a href="#ref-bullock2011statistical" role="doc-biblioref">Bullock, Imai, and Shapiro 2011</a>)</span>. Some subset of subjects will be unaffected by the endorsement because they feel strongly about the policy, and that subset adds substantial noise to endorsement experiments.</p>
</section>
<section id="variantsmodifications-3" class="level3">
<h3 class="anchored" data-anchor-id="variantsmodifications-3">Variants/modifications</h3>
<p>To overcome low power, <span class="citation" data-cites="bullock2011statistical">Bullock, Imai, and Shapiro (<a href="#ref-bullock2011statistical" role="doc-biblioref">2011</a>)</span> recommend using <em>multiple policy questions</em> that are on the same one-dimensional policy space. Multiple questions on one policy space allows the researcher to predict each respondent’s level of support for the policy if it were not endorsed by the group of interest. The researcher can thus model the noise caused by strong feelings towards the policy.</p>
<p>As with priming experiments, to ensure information equivalence and to reduce confounding factors, researchers use endorsement experiments as part of <em>factorial experiments</em> that vary the multiple factors that may be linked in the mind of respondents. Factorial experiments are mainly used to determine causal relationships and are discussed in section 7.</p>
<!--**Conclusion**: Endorsement experiments are a flexible survey experiment that can be used to measure explicit or implicit attitudes.  Designing them takes substantial pilot testing.-->
</section>
</section>
<section id="limitations-of-survey-experiments-as-a-measurement-technique" class="level1">
<h1>6 Limitations of Survey Experiments as a Measurement Technique</h1>
<p>Survey experiments induce less bias than direct questions when measuring sensitive attitudes <span class="citation" data-cites="blair2015design rosenfeld2016empirical lensvelt2005meta">(<a href="#ref-blair2015design" role="doc-biblioref">Blair, Imai, and Zhou 2015</a>; <a href="#ref-rosenfeld2016empirical" role="doc-biblioref">Rosenfeld, Imai, and Shapiro 2016</a>; <a href="#ref-lensvelt2005meta" role="doc-biblioref">Lensvelt-Mulders et al. 2005</a>)</span>. They are not a panacea, however, and researchers must still ask themselves several questions when using survey experiments to measure sensitive outcomes.</p>
<p>The first question is whether the researcher is interested in an explicit or implicit attitude. An explicit attitude is one the respondent is consciously aware of and can report; an implicit attitude is an automatic positive or negative evaluation of an attitude object that the respondent may not be aware of <span class="citation" data-cites="nosek2007implicit">(see <a href="#ref-nosek2007implicit" role="doc-biblioref">Nosek 2007</a> for a more thorough discussion)</span>. A list experiment, for example, may help uncover explicit racial animus, but it will not reveal implicit racial bias.</p>
<p>The next question is what conditions are necessary for a survey respondent to reveal their explicit attitudes. Survey experimental methods for sensitive explicit attitudes focus on ensuring anonymity. But is ensuring anonymity a sufficient condition to obtain honest answers to sensitive questions? In addition to anonymity, a further assumption must be made: respondents want to express their socially undesirable opinion in a way that evades social sanctions. If that assumption is not true, then anonymity is worth little <span class="citation" data-cites="diaz2020survey">(<a href="#ref-diaz2020survey" role="doc-biblioref">Diaz, Grady, and Kuklinski 2020</a>)</span>.</p>
<p>Researchers also need to think about the numerous pitfalls of survey questions that measurement survey experiments do not solve. Survey experiments do not help researchers avoid question ordering effects or contamination from earlier questions in the survey. Nor do they do expose how respondents interpret the survey question or ensure information equivalence. All survey questions assume that the respondent interprets the question in the way intended by researchers; techniques to ensure anonymity may make that interpretation less likely by obfuscating the question’s purpose <span class="citation" data-cites="diaz2020survey">(<a href="#ref-diaz2020survey" role="doc-biblioref">Diaz, Grady, and Kuklinski 2020</a>)</span>.</p>
<p>Lastly, researchers must also ask about measurement validity: how does one verify that a measure accurately represents a concept of interest? For some outcomes, such as voter turnout, researchers can compare their measure with population estimates <span class="citation" data-cites="rosenfeld2016empirical">(<a href="#ref-rosenfeld2016empirical" role="doc-biblioref">Rosenfeld, Imai, and Shapiro 2016</a>)</span>. But for other outcomes, such as racism or the effect that political parties have on citizens’ policy preferences, there exists no population estimate with which to validate measures.</p>
</section>
<section id="survey-experiments-to-determine-a-causal-relationship-vignette-and-factorial-designs" class="level1">
<h1>7 Survey Experiments to Determine a Causal Relationship: Vignette and Factorial Designs</h1>
<p>Not all survey experiments share the goal of accurately measuring one concept of interest. Some survey experiments, like lab experiments, are interested in how an experimental manipulation impacts outcomes of interest. These survey experiments for causal inference randomize a treatment and then measure outcomes. When measuring outcomes, they may use techniques like list experiments.</p>
<p>One of the most common designs of survey experiments for causal inference are vignette and factorial designs <span class="citation" data-cites="auspurg2014factorial sniderman1991new">(<a href="#ref-auspurg2014factorial" role="doc-biblioref">Auspurg and Hinz 2014</a>; <a href="#ref-sniderman1991new" role="doc-biblioref">Sniderman et al. 1991</a>)</span>. In a vignette/factorial experiment, the researcher provides the respondent with a hypothetical scenario to read, varying key components of the scenario. In a typical vignette, the researcher varies only one component of the scenario. In a typical factorial experiment, the researcher varies several components of the scenario.</p>
<p>Both vignette and factorial designs benefit from embedding the survey question in a concrete scenario so that they require little abstraction from the survey respondent. Their concrete nature can make them more interesting and easier to answer than typical survey questions, decreasing survey fatigue. They can also function as priming experiments if the concept of interest is embedded in other concepts.</p>
<p>As an example, <span class="citation" data-cites="winters2013lacking">Winters and Weitz-Shapiro (<a href="#ref-winters2013lacking" role="doc-biblioref">2013</a>)</span> uses factorial vignettes to learn if voters sanction corrupt politicians in Brazil. They posit that corruption could interact with competence, so the authors varied a Brazilian mayor’s corruption, competence, and political affiliation in the vignette. They tell respondents:</p>
<!--<p class="comment">-->
<blockquote class="blockquote">
<p>Imagine a person named Gabriel (Gabriela for female respondents), who is a person like you, living in a neighborhood like yours, but in a different city in Brazil. The mayor of Gabriel’s city is running for reelection in October. He is a member of the [<strong>Partido dos Trabalhadores</strong>/<strong>Partido da Social Democracia Brasileira</strong>]. In Gabriel’s city, it is well known that the mayor [<strong>never takes bribes</strong>/<strong>frequently takes bribes</strong>] when giving out government contracts. The mayor has completed [<strong>few</strong>/<strong>many</strong>/<strong>omit the entire sentence</strong>] public works projects during his term in office. In this city, the election for mayor is expected to be very close.<br><br> In your opinion, what is the likelihood that Gabriel(a) will vote for this mayor in the next election: very likely, somewhat likely, unlikely, not at all likely? <!--</p>--> This design allowed the authors to determine if and when corruption would be punished by voters. If respondents overlooked corruption when the mayor completed many public works, the interpretation is that corruption is acceptable if it gets the job done. If respondents overlooked corruption when the mayor was a copartisan, the interpretation is that voters ignore the corruption of their own. By varying several related aspects of the scenario, <span class="citation" data-cites="winters2013lacking">Winters and Weitz-Shapiro (<a href="#ref-winters2013lacking" role="doc-biblioref">2013</a>)</span> could isolate the conditions under which credible information about corruption would be punished by voters.</p>
</blockquote>
<section id="pitfalls-of-vignettefactorial-experiments" class="level3">
<h3 class="anchored" data-anchor-id="pitfalls-of-vignettefactorial-experiments">Pitfalls of vignette/factorial experiments</h3>
<p>The main pitfall of vignettes – a <em>lack of information equivalence</em> – is dealt with by factorial experiments. Researchers can randomize several aspects of the scenario, standardizing factors that could influence how the main treatment is perceived by respondents. <em>Some combinations of different factors may not be realistic</em>, however. Researchers must be sure that the various possible combinations of their factorial experiments seem credible to respondents.</p>
<p><em>Statistical power is weak</em> when factorial experiments vary many confounding traits. The more traits being varied, the more experimental conditions, the fewer respondents in each experimental condition, and the greater likelihood of imbalance between treatment conditions.</p>
<p>In enumerated surveys, there is also the possibility that certain enumerators are more often assigned certain factorial conditions and that <em>enumerator effects could be mistaken for treatment effects</em> <span class="citation" data-cites="steiner2016designing">(<a href="#ref-steiner2016designing" role="doc-biblioref">Steiner, Atzmüller, and Su 2016</a>)</span>. Imagine the <span class="citation" data-cites="winters2013lacking">Winters and Weitz-Shapiro (<a href="#ref-winters2013lacking" role="doc-biblioref">2013</a>)</span> study, which had six functional treatment groups and ~2,000 respondents. If the survey was enumerated by twenty survey enumerators, then, in expectation, each enumerator has only ~17 subjects in each treatment category. In reality, it is likely that certain enumerators will more often enumerate some conditions than others and differences due to enumerators could appear as treatment effects.</p>
</section>
<section id="variantsmodifications-4" class="level3">
<h3 class="anchored" data-anchor-id="variantsmodifications-4">Variants/modifications</h3>
<p>Researchers can <em>block treatment by enumerator</em> so that enumerator effects cannot confound treatment effects <span class="citation" data-cites="steiner2016designing">(<a href="#ref-steiner2016designing" role="doc-biblioref">Steiner, Atzmüller, and Su 2016</a>)</span>. Blocking and other techniques the authors propose should also increase statistical power by accounting for systematic error.</p>
<p><em>Conjoint experiments</em> maintain many benefits of factorial experiments, but increase power by presenting multiple choice tasks instead of one. We discuss conjoint experiments in the next section.</p>
</section>
</section>
<section id="survey-experiments-to-determine-a-causal-relationship-conjoint-experiments" class="level1">
<h1>8 Survey Experiments to Determine a Causal Relationship: Conjoint Experiments</h1>
<p>Conjoint experiments <span class="citation" data-cites="hainmueller2014causal green1971conjoint">(<a href="#ref-hainmueller2014causal" role="doc-biblioref">Hainmueller, Hopkins, and Yamamoto 2014</a>; <a href="#ref-green1971conjoint" role="doc-biblioref">Green and Rao 1971</a>)</span> have gained popularity in response to the limits of vignette and factorial designs. Vignette and factorial designs suffer from a lack of information equivalence if they do not provide sufficient details about potentially confounding aspects of the scenario or a lack of statistical power if they do vary several traits. A typical conjoint experiment attempts to solve these problems by repeatedly asking respondents to choose between two distinct options and randomly varying the characteristics of those two options. Respondents may also be asked to rate each option on a scale. In both cases, respondents express their preferences towards a large number of pairings with randomized attributes, drastically increasing statistical power to detect effects of any one attribute relative to a one-shot factorial design.</p>
<p><span class="citation" data-cites="hainmueller2014causal">Hainmueller, Hopkins, and Yamamoto (<a href="#ref-hainmueller2014causal" role="doc-biblioref">2014</a>)</span> demonstrate the use of conjoint experiments in a study about support for immigration. The authors showed respondents two immigrant profiles and asked (a) which immigrant the respondent would prefer be admitted to the Unites States and (b) how the respondent rated each immigrant on a scale from 1-7. The authors randomly varied nine attributes of the immigrants (gender, education, employment plans, job experience, profession, language skills, country of origin, reasons for applying, and prior trips to the United States), yielding thousands of unique immigrant profiles. This process was repeated five times so that each respondents saw and rated five pairs of immigrants. Through this procedure, the authors can assess how these randomly varied components influence support for the immigrant.</p>
<p>Respondents saw:</p>
<p><img src="conjoint_image2.png" class="img-fluid"></p>
<p>Through a conjoint experiment, researchers can learn about the average marginal effect of several aspects of a scenario, far more than would be feasible with a typical vignette or factorial design. Though researchers could include and vary an almost infinite number of characteristics, the best practice is to only vary traits that could confound the relationship between a primary explanatory variable and an outcome of interest, rather than varying any trait that might affect the outcome of interest <span class="citation" data-cites="diaz2020survey">(<a href="#ref-diaz2020survey" role="doc-biblioref">Diaz, Grady, and Kuklinski 2020</a>)</span>.</p>
<section id="pitfalls-of-conjoint-experiments" class="level3">
<h3 class="anchored" data-anchor-id="pitfalls-of-conjoint-experiments">Pitfalls of conjoint experiments</h3>
<p>The costs and benefits of conjoint experiments are still being actively researched. Thus far, two classes of critiques are common.</p>
<p>Results from conjoint experiments are <em>difficult to interpret</em>. Results of conjoint experiments’ target estimand, the Average Marginal Component Effect (AMCE), can “indicate the opposite of the true preference of the majority” <span class="citation" data-cites="abramson2019we">(<a href="#ref-abramson2019we" role="doc-biblioref">Abramson, Koçak, and Magazinnik 2019, 1</a>)</span>. Other researchers have noted that AMCE’s depend on the reference category and are not comparable across survey subgroups <span class="citation" data-cites="leeper2020measuring">(<a href="#ref-leeper2020measuring" role="doc-biblioref">Leeper, Hobolt, and Tilley 2020</a>)</span>. <span class="citation" data-cites="bansak2020using">Bansak et al. (<a href="#ref-bansak2020using" role="doc-biblioref">2020</a>)</span> provides guidance on how to interpret conjoint results and argues that AMCE’s do represent quantities of interest to empirical scholars.</p>
<p>Conjoint experiments also create <em>unrealistic combinations</em> and those unrealistic combinations lead to effect estimates that are not representative of the real world <span class="citation" data-cites="incerti2020corruption">(<a href="#ref-incerti2020corruption" role="doc-biblioref">Incerti 2020</a>)</span>. Similarly, the large amount of information provided by conjoint experiments could misrepresent how individuals generally process information they encounter in the world <span class="citation" data-cites="hainmueller2014causal">(<a href="#ref-hainmueller2014causal" role="doc-biblioref">Hainmueller, Hopkins, and Yamamoto 2014</a>)</span>. The large amount of information and demand on respondents has also led to concerns about satisficing, though <span class="citation" data-cites="bansak2018number">Bansak et al. (<a href="#ref-bansak2018number" role="doc-biblioref">2018</a>)</span> and <span class="citation" data-cites="bansak2019beyond">Bansak et al. (<a href="#ref-bansak2019beyond" role="doc-biblioref">2019</a>)</span> suggest satisficing is not a major concern for conjoint experiments.</p>
<p>Other potential pitfalls can occur if the researcher varies too many characteristics. More randomly varied characteristics means a large number of potential hypothesis tests. The necessity of applying multiple hypothesis corrections to the vast number of potential hypothesis tests could decrease statistical power to detect specific effects, especially if researchers are interested in interactions between traits being varied.</p>
</section>
</section>
<section id="limitations-of-survey-experiments-for-causal-identification" class="level1">
<h1>9 Limitations of Survey Experiments for Causal Identification</h1>
<p>Survey experiments to determine causal relationships have the same benefits and drawbacks as other experiments, as well as benefits and drawbacks that derive from the survey context. The biggest three drawbacks generally applicable to survey experiments are confounding, information equivalence, and pre-treatment contamination <span class="citation" data-cites="diaz2020survey">(<a href="#ref-diaz2020survey" role="doc-biblioref">Diaz, Grady, and Kuklinski 2020</a>)</span>. Researchers should think about these factors when designing and interpreting results from survey experiments.</p>
<p><em>Confounding</em>: Any experimental intervention <em>A</em> that is meant to trigger mental construct <em>M</em> could also trigger mental construct <em>C</em>. If <em>C</em> is not varied in the experimental design, researchers cannot determine whether <em>M</em>, <em>C</em>, or a combination of <em>M</em> and <em>C</em> affect outcomes of interest.</p>
<p><em>Information Equivalence</em>: Any experimental intervention <em>A</em> can be interpreted differently by different respondents, effectively giving each respondent a different treatment <span class="citation" data-cites="dafoe2018information">(<a href="#ref-dafoe2018information" role="doc-biblioref">Dafoe, Zhang, and Caughey 2018</a>)</span>. When these interpretations vary systematically by treatment condition, those conditions are not information equivalent and researchers cannot know that their treatment caused the observed effect.</p>
<p><em>Pre-treatment contamination</em>: Respondents may encounter the treatment outside of the experiment, causing similar outcomes in the control group and treatment group even if the treatment affects outcomes <span class="citation" data-cites="gaines2007logic">(<a href="#ref-gaines2007logic" role="doc-biblioref">Gaines, Kuklinski, and Quirk 2007</a>)</span>.</p>
</section>
<section id="considerations-when-using-survey-experiments" class="level1">
<h1>10 Considerations when using Survey Experiments</h1>
<p>Survey experiments can be an effective tool for researchers to measure sensitive attitudes and learn about causal relationships. They are cost-effective, can be done quickly and iteratively, can be included on mass online surveys because they do not require in-person contact to implement. This means that a researcher can plan a sequence of online survey experiments, changing the intervention and measured outcomes from one experiment to the next to learn about the mechanisms behind the treatment effect very quickly <span class="citation" data-cites="sniderman2018some">(<a href="#ref-sniderman2018some" role="doc-biblioref">Sniderman 2018</a>)</span>.</p>
<p>For survey experiments as a measurement technique, the researcher first has to assess if the attitude of interest is explicit (consciously known to the respondent) or implicit (not consciously known to the respondent). If the researcher believes the respondent knows her own attitude but does not want to be identified with it, the researcher should make it possible for the respondent to express that attitude without the researcher knowing that attitude. List experiments, randomized response techniques, and endorsement experiments can help accomplish this task. If the researcher believes the respondent does not know her own attitude, the researcher should make that attitude salient through priming and then ask a question that should be implicitly affected by the prime.</p>
<p>There may be cases where survey experiments are not the best tool for measuring sensitive attitudes. As an alternative to survey experiments to measure explicit attitudes, researchers can use techniques like the Bogus Pipeline <span class="citation" data-cites="jones1971bogus">(<a href="#ref-jones1971bogus" role="doc-biblioref">Jones and Sigall 1971</a>)</span> or phrase questions about a sensitive topic so that they are not considered socially undesirable <span class="citation" data-cites="kinder1981symbolicRacism">(<a href="#ref-kinder1981symbolicRacism" role="doc-biblioref">Kinder and Sears 1981</a>)</span>. As an alternative to survey experiments to measure implicit attitudes, researchers can use measures like the Implicit Association Test (IAT) <span class="citation" data-cites="greenwald1998IAT">(<a href="#ref-greenwald1998IAT" role="doc-biblioref">Greenwald, McGhee, and Schwartz 1998</a>)</span> and physiological measures like skin conductance <span class="citation" data-cites="rankin1955galvanic figner2011using">(<a href="#ref-rankin1955galvanic" role="doc-biblioref">Rankin and Campbell 1955</a>; <a href="#ref-figner2011using" role="doc-biblioref">Figner, Murphy, et al. 2011</a>)</span>. These measures are beyond conscious control of the respondent. Many of these alternative measures are not currently flexible enough to be included on a mass survey, but technology, like heart-rate monitoring watches and other phone sensors, may soon make biometric outcomes measurable in mass surveys.</p>
<!--_For survey experiments to estimate causal relationships_, -->
<p>For all types of survey experiments, researchers should worry about the same issues that hamper other experiments: confounding, information equivalence, and pre-treatment contamination. To deal with confounding and information equivalence, researchers can design the experiment to manipulate characteristics that might confound the treatment. To account for pre-treatment, researchers can think about the everyday context of research subjects and assess whether all or a subset of respondents may already be treated before beginning the experiment. If only a subset will be affected, the researcher can block the experiment on that subset.</p>
<p>Survey experiments for measurement and survey experiments for estimating causal relationships are not binary categories, and the two types of survey experiments can overlap. Priming experiments, for example, can measure implicit attitudes and assess the effect of the prime on other outcomes of interest. Vignette or conjoint experiments can effectively measure a sensitive attitude by priming the sensitive attitude and providing lots of other information to distract respondents from the prime.</p>
<p>For more discussion of survey experiments, see:</p>
<ul>
<li><span class="citation" data-cites="mutz2011population">Mutz (<a href="#ref-mutz2011population" role="doc-biblioref">2011</a>)</span> “Population-Based Survey Experiments.”</li>
<li><span class="citation" data-cites="sniderman2018some">Sniderman (<a href="#ref-sniderman2018some" role="doc-biblioref">2018</a>)</span> “Some Advances in the Design of Survey Experiments” in the Annual Review of Political Science.</li>
<li><span class="citation" data-cites="lavrakas2019experimental">Lavrakas et al. (<a href="#ref-lavrakas2019experimental" role="doc-biblioref">2019</a>)</span> “Experimental Methods in Survey Research: Techniques that Combine Random Sampling with Random Assignment.”</li>
<li><span class="citation" data-cites="diaz2020survey">Diaz, Grady, and Kuklinski (<a href="#ref-diaz2020survey" role="doc-biblioref">2020</a>)</span> “Survey Experiments and the Quest for Valid Interpretation” in the Sage Handbook of Research Methods in Political Science and International Relations.</li>
</ul>
</section>
<section id="references" class="level1">




</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-abramson2019we" class="csl-entry" role="doc-biblioentry">
Abramson, Scott F, Korhan Koçak, and Asya Magazinnik. 2019. <span>“What Do We Learn about Voter Preferences from Conjoint Experiments?”</span> <em>Unpublished Manuscript. Https://Pdfs. Semanticscholar. Org/023a/24a7dfaddfce626d011596b187f26361ee86. Pdf</em>.
</div>
<div id="ref-auspurg2014factorial" class="csl-entry" role="doc-biblioentry">
Auspurg, Katrin, and Thomas Hinz. 2014. <em>Factorial Survey Experiments</em>. Vol. 175. Sage Publications.
</div>
<div id="ref-azfar2009identifying" class="csl-entry" role="doc-biblioentry">
Azfar, Omar, and Peter Murrell. 2009. <span>“Identifying Reticent Respondents: Assessing the Quality of Survey Data on Corruption and Values.”</span> <em>Economic Development and Cultural Change</em> 57 (2): 387–411.
</div>
<div id="ref-bansak2018number" class="csl-entry" role="doc-biblioentry">
Bansak, Kirk, Jens Hainmueller, Daniel J Hopkins, and Teppei Yamamoto. 2018. <span>“The Number of Choice Tasks and Survey Satisficing in Conjoint Experiments.”</span> <em>Political Analysis</em> 26 (1): 112–19.
</div>
<div id="ref-bansak2019beyond" class="csl-entry" role="doc-biblioentry">
———. 2019. <span>“Beyond the Breaking Point? Survey Satisficing in Conjoint Experiments.”</span> <em>Political Science Research and Methods</em>, 1–19.
</div>
<div id="ref-bansak2020using" class="csl-entry" role="doc-biblioentry">
———. 2020. <span>“Using Conjoint Experiments to Analyze Elections: The Essential Role of the Average Marginal Component Effect (AMCE).”</span> <em>Available at SSRN</em>.
</div>
<div id="ref-blair2015design" class="csl-entry" role="doc-biblioentry">
Blair, Graeme, Kosuke Imai, and Yang-Yang Zhou. 2015. <span>“Design and Analysis of the Randomized Response Technique.”</span> <em>Journal of the American Statistical Association</em> 110 (511): 1304–19.
</div>
<div id="ref-boruch1971assuring" class="csl-entry" role="doc-biblioentry">
Boruch, Robert F. 1971. <span>“Assuring Confidentiality of Responses in Social Research: A Note on Strategies.”</span> <em>The American Sociologist</em>, 308–11.
</div>
<div id="ref-brader2008triggers" class="csl-entry" role="doc-biblioentry">
Brader, Ted, Nicholas A Valentino, and Elizabeth Suhay. 2008. <span>“What Triggers Public Opposition to Immigration? Anxiety, Group Cues, and Immigration Threat.”</span> <em>American Journal of Political Science</em> 52 (4): 959–78.
</div>
<div id="ref-bullock2011statistical" class="csl-entry" role="doc-biblioentry">
Bullock, Will, Kosuke Imai, and Jacob N Shapiro. 2011. <span>“Statistical Analysis of Endorsement Experiments: Measuring Support for Militant Groups in Pakistan.”</span> <em>Political Analysis</em> 19 (4): 363–84.
</div>
<div id="ref-cohen2003party" class="csl-entry" role="doc-biblioentry">
Cohen, Geoffrey L. 2003. <span>“Party over Policy: The Dominating Impact of Group Influence on Political Beliefs.”</span> <em>Journal of Personality and Social Psychology</em> 85 (5): 808.
</div>
<div id="ref-dafoe2018information" class="csl-entry" role="doc-biblioentry">
Dafoe, Allan, Baobao Zhang, and Devin Caughey. 2018. <span>“Information Equivalence in Survey Experiments.”</span> <em>Political Analysis</em> 26 (4): 399–416.
</div>
<div id="ref-diaz2020survey" class="csl-entry" role="doc-biblioentry">
Diaz, Gustavo, Christopher Grady, and James H Kuklinski. 2020. <span>“Survey Experiments and the Quest for Valid Interpretation.”</span> In <em>SAGE Handbook of Research Methods in Political Science and International Relations</em>. SAGE Publishing.
</div>
<div id="ref-droitcour2004item" class="csl-entry" role="doc-biblioentry">
Droitcour, Judith, Rachel A Caspar, Michael L Hubbard, Teresa L Parsley, Wendy Visscher, and Trena M Ezzati. 2004. <span>“The Item Count Technique as a Method of Indirect Questioning: A Review of Its Development and a Case Study Application.”</span> <em>Measurement Errors in Surveys</em>, 185–210.
</div>
<div id="ref-druckman2012learning" class="csl-entry" role="doc-biblioentry">
Druckman, James N, and Thomas J Leeper. 2012. <span>“Learning More from Political Communication Experiments: Pretreatment and Its Effects.”</span> <em>American Journal of Political Science</em> 56 (4): 875–96.
</div>
<div id="ref-edgell1982validity" class="csl-entry" role="doc-biblioentry">
Edgell, Stephen E, Samuel Himmelfarb, and Karen L Duchan. 1982. <span>“Validity of Forced Responses in a Randomized Response Model.”</span> <em>Sociological Methods &amp; Research</em> 11 (1): 89–100.
</div>
<div id="ref-figner2011using" class="csl-entry" role="doc-biblioentry">
Figner, Bernd, Ryan O Murphy, et al. 2011. <span>“Using Skin Conductance in Judgment and Decision Making Research.”</span> <em>A Handbook of Process Tracing Methods for Decision Research</em>, 163–84.
</div>
<div id="ref-gaines2007logic" class="csl-entry" role="doc-biblioentry">
Gaines, Brian J, James H Kuklinski, and Paul J Quirk. 2007. <span>“The Logic of the Survey Experiment Reexamined.”</span> <em>Political Analysis</em> 15 (1): 1–20.
</div>
<div id="ref-garcia2020weed" class="csl-entry" role="doc-biblioentry">
Garcia-Sanchez, Miguel, and Rosario Quierolo. n.d. <span>“A Tale of Two Countries: Measuring Drug Consumption in Opposite Contexts - Evidence from Survey Experiments in Colombia and Uruguay.”</span>
</div>
<div id="ref-cpo2015rr" class="csl-entry" role="doc-biblioentry">
Gingerich, Daniel. 2015. <span>“Randomized Response: Foundations and New Developments.”</span> In <em>Newsletter of the Comparative Politics Organized Section of the American Political Science Association</em>.
</div>
<div id="ref-gingerich2010understanding" class="csl-entry" role="doc-biblioentry">
Gingerich, Daniel W. 2010. <span>“Understanding Off-the-Books Politics: Conducting Inference on the Determinants of Sensitive Behavior with Randomized Response Surveys.”</span> <em>Political Analysis</em> 18 (3): 349–80.
</div>
<div id="ref-glynn2013double" class="csl-entry" role="doc-biblioentry">
Glynn, Adam N. 2013. <span>“What Can We Learn with Statistical Truth Serum? Design and Analysis of the List Experiment.”</span> <em>Public Opinion Quarterly</em> 77 (S1): 159–72.
</div>
<div id="ref-green1971conjoint" class="csl-entry" role="doc-biblioentry">
Green, Paul E, and Vithala R Rao. 1971. <span>“Conjoint Measurement-for Quantifying Judgmental Data.”</span> <em>Journal of Marketing Research</em> 8 (3): 355–63.
</div>
<div id="ref-greenwald1995implicit" class="csl-entry" role="doc-biblioentry">
Greenwald, Anthony G, and Mahzarin R Banaji. 1995. <span>“Implicit Social Cognition: Attitudes, Self-Esteem, and Stereotypes.”</span> <em>Psychological Review</em> 102 (1): 4.
</div>
<div id="ref-greenwald1998IAT" class="csl-entry" role="doc-biblioentry">
Greenwald, Anthony G, Debbie E McGhee, and Jordan LK Schwartz. 1998. <span>“Measuring Individual Differences in Implicit Cognition: The Implicit Association Test.”</span> <em>Journal of Personality and Social Psychology</em> 74 (6): 1464.
</div>
<div id="ref-hainmueller2014causal" class="csl-entry" role="doc-biblioentry">
Hainmueller, Jens, Daniel J Hopkins, and Teppei Yamamoto. 2014. <span>“Causal Inference in Conjoint Analysis: Understanding Multidimensional Choices via Stated Preference Experiments.”</span> <em>Political Analysis</em> 22 (1): 1–30.
</div>
<div id="ref-incerti2020corruption" class="csl-entry" role="doc-biblioentry">
Incerti, Trevor. 2020. <span>“Corruption Information and Vote Share: A Meta-Analysis and Lessons for Experimental Design.”</span> <em>American Political Science Review, Forthcoming</em>.
</div>
<div id="ref-jann2011asking" class="csl-entry" role="doc-biblioentry">
Jann, Ben, Julia Jerke, and Ivar Krumpal. 2011. <span>“Asking Sensitive Questions Using the Crosswise Model: An Experimental Survey Measuring Plagiarism.”</span> <em>Public Opinion Quarterly</em> 76 (1): 32–49.
</div>
<div id="ref-jones1971bogus" class="csl-entry" role="doc-biblioentry">
Jones, Edward E, and Harold Sigall. 1971. <span>“The Bogus Pipeline: A New Paradigm for Measuring Affect and Attitude.”</span> <em>Psychological Bulletin</em> 76 (5): 349.
</div>
<div id="ref-kam2005toes" class="csl-entry" role="doc-biblioentry">
Kam, Cindy D. 2005. <span>“Who Toes the Party Line? Cues, Values, and Individual Differences.”</span> <em>Political Behavior</em> 27 (2): 163–82.
</div>
<div id="ref-kinder1981symbolicRacism" class="csl-entry" role="doc-biblioentry">
Kinder, Donald R, and David O Sears. 1981. <span>“Prejudice and Politics: Symbolic Racism Versus Racial Threats to the Good Life.”</span> <em>Journal of Personality and Social Psychology</em> 40 (3): 414.
</div>
<div id="ref-eso2012list" class="csl-entry" role="doc-biblioentry">
Kramon, Eric, and Keith Weghorst. 2012. <span>“List Experiments in the Field.”</span> In <em>Newsletter of the APSA Experimental Section</em>.
</div>
<div id="ref-kramon2019mis" class="csl-entry" role="doc-biblioentry">
———. 2019. <span>“(Mis) Measuring Sensitive Attitudes with the List Experiment: Solutions to List Experiment Breakdown in Kenya.”</span> <em>Public Opinion Quarterly</em> 83 (S1): 236–63.
</div>
<div id="ref-krosnick1991response" class="csl-entry" role="doc-biblioentry">
Krosnick, Jon A. 1991. <span>“Response Strategies for Coping with the Cognitive Demands of Attitude Measures in Surveys.”</span> <em>Applied Cognitive Psychology</em> 5 (3): 213–36.
</div>
<div id="ref-kuklinski1997list" class="csl-entry" role="doc-biblioentry">
Kuklinski, James H, Paul M Sniderman, Kathleen Knight, Thomas Piazza, Philip E Tetlock, Gordon R Lawrence, and Barbara Mellers. 1997. <span>“Racial Prejudice and Attitudes Toward Affirmative Action.”</span> <em>American Journal of Political Science</em>, 402–19.
</div>
<div id="ref-lavrakas2019experimental" class="csl-entry" role="doc-biblioentry">
Lavrakas, Paul J, Michael W Traugott, Courtney Kennedy, Allyson L Holbrook, Edith D de Leeuw, and Brady T West. 2019. <em>Experimental Methods in Survey Research: Techniques That Combine Random Sampling with Random Assignment</em>. John Wiley &amp; Sons.
</div>
<div id="ref-leary1990impression" class="csl-entry" role="doc-biblioentry">
Leary, Mark R, and Robin M Kowalski. 1990. <span>“Impression Management: A Literature Review and Two-Component Model.”</span> <em>Psychological Bulletin</em> 107 (1): 34.
</div>
<div id="ref-leeper2020measuring" class="csl-entry" role="doc-biblioentry">
Leeper, Thomas J, Sara B Hobolt, and James Tilley. 2020. <span>“Measuring Subgroup Preferences in Conjoint Experiments.”</span> <em>Political Analysis</em> 28 (2): 207–21.
</div>
<div id="ref-lensvelt2005meta" class="csl-entry" role="doc-biblioentry">
Lensvelt-Mulders, Gerty JLM, Joop J Hox, Peter GM Van der Heijden, and Cora JM Maas. 2005. <span>“Meta-Analysis of Randomized Response Research: Thirty-Five Years of Validation.”</span> <em>Sociological Methods &amp; Research</em> 33 (3): 319–48.
</div>
<div id="ref-lyall2013explaining" class="csl-entry" role="doc-biblioentry">
Lyall, Jason, Graeme Blair, and Kosuke Imai. 2013. <span>“Explaining Support for Combatants During Wartime: A Survey Experiment in Afghanistan.”</span> <em>American Political Science Review</em> 107 (4): 679–705.
</div>
<div id="ref-macrae1994out" class="csl-entry" role="doc-biblioentry">
Macrae, C Neil, Galen V Bodenhausen, Alan B Milne, and Jolanda Jetten. 1994. <span>“Out of Mind but Back in Sight: Stereotypes on the Rebound.”</span> <em>Journal of Personality and Social Psychology</em> 67 (5): 808.
</div>
<div id="ref-mutz2011population" class="csl-entry" role="doc-biblioentry">
Mutz, Diana C. 2011. <em>Population-Based Survey Experiments</em>. Princeton University Press.
</div>
<div id="ref-nicholson2011dominating" class="csl-entry" role="doc-biblioentry">
Nicholson, Stephen P. 2011. <span>“Dominating Cues and the Limits of Elite Influence.”</span> <em>The Journal of Politics</em> 73 (4): 1165–77.
</div>
<div id="ref-nicholson2012polarizing" class="csl-entry" role="doc-biblioentry">
———. 2012. <span>“Polarizing Cues.”</span> <em>American Journal of Political Science</em> 56 (1): 52–66.
</div>
<div id="ref-nisbett1977telling" class="csl-entry" role="doc-biblioentry">
Nisbett, Richard E, and Timothy D Wilson. 1977. <span>“Telling More Than We Can Know: Verbal Reports on Mental Processes.”</span> <em>Psychological Review</em> 84 (3): 231.
</div>
<div id="ref-norenzayan1999telling" class="csl-entry" role="doc-biblioentry">
Norenzayan, Ara, and Norbert Schwarz. 1999. <span>“Telling What They Want to Know: Participants Tailor Causal Attributions to Researchers’ Interests.”</span> <em>European Journal of Social Psychology</em> 29 (8): 1011–20.
</div>
<div id="ref-nosek2007implicit" class="csl-entry" role="doc-biblioentry">
Nosek, Brian A. 2007. <span>“Implicit–Explicit Relations.”</span> <em>Current Directions in Psychological Science</em> 16 (2): 65–69.
</div>
<div id="ref-rankin1955galvanic" class="csl-entry" role="doc-biblioentry">
Rankin, Robert E, and Donald T Campbell. 1955. <span>“Galvanic Skin Response to Negro and White Experimenters.”</span> <em>The Journal of Abnormal and Social Psychology</em> 51 (1): 30.
</div>
<div id="ref-riambau2019placebo" class="csl-entry" role="doc-biblioentry">
Riambau, Guillem, and Kai Ostwald. 2019. <span>“Placebo Statements in List Experiments: Evidence from a Face-to-Face Survey in Singapore.”</span> <em>Political Science Research and Methods</em>, 1–8.
</div>
<div id="ref-rosenfeld2016empirical" class="csl-entry" role="doc-biblioentry">
Rosenfeld, Bryn, Kosuke Imai, and Jacob N Shapiro. 2016. <span>“An Empirical Validation Study of Popular Survey Methodologies for Sensitive Questions.”</span> <em>American Journal of Political Science</em> 60 (3): 783–802.
</div>
<div id="ref-schwarz1999self" class="csl-entry" role="doc-biblioentry">
Schwarz, Norbert. 1999. <span>“Self-Reports: How the Questions Shape the Answers.”</span> <em>American Psychologist</em> 54 (2): 93.
</div>
<div id="ref-schwarz1983mood" class="csl-entry" role="doc-biblioentry">
Schwarz, Norbert, and Gerald L Clore. 1983. <span>“Mood, Misattribution, and Judgments of Well-Being: Informative and Directive Functions of Affective States.”</span> <em>Journal of Personality and Social Psychology</em> 45 (3): 513.
</div>
<div id="ref-simon2006administrative" class="csl-entry" role="doc-biblioentry">
Simon, Herbert, and James March. 2006. <span>“Administrative Behavior and Organizations.”</span> <em>Organizational Behavior 2: Essential Theories of Process and Structure</em> 2 (41).
</div>
<div id="ref-sniderman2018some" class="csl-entry" role="doc-biblioentry">
Sniderman, Paul M. 2018. <span>“Some Advances in the Design of Survey Experiments.”</span> <em>Annual Review of Political Science</em> 21: 259–75.
</div>
<div id="ref-sniderman1991new" class="csl-entry" role="doc-biblioentry">
Sniderman, Paul M, Thomas Piazza, Philip E Tetlock, and Ann Kendrick. 1991. <span>“The New Racism.”</span> <em>American Journal of Political Science</em>, 423–47.
</div>
<div id="ref-steiner2016designing" class="csl-entry" role="doc-biblioentry">
Steiner, Peter M, Christiane Atzmüller, and Dan Su. 2016. <span>“Designing Valid and Reliable Vignette Experiments for Survey Research: A Case Study on the Fair Gender Income Gap.”</span> <em>Journal of Methods and Measurement in the Social Sciences</em> 7 (2): 52–94.
</div>
<div id="ref-warner1965randomized" class="csl-entry" role="doc-biblioentry">
Warner, Stanley L. 1965. <span>“Randomized Response: A Survey Technique for Eliminating Evasive Answer Bias.”</span> <em>Journal of the American Statistical Association</em> 60 (309): 63–69.
</div>
<div id="ref-winters2013lacking" class="csl-entry" role="doc-biblioentry">
Winters, Matthew S, and Rebecca Weitz-Shapiro. 2013. <span>“Lacking Information or Condoning Corruption: When Do Voters Support Corrupt Politicians?”</span> <em>Comparative Politics</em> 45 (4): 418–36.
</div>
<div id="ref-yu2008two" class="csl-entry" role="doc-biblioentry">
Yu, Jun-Wu, Guo-Liang Tian, and Man-Lai Tang. 2008. <span>“Two New Models for Survey Sampling with Sensitive Characteristic: Design and Analysis.”</span> <em>Metrika</em> 67 (3): 251.
</div>
<div id="ref-zigerell2011you" class="csl-entry" role="doc-biblioentry">
Zigerell, Lawrence J. 2011. <span>“You Wouldn’t Like Me When i’m Angry: List Experiment Misreporting.”</span> <em>Social Science Quarterly</em> 92 (2): 552–62.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>