---
title: "10 Things to Know About Survey Experiments"
author: 
  - name: "Christopher Grady"
    url: https://publish.illinois.edu/cdgrady2/
bibliography: survey-experiments.bib
image: survey-experiments.png
---

<style>
p.comment{
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 25px;
border-radius: 5px;
color: black;
}

</style>

Survey experiments are widely used by social scientists to study individual preferences.  This guide discusses the functions and considerations of survey experiments.


# What is a survey experiment 


A survey experiment is an experiment conducted within a survey.  In an experiment, a researcher randomly assigns participants to at least two experimental conditions.  The researcher then treats each condition differently.  Because of random assignment, any differences between the experimental conditions would result from the treatment.  In a survey experiment, the randomization and treatment occur within a questionnaire.


# Why do a survey experiment


Survey experiments are useful when researchers want to learn about individual perceptions, attitudes, or behaviors.  They are especially useful when a regular survey, without experimentation, may generate biased or even nonsensical responses.  For example, if researchers are interested in studying the effects of policy information on individual preferences for a policy, directly asking each survey respondent "how does this information affect your attitudes toward the policy?" may raise concerns about the accuracy and truthfulness of the responses.  Rather, researchers may find it useful to provide the policy information to a randomized subset of respondents, followed by comparing the policy preferences between those who are subject to the policy information and those who are not.

More generally, survey experiments help to measure individual preferences.  For example, when the preferences of interest are multidimensional, regular surveys may not be able to reliably measure such complex preferences through individual self-reports.  Other preferences, such as racist attitudes and illegal behaviors, may be sensitive --- preferences with which respondents do not want to be publicly associated.  Direct questioning techniques may thus understate the prevalence of these preferences.  In these cases, survey experiments, compared to regular surveys, can be useful to address these measurement challenges.

There are various types of survey experiments.  Five of them --- conjoint experiments, priming experiments, endorsement experiments, list experiments, and randomized response --- are covered in the following sections.

<!-- # Survey Experiments to Determine a Causal Relationship: Vignette and Factorial Designs 


Not all survey experiments share the goal of accurately measuring one concept of interest.  Some survey experiments, like lab experiments, are interested in how an experimental manipulation impacts outcomes of interest.  These survey experiments for causal inference randomize a treatment and then measure outcomes.  When measuring outcomes, they may use techniques like list experiments.

One of the most common designs of survey experiments for causal inference are vignette and factorial designs.^[See @auspurg2014factorial and @sniderman1991new.]  In a vignette/factorial experiment, the researcher provides the respondent with a hypothetical scenario to read, varying key components of the scenario.  In a typical vignette, the researcher varies only one component of the scenario.  In a typical factorial experiment, the researcher varies several components of the scenario.  

Both vignette and factorial designs benefit from embedding the survey question in a concrete scenario so that they require little abstraction from the survey respondent.  Their concrete nature can make them more interesting and easier to answer than typical survey questions, decreasing survey fatigue.  They can also function as priming experiments if the concept of interest is embedded in other concepts.

As an example, @winters2013lacking uses factorial vignettes to learn if voters sanction corrupt politicians in Brazil.  They posit that corruption could interact with competence, so the authors varied a Brazilian mayor's corruption, competence, and political affiliation in the vignette.  They tell respondents:

> Imagine a person named Gabriel (Gabriela for female respondents), who is a person like you, living in a neighborhood like yours, but in a different city in Brazil. The mayor of Gabriel’s city is running for reelection in October. He is a member of the [**Partido dos Trabalhadores**/**Partido da Social Democracia Brasileira**]. In Gabriel’s city, it is well known that the mayor [**never takes bribes**/**frequently takes bribes**] when giving out government contracts. The mayor has completed [**few**/**many**/**omit the entire sentence**] public works projects during his term in office. In this city, the election for mayor is expected to be very close.<br><br>
In your opinion, what is the likelihood that Gabriel(a) will vote for this mayor in the next election: very likely, somewhat likely, unlikely, not at all likely?

This design allowed the authors to determine if and when corruption would be punished by voters.  If respondents overlooked corruption when the mayor completed many public works, the interpretation is that corruption is acceptable if it gets the job done.  If respondents overlooked corruption when the mayor was a copartisan, the interpretation is that voters ignore the corruption of their own.  By varying several related aspects of the scenario, @winters2013lacking could isolate the conditions under which credible information about corruption would be punished by voters.

### Pitfalls of vignette/factorial experiments

The main pitfall of vignettes -- a _lack of information equivalence_ -- is dealt with by factorial experiments.  Researchers can randomize several aspects of the scenario, standardizing factors that could influence how the main treatment is perceived by respondents.  _Some combinations of different factors may not be realistic_, however.  Researchers must be sure that the various possible combinations of their factorial experiments seem credible to respondents.

_Statistical power is weak_ when factorial experiments vary many confounding traits.  The more traits being varied, the more experimental conditions, the fewer respondents in each experimental condition, and the greater likelihood of imbalance between treatment conditions.  

In enumerated surveys, there is also the possibility that certain enumerators are more often assigned certain factorial conditions and that _enumerator effects could be mistaken for treatment effects_ [@steiner2016designing].  Imagine the @winters2013lacking study, which had six functional treatment groups and ~2,000 respondents.  If the survey was enumerated by twenty survey enumerators, then, in expectation, each enumerator has only ~17 subjects in each treatment category.  In reality, it is likely that certain enumerators will more often enumerate some conditions than others and differences due to enumerators could appear as treatment effects.

### Variants/modifications

Researchers can _block treatment by enumerator_ so that enumerator effects cannot confound treatment effects [@steiner2016designing].  Blocking and other techniques the authors propose should also increase statistical power by accounting for systematic error.

_Conjoint experiments_ maintain many benefits of factorial experiments, but increase power by presenting multiple choice tasks instead of one.  We discuss conjoint experiments in the next section. -->


# Conjoint experiments


Conjoint experiments are useful when researchers aim to measure multidimensional preferences (i.e., preferences that are characterized by more than one attribute).  In a typical conjoint experiment, researchers repeatedly ask respondents to choose between two distinct options and randomly vary the characteristics of these two options.^[See @hainmueller2014causal and @green1971conjoint.]  Researchers may also ask respondents to rate each option on a scale.  In both cases, respondents express their preferences toward a large number of pairings with randomized attributes.

@hainmueller2014causal demonstrate the use of conjoint experiments in a study about support for immigration.  The authors showed respondents two immigrant profiles and asked (a) which immigrant the respondent would prefer be admitted to the Unites States and (b) how the respondent rated each immigrant on a scale from 1-7.  The authors randomly varied nine attributes of the immigrants (gender, education, employment plans, job experience, profession, language skills, country of origin, reasons for applying, and prior trips to the United States), yielding thousands of unique immigrant profiles.  This process was repeated five times so that each respondents saw and rated five pairs of immigrants.  Through this procedure, the authors assessed how these randomly varied components influence support for the immigrant.

Respondents saw:

![](survey-experiments_conjoint_image2.png)

The conjoint experiment thus allows us to measure how _multiple_ immigrant characteristics, such as their gender or country of origin, shape respondents' attitudes toward the immigrants.  Another advantage of this survey experiment, compared to a non-experimental survey, is that it preempts the need for respondents to directly express sensitive preferences; instead, they indirectly reveal their preferences.  For example, while respondents who hold sexist attitudes may be less willing to openly express preferences for male immigrants due to social desirability bias, they may find it more comfortable to choose --- and therefore reveal their preferences for --- male immigrant profiles in this less direct setting.^[See @horiuchi2022conjoint.]    Given these advantages, the use of conjoint experiments is not confined to the measurement of immigrant preferences; researchers have also applied conjoint experiments to study other multidimensional preferences, such as candidate choice and policy packages.

<!-- Through a conjoint experiment, researchers can learn about the average marginal effect of several aspects of a scenario, far more than would be feasible with a typical vignette or factorial design.  Though researchers could include and vary an almost infinite number of characteristics, the best practice is to only vary traits that could confound the relationship between a primary explanatory variable and an outcome of interest, rather than varying any trait that might affect the outcome of interest [@diaz2020survey].


### Pitfalls of conjoint experiments

The costs and benefits of conjoint experiments are still being actively researched.  Thus far, two classes of critiques are common.

Results from conjoint experiments are _difficult to interpret_.  Results of conjoint experiments' target estimand, the Average Marginal Component Effect (AMCE), can "indicate the opposite of the true preference of the majority."^[See @abramson2019we[p.1]]  Other researchers have noted that AMCE's depend on the reference category and are not comparable across survey subgroups [@leeper2020measuring].  @bansak2020using provides guidance on how to interpret conjoint results and argues that AMCE's do represent quantities of interest to empirical scholars.

Conjoint experiments also create _unrealistic combinations_ and those unrealistic combinations lead to effect estimates that are not representative of the real world [@incerti2020corruption]. Similarly, the large amount of information provided by conjoint experiments could misrepresent how individuals generally process information they encounter in the world [@hainmueller2014causal].  The large amount of information and demand on respondents has also led to concerns about satisficing, though @bansak2018number and @bansak2019beyond suggest satisficing is not a major concern for conjoint experiments.

Other potential pitfalls can occur if the researcher varies too many characteristics.  More randomly varied characteristics means a large number of potential hypothesis tests.  The necessity of applying multiple hypothesis corrections to the vast number of potential hypothesis tests could decrease statistical power to detect specific effects, especially if researchers are interested in interactions between traits being varied. -->


# Priming experiments


In a priming experiment, researchers expose respondents in the treatment group to a stimulus representing topic _X_ in order to influence their considerations at the top of their head when responding to a survey question about topic _Y_.  The control group, however, is not exposed to the stimulus.   Therefore, the difference in expressed preferences regarding _Y_ between the treatment and control groups is due to exposure to the treatment stimulus.

Priming experiments are a broad class and include any experiment that makes a specific topic salient in the mind of the respondent.  One common method of priming is the use of images.  For example, @brader2008triggers used images as a priming instrument to estimate the role of race in shaping immigration preferences.  The researchers showed subjects a positive or negative news article about immigration paired with a picture of a European immigrant or an Hispanic immigrant.  Subjects expressed negative attitudes about immigration when the negative news article was paired with the Hispanic immigrant picture but not in other conditions.  The picture primed people to think about Hispanic immigrants, and thinking about Hispanic immigrants reduced support for immigration compared to thinking about European immigrants.

More broadly, priming experiments can be useful when researchers are interested in learning about the influence of context.  By making a specific context of interest salient to a randomized subset of respondents, researchers can gauge the impact of this primed context on the measured outcome of interest.

<!-- Survey experiments for measurement and for causal identification overlap in priming experiments.  Researchers can use them to measure implicit attitudes or to assess how the activation of implicit attitudes affects another outcome, like attitudes towards immigration.

### Pitfalls of priming experiments

_Priming experiments are difficult_.  Priming attitudes experimentally is difficult because the researcher cannot be certain that the prime affects subjects as the researcher intended.  A prime intended to induce fear, for example, may induce fear in some subjects and excitement in others.  Priming sensitive attitudes is especially difficult because the researcher must prime a sensitive attitude without the respondent becoming aware that the researcher is interested in the sensitive attitude.  If respondents realize what the priming experiment is about, the experiment fails  because respondents will consciously censor their attitude, rather than passively allow their implicit attitude to influence their response.^[See @macrae1994out and @schwarz1983mood.]  To prevent subjects from ascertaining the goal of the study, researchers try to hide the prime amid other, ostensibly more important, information.

_Priming experiments can suffer from confounding and lack of "information equivalence" between treatment groups_ [@dafoe2018information].  The researchers may prime topic $X$ with the intent of learning about respondents' implicit attitudes towards topic $X$, but if topic $X$ is strongly linked with topic $Y$ then the researcher will estimate the effect of $X$ and $Y$, not just $X$.  For example, priming a partisan group may also prime ideological and policy views associated with the partisan group [@nicholson2011dominating]. A basic priming experiment cannot differentiate the effect of priming the partisan group from the effect of priming the ideological and policy views associated with the partisan group.

_Respondents may be pretreated before the experiment_.  Individuals are exposed to stimuli that prime attitudes during their daily lives.  News broadcasts prime people to think about issues covered on the news, and anti-racism protests prime people to think about racial issues.  Even words seen immediately before answering survey questions influences responses to those survey questions [@norenzayan1999telling].  If subjects, before participating in the experiment, encounter the stimuli that the researcher wants to prime, there may be no difference between treatment and control groups because all subjects were "pretreated" with the prime, even subjects in the control group.^[See @gaines2007logic and @druckman2012learning.]  If the issue being primed is already salient in the mind of the respondent, priming experiments fail.

### Variants/modifications

To ensure information equivalence and to reduce confounding the prime with an associated factor, researchers utilize priming experiments as part of _factorial experiments_.  Factorial experiments vary multiple factors that may be linked in the minds of respondents.  @nicholson2011dominating, for example, asked respondents about support for a policy.  He varied both partisan endorsement and policy details to learn how partisan bias influenced respondents' attitudes beyond any assumptions about the party's policy positions.  Factorial experiments are mainly used to determine causal relationships and are discussed in section 7. -->

<!--**Conclusion**: Priming experiments can help determine respondents' attitudes in cases where respondents do not have conscious access to the attitude that the researcher is interested in.
-->


# Endorsement experiments

 
Endorsement experiments measure attitudes toward a sensitive object, usually a controversial political actor or group.  <!-- They were first developed to study partisan bias,^[See @cohen2003party and @kam2005toes.] but have since been used to measure support for militant groups.^[See @bullock2011statistical and @lyall2013explaining.]  They have also been inverted to measure support for a policy rather than a political actor [@rosenfeld2016empirical]. --> In a typical endorsement experiment, respondents are asked how much they support a policy.  In the treatment condition, the policy is said to be endorsed by an actor or a group.  In the control condition, however, this endorsement information is omitted.  The average difference in support between the endorsed and unendorsed policy represents the change in support for the policy because of the endorsement of the controversial figure.

<!-- Endorsement experiments can measure implicit attitudes or explicit attitudes.  They measure implicit attitudes like a priming experiment if respondents do not realize the group's endorsement is what the researcher is interested in.  They measure explicit attitudes like a list experiment if respondents realize the group's endorsement is what the researcher is interested in.  Whereas list experiments hide the respondent’s opinion by pairing the sensitive item with non-sensitive control items, endorsement experiments hide the respondent’s opinion by pairing the sensitive item with a policy that could feasibly be responsible for the respondent's attitude. -->  
<!--where individuals can freely express their support for the group through supporting the policy because the researcher cannot differentiate policy support from group support at an individual level.-->

<!--chris: example here -->
For example, @nicholson2012polarizing used an endorsement experiment to study partisan bias in the United States during the 2008 Presidential campaign.  The researchers asked respondents about policies, varying whether the policy was endorsed by the Presidential candidates of the two main political parties, Barack Obama (Democrat) and John McCain (Republican).  Respondents were told:

> As you know, there has been a lot of talk about immigration reform policy in the news. One proposal [**backed by Barack Obama**/**backed by John McCain**] provided legal status and a path to legal citizenship for the approximately 12 million illegal immigrants currently residing in the United States.  What is your view of this immigration reform policy?

On one hand, the difference between the control condition and the Obama (McCain) condition for Democrats (Republicans) indicates in-party bias.  On the other, the difference between the control condition and the Obama (McCain) condition for Republicans (Democrats) indicates out-party bias.  This experiment helps researchers gauge the favorability toward the potentially sensitive item (i.e., the political actor), as other well-designed endorsement experiments also do.  Because endorsement experiments preempt the need for respondents to self-report their support for a controversial object, they are especially useful in politically sensitive contexts.  For example, they have been used to measure public support for militant groups (e.g., @bullock2011statistical; @lyall2013explaining).

<!-- ### Pitfalls of endorsement experiments

As with priming experiments, endorsement experiments suffer from _confounding and a lack of information equivalence_.  Researchers cannot be certain if differential support for the policy is due to the endorsement or due to different substantive assumptions about the policy that respondents make as a result of the endorsement.

_Choosing a policy is difficult_.  The value of the endorsement experiments depends largely on the characteristics of the policy being (or not being) endorsed.  The chosen policy must not possess too much or too little support in the survey population, otherwise attitudes towards the policy will wipe out the effect of the group's endorsement.  Too much or too little support could also reduce perceived anonymity if respondents think that no one would support/oppose the policy unless they liked/disliked the endorsing group.

Endorsement experiments can have _low power to detect effects_, even relative to other survey experiments [@bullock2011statistical]  Some subset of subjects will be unaffected by the endorsement because they feel strongly about the policy, and that subset adds substantial noise to endorsement experiments.

### Variants/modifications

To overcome low power, @bullock2011statistical recommend using _multiple policy questions_ that are on the same one-dimensional policy space.  Multiple questions on one policy space allows the researcher to predict each respondent's level of support for the policy if it were not endorsed by the group of interest.  The researcher can thus model the noise caused by strong feelings towards the policy.

As with priming experiments, to ensure information equivalence and to reduce confounding factors, researchers use endorsement experiments as part of _factorial experiments_ that vary the multiple factors that may be linked in the mind of respondents.  Factorial experiments are mainly used to determine causal relationships and are discussed in section 7. -->

<!--**Conclusion**: Endorsement experiments are a flexible survey experiment that can be used to measure explicit or implicit attitudes.  Designing them takes substantial pilot testing.-->


# List Experiments


List experiments (also known as the item count technique) measure a sensitive attitude or behavior when researchers expect respondents to falsify it if it is solicited using a direct question.  For example, respondents may be reluctant to admit that they hold racially conservative views [@kuklinski1997list] or engage in illegal behaviors [@garcia2021list] even after being assured of the survey's anonymity.

In a list experiment, the researcher randomly assigns respondents to a control or treatment condition.  The control condition presents respondents with a list of items; the treatment condition presents respondents with the same list plus a treatment item measuring the attitude or behavior of interest.  Respondents are then asked how many of these items apply to them.  The average difference between the treatment and control conditions represents the percentage of respondents for whom the treatment item applies.  A list experiment does not tell the researcher about the attitude or behavior of any individual respondent, but it tells her about the prevalence of the sensitive attitude in her sample population.  Answers to this question are anonymous because the respondent's attitude toward each item cannot be determined unless the respondent answers that all or none of the items apply to them.

For example, @kuklinski1997list studied racial animus with a list experiment.  They told respondents:

<!--<p class="comment">-->

> Now I am going to read you three things that sometimes make people angry or upset.  After I read all three, just tell me HOW MANY of them upset you.  I don't want to know which ones, just HOW MANY. <br><br>
(1) the federal government increasing the tax on gasoline <br>
(2) professional athletes getting million-dollar contracts<br>
(3) large corporations polluting the environment<br>
_(4) a black family moving in next door_

In the above example, the fourth item was withheld from the control condition.  The authors found that the mean number of items chosen in the treatment group was 2.37, compared to 1.95 in the control group.  The difference of 0.42 between treatment and control suggests that 42% of respondents would be upset by a black family moving in next door.

Despite the anonymity provided by a list experiment, respondents may still worry that their response reflects their attitudes about the sensitive item.  When respondents worry about a lack of anonymity, they may increase or decrease their response to portray themselves in the best light possible, rather than answer honestly [@leary1990impression].  Given this limitation, researchers have developed other types of list experiments, including _double list experiments_ and _placebo-controlled list experiments_.  Interested readers may consult @glynn2013double and @riambau2019placebo for detailed discussions about their implementation, as well as how they help to overcome some of the potential pitfalls of simple list experiments.

<!-- ### Pitfalls of list experiments

List experiments are _vulnerable to satisficing_.  Satisficing occurs when respondents put in minimal effort to understand and answer a survey question.^[See @krosnick1991response and @simon2006administrative.] In a list experiment, satisficing manifests when respondents do not count the number of items that apply to them, instead answering with a number of items that seems reasonable.^[See @eso2012list and @schwarz1999self.]

Respondents may _perceive a lack of anonymity_.  Despite the anonymity provided by a list experiment, respondents may still worry that their response reflects their attitudes about the sensitive item.  When respondents worry about a lack of anonymity, they may increase or decrease their response to portray themselves in the best light possible, rather than answer honestly [@leary1990impression].  For example, the addition of a treatment item about race can _decrease_ the number of items that respondents report because being associated with “three of the four [list items] may be interpreted as a 75% chance that they are racist."^[See @zigerell2011you[p. 544]]

The lack of anonymity is most obvious when all or none of the list items apply to the respondent.  Researchers can reduce this possibility by using _uncorrelated or negatively correlated control items_ that are unlikely to apply to one respondent.  In the @kuklinski1997list example above, the type of person who is upset by pollution is unlikely to also be upset by a gasoline tax.  Negatively correlated items also reduce likelihood that respondent will satisfice because negatively correlated items are unlikely to be interpreted as a scale measuring one concept.  The control items should also fit with the treatment item in some way so that the treatment item does not jump out to respondents as the real item of interest to researchers. -->
<!--More attention to piloting to find control items that not only fit with the treatment item, but that are negatively correlated with other control items . Negatively correlated control items minimize the number of people who will score very high or very low on the control list, a problem that can compromise anonymity.-->

<!-- ### Variants/Modifications


_Double list experiments_ help overcome some pitfalls of single list experiments.^[See @glynn2013double and @droitcour2004item.]  In a double list experiment, the treatment item is randomly selected to appear on either the first or the second control list, so that some respondents see it on the first list and some respondents see it on the second. If researchers observe the same treatment effect on both lists, there is less risk that the effect depends on a particular control list or on how respondents interpret the list.  The double list experiment is also more statistically efficient than a single list experiment [@glynn2013double].

_Placebo-controlled list experiments_ ensure that the difference in responses to the treatment and control lists is due to the treatment item and not due to the treatment list having more items than the control list.  A placebo-controlled list experiment uses an additional item as a placebo on the control list; unlike the additional item on the treatment list, the additional item on the control list is something innocuous that would not apply to any respondent. The placebo item ensures that the difference between the two lists is due to the treatment item, not the presence of an additional item [@riambau2019placebo].

_Visual aids_ also help reduce satisficing and ensure that respondents follow the instruction to count list items instead of satisfice.  If enumerators can carry a laminated copy of the list and a dry erase marker, respondents can check off items on the list to get an exact count and erase it before handing it back to the enumerator.^[See @eso2012list and @kramon2019mis.] -->

<!--**Conclusion**: Can be effective; needs attention to design; needs attention to psychological biases that prevent lists from working.-->


# Randomized Response 


The randomized response technique is also used to measure a sensitive attitude or behavior when the researcher expects respondents to lie about it if asked a direct question.^[See @warner1965randomized, @boruch1971assuring, @cpo2015rr, and @gingerich2010understanding.] In the most common version of the randomized response technique, respondents are directly asked a yes or no question about a sensitive topic. The respondent is also given some randomization device, such as a coin or die. The respondent is told to answer the question when the randomization device takes on a certain value (e.g., tails) or to simply say "yes" when the randomization device takes a different value (e.g., heads).  Researchers assume that respondents will believe their anonymity is protected because the researcher cannot know whether a "yes" resulted from agreement with the sensitive item or the randomization device.

For example, @blair2015design studied support for militants in Nigeria with the randomized response technique.  They gave respondents a die and had the respondent practice throwing it.  They then told respondents:

<!--<p class="comment">-->

> For this question, I want you to answer yes or no.  But I want you to consider the number of your dice throw.  If 1 shows on the dice, tell me no.  If 6 shows, tell me yes.  But if another number, like 2 or 3 or 4 or 5 shows, tell me your opinion about the question that I will ask you after you throw the dice. <br><br>
[ENUMERATOR TURN AWAY FROM THE RESPONDENT]<br><br>
Now throw the dice so that I cannot see what comes out.  Please do not forget the number that comes out.<br><br>
[ENUMERATOR WAIT TO TURN AROUND UNTIL RESPONDENT SAYS YES TO]: Have you thrown the dice?  Have you picked it up?<br><br>
Now, during the height of the conflict in 2007 and 2008, did you know any militants, like a family member, a friend, or someone you talked to on a regular basis?  Please, before you answer, take note of the number you rolled on the dice.

In expectation, one-sixth of respondents answer "yes" due to the die throw.  The researcher can thus determine what percentage of respondents engaged in the sensitive behavior. <!--by subtracting 1/6 from the survey mean of the randomized response question and multiplying by 1/6.--> Here, however, respondents might not feel that their answers to randomized response questions were truly anonymous.  This is because if a respondent answered yes, the answer could have been dictated by the randomization device, but it could also signal agreement with the sensitive item.^[See @edgell1982validity and @yu2008two.]  Indeed, there are other types of randomized response techniques that address this limitation, including the _repeated randomized response technique_ and the _crosswise model_. We refer interested readers to @azfar2009identifying and @jann2011asking for the logic and implementation of these techniques.

<!-- ### Pitfalls of the randomized response technique

_Some versions are complicated_.  Even the common version described above, valued in part for its simplicity, requires respondents to use some randomization device and remember the outcome of the randomization device.  Other versions use more complicated techniques to ensure anonymity; these versions may be difficult both for the respondent and the enumerator.^[See @blair2015design and @gingerich2010understanding.]  It is possible that some respondents do not understand the instructions and some enumerators do not implement the randomized response technique properly.

Respondents may _perceive a lack of anonymity_.  As was true for list experiments, respondents may not feel that their answers to randomized response questions are truly anonymous.  If a respondent answers “yes”, the answer could have been dictated by the randomization device, but it could also signal agreement with the sensitive item.^[See @edgell1982validity and @yu2008two.] Thus, answering “yes” is not unequivocally protected by the design.  @edgell1982validity surreptitiously set the randomization device to always dictate “yes” or “no” for specific questions and observed as high as 26% of respondents say “no” even when the randomization device dictated they say “yes”.

### Variants/modifications

The _repeated randomized response technique_ helps researchers identify respondents who lie on randomized response questions [@azfar2009identifying]. The repeated technique asks a series of randomized response questions with sensitive and non-sensitive items.  The probability of the randomization device dictating that the respondent should answer "no" for all of the _sensitive_ items is very low. The technique thus allows researchers to identify and remove from analysis the respondents who are likely saying “no” even when their coin flip dictates they say “yes”.  Researchers can also determine if certain questions induce widespread lying if the "yes" rate for that question is lower than the randomization device would dictate.  The repeated randomized response technique, however, may be impractical to include on a large survey.

The _Crosswise model_ modifies the randomized response technique so that respondents have no incentive to answer "yes" or "no."^[See @yu2008two and @jann2011asking.] In the Crosswise model, respondents are presented with two statements, one sensitive statement and one non-sensitive statement for which the population mean is known. The respondent is asked to say if (a) neither or both statements are true, or (b) one statement is true. Unlike a typical randomized response question, where individuals who agree with the sensitive statement only occupy the “yes” group, people who agree with the sensitive statement could occupy either group using the Crosswise model.  Since being in category (a) and (b) are equally uninformative about the respondent's agreement with the sensitive statement, the Crosswise model removes a respondent's incentive to lie.  The Crosswise model can be used any time researchers know the population mean of a non-sensitive statement, such as "My mother was born in April."-->

<!--**Conclusion**: Crosswise model should ensure anonymity and is minimally confusing.  But it, and all RR techniques, require some randomization device, either a physical item like a coin or dice or a non-sensitive question for which the population mean is known.-->


# Implementation


To implement survey experiments, researchers need to write up multiple versions of a survey: at least one for the control condition(s) and at least one for the treatment condition(s).  Then, researchers need a randomization device that allows them to randomize the survey version shown to the respondents.  There are many platforms that facilitate the implementation of survey experiments, with Qualtrics being one of the most popular tools among survey researchers.

While the treatment is typically imposed through text, the treatment stimulus can also be of other forms, including images and videos.  The key is to map the treatment directly onto the theoretical variable of interest.  That is, if the researcher is interested in studying the effect of _X_ on _Y_, the text, image, or video (or any of their combination) should induce a change in _X_ and not in other confounding variables.^[See @dafoe2018information on information equivalence.]  Visual aids, if carefully provided, can be helpful in different settings.  For example, researchers have used photos as experimental stimuli to investigate the impact of candidate appearance on vote choice [@ahler2017appearance] and the effects of gender and racial diversity in shaping the legitimacy of international organizations [@chow2023diversity].


# Considerations


Survey experiments can be an effective tool for researchers to measure sensitive attitudes and learn about causal relationships.  Not only can they be done quickly and iteratively, but they may also be included on mass online surveys because they do not require in-person contact to implement.  This means that a researcher can plan a sequence of online survey experiments, changing the intervention and measured outcomes from one experiment to the next to learn about the mechanisms behind the treatment effect very quickly [@sniderman2018some].

But researchers need to be careful about _survey satisficing_, which occurs when respondents put in minimal effort to understand and answer a survey question.^[See @krosnick1991response and @simon2006administrative.]  In the presence of satisficing behavior, the treatment embedded in the survey experiments may not be received by respondents as intended.  As such, the measured preferences will be unreliable.  Given this concern, researchers should always design survey experiments that are easy to understand.  The length and complexity of the survey and experimental stimuli should also be kept at a minimum level, whenever possible.  A related consideration is _respondent attentiveness_, an issue that is extensively discussed by @alvarez2019attention.

Researchers also need to consider the _strength_ of their treatment.  Sometimes the experimental stimulus is unable to generate a meaningful change in the subsequently measured attitude or behavior not because the treatment is unrelated to the outcome variable of interest, but because the treatment itself is too weak.  For example, for an information provision experiment where the experimental stimulus is some factual information related to topic _Y_, the treatment may fail to change views on _Y_ not because the information plays no role in shaping individual attitudes toward _Y_, but because respondents have already been exposed to this information in the real world.^[See @haaland2023infoexp for a review of information provision experiments.]  More generally, researchers need to watch out for _preatreatment effects_ [@druckman2012learning].  If respondents, before participating in the survey experiment, have already encountered the experimental stimulus, there may be no measured difference between treatment and control groups because all respondents were "pretreated" with the stimulus, including those in the control group.

When designing survey experiments, researchers should pay attention to the question _wording_ and _ordering_.  Some terms, for example, may be unfamiliar to certain respondents or interpreted by different respondents in different ways.  As such, measurement invariance may set in, such that the same construct is measured differently for different groups of individuals.  In other cases, the question ordering itself may bias how individuals provide their responses [@gaines2007logic].  These considerations are all important to bear in mind when researchers design their survey experiments, since they fundamentally shape the inferences one can draw from the data.

<!-- For survey experiments as a measurement technique, the researcher first has to assess if the attitude of interest is explicit (consciously known to the respondent) or implicit (not consciously known to the respondent).  If the researcher believes the respondent knows her own attitude but does not want to be identified with it, the researcher should make it possible for the respondent to express that attitude without the researcher knowing that attitude.  List experiments, randomized response techniques, and endorsement experiments can help accomplish this task.  If the researcher believes the respondent does not know her own attitude, the researcher should make that attitude salient through priming and then ask a question that should be implicitly affected by the prime.

There may be cases where survey experiments are not the best tool for measuring sensitive attitudes.  As an alternative to survey experiments to measure explicit attitudes, researchers can use techniques like the Bogus Pipeline [@jones1971bogus] or phrase questions about a sensitive topic so that they are not considered socially undesirable [@kinder1981symbolicRacism].  As an alternative to survey experiments to measure implicit attitudes, researchers can use measures like the Implicit Association Test (IAT) [@greenwald1998IAT] and physiological measures like skin conductance.^[See @rankin1955galvanic and @figner2011using.] These measures are beyond conscious control of the respondent.  Many of these alternative measures are not currently flexible enough to be included on a mass survey, but technology, like heart-rate monitoring watches and other phone sensors, may soon make biometric outcomes measurable in mass surveys.

For all types of survey experiments, researchers should worry about the same issues that hamper other experiments: confounding, information equivalence, and pre-treatment contamination.  To deal with confounding and information equivalence, researchers can design the experiment to manipulate characteristics that might confound the treatment.  To account for pre-treatment, researchers can think about the everyday context of research subjects and assess whether all or a subset of respondents may already be treated before beginning the experiment.  If only a subset will be affected, the researcher can block the experiment on that subset.

Survey experiments for measurement and survey experiments for estimating causal relationships are not binary categories, and the two types of survey experiments can overlap.  Priming experiments, for example, can measure implicit attitudes and assess the effect of the prime on other outcomes of interest.  Vignette or conjoint experiments can effectively measure a sensitive attitude by priming the sensitive attitude and providing lots of other information to distract respondents from the prime. -->


# Limitations


While survey experiments offer a fruitful way to measure individual preferences, researchers are often more concerned about real-world outcomes.  When preferences are measured --- and treatments are delivered --- in a survey setting, there is no guarantee that the survey-experimental findings will translate into the real world.  Therefore, researchers should be cautious when they extrapolate from survey experiments [@barabas2019external].  For more discussions on the strengths and limitations of survey experiments, see:

- @mutz2011population "Population-Based Survey Experiments."
- @sniderman2018some "Some Advances in the Design of Survey Experiments" in the _Annual Review of Political Science_.
- @lavrakas2019experimental "Experimental Methods in Survey Research: Techniques that Combine Random Sampling with Random Assignment."
- @diaz2020survey "Survey Experiments and the Quest for Valid Interpretation" in the _Sage Handbook of Research Methods in Political Science and International Relations_.

<!-- Survey experiments induce less bias than direct questions when measuring sensitive attitudes.^[See @blair2015design, @rosenfeld2016empirical, and @lensvelt2005meta.]  They are not a panacea, however, and researchers must still ask themselves several questions when using survey experiments to measure sensitive outcomes.

The first question is whether the researcher is interested in an explicit or implicit attitude.  An explicit attitude is one the respondent is consciously aware of and can report; an implicit attitude is an automatic positive or negative evaluation of an attitude object that the respondent may not be aware of (see @nosek2007implicit for a more thorough discussion).  A list experiment, for example, may help uncover explicit racial animus, but it will not reveal implicit racial bias.  

The next question is what conditions are necessary for a survey respondent to reveal their explicit attitudes.  Survey experimental methods for sensitive explicit attitudes focus on ensuring anonymity.  But is ensuring anonymity a sufficient condition to obtain honest answers to sensitive questions?  In addition to anonymity, a further assumption must be made: respondents want to express their socially undesirable opinion in a way that evades social sanctions.  If that assumption is not true, then anonymity is worth little [@diaz2020survey].

Researchers also need to think about the numerous pitfalls of survey questions that measurement survey experiments do not solve.  Survey experiments do not help researchers avoid question ordering effects or contamination from earlier questions in the survey.  Nor do they do expose how respondents interpret the survey question or ensure information equivalence.  All survey questions assume that the respondent interprets the question in the way intended by researchers; techniques to ensure anonymity may make that interpretation less likely by obfuscating the question’s purpose [@diaz2020survey].

Lastly, researchers must also ask about measurement validity: how does one verify that a measure accurately represents a concept of interest?  For some outcomes, such as voter turnout, researchers can compare their measure with population estimates [@rosenfeld2016empirical]. But for other outcomes, such as racism or the effect that political parties have on citizens’ policy preferences, there exists no population estimate with which to validate measures.


# Limitations of Survey Experiments for Causal Identification 


Survey experiments to determine causal relationships have the same benefits and drawbacks as other experiments, as well as benefits and drawbacks that derive from the survey context.  The biggest three drawbacks generally applicable to survey experiments are confounding, information equivalence, and pre-treatment contamination [@diaz2020survey]. Researchers should think about these factors when designing and interpreting results from survey experiments.

*Confounding*: Any experimental intervention _A_ that is meant to trigger mental construct _M_ could also trigger mental construct _C_.  If _C_ is not varied in the experimental design, researchers cannot determine whether _M_, _C_, or a combination of _M_ and _C_ affect outcomes of interest.

*Information Equivalence*: Any experimental intervention _A_ can be interpreted differently by different respondents, effectively giving each respondent a different treatment [@dafoe2018information].  When these interpretations vary systematically by treatment condition, those conditions are not information equivalent and researchers cannot know that their treatment caused the observed effect.

*Pre-treatment contamination*: Respondents may encounter the treatment outside of the experiment, causing similar outcomes in the control group and treatment group even if the treatment affects outcomes [@gaines2007logic]. -->


# References