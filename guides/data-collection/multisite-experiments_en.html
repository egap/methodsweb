<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kristen Hunter">

<title>10 Things to Know About Multisite or Block-Randomized Trials – Methods</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../img/egap-logo.svg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Methods</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../guides.html"> 
<span class="menu-text">Methods guides</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../software.html"> 
<span class="menu-text">Software</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching.html"> 
<span class="menu-text">Teaching</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">10 Things to Know About Multisite or Block-Randomized Trials</li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">10 Things to Know About Multisite or Block-Randomized Trials</h1>
</div>

<div class="quarto-title-meta">
    
    
    
    
</div>
  
</header>

<nav id="TOC" role="doc-toc">
    <h4 id="toc-title">Contents</h4>
   
  <ul>
  <li><a href="#what-is-a-multisite-or-block-randomized-trial" id="toc-what-is-a-multisite-or-block-randomized-trial"><span class="header-section-number">1</span> What is a multisite or block randomized trial?</a></li>
  <li><a href="#a-multisite-trial-is-a-type-of-a-blocked-or-stratified-randomized-experiment." id="toc-a-multisite-trial-is-a-type-of-a-blocked-or-stratified-randomized-experiment."><span class="header-section-number">2</span> A multisite trial is a type of a blocked or stratified randomized experiment.</a></li>
  <li><a href="#analysis-can-either-target-the-population-in-the-experiment-or-a-broader-population." id="toc-analysis-can-either-target-the-population-in-the-experiment-or-a-broader-population."><span class="header-section-number">3</span> Analysis can either target the population in the experiment, or a broader population.</a></li>
  <li><a href="#the-average-site-effect-is-not-the-same-as-the-average-person-effect." id="toc-the-average-site-effect-is-not-the-same-as-the-average-person-effect."><span class="header-section-number">4</span> The average site effect is not the same as the average person effect.</a></li>
  <li><a href="#there-are-many-widely-used-estimators-that-target-the-same-estimands-including-design-based-linear-regression-and-multilevel-models." id="toc-there-are-many-widely-used-estimators-that-target-the-same-estimands-including-design-based-linear-regression-and-multilevel-models."><span class="header-section-number">5</span> There are many widely-used estimators that target the same estimands, including design-based, linear regression, and multilevel models.</a></li>
  <li><a href="#some-estimators-attempt-to-reduce-variance-by-increasing-bias." id="toc-some-estimators-attempt-to-reduce-variance-by-increasing-bias."><span class="header-section-number">6</span> Some estimators attempt to reduce variance by increasing bias.</a></li>
  <li><a href="#for-each-estimator-that-achieves-a-point-estimator-there-may-be-multiple-options-for-estimating-standard-errors." id="toc-for-each-estimator-that-achieves-a-point-estimator-there-may-be-multiple-options-for-estimating-standard-errors."><span class="header-section-number">7</span> For each estimator that achieves a point estimator, there may be multiple options for estimating standard errors.</a></li>
  <li><a href="#the-analysts-choices-of-estimand-estimator-and-standard-error-estimator-matter-in-some-cases-and-matter-less-in-others." id="toc-the-analysts-choices-of-estimand-estimator-and-standard-error-estimator-matter-in-some-cases-and-matter-less-in-others."><span class="header-section-number">8</span> The analyst’s choices of estimand, estimator, and standard error estimator matter in some cases, and matter less in others.</a></li>
  <li><a href="#the-choice-of-estimator-impacts-power." id="toc-the-choice-of-estimator-impacts-power."><span class="header-section-number">9</span> The choice of estimator impacts power.</a></li>
  <li><a href="#takeaway-advice-for-researchers-on-multisite-trials" id="toc-takeaway-advice-for-researchers-on-multisite-trials"><span class="header-section-number">10</span> Takeaway advice for researchers on multisite trials</a></li>
  <li><a href="#references" id="toc-references"><span class="header-section-number">11</span> References</a></li>
  </ul>
</nav>
<section id="what-is-a-multisite-or-block-randomized-trial" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> What is a multisite or block randomized trial?</h1>
<p>A multisite or block-randomized trial is a randomized experiment “in which sample members are randomly assigned to a program or a control group <em>within</em> each of a number of sites” <span class="citation" data-cites="Raudenbush2015">(<a href="#ref-Raudenbush2015" role="doc-biblioref">S. W. Raudenbush and Bloom 2015</a>)</span>.</p>
<p>This guide focuses on multisite educational trials for illustration, although multisite trials are not unique to education. Multisite trials are a subset of multilevel randomized controlled trials (RCTs), in which units are nested within hierarchical structures, such as students nested within schools nested within districts. This guide uses as an illustrative example the case where each site is a school, although they could also be districts or classrooms; thus the term “site” and “school” are used interchangeably.</p>
<p>An advantage of multisite trials is that they allow a researcher to study average impact across units or sites, while also getting a sense of heterogeneity across sites <span class="citation" data-cites="Raudenbush2015">(<a href="#ref-Raudenbush2015" role="doc-biblioref">S. W. Raudenbush and Bloom 2015</a>)</span>. However, the opportunities provided by multisite trials also come with their own challenges. Much of the rest of this guide will discuss the choices that researchers must make when analyzing multisite trials, and the consequences of these choices.</p>
<section id="preliminaries-estimands-estimators-and-estimates" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="preliminaries-estimands-estimators-and-estimates"><span class="header-section-number">1.1</span> Preliminaries: estimands, estimators, and estimates</h2>
<p>Before diving in, let’s introduce the definitions of estimand, estimator, and estimate. These concepts are sometimes conflated, but disentangling them increases clarity and understanding. The main distinction is that the <em>estimand</em> is the goal, while the <em>estimator</em> is the analysis we do in order to reach that goal.</p>
<p>An <strong>estimand</strong> is an unobserved quantity of interest about which the researcher wishes to learn. In this guide, the only type of estimand considered is the overall average treatment effect (ATE). Other options include focusing on treatment effect for only a subgroup, or calculating a different summary, such as an odds ratio. After choosing an estimand, the researcher chooses an <strong>estimator</strong>, which is a method used to calculate the final <strong>estimate</strong> which should tell the researcher something about the estimand. Finally, the researcher must also choose a standard error estimator if she wants to summarize how the estimates might vary if the research design or underlying data generating process were repeated.</p>
<p>First, to provide context, let’s consider an example. The researcher decides their <em>estimand</em> will be the average treatment effect for the pool of subjects in the experiment. In this example, the researchers observe all of the subjects for whom they want to estimate an effect. As with any causal analysis, the researchers do not observe the control outcomes of the subjects assigned to the active treatment, or the treated outcomes of the subjects assigned to the control treatment. Thus, causal inference is sometimes referenced as a missing data problem, because it is impossible to observe both potential outcomes (the potential outcome given active treatment and the potential outcome given control treatment). See <a href="https://methods.egap.org/guides/causal-inference/causal-inference_en.html">10 Things to Know About Causal Inference</a> and <a href="https://methods.egap.org/guides/research-questions/effect-types_en.html">10 Types of Treatment Effect You Should Know About</a> for a discussion of other common estimands.</p>
<p>Given an estimand, the researchers choose their <em>estimator</em> to be the coefficient from an OLS regression of the observed outcome on site-specific fixed effects and the treatment indicator. To calculate standard errors, they use Huber-White robust standard errors. All these choices result in a point <em>estimate</em> (e.g.&nbsp;the program increased reading scores by <span class="math inline">\(5\)</span> points) and a measure of uncertainty (e.g.&nbsp;a standard error of <span class="math inline">\(2\)</span> points).</p>
<p>We’ll also need some notation. This guide follows the Neyman-Rubin potential outcomes notation (<span class="citation" data-cites="Neyman1923">Splawa-Neyman, Dabrowska, and Speed (<a href="#ref-Neyman1923" role="doc-biblioref">1923/1990</a>)</span>, <span class="citation" data-cites="Imbens2015">Imbens and Rubin (<a href="#ref-Imbens2015" role="doc-biblioref">2015</a>)</span>). The observed outcomes are <span class="math inline">\(Y_{ij}\)</span> for unit <span class="math inline">\(i\)</span> in site <span class="math inline">\(j\)</span>. The potential outcomes are <span class="math inline">\(Y_{ij}(1)\)</span>, the outcome given active treatment, and <span class="math inline">\(Y_{ij}(0)\)</span>, the outcome given control treatment. The quantity <span class="math inline">\(B_{ij}\)</span> is the unit-level intention-to-treat effect (ITT) <span class="math inline">\(B_{ij} =  Y_{ij}(1) - Y_{ij}(0)\)</span>. If there is no noncompliance, the ITT is the ATE, as defined above. Then <span class="math inline">\(B_j\)</span> is the average impact at site <span class="math inline">\(j\)</span>, <span class="math inline">\(B_j = 1/N_j \sum_{i = 1}^{N_j} B_{ij}\)</span> where <span class="math inline">\(N_j\)</span> is the number of units at site <span class="math inline">\(j\)</span>. Finally, <span class="math inline">\(N = \sum_{j = 1}^{J} N_j\)</span>.</p>
<p>This guide is structured around the choices an analyst must make concerning estimand and estimators, and the resulting consequences. The choice of estimand impacts the substantive conclusion that a researcher makes. The choice of estimator and standard error estimator results in different statistical properties, including a potential trade off between bias and variance. This guide summarizes material using the framework provided by <span class="citation" data-cites="Miratrix2020">Miratrix, Weiss, and Henderson (<a href="#ref-Miratrix2020" role="doc-biblioref">2021</a>)</span>.</p>
</section>
</section>
<section id="a-multisite-trial-is-a-type-of-a-blocked-or-stratified-randomized-experiment." class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> A multisite trial is a type of a blocked or stratified randomized experiment.</h1>
<section id="a-multisite-trial-is-fundamentally-a-blocked-or-stratified-rct." class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="a-multisite-trial-is-fundamentally-a-blocked-or-stratified-rct."><span class="header-section-number">2.1</span> A multisite trial is fundamentally a blocked or stratified RCT.</h2>
<p>A multisite trial is a blocked RCT with 2 levels: randomization occurs at the student level (level 1) within blocks defined by sites/schools (level 2). For example, in a study of a new online math tool for high school students, randomization occurs at the student level within blocks defined by sites/schools. Perhaps half of students at each school are assigned to the status quo / control treatment (no additional math practice), and half are assigned to the active treatment (an offer of additional math practice at home using an online tool).</p>
<p>Because of the direct correspondence between multisite trials and blocked experiments, statistical properties of blocked experiments also translate directly to multisite experiments. The main difference between a traditional blocked RCT and a multisite experiment is that in many blocked RCTs, the researcher is able to choose the blocks. For example, in a clinical trial, a researcher may decide to block based on gender or specific age categories. Blocking can help increase statistical power overall or ensure statistical power to assess effects within subgroups (such as those defined by time of entering the study, or defined by other important covariates that might predict the outcome) (<span class="citation" data-cites="moore2012multivariate">Moore (<a href="#ref-moore2012multivariate" role="doc-biblioref">2012</a>)</span>, <span class="citation" data-cites="moore2013">Moore and Moore (<a href="#ref-moore2013" role="doc-biblioref">2013</a>)</span>, <span class="citation" data-cites="bowers2011mem">Bowers (<a href="#ref-bowers2011mem" role="doc-biblioref">2011</a>)</span>). <span class="citation" data-cites="Pashley2021">Pashley and Miratrix (<a href="#ref-Pashley2021" role="doc-biblioref">2021</a>)</span> makes the distinction between <strong>fixed blocks</strong>, where the number and covariate distribution of blocks is chosen by the researcher, and <strong>structural blocks</strong>, where natural groupings determine the number of blocks and their covariate distributions. Multisite experiments have structural blocks, such as districts, schools, or classrooms. The type of block can impact variance estimation, as shown in <span class="citation" data-cites="Pashley2021">Pashley and Miratrix (<a href="#ref-Pashley2021" role="doc-biblioref">2021</a>)</span> and <span class="citation" data-cites="Pashley2022">Pashley and Miratrix (<a href="#ref-Pashley2022" role="doc-biblioref">2022</a>)</span>.</p>
<p>The <a href="https://egap.org/our-work-0/the-metaketa-initiative/">EGAP Metaketa Projects</a> are also multisite trials: the 5 to 7 countries that contain sites for each study are fixed and chosen in advance by the different research teams.</p>
</section>
<section id="a-multisite-trial-is-not-a-cluster-randomized-trial" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="a-multisite-trial-is-not-a-cluster-randomized-trial"><span class="header-section-number">2.2</span> A multisite trial is not a cluster-randomized trial</h2>
<p>A different type of RCT is a <a href="https://methods.egap.org/guides/data-collection/cluster-randomization_en.html">cluster-randomized design</a>, in which entire schools are assigned to either the active treatment or control treatment. <a href="https://youtu.be/bL2U9z8hX1k">This video explains the difference between cluster and block-randomized designs</a>. In a multisite trial, treatment is assigned <strong>within a block to individual units</strong>. In a cluster-randomized trial, treatment is assigned to <strong>groups</strong> of units. Some designs <a href="https://declaredesign.org/r/designlibrary/reference/block_cluster_two_arm_designer.html">combine cluster- and block-randomization</a>.</p>
<p>Another design that is not a multisite or block-randomized trial is an experiment that takes place in only one school and assigns individual students to active treatment and control treatment. This type of study has only one site and thus differences between sites do not matter in this design.</p>
</section>
<section id="why-choose-a-multisite-or-block-randomized-trial-design" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="why-choose-a-multisite-or-block-randomized-trial-design"><span class="header-section-number">2.3</span> Why choose a multisite or block-randomized trial design?</h2>
<p>In most contexts, blocking reduces estimation error over an unblocked (completely randomized) experiment (<span class="citation" data-cites="moore2012multivariate">Moore (<a href="#ref-moore2012multivariate" role="doc-biblioref">2012</a>)</span>, <span class="citation" data-cites="gerber2012field">Gerber and Green (<a href="#ref-gerber2012field" role="doc-biblioref">2012</a>)</span>). Thus, blocked experiments generally offer higher statistical power than unblocked experiments. Blocking is most helpful in increasing precision and statistical power in the setting where there is variation in the outcome, and where the blocks are related to this variation.</p>
<p>In multisite trials as compared to block-randomized trials, the researcher typically cannot purposely construct blocks to reduce variation, because they are defined by pre-existing sites. However, the researcher can hope, and often expect, that sites naturally explain some between-site variation. For example, if some schools tend to have higher outcomes than others, then blocked randomization using the school as a block improves efficiency over complete randomization.</p>
<p>Randomizing with purposefully created blocks or pre-existing sites also helps analysts learn about how treatment effects may vary across the sites or groups of people categorized into the blocks. If a new treatment should help the lowest performing students most, but in any given study most students are not the lowest performing, then researchers may prefer to create blocks of students within schools with the students divided by their previous performance. This blocking within site would allow comparisons of the treatment effects on the relatively rare lowest performing students with the treatment effects on the relatively rare highest performing students.</p>
</section>
<section id="why-not-block" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="why-not-block"><span class="header-section-number">2.4</span> Why not block?</h2>
<p>Often, in a multisite trial with treatment administered by site administrators (like principals of schools), an analyst has no choice but to randomize within site. In other studies, the construction and choice of blocking criteria is a choice. <span class="citation" data-cites="Pashley2022">Pashley and Miratrix (<a href="#ref-Pashley2022" role="doc-biblioref">2022</a>)</span> shows that blocking is generally beneficial, but also explores settings in which it may be harmful. Blocking does result in fewer degrees of freedom, but in practice this reduction is rarely an issue, unless an experiment is very small <span class="citation" data-cites="Imai2008">(<a href="#ref-Imai2008" role="doc-biblioref">Imai, King, and Stuart 2008</a>)</span>. Any use of blocking requires that an analyst keep track of the blocks and also that an analyst reflect the blocks in subsequent analysis: in many circumstances estimating average treatment effects from a block-randomized experiment while ignoring the blocks will yield biased estimates of the underlying targeted estimands (see <a href="https://declaredesign.org/blog/biased-fixed-effects.html">“The trouble with ‘controlling for blocks’”</a> and <a href="https://egap.org/resource/sd-block-rand">“Estimating Average Treatment Effects in Block Randomized Experiments”</a> for demonstrations of bias arising from different approaches to weighting by blocks).</p>
</section>
</section>
<section id="analysis-can-either-target-the-population-in-the-experiment-or-a-broader-population." class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Analysis can either target the population in the experiment, or a broader population.</h1>
<p>The first choice a researcher must make in defining their estimand is the population of interest. The researcher may want to focus on the <strong>finite population</strong>: only those units in the experimental pool or sample. Alternatively, they can expand their estimand to consider the <strong>super population</strong>. A super population framework considers the units in the experiment to be a sample from a broader, unobserved population, and targets the impact in this larger population.</p>
<p>A researcher might be interested in a finite population framework if most or all of the population is included in the study. For example, a state-level policymaker considering results from a statewide trial may only be interested in the impact on schools in their own state. Similarly, if an organization is evaluating itself and includes all of its own sites, they would use a finite population framework. An additional common case of a finite population framework is for proof-of-concept or pilot studies. A researcher may be running a small study to test whether an intervention is worth exploring in a larger trial. They may have even specifically selected a set of units assumed to be a worst case scenario to see whether there is still a measurable impact in such a group. Finally, many field experiments use a finite population framework out of necessity. The units and sites available for study may not arrive via any known or replicable sampling process, sometimes called a “convenience sample.”</p>
<p>A super population framework is of interest when a researcher plans to report estimates of the effect on units not included in the given study. For many trials, the end goal is not to study the units at hand, but rather to provide predictions of the likely impacts if the intervention were expanded. For example, a state-level policymaker with access to a trial performed on only a subset of schools in their state might prefer a super population framework. However, one challenge of the super population framework is that it assumes that sites are randomly sampled from the broader population of interest. As noted above, sites are often selected based on availability rather than a random sampling approach. Thus, when taking a super population framework when sites are not randomly sampled, the population we are making inference about becomes fuzzy. We may not be able to generalize to the whole population of interest, but instead can only generalize to a broader population of units that could have feasibly included in the study.</p>
<p>One of the main consequences of the choice of population framework is the amount of uncertainty in the final estimates. This topic will be discussed in more detail later in the guide. When accounting for sites randomly sampled via a known sampling process from a super population, we naturally have an additional source of uncertainty deriving from which units were selected for the study at hand: randomization to treatment is one source of randomness, and sampling from the population is another source of randomness. Although the point estimates from either perspective will often be the same, the breadth of intervals will generally be larger for super population studies.</p>
<p>For more discussion of the consequence of the super population and finite population frameworks, see <span class="citation" data-cites="Schochet2016">Schochet (<a href="#ref-Schochet2016" role="doc-biblioref">2016</a>)</span> and <span class="citation" data-cites="Pashley2021">Pashley and Miratrix (<a href="#ref-Pashley2021" role="doc-biblioref">2021</a>)</span>.</p>
</section>
<section id="the-average-site-effect-is-not-the-same-as-the-average-person-effect." class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> The average site effect is not the same as the average person effect.</h1>
<p>The second choice a researcher makes is the target of inference: is the researcher interested in the <strong>average student</strong>, or the <strong>average site</strong> <span class="citation" data-cites="Miratrix2020">(<a href="#ref-Miratrix2020" role="doc-biblioref">Miratrix, Weiss, and Henderson 2021</a>)</span>?</p>
<p>When we consider the average student impact, we weigh each student equally. Thus, larger sites have a larger impact on the outcome. For example, if one very large site is an outlier, the impact at that site will heavily drive the final results. Taking this approach makes sense from a utilitarian perspective, i.e., if the benefit of the intervention is equal to the total sum of benefits across all people. Average student impact might be of interest to a high-level policymaker, such as a state official. The average student impact is <span class="math inline">\(\frac{1}{N} \sum_{j = 1}^{J} \sum_{i = 1}^{N_j} B_{ij} = \sum_{j = 1}^{J} \frac{N_j}{N} B_j.\)</span></p>
<p>When we consider the average site impact, we weigh each site equally. Thus, larger sites will be equally weighted to smaller sites. A site-level decision maker, such as a school principal, might be more interested in the average site impact, so that site size does not influence the final answer. The average site impact is <span class="math inline">\(\frac{1}{J} \sum_{j = 1}^{J} B_j.\)</span></p>
<p>Note that in the case where all sites are of the same size, or all sites have the same impact, then these two estimands are the same.</p>
<p>To summarize, this section and the prior section have given two axes of choices: the population of interest (FP or SP for finite and super population), and the target of inference (persons or sites). These choices result in four possible estimands: FP-persons, SP-persons, FP-sites, and SP-sites.</p>
</section>
<section id="there-are-many-widely-used-estimators-that-target-the-same-estimands-including-design-based-linear-regression-and-multilevel-models." class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> There are many widely-used estimators that target the same estimands, including design-based, linear regression, and multilevel models.</h1>
<p>After choosing an <em>estimand</em>, the researcher must then choose an <em>estimator</em>, a process to arrive at the estimate of interest. There are three main categories of estimators: <strong>design based</strong>, <strong>linear regression</strong>, and <strong>multilevel modeling</strong>. Linear regression and multilevel modeling are both model-based approaches to statistical inference.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> In model-based approaches, the researcher estimates the parameter in a likelihood function that is chosen to represent the natural stochastic process that generates the outcomes in the study. See <span class="citation" data-cites="rubin1990b">Rubin (<a href="#ref-rubin1990b" role="doc-biblioref">1990</a>)</span> for more discussion of the differences between design- and model-based approaches to statistical inference.</p>
<p>The different categories of estimator differ both philosophically and practically. Each category assumes a different source of randomness, and thus has a different statistical justification.</p>
<p><strong>Design-based</strong> estimators specifically target the four estimands outlined above. The main source of uncertainty is assumed to be the treatment assignment: which units happened to be assigned to the active treatment, and which happened to be assigned to the control treatment. This assumption is the reason for their name; the uncertainty in the estimates is by design, from the purposeful randomization of units. Using design-based estimators is also sometimes called Neymanian inference, as the estimators and properties were first introduced by <span class="citation" data-cites="Neyman1923">Splawa-Neyman, Dabrowska, and Speed (<a href="#ref-Neyman1923" role="doc-biblioref">1923/1990</a>)</span>. Design-based estimators can also incorporate uncertainty from sampling when using a super population framework.</p>
<p><strong>Linear regression</strong> estimators are the most familiar to many researchers. With these estimators, the observed outcomes are assumed to be a linear function of the treatment assignment, (optionally) site-specific effects, (optionally) covariates, and random error. In standard regression theory, the only source of randomness is the error term. The covariates, which in the case of RCTs includes the treatment indicator, are considered fixed. This assumption is in direct contrast to the design-based framework, in which the treatment assignment is considered random. In econometric theory, the randomness in the error term in regression models is sometimes viewed as deriving from sampling from a larger population.</p>
<p><strong>Multilevel model</strong> estimators are a generalization of linear regression. When assuming <em>fixed effects</em>, as in a standard regression model, each site’s parameter is considered to be fixed and independent. When assuming <em>random effects</em>, as in a multilevel model, each site’s parameter is assumed to be drawn from a shared distribution of site impacts. Most standard statistical software assumes a Normal distribution to model the site-specific impacts. Multilevel models can incorporate both random site-level intercepts, and random site-level coefficients (in our cases, these are site-specific treatment impacts). Now, uncertainty stems both from the individual-level random error term, and from the additional uncertainty of site-level parameters being considered random. In general, multilevel models naturally lend themselves to a super population framework, because they already incorporate the assumption that sites are being randomly drawn from a broader, unobserved population. Multilevel models are also called mixed effects models or mixed models, where a mixed model has a combination of fixed and random effects. For a more comprehensive look at multilevel models, see <span class="citation" data-cites="Raudenbush2015">S. W. Raudenbush and Bloom (<a href="#ref-Raudenbush2015" role="doc-biblioref">2015</a>)</span>.</p>
<p>Let’s examine a few popular models among linear regression and multilevel models in more detail. Note that these models as presented do not include covariates, but covariates can easily be incorporated to increase power if the analyst is willing to increase bias by a small amount in exchange (often a very small amount if the experiment is large enough) (see <span class="citation" data-cites="lin2013agnostic">Lin (<a href="#ref-lin2013agnostic" role="doc-biblioref">2013</a>)</span>).</p>
<section id="common-linear-regression-models" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="common-linear-regression-models"><span class="header-section-number">5.1</span> Common linear regression models</h2>
<p><strong>Fixed effects with a constant treatment (FE)</strong></p>
<p>With this model, the researcher assumes that there are site-specific fixed effects (intercepts), but a common overall ATE. The assumed model is <span class="math inline">\(Y_{ij} = \sum_{k = 1}^{J} \alpha_k \text{Site}_{k,ij} + \beta T_{ij} + e_{ij}\)</span>, where <span class="math inline">\(\text{Site}_{k,ij}\)</span> is an indicator for unit <span class="math inline">\(ij\)</span> being in site <span class="math inline">\(k\)</span> (out of <span class="math inline">\(J\)</span> sites), <span class="math inline">\(T_{ij}\)</span> is a treatment indicator, and <span class="math inline">\(e_{ij}\)</span> is an <span class="math inline">\(iid\)</span> error term. For more discussion, see <span class="citation" data-cites="Raudenbush2015">S. W. Raudenbush and Bloom (<a href="#ref-Raudenbush2015" role="doc-biblioref">2015</a>)</span>.</p>
<p><strong>Fixed effects with interactions (FE-inter)</strong></p>
<p>With this model, the researcher assumes site-specific heterogeneous treatment effects, so in addition to fitting a separate fixed effect for the <em>intercepts</em> for each site, a separate treatment impact <em>coefficient</em> is found for each site. <span class="math display">\[Y_{ij} = \sum_{k = 1}^{J} \alpha_k \text{Site}_{k,ij} +
\sum_{k = 1}^{J} \beta_k \text{Site}_{k,ij} T_{ij} + e_{ij}\]</span></p>
<p>Given a series of site-specific treatment estimates <span class="math inline">\(\hat{\beta}_j\)</span>, these estimates are then averaged, with weights by either simple weighting (see <span class="citation" data-cites="Clark2011">Clark and Silverberg (<a href="#ref-Clark2011" role="doc-biblioref">2011</a>)</span>) or by site size.</p>
</section>
<section id="common-multilevel-models" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="common-multilevel-models"><span class="header-section-number">5.2</span> Common multilevel models</h2>
<p>Once an analyst selects multilevel modeling, for site intercepts and site impacts they must decide: what is considered random, and what is considered fixed?</p>
<p><strong>Fixed intercept, random treatment coefficient (FIRC)</strong></p>
<p>This model is similar to the fixed effects models above, but assumes that the site impact <span class="math inline">\(\beta_j\)</span> is drawn from a shared distribution. The FIRC model was more recently designed to handle bias issues that arise when the proportion of units treated varies across sites.</p>
<p><span class="math display">\[\begin{align*}
\text{Level 1}\qquad &amp; Y_{ij} = \sum_{k = 1}^{J} \alpha_k
\text{Site}_{k,ij} + \beta_j T_{ij} + e_{ij}\\
\text{Level 2}\qquad &amp; \beta_j = \beta + b_j
\end{align*}\]</span> See <span class="citation" data-cites="Raudenbush2015">S. W. Raudenbush and Bloom (<a href="#ref-Raudenbush2015" role="doc-biblioref">2015</a>)</span> and <span class="citation" data-cites="Bloom2017">Bloom and Porter (<a href="#ref-Bloom2017" role="doc-biblioref">2017</a>)</span>.</p>
<p><strong>Random intercept, random treatment coefficient (RIRC)</strong></p>
<p>This model is an older version of multilevel models, and assumes that both the site intercept and site impact are drawn from shared distributions. <span class="math display">\[\begin{align*}
\text{Level 1}\qquad &amp; Y_{ij} = A_j + \beta_j T_{ij} + e_{ij}\\
\text{Level 2}\qquad &amp; \beta_j = \beta + b_j\\
&amp; A_j = \alpha + a_j
\end{align*}\]</span></p>
<p><strong>Random intercept, constant treatment coefficient (RICC)</strong></p>
<p>Finally, this model assumes that the site intercepts are drawn from a shared distribution, but the treatment impact is shared. <span class="math display">\[\begin{align*}
\text{Level 1}\qquad &amp; Y_{ij} = A_j + \beta T_{ij} + e_{ij}\\
\text{Level 2}\qquad &amp; A_j = \alpha + a_j\\
\end{align*}\]</span> As noted previously, the multilevel framework generally naturally corresponds to the super population perspective. However, for RICC models, the site <em>impacts</em> are not assumed to be drawn from a super population; only the site <em>intercepts</em> are assumed to be random. Thus, when it comes to estimating treatment impacts, RICC models actually take a finite population perspective.</p>
<p>There are also weighted versions of both traditional regressions and multilevel models. For example, a fixed-effects model can weigh each person by their inverse chance of treatment to help increase precision. Weighted regression for traditional regression is discussed in <span class="citation" data-cites="Miratrix2020">Miratrix, Weiss, and Henderson (<a href="#ref-Miratrix2020" role="doc-biblioref">2021</a>)</span>, and weighted regression for multilevel models is discussed in <span class="citation" data-cites="Raudenbush2020">Raudenbush S. W. and Schwartz (<a href="#ref-Raudenbush2020" role="doc-biblioref">2020</a>)</span>.</p>
</section>
</section>
<section id="some-estimators-attempt-to-reduce-variance-by-increasing-bias." class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Some estimators attempt to reduce variance by increasing bias.</h1>
<p>Each category of estimator (design, regression, and multilevel) results in a different estimation approach. One way to characterize the categories is the weights induced by the choice of estimator. The properties of each estimator also result in different consequences for bias and variance. Design-based estimators are generally unbiased, but may not always afford the most precise estimates. In general, model-based estimators trade bias for variance. Thus, they can sometimes have a lower mean squared error than design-based estimators. One way that model-based estimators increase precision is through the easy incorporation of covariates. Covariate adjustment methods that incorporate covariates result in the equivalent to a weighted regression approach.</p>
<section id="design-based-estimators" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="design-based-estimators"><span class="header-section-number">6.1</span> Design-based estimators</h2>
<p>Design-based estimators are the most straightforward, as they are composed of simple weighted combinations of means. First, the site-specific treatment impact estimates <span class="math inline">\(\hat{B_j}\)</span> are calculated by taking differences in means between the active treatment and control treatment groups for each site. Then, the overall estimate is a weighted combination of these estimates, weighted by either person or site weighting.</p>
<p>The design-based estimators are <span class="math display">\[\begin{align*}
\hat{\beta}_{DB-persons} &amp;= \sum_{j = 1}^{J} \frac{N_j}{N} \hat{B_j} \\
\hat{\beta}_{DB-sites} &amp;= \sum_{j = 1}^{J} \frac{1}{J} \hat{B_j}.
\end{align*}\]</span> Design-based estimators are generally <em>unbiased</em> for their corresponding estimands (person-weighted or site-weighted). Unbiasedness does not hold for one super population model; see <span class="citation" data-cites="Pashley2022">Pashley and Miratrix (<a href="#ref-Pashley2022" role="doc-biblioref">2022</a>)</span> for more details.</p>
</section>
<section id="linear-regression-estimators" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="linear-regression-estimators"><span class="header-section-number">6.2</span> Linear regression estimators</h2>
<p>Consider the FE model (fixed effects with a constant treatment). This regression model results in a <em>precision-weighted</em> estimate, in which each site impact is weighted by the estimated precision of estimating that site’s impact. The estimator is <span class="math inline">\(\hat{\beta}_{FE} = \sum_{j = 1}^{J} \frac{N_j p_j (1 - p_j)}{Z} \hat{B_j}\)</span>, where <span class="math inline">\(p_j\)</span> is the proportion treated at site <span class="math inline">\(j\)</span>. The quantity <span class="math inline">\(Z\)</span> is a normalizing constant, so <span class="math inline">\(Z\)</span> is defined as <span class="math inline">\(\sum_{j = 1}^{J} N_j p_j (1-p_j)\)</span> to ensure the weights sum to one. The weights are <span class="math inline">\(N_j p_j (1 - p_j)\)</span>, which is the inverse of <span class="math inline">\(Var(\hat{\beta_j})\)</span>, so the weights are related to the precision of the estimate for each site. This expression shows that sites with larger <span class="math inline">\(N_j\)</span>, or that have <span class="math inline">\(p_j\)</span> closer to <span class="math inline">\(0.5\)</span>, have larger weights.</p>
<p>The FE estimator is not generally unbiased for either person-weighted or site-weighted estimands. If the impact size <span class="math inline">\(B_j\)</span> is related to the weights (<span class="math inline">\(N_j p_j (1 - p_j)\)</span>), then the estimator could be biased. For example, if sites that treat a higher proportion of treated units also experience a larger treatment impact, then <span class="math inline">\(B_j\)</span> can be related to <span class="math inline">\(p_j (1- p_j)\)</span>. This setting is plausible for example if sites with more resources to intervene on more students also implement the intervention more effectively. If larger sites are more effective, then <span class="math inline">\(B_j\)</span> can be related to <span class="math inline">\(N_j p_j (1- p_j)\)</span>.</p>
<p>Instead, the FE estimator is unbiased for an estimand that weights the site impacts by <span class="math inline">\(N_j p_j (1- p_j)\)</span>. However, this estimand does not have a natural substantive interpretation. Although the FE estimator is generally biased for the estimands of interest, it may have increased precision and thus a lower mean squared error.</p>
<p>In contrast, the FE-inter model ends up with weights identical to the design-based estimators, depending on if the estimated site impacts are weighted equally or by size.</p>
</section>
<section id="multilevel-model-estimators" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="multilevel-model-estimators"><span class="header-section-number">6.3</span> Multilevel model estimators</h2>
<p>Multilevel models also result in precision weighting, but in these models the estimated precision also takes into account the assumed underlying variance in site impacts. For example, the FIRC model can be expressed roughly as: <span class="math display">\[\hat{\beta}_{ML-FIRC*} = \sum_{j = 1}^{J} \frac{1}{Z}
\left(\frac{\sigma^2}{N_j p_j ( 1 - p_j)} + \tau^2\right)^{-1} \hat{B_j}\]</span>, where <span class="math inline">\(Z\)</span> is again a normalizing constant, <span class="math inline">\(Z = \sum_{j = 1}^{J} \left(\frac{\sigma^2}{N_j p_j ( 1 - p_j)} + \tau^2\right)^{-1}\)</span>. This equation assumes that the <span class="math inline">\(b_j\)</span> have known variance <span class="math inline">\(\tau^2\)</span>, and the <span class="math inline">\(e_{ij}\)</span> have known variance <span class="math inline">\(\sigma^2\)</span>. In general, we do not know these quantities, and instead must estimate them. However, we can see that the implied precision weights incorporate the additional uncertainty assumed in the value of <span class="math inline">\(b_j\)</span>.</p>
<p>The RIRC model imposes the same structure on the site impacts, and thus the weights are similar to the FIRC model. The RICC model assumes a constant treatment impact, and thus is essentially equivalent to the precision-weighted fixed effects with constant treatment model (FE) when it comes to estimating the site impacts.</p>
<p>We summarize the weights in the table below. The following table includes additional estimators that are not discussed in this guide; for more information about these additional estimators, see <span class="citation" data-cites="Miratrix2020">Miratrix, Weiss, and Henderson (<a href="#ref-Miratrix2020" role="doc-biblioref">2021</a>)</span>.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Weight name</th>
<th>Weight</th>
<th>Estimators</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Unbiased person-weighting</td>
<td><span class="math inline">\(w_j \propto N_j\)</span></td>
<td><span class="math inline">\(\hat{\beta}_{DB-FP-person}\)</span>, <span class="math inline">\(\hat{\beta}_{DB-SP-person}\)</span>, <span class="math inline">\(\hat{\beta}_{FE-weight-person}\)</span>, <span class="math inline">\(\hat{\beta}_{FE-inter-person}\)</span></td>
</tr>
<tr class="even">
<td>Fixed-effect precision-weighting</td>
<td><span class="math inline">\(w_j \propto N_j p_j (1 - p_j)\)</span></td>
<td><span class="math inline">\(\hat{\beta}_{FE}\)</span>, <span class="math inline">\(\hat{\beta}_{FE-HW}\)</span>, <span class="math inline">\(\hat{\beta}_{FE-CR}\)</span>, <span class="math inline">\(\hat{\beta}_{ML-RICC}\)</span> (approximately)</td>
</tr>
<tr class="odd">
<td>Random-effect precision-weighting</td>
<td><span class="math inline">\(w_j \propto \left[\hat{\tau} + N_j p_j (1 - p_j)\right]^{-1}\)</span> &nbsp;(approximately)</td>
<td><span class="math inline">\(\hat{\beta}_{ML-FIRC}\)</span>, <span class="math inline">\(\hat{\beta}_{ML-RIRC}\)</span></td>
</tr>
<tr class="even">
<td>Unbiased site-weighting</td>
<td><span class="math inline">\(w_j \propto 1\)</span></td>
<td><span class="math inline">\(\hat{\beta}_{DB-FP-site}\)</span>, <span class="math inline">\(\hat{\beta}_{DB-SP-site}\)</span>, <span class="math inline">\(\hat{\beta}_{FE-weight-site}\)</span>, <span class="math inline">\(\hat{\beta}_{FE-inter-site}\)</span></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="for-each-estimator-that-achieves-a-point-estimator-there-may-be-multiple-options-for-estimating-standard-errors." class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> For each estimator that achieves a point estimator, there may be multiple options for estimating standard errors.</h1>
<p>The difference between the finite population and super population framework comes into focus when calculating the standard error of various estimators. In general, the super population framework results in larger estimates of error because of the additional uncertainty induced by assuming the sites observed are randomly drawn from a larger population. In general, variation in the control outcome can be broken down into <em>within site</em> variation and <em>between site</em> variation. In the finite population framework, estimators calculate variation <em>within</em> sites, and then estimators average this variation across sites. In the super population framework, estimators look at the variation <em>between</em> sites to “capture both any within-site estimation error along with the uncertainty associated with sampling sites from a larger population” <span class="citation" data-cites="Miratrix2020">(<a href="#ref-Miratrix2020" role="doc-biblioref">Miratrix, Weiss, and Henderson 2021</a>)</span>. For both approaches, modeling assumptions can stabilize uncertainty estimation procedures, but also risk inducing bias if the modeling assumptions are wrong.</p>
<p>For design-based estimators, for the finite population framework Neyman developed a conservative estimator for the standard error using the observed outcomes. First, within-site uncertainty is estimated for each site, and then these estimates are averaged with weights according to the target estimand. The super population framework induces more complicated expressions that take into account the additional population variance. The details of standard errors for super population design-based estimators are beyond the scope of this guide.</p>
<p>For linear regression estimators, the traditional way to calculate standard errors is using classical regression theory. We term this a model-based standard error approach, as they rely on the assumed model of <span class="math inline">\(iid\)</span> standard errors. Alternatively, heteroscedastically robust standard errors (Huber-White) or cluster robust standard errors relax this <span class="math inline">\(iid\)</span> assumption (see <span class="citation" data-cites="Weiss2019">Weiss and Gupta (<a href="#ref-Weiss2019" role="doc-biblioref">2017</a>)</span> and <span class="citation" data-cites="Richburg-Hayes2008">Richburg-Hayes and Bloom (<a href="#ref-Richburg-Hayes2008" role="doc-biblioref">2008</a>)</span>). Robust standard errors fall into a design-based approach instead of a model-based approach (see <span class="citation" data-cites="lin2013agnostic">Lin (<a href="#ref-lin2013agnostic" role="doc-biblioref">2013</a>)</span>, Chapter 3 of <span class="citation" data-cites="gerber2012field">Gerber and Green (<a href="#ref-gerber2012field" role="doc-biblioref">2012</a>)</span>). Huber-White standard errors correspond to the finite population framework, while the asymptotic theory justifying traditional cluster robust standard errors corresponds to the super population framework in regards the clusters. In a <a href="https://methods.egap.org/guides/data-collection/cluster-randomization_en.html">cluster-randomized trial</a>, treatment is assigned to clusters, so there is also a finite-population-of-clusters perspective on cluster robust standard errors that is approximated in what are commonly known as CR2 standard errors (see <span class="citation" data-cites="Pustejovsky2018">Pustejovsky and Tipton (<a href="#ref-Pustejovsky2018" role="doc-biblioref">2018</a>)</span>).</p>
<p>To briefly summarize the correspondence between standard error estimators and the assumed population, first consider the motivation behind robust standard error estimators. In the FE model, treatment effects are assumed to be constant across sites. Thus, if there is truly treatment effect heterogeneity, units in different sites will have different amounts of variation, and this variation will be incorporated into the error term. The assumption of <span class="math inline">\(iid\)</span> standard errors will be broken. Huber-White standard errors allow for heteroscedasticity in the residuals while still assuming that sites are fixed, which fits into a finite population framework: in fact <span class="citation" data-cites="lin2013agnostic">Lin (<a href="#ref-lin2013agnostic" role="doc-biblioref">2013</a>)</span> shows that the standard error derived by <span class="citation" data-cites="Neyman1923">Splawa-Neyman, Dabrowska, and Speed (<a href="#ref-Neyman1923" role="doc-biblioref">1923/1990</a>)</span> on finite-population and design-based principles is the same as the HC2 standard error.</p>
<p>In contrast, for cluster robust standard errors, “the conventional adjustments, often implicitly, assume that the clusters in the sample are only a small fraction of the clusters in the population of interest” <span class="citation" data-cites="Abadie2017">(<a href="#ref-Abadie2017" role="doc-biblioref">Abadie et al. 2017</a>)</span>. Using cluster robust standard errors accounts for both correlation of individuals within sites, and different amounts of variation across sites. This strategy generally results in larger standard errors. For more discussion of cluster robust standard errors, see <span class="citation" data-cites="Abadie2017">Abadie et al. (<a href="#ref-Abadie2017" role="doc-biblioref">2017</a>)</span> and <span class="citation" data-cites="Pustejovsky2018">Pustejovsky and Tipton (<a href="#ref-Pustejovsky2018" role="doc-biblioref">2018</a>)</span>.</p>
<p>Finally, the details for standard error estimation for multilevel modeling are outside the scope of this guide. Generally, maximum likelihood theory is applied, which “requires a complete model for both the random effects and the residual variances” (see <span class="citation" data-cites="Miratrix2020">Miratrix, Weiss, and Henderson (<a href="#ref-Miratrix2020" role="doc-biblioref">2021</a>)</span>). FIRC and RIRC models naturally produce standard errors under the super population framework, while RICC essentially takes a finite population framework because the treatment impacts are not assumed to be drawn from a super population, as they are assumed to be consistent across sites.</p>
</section>
<section id="the-analysts-choices-of-estimand-estimator-and-standard-error-estimator-matter-in-some-cases-and-matter-less-in-others." class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> The analyst’s choices of estimand, estimator, and standard error estimator matter in some cases, and matter less in others.</h1>
<p>After discussing the different choices a researcher can make in analyzing a multisite trial, a big question remains: how do these choices impact empirical results? Which of these choices have a substantial impact on the conclusion we reach, and which do not matter as much? <span class="citation" data-cites="Miratrix2020">Miratrix, Weiss, and Henderson (<a href="#ref-Miratrix2020" role="doc-biblioref">2021</a>)</span> conducted an empirical study to investigate these questions using 12 large multisite trials, backed up by simulation studies in certain cases.</p>
<section id="point-estimates" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="point-estimates"><span class="header-section-number">8.1</span> Point estimates</h2>
<p>First, they consider the impact of choices on point estimates. The authors ask, “to what extent can the choice of estimator of the overall average treatment effect result in a different impact estimate?” In general, the authors find that the choice of estimator can substantially impact the point estimates, although the degree of impact depends on the choice. The authors reach the following conclusions.</p>
<p><strong>Person-weighted estimands can result in a different conclusion than site-weighted estimands.</strong></p>
<p>In some trials, estimates resulting from person-weighted estimands differed substantially from estimates resulting from site-weighted estimands. These discrepancies could be due to a difference in the true underlying values of the estimands, but they could also be due to estimation error from the estimation procedure. Through empirical exploration, they found that the difference is likely due to the estimands themselves being different. They found that “the range of estimates across all estimators is rarely meaningfully larger than the range between the person- and site-weighted estimates alone.”</p>
<p><strong>For person-weighted estimands, the choice of estimator generally does not matter.</strong></p>
<p>The unbiased design-based estimator and the precision-weighted fixed effect estimate both target the person-weighted estimand. There was little difference in estimates between these estimators. Most likely, “this implies that the potential bias in the bias-precision trade off to the fixed effect estimators is negligible in practice.” Other <a href="https://egap.org/resource/sd-block-rand/">authors</a> have been able to create situations in which the bias-precision trade off is more severe.</p>
<p><strong>For site-weighted estimands, the choice of estimator can matter.</strong></p>
<p>FIRC estimates did differ from the unbiased design-based site estimator. FIRC can be seen as an adaptive estimator: when there is little estimated variation in impacts between sites, it tends to be more similar to the person-weighted estimate instead of the site-weighted estimate.</p>
<p><strong>Different estimators have different bias-variance trade offs.</strong></p>
<p>Finally, the authors consider the empirical bias-variance trade off of different estimators, and find:</p>
<ul>
<li>FE estimators have little bias, but also do not improve precision much over design-based estimators.</li>
<li>FIRC tends to have lower mean squared error than design-based estimators.</li>
<li>Larger site impact heterogeneity results in more biased estimates for FIRC.</li>
<li>Even with more site impact heterogeneity, the mean squared error for FIRC estimators is still generally lower.</li>
<li>Coverage for design-based estimators is more reliable, especially when site size is variable and site size is correlated with impact.</li>
</ul>
</section>
<section id="standard-errors" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="standard-errors"><span class="header-section-number">8.2</span> Standard errors</h2>
<p>The second question concerns the choice of standard error estimators. The authors ask, “to what extent can the choice of estimator of the standard error of the overall average treatment effect result in a different estimated standard error?”</p>
<p>The choice of standard error estimator can substantially impact the estimated standard error. The authors reach the following conclusions.</p>
<p><strong>The choice of estimand impacts the standard error.</strong></p>
<p>Super population estimators generally have larger standard errors than finite population estimators. Site-weighted estimators generally have larger standard errors than person-weighted estimators.</p>
<p><strong>Given a particular estimand, the choice of estimator matters in some contexts and not others.</strong></p>
<p>For finite population estimands (including both person and site-weighted estimands) or super population person-weighted estimands, the choice of standard error estimator generally does not matter. In practice, <span class="citation" data-cites="Miratrix2020">Miratrix, Weiss, and Henderson (<a href="#ref-Miratrix2020" role="doc-biblioref">2021</a>)</span> found that estimators that attempt to improve precision by trading bias may not actually result in gains in precision in practice. The use of robust standard errors also does not differ much from non-robust standard errors in practice.</p>
<p>For super population site-weighted estimands, the choice of standard error estimator can matter a lot. In most cases, standard error estimates differed substantially between the design-based super population estimator and FIRC. The authors further conclude that for super population site-weighted estimands, the wide-ranging standard error estimates stem from instability in estimation. Through a simulation study, they find that super population standard errors can underestimate the true error. The design-based super population standard error estimator is particularly prone to underestimate the standard error compared to multilevel models, and can be unstable, in that it estimates a wide range of different values across simulations.</p>
</section>
</section>
<section id="the-choice-of-estimator-impacts-power." class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> The choice of estimator impacts power.</h1>
<p>Given the discussion thus far, it is not surprising that modeling choices made by the analyst also impacts statistical power.</p>
<p>To further understand power, we define an important quantity in power calculations: the intraclass correlation coefficient (ICC). Broadly, variation in the observed control outcomes can be categorized into <em>within</em>-site variation, and <em>between</em>-site variation. In educational trials, the ICC is the proportion of variation in the outcome that lies <em>between</em> sites <span class="citation" data-cites="Schochet2016">(<a href="#ref-Schochet2016" role="doc-biblioref">Schochet 2016</a>)</span>. The ICC is defined as the ratio of the variance at the site level divided by the overall variance of the individual outcomes. This quantity plays a different role in block-randomized trial power analysis depending on the target of inference chosen by the analyst. ICC is also used in the design and analysis of cluster-randomized trials.</p>
<p>We consider two different estimators and how they impact power. First, consider a version of the finite population FE model that has been expanded to include level 1 (student) covariates. The standard error for the ATE estimator is <span class="math inline">\(SE = \sqrt{\frac{(1-\text{ICC})(1-R^2_{1})}{\bar{T}(1 - \bar{T}) J \bar{n}}}\)</span>, where <span class="math inline">\(ICC\)</span> is the intraclass correlation, <span class="math inline">\(R_1^2\)</span> is the proportion of variation explained by level 1 (student) covariates, <span class="math inline">\(\bar{T}\)</span> is the average number of treated units per site, <span class="math inline">\(J\)</span> is the number of sites, and <span class="math inline">\(\bar{n}\)</span> is the average number of students per site. For more information about this standard error expression, see the technical appendix of <span class="citation" data-cites="Hunter2022">Hunter, Miratrix, and Porter (<a href="#ref-Hunter2022" role="doc-biblioref">2022</a>)</span>.</p>
<p>In contrast, consider the super population RIRC model. The standard error for the ATE estimator is <span class="math inline">\(SE = \sqrt{\frac{\text{ICC} \omega}{J} + \frac{(1-\text{ICC})(1-R^2_{1})}{\bar{T}(1 - \bar{T}) J \bar{n}}}\)</span>, where <span class="math inline">\(\omega\)</span> is the ratio between the cross-site impact variation and the control outcome variation. We can see that in doing super population inference, the standard error has an additional term which is non-negative, so it will be as least as large as the standard error from finite population inference. A larger standard error will result in lower power.</p>
<p>Examining these standard error formulae also gives a better understanding of what factors impact power. For example, having more explanatory power of student-level covariates (higher <span class="math inline">\(R_1^2\)</span>) decreases the standard error, and thus increases power. Additionally, the individual-level covariates do not impact the super population term; they only help to reduce the component of standard error corresponding to the finite population. However, site-level covariates, which would be denoted <span class="math inline">\(R_2^2\)</span>, do not impact power. Site-level covariates are not useful in these models because there are already site-level effects, so the addition of covariates at that level does not provide more information.</p>
<p>To calculate power for multisite trials, users can use the PowerUpR! package <span class="citation" data-cites="Dong2013">(<a href="#ref-Dong2013" role="doc-biblioref">Dong and Maynard 2013</a>)</span>. The package also calculates sample size requirements and minimum detectable effect size. The newly-developed PUMP package (from <span class="citation" data-cites="Hunter2022">Hunter, Miratrix, and Porter (<a href="#ref-Hunter2022" role="doc-biblioref">2022</a>)</span>) extends the functionality of PowerUpR! to experiments with multiple outcomes, in addition to providing user-friendly tools for exploring the sensitivity of power to different assumptions.</p>
</section>
<section id="takeaway-advice-for-researchers-on-multisite-trials" class="level1" data-number="10">
<h1 data-number="10"><span class="header-section-number">10</span> Takeaway advice for researchers on multisite trials</h1>
<p>Many research plans and analyses do not clearly specify an estimand. This lack of clarity can both obscure the goal, and result in poor analysis choices. For example, different estimands imply different power analyses, but the choice is often not taken into account; super population estimands in particular result in larger standard errors, and thus often require larger sample sizes to be adequately powered. Additionally, different estimands require different estimators, so not defining an estimand makes it difficult for readers to judge the validity of the analysis. <span class="citation" data-cites="Miratrix2020">Miratrix, Weiss, and Henderson (<a href="#ref-Miratrix2020" role="doc-biblioref">2021</a>)</span> shows that the choice of estimand, estimator, and standard error estimator can matter (albeit in some cases more than others), in that the choice can impact the final conclusion reached by a study.</p>
<p>This guide did not focus on the problem of model misspecification. For the empirical estimates from the multisite trials considered, model-based and design-based approaches did not result in substantially different answers. However, it is conceivable that there are contexts when these estimators could differ, and further investigation into this area is warranted.</p>
<p>Though this guide set up analyzing an RCT as a series of dichotomous choices, one way forward is for researchers to report more than one estimand. Presenting a finite population person-weighted estimand is almost always compelling. Then, the researcher may choose to also present a site-weighed estimand, or to expand their conclusion to a super population estimand. In some cases, different estimands may result in the same conclusion. However, it is possible that for some studies, there is evidence of a significant effect in the finite population, but the additional uncertainty of the super population estimation means there is insufficient evidence concerning the impact in a broader population. In these cases, only reporting one of the finite population or super population estimands does not portray the full nuance of the results.</p>
</section>
<section id="references" class="level1" data-number="11">




</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">11 References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Abadie2017" class="csl-entry" role="listitem">
Abadie, Alberto, Susan Athey, Guido W. Imbens, and Jeffrey Wooldridge. 2017. <span>“When Should You Adjust Standard Errors for Clustering?”</span> NBER.
</div>
<div id="ref-Bloom2017" class="csl-entry" role="listitem">
Bloom, Raudenbush, H. S., and K. Porter. 2017. <span>“Using Multisite Experiments to Study Cross-Site Variation in Treatment Effects: A Hybrid Approach with Fixed Intercepts and a Random Treatment Coefficient.”</span> <em>Journal of Research on Educational Effectiveness</em> 10 (4): 817–42.
</div>
<div id="ref-bowers2011mem" class="csl-entry" role="listitem">
Bowers, Jake. 2011. <span>“Making Effects Manifest in Randomized Experiments.”</span> In <em>Cambridge Handbook of Experimental Political Science</em>, edited by James N. Druckman, Donald P. Green, James H. Kuklinski, and Arthur Lupia. New York, NY: Cambridge University Press.
</div>
<div id="ref-Clark2011" class="csl-entry" role="listitem">
Clark, Gleason, M. A., and M. K. Silverberg. 2011. <span>“Do Charter Schools Improve Student Achievement? Evidence from a National Randomized Study.”</span> Mathematica Policy Research, Inc.
</div>
<div id="ref-Dong2013" class="csl-entry" role="listitem">
Dong, Nianbo, and Rebecca Maynard. 2013. <span>“PowerUP!: A Tool for Calculating Minimum Detectable Effect Sizes and Minimum Required Sample Sizes for Experimental and Quasi-Experimental Design Studies.”</span> <em>Journal of Research on Educational Effectiveness</em> 6 (1): 24–67.
</div>
<div id="ref-gerber2012field" class="csl-entry" role="listitem">
Gerber, Alan S, and Donald P Green. 2012. <em><span class="nocase">Field experiments: Design, analysis, and interpretation</span></em>. WW Norton.
</div>
<div id="ref-Hunter2022" class="csl-entry" role="listitem">
Hunter, Kristen, Luke Miratrix, and Kristin Porter. 2022. <span>“Power Under Multiplicity Project (PUMP): Estimating Power, Minimum Detectable Effect Size, and Sample Size When Adjusting for Multiple Outcomes.”</span> <em>arXiv</em>. <a href="https://arxiv.org/abs/2112.15273">https://arxiv.org/abs/2112.15273</a>.
</div>
<div id="ref-Imai2008" class="csl-entry" role="listitem">
Imai, Kosuke, Gary King, and Elizabeth A. Stuart. 2008. <span>“Misunderstandings Between Experimentalists and Observationalists about Causal Inference.”</span> <em>Journal of the Royal Statistical Society: Series A</em> 171 (2): 481–502.
</div>
<div id="ref-Imbens2015" class="csl-entry" role="listitem">
Imbens, Guido W., and Donald B. Rubin. 2015. <em>Causal Inference for Statistics, Social, and Biomedical Sciences: <span>A</span>n Introduction</em>. Cambridge University Press.
</div>
<div id="ref-lin2013agnostic" class="csl-entry" role="listitem">
Lin, Winston. 2013. <span>“<span class="nocase">Agnostic notes on regression adjustments to experimental data: Reexamining Freedman’s critique</span>.”</span> <em>The Annals of Applied Statistics</em> 7 (1): 295–318.
</div>
<div id="ref-Miratrix2020" class="csl-entry" role="listitem">
Miratrix, Luke E., Michael Weiss, and Brit Henderson. 2021. <span>“An Applied Researcher’s Guide to Estimating Effects from Multisite Individually Randomized Trials: Estimands, Estimators, and Estimates.”</span> <em>Journal of Research on Educational Effectiveness</em> 14.
</div>
<div id="ref-moore2012multivariate" class="csl-entry" role="listitem">
Moore, Ryan T. 2012. <span>“Multivariate Continuous Blocking to Improve Political Science Experiments.”</span> <em>Political Analysis</em> 20 (4): 460–79.
</div>
<div id="ref-moore2013" class="csl-entry" role="listitem">
Moore, Ryan T, and Sally A Moore. 2013. <span>“<a href="https://www.ncbi.nlm.nih.gov/pubmed/24143061">Blocking for Sequential Political Experiments</a>.”</span> <em>Political Analysis</em> 21: 507–23.
</div>
<div id="ref-Pashley2021" class="csl-entry" role="listitem">
Pashley, Nicole E., and Luke W. Miratrix. 2021. <span>“Insights on Variance Estimation for Blocked and Matched Pairs Designs.”</span> <em>Journal of Educational and Behavioral Statistics</em> 46 (3): 271–96.
</div>
<div id="ref-Pashley2022" class="csl-entry" role="listitem">
———. 2022. <span>“Block When You Can, Except When You Shouldn’t.”</span> <em>Journal of Educational and Behavioral Statistics</em> 47 (1).
</div>
<div id="ref-Pustejovsky2018" class="csl-entry" role="listitem">
Pustejovsky, James E., and Elizabeth Tipton. 2018. <span>“Small-Sample Methods for Cluster-Robust Variance Estimation and Hypothesis Testing in Fixed Effects Models.”</span> <em>Journal of Business &amp; Economic Statistics</em> 36 (4).
</div>
<div id="ref-Raudenbush2020" class="csl-entry" role="listitem">
Raudenbush, S. W., and D. Schwartz. 2020. <span>“Randomized Experiments in Education, with Implications for Multilevel Causal Inference.”</span> <em>Annual Review of Statistics and Its Application</em> 7 (1).
</div>
<div id="ref-Raudenbush2015" class="csl-entry" role="listitem">
Raudenbush, Stephen W., and Howard S. Bloom. 2015. <span>“Learning about and from a Distribution of Program Impacts Using Multisite Trials.”</span> <em>American Journal of Evaluation</em> 36: 475–99.
</div>
<div id="ref-Richburg-Hayes2008" class="csl-entry" role="listitem">
Richburg-Hayes, Visher, L., and D. Bloom. 2008. <span>“Do Learning Communities Effect Academic Outcomes? Evidence from an Experiment in a Community College.”</span> <em>Journal of Research on Educational Effectiveness</em> 1 (1): 33–65.
</div>
<div id="ref-rubin1990b" class="csl-entry" role="listitem">
Rubin, D. B. 1990. <span>“Formal Modes of Statistical Inference for Causal Effects.”</span> <em>Journal of Statistical Planning and Inference</em> 25: 279–92.
</div>
<div id="ref-Schochet2016" class="csl-entry" role="listitem">
Schochet, Peter Z. 2016. <span>“Statistical Theory for the RCT-YES Software: Design-Based Causal Inference for RCTs.”</span>
</div>
<div id="ref-Neyman1923" class="csl-entry" role="listitem">
Splawa-Neyman, Jerzy, Dortoa M Dabrowska, and Terence P. Speed. 1923/1990. <span>“On the Application of Probability Theory to Agricultural Experiments. <span>E</span>ssay on Principles. <span>S</span>ection 9.”</span> <em>Statistical Science</em> 5 (4): 465–72.
</div>
<div id="ref-Weiss2019" class="csl-entry" role="listitem">
Weiss, Ratledge, M. J., and H. Gupta. 2017. <span>“Supporting Community College Students from Start to Degree Completion: Long-Term Evidence from a Randomized Trial of CUNY’s ASAP.”</span> <em>Annual Economic Journal: Applied Economics</em> 11 (3).
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p> Linear regression can be used as a tool in both design-based approaches (to calculate the difference in means) and model-based approaches (to estimate the parameters of a Normal data-generating process). In general, this guide considers linear regression as used in a model-based approach.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/methods\.egap\.org");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>