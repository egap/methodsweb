---
title: 10 Things to Know About Indices
editor: visual
format: 
  html:
    fig-width: 6
    fig-height: 6
---

\bigskip

Saki Kuzushima

# 1. What are indices?

An index is a composite statistic where multiple indicators are aggregated to measure one concept. Researchers often combine multiple observable indicators to measure concepts not directly observable, such as attitudes, perceptions, and beliefs. Rather than forcing one indicator to measure the concept, using multiple indicators can reduce measurement error and measure the concept more accurately.

Examples of such measures include both established measures and researcher-defined measures. Established measures include personality traits, such as right-wing authoritarianism and narcissism, and they have established sets of questions [@altemeyer1988enemies; @raskin1988principal]. By contrast, many other concepts do not have an established set of indicators to measure them, and thus researchers need to choose which observable indicators to use and how to combine them. There are many aggregation methods, such as simple and weighted averages, principle component analysis (PCA), factor analysis, and item response theory (IRT) methods, and they have different advantages and disadvantages. Researchers need to choose which method to use depending on the context and the purpose of the study.

# 2. Why are indices important?

Substantively, indices allow researchers to measure the concepts not directly observable. When researchers conduct survey and field experiments, they are typically interested in how treatments affect outcomes. While treatments are often easily measured because researchers have complete control over them in most experimental setups, measuring outcomes can be challenging. This is because many outcomes political scientists and policymakers are interested in are not directly observable and are difficult to measure using a single observable indicator. For example, individual socioeconomic values, or how much people value economic equality and fairness, is an important concept in social science, but they are not directly observable. In addition, it is difficult to measure them using a single observable indicator because the concept itself is complex and multi-dimensional. Researchers often use responses to survey questions to measure these complex concepts, there is often no single question that can capture the concept well. It is not sensible to ask people "what socioeconomic values do you have?" because respondents may have different understandings about what socioeconomic values mean, or they simply may not understand the question well.

To overcome this challenge, combining several observable indicators that are more concrete and reflect the definition of the concept is often the best way to measure them. For example, [@margalit2021markets] study the effect of investment experience on socioeconomic values, and they measure socioeconomic values by combining respondents' agreement with statements such as "economic positions are legitimate reflections of people's achievements" and "there are no inherent differences between rich and poor; it is purely a matter of the circumstances into which you are born." Combining these questions together allows researchers to measure the concept more accurately than asking a single question about socioeconomic values.

# 3. When do we want to use indices?: Substantive reasons

When researchers design survey or field experiments, they may face a choice about the outcome variables. Consider an experiment where outcomes are survey responses to multiple questions. Researchers have two choices about the outcome variables. The first option is to combine the multiple questions into a single index, and use the index as a single outcome. The second option is to use multiple observable indicators as multiple outcomes. Substantively, researchers should consider which can address their research question better. Statistically, using an index as an outcome can increase the power of the study, but it comes at the cost of losing the information about the treatment effect on each observable indicator.

For example, [@anjum2021united] conduct a survey experiment that examines whether an endorsement from the UN influences public support for policies to improve women's rights in Pakistan. In their experiment, their outcome variables are the support for different policies related to women's rights, such as increasing legislative quotas and raising the minimum marriage age, and the treatment is information about the UN endorsement. Which outcomes are more appropriate? 1) A single composite index that combines the support for different policies or 2) the support for each policy separately?

Substantively, research questions should guide whether the appropriate outcome is an index that combines the support for different policies or the support for each policy separately. If researchers are interested in the endorsement effect on any kind of women's rights, researchers can choose to use the index as an outcome. Alternatively, if researchers are interested in the variation in the effect of the UN endorsement on different policies, researchers should use the support for each policy as an outcome.

Practically, researchers may interested in both the effect on the concept measured by the composite index and the effect of treatment on each observable indicator. This is certainly a reasonable choice, but researchers should make sure to pre-register their hypotheses and analysis plan, including whether and how to combine outcome variables before conducting the experiment. <!--
In short, whether to use an index as an outcome depends on the research goals and substantive questions, and researchers should carefully consider what they want to learn from the study. For this reason, we strongly encourage researchers to pre-register their hypotheses and analysis plan that specifies whether and how to combine outcome variables before conducting the experiment.
-->

# 4. When do we want to use indices?: Statistical reasons

Statistically, using an index can reduce measurement error [@ansolabehere2008strength]. Let $X$ be a scalar value that represents true socioeconomic value but it is not directly observable. Instead, we observe responses to the two survey questions, $W_1$ and $W_2$, which are noisy measures of $X$. In other words, these observe responses come with additive noise $\epsilon_1$ and $\epsilon_2$ such that $W_1 = X + \epsilon_1$ and $W_2 = X + \epsilon_2$. We further assume that the noise has a mean zero and is not correlated with either the true value $X$ or each other, and that the noise has the same variance. In other words, we assume that $E[\epsilon_1] = E[\epsilon_2] = 0$, $E[\epsilon_1 X] = E[\epsilon_2 X] = 0$, $E[\epsilon_1 \epsilon_2] = 0$, and $Var[\epsilon_1] = Var[\epsilon_2] = \sigma^2$.

We can compare the variance of two possible outcome variables: a single observable indicator $W_1$ and a composite index computed as the average of two indicators $\bar{W} = \frac{1}{2}(W_1 + W_2)$. Under our assumption, we can show that $Var[W_1] = Var[X] + \sigma^2$ and $Var[\bar{W}] = Var[X] + \frac{1}{2}\sigma^2$, and therefore $Var[\bar{W}] < Var[W_1]$.This implies that the variance in the outcome variable due to the measurement error can be reduced by using an index that combines multiple observable indicators.

Due to the reduced measurement error, a composite index as an outcome can increase the power of the study. Because the indices can reduce the measurement error, and thus the variance in the outcome variable, the variance in the estimated treatment effect is likely to be also reduced, which increases the power of the study. In addition, using indices can alleviate concerns about multiple testing that arise when multiple outcomes are used.[^1] 

By contrast, the major disadvantage of using an index as a single outcome is that we would lose the information about the treatment effect on each observable indicator, and thus we may not be able to learn about the variation in the treatment effect on different indicators. The variation across different indicators can be important because it can provide insights into research questions, such as which aspect of the concept is affected by the treatment and which is not. Therefore, researchers should carefully consider whether they want to use an index as an outcome or multiple observable indicators as outcomes depending on the research questions and hypotheses to be tested.

[^1]: Please refer to the EGAP method guide on the multiple testing problem on how to conduct multiple testing corrections.

# 5. How to pick different methods to construct indices

Suppose researchers decided to combine multiple questions to construct an index to measure the concept of interest, and then they want to use it as an outcome of an experiment. This process typically involves several steps: 1) define the concept well, 2) choose what observable indicators to use, 3) decide how to combine them together. In this section, we focus on the last step: how to combine multiple observable indicators to measure the concept. A simple and commonly used way is to use a simple average of the observable indicators as an index. While this is the most straightforward and simple method, one of the disadvantage is that it assumes that all observable indicators are equally important, which may not be the case. Researchers may have a theory about which observable indicator should be weighted more than others, or they may want to choose a method that maximizes the efficiency of the available data. Among many existing methods, researchers need to choose which method to use depending on the context and the purpose of the study. There are two approaches to guide this decision: theory-driven and data-driven approaches.

Researchers may have a theory about how the observable indicators are linked to the concept of interest. In this case, researchers may want to use a theory-driven approach to construct the index. For example, Polity IV is a measure of democracy that combines multiple observable indicators, such as the competitiveness of political participation, the openness and competitiveness of executive recruitment, and the constraints on the chief executive. These observable indicators are combined as a weighted average with pre-determined weights [@polityiv]. While taking a weighted average with pre-determined weights is a simple and transparent method, it raises the concern about the subjectivity of the weights. Because the way each indicator is weighted has undoubtedly a large impact on the index values, the weights need to be chosen with care and justified by theory.

Another perspective is a data-driven approach. In this approach, researchers can use statistical tools to combine the observable indicators to construct the index. For example, principle component analysis (PCA) and factor analysis are two common methods that can increase the efficiency of the data compared to the simple average. Intuitively, these methods allow researchers can exploit the correlation information among the observable indicators to construct the index. While the data-driven approach may seem more objective and transparent compared to the theory-driven approach, researchers should be aware that PCA and factor analysis also involve researchers' decisions in practice, such as how many dimensions to keep and whether to use rotation. Thus, it is recommended that researchers decide how exactly they apply these methods prior to conducting the experiment. Pre-registering an analysis code that specifies the exact method and parameters used to compute the index can remove this concern.

# 6. How to construct indices

We briefly review how to construct indices with a data-driven approach. In particular, we review the assumptions that each method makes and discuss the advantages and disadvantages of each method.

The most straightforward way to construct an index is to take a simple average of the observable indicators. Suppose that $X$ is the true value of the concept, and $W_1, W_2, \ldots, W_k$ are the observable indicators. The simple average index is defined as $\bar{W} = \frac{1}{k}\sum_{i=1}^k W_i$. This method is simple and transparent, and it is often used in practice. However, its downside is that it assumes that all observable indicators are equally important, which may not be the case.

To relax the assumption of common weights, researchers may want to use a weighted average. The weighted average index is defined as $\bar{W} = \sum_{i=1}^k v_i W_i$, where $v_i$ is the weight for the $i$-th observable indicator. If $v_i = 1/k$ for all $i$, the weighted average index is equivalent to the simple average index. There are several methods to estimate the weights. Common methods, such as PCA and factor analysis, can be used to estimate the weights. PCA is a dimension reduction technique that finds the weights that maximize the variance of the estimated index. Specifically, it replaces the original $W_1, W_2, \dots, W_k$ with a smaller number of linear combinations of the original indicators, and the weights are chosen to find the direction that maximizes the variance of the data. Factor analysis is similar to PCA, except that it is often considered as a model-based approach. While both PCA and factor analysis are dimension reduction methods, one main difference between the two is that PCA uses covariance information and factor analysis use correlation information. The advantage of using these methods is that we can use covariance or correlation information, and thus we can obtain more efficient indices. However, the disadvantage is that it requires the estimation of weights, and the choice of the method to estimate the weights may affect the results. Therefore, it is important for researchers to justify the method used to estimate the weights and pre-register the analysis plan.

One limitation of the weighted average approach is that the data is treated as numerical, but the survey response data is often categorical. For example, the survey response data is often categorical, such as "strongly agree," "agree," "neutral," "disagree," and "strongly disagree." With these response categories, the distance between "strongly agree" and "agree" may not the same as the distance between "agree" and "neutral." However, with PCA and factor analysis, these response categories are treated as numbers (i.e., 1, 2, 3, 4, 5), and thus the assumption of common distance between response categories is implicitly made. One method that can relax the assumption of common distance between response categories is item response theory (IRT) models and its variants. While IRT models are the most flexible and can account for the uncommon distance between response categories, the same disadvantage as PCA and factor analysis remains as it requires estimation of weights.

# 7. How to use an index as a dependent variable in a regression: Variance

After the index is constructed, researchers want to estimate the treatment effect on the index. A common approach is a two-step approach: First, construct the index using the method described in the previous section, and then estimate the treatment effect on the constructed index. For example, researchers could construct an index using factor analysis, and then estimate the treatment effect by taking a difference-in-means of the estimated index between the treatment and control groups. This approach is simple and straightforward, but there are several issues that researchers should be aware of.

First, the variance of the estimated treatment effect is likely to be underestimated because the uncertainty of the measurement error in the index is not taken into account when estimating the treatment effect. After we construct the index, researchers often use the estimated index as an outcome in a regression model, and then estimate the treatment effect on the index. When they use the estimated index as an outcome, they often consider the estimated index as a fixed value, and disregard the uncertainty of the measurement error in the index. Therefore, the variance in the estimated treatment effect is likely to be underestimated.

One solution to this problem is to use the hierarchical model to estimate the treatment effect on the index. [@stoetzer2024causal] introduce a method that can incorporate the uncertainty, by using a hierarchical item response theory (IRT) model. In their method, the index is constructed using the IRT model, and then the treatment effect is estimated as a hierarchical structure. This method can account for the uncertainty of the measurement error in the index, and thus the variance in the estimated treatment effect is likely to be more accurately estimated.

# 8. How to use an index as a dependent variable in a regression: Bias

Even if the uncertainty is incorporated, the estimated treatment effect may be biased if the measurement error is correlated with the treatment. One way to understand why the estimated treatment effect is biased is to consider the systematic measurement error in the index. Suppose that the true value of the concept is $X_i$, and the observed value of the concept is $W_i = X_i + \epsilon_i$, where $\epsilon_i$ is the measurement error. Unless the expectation of the measurement error given the treatment status is zero, the estimated treatment effect on the index is likely to be biased. Suppose we want to estimate the average treatment effect (ATE) of the binary treatment, $T_i$, on the concept, $X_i$. The ATE is then defined as $E[X_i | T_i = 1] - E[X_i | T_i = 0]$. However, the difference in means of the observed index, $W_i$, between the treatment and control groups is given by the following.0 $$\begin{split}
  &E[W_i | T_i = 1] - E[W_i | T_i = 0] \\
  &=E[X_i + \epsilon_i | T_i = 1] - E[X_i + \epsilon_i | T_i = 0] \\
  &= \underbrace{E[X_i | T_i = 1] - E[X_i | T_i = 0]}_{ATE} + \underbrace{E[\epsilon_i | T_i = 1] - E[\epsilon_i | T_i = 0]}_{Bias}.
\end{split}$$ The last term, $Bias$, is the bias in the estimated treatment effect, and it may be non-zero.

When does the bias occur? One example is social desirability bias induced by treatment assignment, where respondents tend to give answers that are more socially acceptable when treated. For example, [@clayton2019all] study the effect of gender-compositions of committees on the perceived legitimacy of the committee's decision. While they found that having a gender-balanced committee increases the perceived legitimacy of the committee's decision, it is possible that the results are driven by the social desirability bias. In other words, respondents might have answered that the gender-balanced committee's decision is more legitimate simply because they think it is better to say so, rather than because they truly believe that the gender-balanced committee is more legitimate.

# 9. Pre-registration: Design stage

Given that multiple methods are available to construct indices and estimate the treatment effect on them, researchers should carefully consider which method to use depending on the context and the purpose of the study. In particular, it is important for researchers to pre-register their hypotheses and analysis plan before conducting the experiment. This ensures the credibility of the study because it can constrain researchers from choosing the method that produces the most favorable results after seeing the data. In addition, it can increase the transparency of the study because it allows others to understand how the index is constructed and the treatment effect is estimated.

For example, a pre-registration by [@kalla2020reducing] is a great example of how to pre-register the hypotheses and analysis plan with indices as an outcome.[^2] In their pre-registration, they not only specify the research questions and the hypotheses but also the analysis plan, including how to construct the index and estimate the treatment effect. In addition, they provide the analysis code that can be executed once the data is collected, which ensures that the analysis is reproducible and transparent.

[^2]: <https://osf.io/b4t3q>

# 10. Pre-registration: Analysis stage

After the experiment is conducted and the data is collected, researchers should stick to the pre-registered analysis plan as much as possible. However, it is often the case that researchers have to deviate from the plan for some reasons. In such cases, it is important to be transparent and explain why and how the plan is deviated.

For many reasons, researchers may have to change the method to construct the index, they should explain why the original method did not work and how the new method is chosen. For example, [@kalla2020reducing] pre-registered that they would use factor analysis to construct the index, but they found that the method did not work well with the data collected due to the missingness. Then, they updated their analysis plan and declared that they decided to use simple average method to construct the index. This is a great example of being transparent and explaining why and how the method is changed from the pre-registered plan.

Furthermore, when analyzing data, researchers may notice some interesting findings that they did not expect at the design stage. In such cases, it is important to distinguish whether the findings are confirmatory and explanatory. In order to avoid confusion, it is recommended to explain the new findings are exploratory and not confirmatory.

# Example R code

This section provides an example R code to construct indices using the simple average, PCA, and factor analysis methods. We use the replication data from [@margalit2021markets] which studies the effect of investment experience on socioeconomic values.[^3] The observable indicators are the responses to the following questions. The response categories are "strongly agree," "agree," "neutral," "disagree," and "strongly disagree" for the first three questions, and 1-10 scale for the last question.

[^3]: Replication data and code are available at <https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/UEARI0>.

-   "Most people who don't get ahead in our society should not blame the system; they have only themselves to blame."

-   "Economic positions are legitimate reflections of people's achievements."

-   "There are no inherent differences between rich and poor; it is purely a matter of the circumstances into which you are born."

-   Place the views on a 1--10 scale: (1) "We need larger income differences as incentives for individual effort." (10) "Incomes in the UK should be made more equal".

While the original study uses PCA to construct the index, we also provide the code to construct the index using the simple average, factor analysis, IRT with two-step approach, and hierarchical IRT methods.

\centering


```{r, message=F, warning=F, output=F}
library(haven)
library(hIRT)
data <- read_dta("indices_markets_SEV_repdata.dta")

# Simple average ################################
items <- data[,c("blame_system_w3", "bjw_w3", "r_luck_w3", "equality_1_w4")]
items_std <- apply(items, 2, scale)
score_ave <- rowMeans(items_std, na.rm=T)

# match the direction so that the higher score means right-wing
data$score_ave <- -score_ave

# average treatment effect
ate_ave <- lm(score_ave ~ stockreal, data=data)

# PCA ################################################
out_pca <- princomp(~ blame_system_w3 + bjw_w3 +
                    r_luck_w3 + equality_1_w4,
                    data=data,
                    cor=T,
                    na.action=na.exclude)
score_pca <- out_pca$scores[,1]

data$score_pca <- -score_pca

ate_pca <- lm(score_pca ~ stockreal, data=data)

# Factor analysis ##########################################
out_fa <- factanal(~ blame_system_w3 + bjw_w3 +
                    r_luck_w3 + equality_1_w4,
                    factors=1,
                    data=data,
                    scores="regression",

                    na.action=na.exclude)
score_fa <- out_fa$scores[,1]

data$score_fa <- -score_fa

ate_fa <- lm(score_fa ~ stockreal, data=data)

# IRT (two step) ############################################
out_irt <- hgrm(y=items, x=NULL)
score_irt <- -out_irt$scores$post_mean
ate_irt <- lm(score_irt ~ stockreal, data=data)

# Hierarchical IRT ##########################################
out_hirt <- hgrm(y=items, x=as.matrix(data[,c("stockreal")]))
ate_hirt <- out_hirt$coefficients["xstockreal",] 
```
```{r, fig.cap="**Comparison of the Average Treatment Effect with Different Methods to Construct the Index**: This figure compares the estimated average treatment effect (ATE) of investment experience on the outcome index constructed with different methods. X-axis represents the estimated ATE, and the horizontal line represents the 95% confidence interval. The outcome index is constructed with 5 different methods: simple average, PCA, factor analysis, IRT, and hierarchical IRT. For the simple average, PCA, factor analysis methods, and IRT, the index is constructed first, and a simple linear regression is used to estimate the treatment effect of investment experience on the index. For the hierarchical IRT method, both the index construction and the effect estimation are done by the hierarchical IRT model. Across all methods other than hierarchical IRT model, the estimated treatment effect is positive statistically significant at the 5% level, which is consistent with the original study."}
#| echo: false
# Results ###################################################
par(mar=c(5, 8, 4, 2))
plot(x=NA, y=NA, ylim=c(0, 6), xlim=c(-0.5, 0.5), xlab="Treatment effect", ylab="Methods",
     yaxt="n", xaxt="n")
points(x=ate_ave$coefficients["stockreal"], y=1, pch=19)
lines(x=c(confint(ate_ave)[2,1], confint(ate_ave)[2,2]), y=c(1, 1), lwd=2)
points(x=ate_pca$coefficients["stockreal"], y=2, pch=19)
lines(x=c(confint(ate_pca)[2,1], confint(ate_pca)[2,2]), y=c(2, 2), lwd=2)
points(x=ate_fa$coefficients["stockreal"], y=3, pch=19)
lines(x=c(confint(ate_fa)[2,1], confint(ate_fa)[2,2]), y=c(3, 3), lwd=2)
points(x=ate_irt$coefficients["stockreal"], y=4, pch=19)
lines(x=c(confint(ate_irt)[2,1], confint(ate_irt)[2,2]), y=c(4, 4), lwd=2)
points(x=ate_hirt$Estimate, y=5, pch=19)
lines(x=c(ate_hirt$Estimate - 1.96*ate_hirt$Std_Error, ate_hirt$Estimate + 1.96*ate_hirt$Std_Error), y=c(5, 5), lwd=2)
abline(v=0, lty=2)
axis(1, at=seq(-0.5, 0.5, by=0.1), labels=seq(-0.5, 0.5, by=0.1))
axis(2, at=1:5, labels=c("Average", "PCA", "FA", "IRT", "HIRT"), las=1)

```

\clearpage
