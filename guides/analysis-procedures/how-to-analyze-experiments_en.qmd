---
title: " 10 Things on Your Checklist for Analyzing an Experiment"
author: 
  - name: "name"
    url: https://methods.egap.org/guides.html
image: covariates.png
bibliography: how-to-analyze-experiments.bib
---

This guide will provide practical tools on what to do with your data once you've run an experiment. This guide is geared towards anyone involved in the life cycle of an experimental project: from analysts to implementers to project managers. If you're not the one directly involved in analyzing the data, these are some key things to look for in reports of analysis of experimental data. If you are analyzing the data, this guide provides instructions and `R` code on how to do so.

# Treatment variable: Confirm its randomization
==

First things first, it is important to verify that your treatment was successfully randomized according to what you'd planned in your design. Randomization plays a key role in the fundamentals of experimental design and analysis, so it's key that you (and others) can verify that your treatment was assigned randomly in the way that you planned. Without this, it's hard for the research community and policymakers to have confidence in your inferences. 

First, you should trace back to the code or software that was responsible for generating the randomization. If this was done in Qualtrics or on SurveyCTO, this would involve going back to the original software and ensuring that the particular survey module was coded correctly. If you worked with a provider to code up your online survey, you could schedule a meeting with them to walk through verifying the coding of the randomization. If this was done in `R`, there should be at least 2 lines of code: one that set a seed so that the randomization would be reproducible, and another that actually randomizes the treatment assignment. Note that the second line of code depends on the type of randomization you'd conducted (see [10 Things You Need to Know About Randomization](https://methods.egap.org/guides/analysis-procedures/randomization-inference_en.html)).




# Variable inspection: Locating and understanding your data variables
==

In addition to verifying that the treatment was correctly assigned in the software that you used, it's important to verify what the treatment variable in your data set - produced by the code or software - actually means. This means checking that values of the treatment assignment variable correspond to the values assigned in the randomization process in `R` or in the software used to assign treatment. Does a 1 produced by the software correspond to treatment, and 0 to control? For a three-arm experiment, softwares like SurveyCTO may produces values 1, 2, and 3 for the treatment assignment variable. It's crucial that analysts verify that what they think they're analyzing corresponds to the actual treatment assigned.

If you are in a context where non-compliance with treatment is possible, you should also inspect the variable that indicates treatment receipt. In a field experiment, if a subject was assigned to a skills training (treatment assignment = 1), did they actually attend and receive the intended information on skills (treatment receipt = 1)? In a survey experiment, did a subject assigned to receive watch a video to provoke positive emotions (treatment assignment = 1) actually report watching the video and internalizing its content (treatment receipt = 1)? A variable that indicates whether a subject actually _received_ the treatment it was assigned to is key for thinking about things like complier average causal effects (see [10 Things You Need to Know About the Local Average Treatment Effect](https://methods.egap.org/guides/research-questions/late_en.html)). Understanding what the treatment receipt variable means, and what each of its values corresponds to (does 1 correspond to treatment receipt, and 0 to lack of receipt?), is crucial.

It's also useful to inspect the outcome and covariate variables that you plan on using in your analysis. This might include renaming them for easier coding and interpretation of your analysis code. If someone looks at your analysis code, it is generally easier for them to understand what a variable called "gender" means, rather than if it is called "question_26_c_1". In addition, making sure that missing data are consistently coded with "NA" rather than a numeric value is important to ensure the integrity of data analysis. Sometimes, survey software may code missing data with 0 or 888; if you do not change these values to "NAs", your data analysis will think that they correspond to numeric values of the variable of interest.

# Did treatment assignment go according to plan?
==

It's generally good practice to inspect whether or not your treatment assignment did in fact work in accordance with your expectations, depending on the design. This is separate from verifying the code or software that produced the randomization, as in 1 above. Instead, you can check the distribution of the values of treatment assignment; you should see whether it corresponds to what you'd designed (see [10 Things You Need to Know About Randomization](https://methods.egap.org/guides/analysis-procedures/randomization-inference_en.html)). 

For example, if you conducted a complete randomization where 25 experimental subjects were to be assigned to control, and 25 to treatment, did this play out in practice? You can run a simple cross-tabulation of the treatment assignment variable, and observe whether or not this is the case. Alternatively, if you conducted a block randomization by gender and wanted to assign half of women to treatment and the other half to control (and the same for men), you can look at the distribution of treatment assignment values by gender. Though uncommon in field experimental design where complete randomization prevails, some survey experimental software conducts simple randomization. In a survey experiment with two arms, the software will assign each respondent to treatment or control by flipping a Bernoulli coin that has a 50 percent chance of landing on heads (treatment), and a 50 percent chance of landing on tails (control).^[Note: the Bernoulli coin need not be defined by 50-50 chances; the researcher defines ex-ante the probability of receiving treatment versus control, which could be 70-30 instead.] In this case, it's okay if exactly 50 percent of your respondents don't end up in treatment - we'd expect some variability due to the nature of the randomization. However, the percentage of units treated shouldn't be _too far_ from 50 percent - this would induce doubt that the randomization actually worked.


# Checking outcome and covariate variables for outliers
==

Next, one should inspect the outcome and covariate variables for outlier values and decide what to do about these outliers. Plotting the distribution of the values of each variable (via a histogram or boxplot, for example), or asking your statistical software to produce a "summary" of the variable, are ways of going about this.

To adjudicate about outliers requires substantive knowledge about what reasonable ranges are for the variables. For an age variable of a survey of only adults, for example, ranges from 18 to 85 might be reasonable. If a subject has a reported age of 2 or 1000, for example, this would raise concerns. Similarly, reported salary values outside of the range reasonable for the context that you work in might raise questions. As a note, it's possible to program your survey software such that values entered must abide by certain rules (for example, percent of the time one spends doing household labor can only be between 0 and 100, inclusive). See guide on [10 Things to Know About Survey Design](https://methods.egap.org/guides/planning/survey-design_en.html) for more on designing surveys smartly.

If you think a value of a variable is incorrectly coded, you can try to figure out what the correct value was. First, it's possible to do detective work to discover what the correct value was. Sometimes an enumerator may accidentally add an extra digit to an otherwise correct value, for example. One can follow-up with enumerators to clarify whether or not they made a mistake. Alternatively, one can go back to the source to verify the correct value of the variable for a certain observation. Imagine that an outcome variable is the result of an election, which was coded manually from archival resources or online images. In this case, it is relatively straightforward to return to the original newspaper source and/or image to verify the result of the election for the given observation. 

However, much of the time it's not possible to retrospectively find the correct value of a variable for a given unit. Further, it may not always be clear that the value in question is in fact an incorrect - perhaps there really was a respondent who had an annual salary of 1 million dollars. Many times, best practice is to leave the value alone; editing the value would bias results. In the case that, thanks to contextual knowledge, you are certain that a value has been incorrectly coded and you are unable to locate the correct value, you may consider marking the value as missing. Other times, researchers use techniques like winsorizing a variable. This approach transforms data with the goal of limiting the impact of outliers. Concretely, winsorization changes extreme values to less extreme values. Another approach is to trim outlier values, which means to remove them entirely. 

If you choose to make edits to values in your data, you should make sure you're doing 2 things. First, you should be blind to results and the treatment condition of the observation(s) when you edit or inspect data. See [Standard operating procedures for Don Greenâ€™s lab at Columbia
](https://alexandercoppock.com/Green-Lab-SOP/Green_Lab_SOP.html#missing-covariate-values) for more helpful insight on this. Second, you should never completely overwrite the raw data as you make edits. Instead, you should keep a code file that documents exactly the changes you make, and producing a cleaned data file (while retaining the raw data file). This is key to the transparency and reproducibility of your results. One way to organize the data and code for your experiment is to have a raw data file, then the code scripts that clean the data, and the intermediate/final (clean) data that you then run the analysis on. _Never overwrite the raw data_.


# Checking for balance on pre-treatment covariates
==

After inspecting and cleaning treatment assignment, treatment receipt, covariate, and outcome data, it's crucial to check for balance on pre-treatment covariates between treatment arms. "Balance tests" are a way of providing evidence that your randomization worked as expected. They seek to probe an observable implication of random assignment of treatment: that treatment and control groups look the same, on average, in terms of pre-treatment characteristics.^[See the methods guide on balance tests for more on this. However, the key (unobservable) assumption that we seek to test is whether or not treatment assignment is independent of potential outcomes.] Balance tests, which can be implemented in a number of different ways, ask whether imbalances in pre-treatment covariates across different experimental conditions are larger than what would be expected by chance.

The balance test you run depends on your experimental design. In a set-up where you have a binary treatment and all units have the same probability of assignment to treatment, you can run a regression of the treatment assignment variable on your covariates, and run an F-test that tests the null hypothesis that all covariates are jointly 0. The p-value of the F-statistic can be found via randomization inference. If, for example, you conducted a block randomization with treatment probabilities that vary by block, you should adjust for this in the regression specification. 

We are interested in the F-test because we care about overall balance of covariates. However, you are likely to come across covariate-by-covariate analyses that conduct t-tests comparing means among control units and treated units. Though this is standard practice, it is undesirable for several reasons. First, it is not unlikely that a covariate will come up as significantly different among treated and control units: due to the often numerous number of tests conducted, statistical significance can arise by chance. Here, it is important to emphasize that if you detect imbalance on a covariate that is significantly different than what would arise by chance, it's not always cause for panic. Second, this approach treats each covariate as equally important and informative about potential outcomes, rather than taking into account the prognosticness of the covariate. One may consider running equivalence tests instead of conventional balance tests.^[@hartman_hidalgo_2018 write on the meaning and implications of equivalence tests, vis-a-vis conventional balance tests.] 


What you expect to see and what to do if it's okay, good to go

What to do if there's evidence of imbalance


things to look into:

- ex balance
- RItools, xbalance that can do with and without stratification, kind of complicated
- cobalt
- look at cluster randomization guide by Jake
- add the overall p value from the F test

```{r}
## insert code for balance test here
## t tests ? 
## permutation tests ?
## equivalence tests ? 
## simply showing standardized differences ?
```




# Example code
==

In this example, we use data from @gaikwad_nellis_2021. They employ a door-to-door field experiment in two Indian cities to increase the political inclusion of migrants. The treatment provided intensive assistance in applying for a voter identification card. They conduct a simple randomization where 2306 migrants were either assigned to treatment or control with a 50 percent probability. They look at the impact of the treatment on several outcomes, one of which whether an individual voted in India's 2019 national election.

```{r}
# load in the experimental data from Gaikwad and Nellis (2021)
# this coad draws heavily on their reproduction code, found in their
# supplementary materials here: 
# https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/G1JCKK

t1_experiment_df <- readRDS("gaikwad-nellis-2021-data-t1-experiment-df.Rds")

### step 1 ###
# inspect the treatment variable

# values are 0 and 1
# around 50 percent assigned to each condition, consistent with simple randomization
# with p = 0.5 for all subjects

prop.table(table(t1_experiment_df$i_t1))
```


```{r}
### step 2 ###
# outcome and covariate inspection

# outcome variable, turnout in 2019 national election
# values of 0 or 1, no strange values
# 186 NAs which we'll investigate more later
summary(t1_experiment_df$e_voted_2019)

# covariates
# they use 10 of them in their main specification:
# 1) lagged outcome (voted previously), 2) whether female,
# 3) age, 4) whether muslim, 5) whether SC/ST, 6) whether has primary education,
# 7) income, 8) whether married, 9) length of residence, 10) whether owns a home
# here, we investigate 3 of them: age, female, and income

# age
# range 18-88, no NAs
  summary(t1_experiment_df$b_age)

# female
# range 0-1, no NAs 
  summary(t1_experiment_df$b_female)

# income
# authors are interested in income as 000s of rupees, so we transform the variable
# range 1-150, no NAs
# there seems to be some extreme outliers
  t1_experiment_df %<>% mutate(
    b_income_000 = b_income/1000)
  
  summary(t1_experiment_df$b_income_000)

  t1_experiment_df %>% 
    select(b_income_000) %>% 
    pivot_longer(
      cols = c("b_income_000"),
      values_to = "count") %>% 
    ggplot(aes(x = factor(name), y = count, fill = factor(name))) +
    geom_boxplot() +
    labs(xlab = "") +
    scale_fill_grey() + 
    scale_color_grey() +
    theme_bw() +  
    theme(legend.position="none") + 
    scale_x_discrete(labels = c("Original")) +
    xlab("") +
    ylab("Income (000s INR)")
```


```{r}
### step 3 ###
# dealing with outliers

# income
# as we mentioned before, and as Gaikwad and Nellis (2021) mention in their paper
# there are some outliers in the income variable that we want to address
# they choose to winsorize it 

# winsorize income variable
# using the "squish" function via the "scales" package
# Gaikwad and Nellis winsorize by setting all values higher than the 
# 99th percentile at the 99th percentile
  t1_experiment_df %<>%
    mutate(b_income_000_winsorized = squish(b_income_000, 
                                                quantile(b_income_000, 
                                                         c(0, .99))))

# inspect comparison of distributions of original vs. winsorized variable

  t1_experiment_df %>% 
    select(b_income_000, b_income_000_winsorized) %>% 
    pivot_longer(
      cols = c("b_income_000", "b_income_000_winsorized"),
      values_to = "count") %>% 
    ggplot(aes(x = factor(name), y = count, fill = factor(name)))+
    geom_boxplot() +
    labs(xlab = "") +
    scale_fill_grey() + 
    scale_color_grey() +
    theme_bw() +  
    theme(legend.position="none") + 
    scale_x_discrete(labels = c("Original", "Winsorized")) +
    xlab("") +
    ylab("Income (000s INR)")
  
```


```{r}
### step 4 ###
# checking for imbalance

# Gaikwad and Nellis inspect balance on a number of additional variables,
# which we include below

# create a vector of variable names

names <-
      list(
      # experimental covariates
      "b_female" = "Female",
      "b_age" = "Age",
      "b_muslim" = "Muslim",
      "b_sc_st" = "SC/ST",
      "b_primary_edu" = "Primary education",
      "b_income_000_OUTLIER_FIXED" = "Income (INR 000s)",
      "b_married" = "Married",
      "b_length_residence" = "Length of residence in city",
      "b_owns_home" = "Owns home in city",
  
    # lagged DVs
      "b_not_voted_previously" = "Hadn't voted previously",
      "b_vote_mc_elecs_how_likely" = "How likely to vote in city if registered",
      "b_political_interest" = "Political interest",
      "b_political_efficacy" = "Sense of political efficacy",
      "b_political_trust" = "Political trust index",
      "b_inter_ethnic_tolerance_meal" = "Shared meal with non-coethnic",
    
    # summary variables
      "b_has_village_voter_id" = "Has hometown voter ID",
      "b_pol_active_village" = "Returned to vote in hometown",
      "b_more_at_home_village" = "More at home in hometown",
      "b_kms_to_home" = "Straight-line distance to home district",
      "b_still_gets_village_schemes" = "Still receives hometown schemes",
      "b_owns_village_property" = "Owns hometown property")

# we regress the treatment indicator on the covariates above
# using lm_robust we get robust standard errors 

t1bal <- 
    summary(lm(as.formula(paste("i_t1 ~", paste(c(names(names)), collapse = "+"))),
       data = t1_experiment_df, subset = i_attrition==0))

# to ask, whether or not to have robust lm in order to get the F stat
# look at donald green SOP
# versus what is shown in the ri guide

# TO ASK should we subset on attrition here?

# we can display 





```




# References