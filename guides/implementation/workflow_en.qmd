---
title: "10 Things You Need to Know About Research Workflow for Experiments"
author: 
  - name: "Graeme Blair"
    url: https://graemeblair.com
  - name: "Matthew Lisiecki"
# add a fn like: @bowers_voors_2016 provide a framework and steps for improving project’s workflow; this guide draws upon their paper and upon additional tools aimed at documenting the important choices made by researchers and effectively communicating those choices to the project team.
bibliography: workflow.bib
image: workflow.png
---

# What is a reproducible workflow?

An experiment that is reproducible is one that someone else -- another scientist or yourself in the future -- could run again from start to finish just with the details you provide about the study. The idea is that they could do so without talking to you, so a researcher on the other side of the world could run it again it ten years. Making an experiment reproducible typically requires a workflow in which details are recorded along the way, rather than trying to recall and write them down after the fact. An experiment is fully reproducible if the data strategy -- sampling, treatment assignment, treatment implementation, and measurement procedures -- could be run again and if the analysis strategy can be rerun with the data that resulted from the experiment and return the same results as that of the original authors. An archive is often produced to aid reproducibility that includes code, treatment protocols, measurement materials such as survey instruments, and the data.

# Why should you adopt a reproducible workflow?

There are limits to human memory. Since most experimental research requires at least months if not years of design, monitoring, analysis, and reporting, few researchers can maintain a mental overview of all of a project’s moving pieces over time. Introduce additional investigators into the mix, and the questions of who did what, when, and why (if not how) multiply and become harder to answer. Maintaining a written record of decisions, actions, and questions is essential.

# Coding is communication to yourself, to co-investigators, and to the future 

<!-- Implementing an experiment requires a series of decisions, large and small. These decisions will be made both by individual researchers and by research teams. The why and how questions that will arise when you or others attempt to reproduce your work can only be answered if you can remember and communicate about these decisions. -->

The best way to leave an easy-to-retrace path for your future self and others in designing, implementing, and analyzing the data from an experiment is to produce all outputs through code. Opening Excel to make one change to one table may seem the quickest way to complete the task, but when you are trying to remember what you did months later, you will be glad to have updated your code to make the desired edit. To maximize re-traceability, leave comments in your code file to explain the purpose of each line.

Coding has other benefits. It saves time by systematically completing repetitive tasks that could be done manually but with greater time investment. It leads to smoother collaboration by communicating progress and next steps. Finally, it helps researchers avoid making mistakes by combining multiple steps into one file where the order of steps and the relationships among steps is clear.

# Organize code to communicate with yourself, collaborators, and future analysts

As valuable as coding is, there is little benefit if the code cannot be read and interpreted by you or others later on. Four rules of thumb can help make code easier to understand and reuse: (1) give files and folders names that state their purpose and; (2) add a README file to each folder to indicate what files should be run in what order and the collective purpose of the code files in the folder; (3) add a section to each file explaining its purpose and what it accomplishes; and (4) comment each file so that someone could read the comments alone and understand each step of the code. Together, these are often known as literate programming. 

# Code your research plans in advance to diagnose them--and you're ready to implement

Experimental researchers often operate under tight timelines and immovable deadlines like elections. If you need to write the code to randomly assign units to treatment the night before you implement the treatment it will be difficult to comment and document the process. But there is another reason to write your code in advance for data strategy and analysis procedures: to assess whether your design is the best one you can choose given financial, ethical, and practical constraints. Writing code and then using simulation to assess the statistical power, bias, cost, and risk to participants can highlight problems before it is too late, and enable pivoting to a better design. Once written to diagnose the design in this way, the code can then be used in implementing the experiment, too. When written in advance and tested in this way, there is time to document and test the code, making it more likely to be reproducible later. 

# Use random seeds to be sure you can reproduce sampling and randomization

If we could provide one single piece of advice to make your code more reproducible for an experiment it is: set a random seed before you sample units and randomly assign them to treatment. Documenting that the treatment was, in fact, randomized, and which specific procedure was used is an important step both in demonstrating that an estimate of a treatment effect is credible and ensuring the analysis procedure correctly follows from the random assignment procedure. A time-stamped code file with a random seed that takes in a dataset of the study units and constructs the random assignment of units to treatments is the best evidence you can have. Without it, researchers can second-guess themselves about what specific procedure was used and potentially misanalyze the data as a result. The same advice goes for any procedure that involves randomness in code. With a random seed, you can reproduce the results and be sure your code documents that procedure. 

# You can also make your treatment and data collection plans reproducible

To reproduce a study from start to finish, more than code is needed. Study materials are needed that document how to implement the treatment and the research procedures. And logs of who received treatments, who interviewed respondents, and other details of how the study was in fact implemented can become important in analyzing the data.

For sampling procedures, how study units are sampled from a population can be documented in code, but how and when they are approached can affect who joins the study. When reproducing the treatment, this same set of details are needed about how treatment is delivered to study units. If these details are not reproduced, which units receive the treatment may differ. In addition, the content of the treatment itself should be documented, otherwise the treatment effects may differ. Finally, the materials used to collect outcome measures, such as survey instruments, are needed since the same treatment may affect different outcomes differently.

Records about how the study was realized, separate from the materials that document what was planned, can change how the data should be analyzed. The spreadsheets that record when attempts were made to sample units can be used to estimate contact rates and those that record who answered the door or opened an email can be used to calculate compliance rates. Information on enumerator names and identities can help assessing whether responses to surveys differ by enumerator or according to whether the enumerator and respondent have the same gender. In short, any implementation details that can be recorded should be, and retained as the experiment goes along. Many turn out to be useful later, certainly to analyze the data correctly but also to understand how to reproduce it.

# Data analysis requires clear routes between inputs and outputs 

Nearly every script begins by calling in the relevant data for analysis. This should be done with a line that produces the same output on every computer. This, in turn, requires that collaborators need to have access to a shared file structure for storing project materials, about which more in section 7.

Within that shared file structure, the organization of files can be a very helpful map for identifying what went where. @nagler_1995 outlines the principle of modularity, which suggest that you separate files by function, namely making data cleaning/processing/recoding/merging distinct from analysis. Storing the original data file in a folder labeled “raw” and the cleaned/recoded datafile in one marked “clean” tells you and your collaborators where to find each without searching too hard.

# Version control: Keep track of who did what and when, and makes sure that work is not overwritten 

Even teams with perfect file structure in their shared folder can run into issues without effective version control. Version control allows teams to:

* Determine what changed between different versions of the same document;
* Experiment within a document, delete the parts that did not work, and merge in the parts that did;
* Produce multiple versions of the same document (such as one for a journal and one for a conference);
* Work on a file without undoing the work that someone else on your team happened to do at the same time.

One way to practice effective version control is to use the “track changes” function within word processors. This allows one author to make edits in a way that others can quickly identify, review, and “accept” or “reject.” In Google Docs^[https://docs.google.com], this feature (which Google calls “suggestion” mode) can be combined with the ability for multiple authors to work on the document at the same time, assuming that everyone has a reliable internet connection.
 
In situations with less reliable internet access, Dropbox^[https://www.dropbox.com] can provide shared access to a folder or series of folders where project materials are stored. One author can make an edit and sync their account, and then another can view the latest version. Authors cannot make changes at the same time, but they can hand off to each other with greater ease. This can be aided by adopting a file naming structure that tells you when the latest changes were made. Naming a file ```document.docx``` does not communicate this; naming it ```yyyymmdd_document.docx``` does. It also allows you to sort the folder by name and see the most recently edited version first. If the versions start to pile up, consider creating an archive subfolder to store all but the most recent version.

When collaboration involves plain text files (i.e., code or files that combine code and prose), you might consider following the best practices of community of software developers, who make a living from working together in teams to write reliable code in an efficient way. Currently, the standard tool to enable this collaboration is Github.

GitHub is a code hosting platform to streamline issues related to version control and working as a group. It lets you and others work together on projects from anywhere. The core GitHub workflow includes each user copying, or “cloning”, a version of a folder (or folder structure) to their own desktop, so that the latest version from the repository is on a local device, making changes to the desired files, and pushing those files to the shared repository. At the beginning of a work session one “pulls” or “synchronizes” any changes made by other team members online with the local copy. The git system tracks all changes and contains functionality to revert back to previous versions as needed.

# To minimize errors, build testing into the code 

Testing ensures that errors from bugs and typos are caught before it is too late, and including that testing in your coding makes sure that it is done.



<!-- Below is a simple example of this testing on a function designed to multiply a number by 2.  -->

<!-- This tells you to go back and review the function, where we see a + instead of a *. Testing can be even more effective if we code the script to stop when an error occurs instead of generating an error message. In R, the stopifnot function will do just that. -->

# Your future self -- and others -- will want to be able to access your data so use an archive

Science involves learning from the work of others, in addition to remembering one's own past work. If one works in such a way that a future self can remember, then, often that work will be easy to use by others as they themselves attempt to extend the science or even just learn how to apply your good ideas to their own context.^[See @king_1995.] There are a number of tools to make storing and sharing files for replication easier, including GitHub and the Center for Open Science’s Open Science Framework (OSF). OSF recently ran a [replication project](http://www.socialsciencesreplicationproject.com) aimed at understanding “predictors of reproducibility of research results, and how low reproducibility may inhibit efficient accumulation of knowledge.”

Data should be stored in archives like the Open Science Framework, Dataverse, the ICPSR, or other places with a plan for daily backups and also a plan for data to be preserved and made available for many years into the future (in contrast to a homemade website that may disappear). You also want to have a permanent identifier like a DOI for your archive, and use that identifier rather than a URL to share the archive so it can be found even if the URL for an archive changes.

# References
